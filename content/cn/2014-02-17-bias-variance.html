---
title: 漫谈统计学习中的 Bias-Variance Trade-Off思想
date: 2014-02-17
slug: bias variance
tags:
  - data
---



<p>最近在听一门统计学习的公开课，由<a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">ESL</a>的两个作者Trevor Hastie与Robert Tibshirani讲授，不过使用的教材是ESL的简化版——<a href="http://www-bcf.usc.edu/~gareth/ISL/">《An Introduction to Statistical Learning》</a>。虽说是简化版但写作风格明显是面对不同群体的，ISL偏应用，ESL更侧重数学原理解析，更抽象些。但这两本书的核心都是围着统计学习转的，而统计学习的一个核心论题就在Bias-Variance的取舍上，简单说就是偏差与方差的取舍。</p>
<p>初涉统计学习最直观的图形就是过拟合与欠拟合训练集的模型对测试集残差的倒U型曲线，解释上也会偏重说明过拟合问题，下面就从原理层解释下这个问题：</p>
<p>首先，我们知道统计学习实际上关注的是模型与现实问题，将现实的数据转化提炼出抽象的模型用来预测或做关系推断。那么先把无监督学习跟关系推断放一边，我们面对的是<span class="math inline">\(Y = f(X) + \epsilon\)</span> 的回归预测问题，这里<span class="math inline">\(f(x)\)</span>是理想模型。那么从公式上看我们可以想像，即使是理想模型，也存在<span class="math inline">\(\epsilon\)</span>，这是由Y背后的分布决定的，即预测值存在于一个范围而不是定值，通过其出现的概率密度函数，我们可以得到取值的变动范围。</p>
<p>其次，要清楚实际的统计学习过程是借助<span class="math inline">\(\hat f(x)\)</span>来进行的，而<span class="math inline">\(\hat y = \hat f(x)\)</span>,也就是每一种建模方法实际对应了一种对理想<span class="math inline">\(f(x)\)</span>的估计。那么理想<span class="math inline">\(f(x)\)</span>是没有偏差的，但实际的建模过程是存在偏差的，这个偏差是可以有方向的，用bias表示。而在建模过程中模型对理想模型<span class="math inline">\(f(x)\)</span>也存在变动范围，受训练集影响大的模型变动大，Variance就高，这就是过拟合的来源。</p>
<p>再次，不要忘记我们建模实际关心的不是<span class="math inline">\(\hat y\)</span>而是<span class="math inline">\(y\)</span>，那么在评价建模过程时我们引入一个统计量<span class="math inline">\(MSE\)</span>来衡量预测值与实际值的差异，有<span class="math inline">\(MSE = E(Y - \hat Y)^2 = [f(x) - \hat f(x)]^2 + Var(\epsilon)\)</span>，这个公式的证明很重要，但基本就是一个代数过程。从结果上看，差异的来源由两部分构成，一部份源于模型的偏差，另一部分源于<span class="math inline">\(y\)</span>自身的偏差，前者想想办法可以降低，后者由你响应变量的自身属性决定，基本不可约了。</p>
<p>最后，关注下<span class="math inline">\([f(x) - \hat f(x)]^2\)</span>，这一部份实际由两部分构成，就是上面提到的variance与bias。这两部分都低的建模方法更接近实际，但统计学习的一些方法是面临一个取舍问题的，那就是 Bias-Variance Trade-Off。</p>
<p>这里用交叉验证中的留一法与k叠交叉检验来说明下这个问题，留一法的核心在于留下一个作为验证集，用其余的数据建模，建n个模型求验证集的<span class="math inline">\(MSE\)</span>来进行模型评价。k叠交叉检验则将数据分为k份，大部份拿来建模，留一部分拿来验证。从Bias角度，由于留一法实际上使用了近乎所有数据，而k叠交叉检验会有<span class="math inline">\(n/k\)</span>部分不参与建模，所以相同训练集条件下留一法的Bias会很小。但从variance角度，使用的数据基本相同，过于依赖同一套数据，在测试集上会表现模型方差偏大，反观k叠交叉检验，Bias可能由于训练不够偏高，但构建的模型独立性较好，抵消掉了对同一数据集的依赖，得到的模型在variance上会相对小。这样我们会看到一个有意思的现象，伴随模型复杂度的提高，留一法与k叠交叉检验的<span class="math inline">\(MSE\)</span>实际是差不多的，但其中Bias与variance的组成是不一样的，从预测效果上看，可能variance低的模型更具吸引力，因此k叠交叉检验会比留一法更受欢迎。</p>
<p>另一个解释的角度是从残差相关度上理解的，如果预测值与实际值的差距在训练集上高度相关，这样如果假设建模过程的variance对某一特定模型是固定的，那么训练集上的高度相关就反过来导致测试集变动范围增大。此外，数据生成过程如果存在自变量的相关，其势必造成参数标准误估计的偏低，进而影响模型的稳健性，这一过程还是比较常见的：时间序列分析。</p>
<p>总之，Bias-Variance Trade-Off 贯穿在近乎每一个建模过程之中，统计学习的一个全局观就体现在其目标针对的是全局最优的理想模型。从这个角度出发，技术上的改进本质上是发现并平衡掉各种实际情况中的不完美或不理想。在这个层面上用好数学工具是可以从根上将问题分析透彻的。</p>
