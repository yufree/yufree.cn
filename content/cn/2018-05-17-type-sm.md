---
title: 两种结论错误与研商
author: ''
date: '2018-05-17'
slug: type-sm
categories: []
tags:
  - sci
  - data
---
读论文结论时其实我们都在跟着作者的事实推理逻辑进行决策，而决策就有对有错，这与事实或规律本身无关，只代表当下的认知水平。正是因为承认这一点，科研才不会纠结于错误，或者说科研就是在错误中前行的。同样的数据是有可能得到完全不同结论的，这是个时间的函数，逼近而不是揭示真相。所以，在这个有决策的过程中错误是可以用概率来描述的，p值的流行很大程度上是因为它给了一个通用版的决策方法与阈值，随之而来的就是两种错误，一种是假阳性，一种是假阴性。

所谓真假，必有对照，多数假设检验的空假设就是个对照基础，这个基础一般是一个分布或就是随机条件。多数对这种判断诟病的根源也在这里，因为真实实验或观察中基线往往不服从分布或随机，为此统计学家提供了大量手段来平衡掉不随机的部分让随机成为基线，在此基础上进行的差异比对就是一个令人信服的相对正确结论。在结论的修饰语中，相对正确是理想化的，令人信服才是被发表出来的原因，多数人没搞懂这一点去解读文献其实是一种科黑。

显然，平衡掉不随机的部分需要你事先知道这部分是什么，很遗憾，目前科研特别是基于观察的研究并不能事先知道，有时候就是想发现这些不知道自己不知道的东西。这种情况下基于p值或空假设的假设检验其实是不应该用的，打个比方，你发现观测数据中A基因与甲疾病相关，但究竟是不是A基因引发甲疾病还是需要用控制变量来验证的，很有可能A基因与甲疾病同样被B基因调控，但你根本就没测B基因，所以研究本身就是不完整的。那么通过组学技术知道的不知道的我一起去测不就完整了吗？也不是，当你测量数量增加时，假设检验的个数也增加了，此时你的p值阈值如果是0.05，那么10000个测量变量中会有500个即使随机测定都会出现差异的基因。去年有人建议把p值阈值设到0.005，但这根本不解决问题，只是把需要核实的数量减少了，虽然这也有一定意义。举个例子，10000个基因中有一个是真实的，你测定后按照0.05发现了501个，按照0.005发现了51个，也就是说需要验证的数量减少了。但真实研究中，你会遇到0.05发现了501个但0.005只发现了50个的情况，真实差异由于效应量或造成的差异量不够大而被你的决策方法给漏掉了。甚至也会出现0.05发现了480个而0.005只发现了48个的情况。也就是说，当你观察的问题效应不大时，p值有可能不管怎么调整都无法发现。这个锅不在p值，在于你要研究的效应效应太低而你用了不恰当的研究方法与假设来检验这个现象。这类效应大小问题就是 type M 型错误，只要你假设检验很多，这个问题就很难规避。

读博期间跟室友卧谈时我曾说过，现在只能相信强结论，也就是说无论你用哪种统计方法去进行检验，这个现象都是客观存在的，不会因为决策方法的变化而出现结论差异。不过这个提法现在看还是太理想了，因为强结论真的很强或显而易见，属于科研里低垂的果实，前人都摘的差不多了。如果一个现象足够强，p值一定会发现，贝叶斯方法也一定会发现，此时不存在效应大小问题。但更多的事实或规律是埋藏在当前认为的随机或噪音之中的，我们的分析水平也就刚刚好能把疑似信号与噪音进行区分，而这个区分是否靠谱则完全成了迷，统计学在这里帮不上忙，技术进步倒成了关键。我看到一些研究寄希望于数据挖掘技术解决学科内现象发现问题，这里我只能说对于显而易见但被忽视的现象是有帮助的，但对于高噪音数据，降低测量噪音对结论的帮助要远大于遴选能发现差异统计方法的努力。数据迷信会让你看到伪规律，而测量技术进步才会真的发现价值规律。我曾经也想把生活完全量化，但后来发现测量与传感方面的误差会让量化数据变成垃圾，大数据很美但也可能很虚。

另一个则是方向问题，p值经常是双边概率取中间那一部分，所以当你看到一个很小的p值时，你并不知道这个效应的方向是更大还是更小，此时你还是需要去看效应值。在这个情况下，如果报导p值不报道效应，那么就好比我告诉你明天要变天但又不告诉你变成什么一样毫无意义。在多数实验设计中，变化几乎是一定存在的，例如我敲掉了某个基因去验证功能，基因的变化与功能肯定有区别，大都来源于观察实验，更有意义的是影响大小，这个大小更多需要专业判断而不是简单的p值。如果理科学生学了半天最后就知道用p值来判断结论，那么这个学位不给也罢。这类搞不清楚效应方向的问题是 type S 型错误，验证性实验特别需要注意。

今天特意讲这个是因为我去年年底看了一篇论文，上面测量了很多种污染物的浓度，然后就对着很多健康指标进行了相关分析。这是一种多对多的结果遴选，在组学研究中也很常见，需要承认的是这是很多环境健康研究的惯用套路，然后只报道那些差异显著的结果。我将这篇论文转给了哥伦比亚大学的 Gelman 教授，询问他从数据分析角度有没有什么建议，他告诉我会在半年后在博客上公开回复这个问题（他档期真的很满）。然后这个月我看到了[回复](http://andrewgelman.com/2018/05/15/reduce-type-m-errors-exploratory-research/)，总结下就是 Gelman 教授认为1）显著性检验是不靠谱的，2）通过多层模型来减小M型错误影响（这是一种我认为很符合中庸之道的模型）并且3）尽可能多的平衡掉已知效应。更重要的是， Gelman 教授指出这属于探索性分析而非验证性分析，对于结论不应该太过信赖。这个回复是很中肯的，但一线研究人员能否理解并应用就不好说了。如果把对当今科研中的问题理解程度量化为“研商”，我想国内对于研商的培养是缺失非常严重的，从学生到老师职业功利性都远大于
对研究本身的理解，或者说我们缺少一个氛围。如果你去看 Gelman 教授的回复，你会发现博客下面的评论中引发了更多对科研成果报导、开放获取期刊等问题的讨论。而国内的科研博客评论里普遍理性讨论少，简单评价多，这个氛围的形成需要包括你我在内的一代甚至几代人的努力。

## 小结

- 除了假阳性与假阴性错误，科研结论中还存在效应大小错误与方向的掩盖
- p值对于后面两种错误的解决帮助不大，贝叶斯分层模型有助于问题部分解决
- 强结论很美好，但同时依赖数据分析与测量技术，后者容易被忽略但更为关键
- 研商是区别科学家与科研从业人员的重要指标，国内对此培养欠缺
- 在线公开讨论问题对于问题的理解与解决是有帮助的，这是互联网时代的研究红利
