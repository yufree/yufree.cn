---
title: 模型参数知多少
author: Miao Yu
date: '2023-11-27'
slug: number-of-model-parameters
categories: []
tags:
  - data
---

我的身体大概用了三周多才从指标上恢复正常，但我的脑子可能退烧后就自认为痊愈了，到底信生理指标还是个人感受其实是很奇妙的感觉。我自然信生理指标，不是因为这个准，而是这个借口可以给自己放松。我属于那种别人说啥都不听的货，所以外来压力很难直接影响我，但多数情况下是外在压力远小于我对自己的要求，别人能想到的我多数情况已经想过了，没想过的多半我本来也不感兴趣，毕竟年龄放在那里了，所以这种五六年病一次的机会自然就躺平了。我的个人意志只是这套生理系统的副产品，生理系统出问题了，我这边主观能动性没有任何讨价还价的余地。不过，现实情况确实是脑子更轻松了。身体上缓慢的恢复没妨碍到脑子的运转，因此也可以去思考一些荒唐的事，这里摘录其中一段关于对模型的思考。

今年大语言模型特别火，但我总感觉人们在追热点时经常忘了常识。大语言模型并不是人工智能的巅峰，更像是拟人智能的起点。然而，拟人智能最大的问题就在于越是拟人，越没法脱离人语言的框架，而很多真实存在的难题并不是人类语言可以描述的，人与人间在解决具体事物上的交流里一多半都是冗余的，甚至都不具备明确的目的性。现在人们惊叹于大语言模型可以替代的那部分工作其实也带有冗余且目的不清晰的特点，例如公务员的公文，很多时候就是在车轱辘话说很简单的事，现在人工智能也会了这套，也许会体现公文专业性，但很难说能提高行政效率。

这里隐含的问题就在大语言模型的“语言”二字上。自然语言是纯人类交流使用的，带有模糊性、多义性等各种缺点。因此，自然语言处理对于计算机而言就属于灾难，不过，当语料足够高质量后，一个复杂的模型也可以去模拟自然语言中的这些特性来实现跟人交流。这里跟人交流是一个没法优化的问题，人们日常交流本就存在压不下去的噪音与无处不在的误解与冗余，我们可以设计一个图灵测试来让对方感觉不出谈话的是人是机器，但这并不代表谈话能准确传达意义。而精准传达并执行本来就是计算机的强项，现在我们让计算机自费武功去理解人类语言，也许能替代很多工作，但很可能这些工作本来就不应该存在。人工智能里最拖后腿的其实是人的那部分，毕竟肉体凡胎的人很难脱离自然语言里的概念想象社会运转。因此，大语言模型的意义更多可能在于实现了更直接的人机对话，我们连碳基生物那点事都没搞清楚，别提复杂模型了。

但我终究还是搞科研的，对于具体的科学问题，大语言模型是锦上添花的东西，但研究用的深度学习模型我认为不可走语言图像模型的老路，也就是不断提高算力并堆砌层数来大力出奇迹。原因很简单，具体的科学问题你能采集到的高质量数据远少于高质量语料数据。环境顶级期刊里涉及的研究很多样本量也就几十到几百这个量级，你去用迁移学习搞一个预训练的ResNet模型，里面有两千万个参数，我很难不认为会出现过拟合。

这里我举个具体场景，人也就两万多个基因，如果你不是搞突变的，我们默认每个人体内都能测到两万个基因的表达。然而，基因跟基因之间很多是相关的，也就是不独立的，也就是人的基因组里独立变量不超过两万个。在具体的科学问题下，例如找特定疾病的标记基因，多数基因比较保守根本没差异，到最后可能只有四五个基因有差异。那么也就是说，对于这个疾病，我们只需要一个包含四五个参数的线性模型就可以描述了。那么此时你告诉我说要上变形金刚训练一个含有几十亿参数的模型来描述这种疾病，我会拿着奥卡姆剃刀追着砍你。

语言的变化取决于语素的排列组合，图片的变化取决于红绿蓝三通道在平面上的排列组合，其排列组合后还有意义的计算空间是比较大的，因此大语言模型去卷参数量是有道理的。但相比语言或图像的变化，科学问题里涉及的单元可能是分子、细胞与组织，我很难相信你身上的葡萄糖跟我身上的有啥差异，那么这样的有生物意义的小分子有多少呢？大概在几万个这个水平，但具体到不同的科学问题，基本都会缩减到几百甚至几个小分子上。就这几块料做个凉拌菜，也值当上个太上老君的炼丹炉？

不过话说回来，如果不考虑还没被发现的小分子，我们假定目前能测到的小分子里已经包含了解释大多数健康状态的信息，那么理论上我们的模型在构架构架合适的前提下并不需要很大。其验证方法也很简单，就是训练好后，无论我们怎么提高样本量，最后得到的预测效果都不会再有变化，此时这个模型在默认小分子信息量充足的前提下应该可以回答大多数相关科学问题，这其实就是各种组学的终极目标。那么问题来了，这样一个完美模型，在几十到几百的样品量限制下，会需要多少参数呢？

ChatGPT大概是200亿的参数量就基本实现了人机对话，大语言模型似乎固定好了模型参数量不断往里送数据效果还会提升。这里我的理解是更多的数据量提高的是模型的解析度，类似从模糊到锐化的过程。而进一步提高参数量可能也会带来提高，但似乎存在一个飞跃式提高的参数量节点，对于自然语言自监督训练，这个量可能100亿这个量级上。自然语言里基本语素是单词，这个总量是3000亿，那么我大概估计数据量要是模型参数量的10倍以上才能训练出一个性能突飞猛进的模型。

回到前面说的基因组的例子，假如我有100个样品，每个样品测1万个基因，那么就有总量一百万的数据，按照上面所说的估计，出现一个完美模型其参数量在10万这个量级应该就足够了。考虑到基因里共相关的比例很高，模型参数可能在几万这个水平就足够利用组学数据里的信息了。当然，前提是这里面真的有你关心的那一个问题。如果没有，考虑上突变，那么模型相应就要变大。

那么什么样的模型有几万个参数？甚至都用不上深度学习，随机森林模型大一点都能有几万个参数。而深度学习有些模型构架例如图神经网络甚至可能只有几千个参数。只不过在深度学习里模型构架没法如随机森林一样直观描述出来，但也正是这个原因模型会在训练过程中提取出要被训练的参数，变化更多一些。

我觉得在具体的科学领域里，最先要做的就是搞清楚自己能测到多少独立变量，然后可以利用独立变量的数目与实际可获取样本量估计出需要多大的模型，之后就可以写个脚本让电脑试错找出最佳的领域内通用模型，很可能对算力要求不高，但能解决八成的已知问题。当然，这都是我的幻想，但我查了下文献，很少看到这方面的讨论，隐约感觉这个思路比现在为了上模型而上模型要靠谱些。起码先圈出一个大概的方向，对不对要上仿真，我现在是没空，等有空了试一下，看看是不是领域内通用模型可以用这个思路来设计。
