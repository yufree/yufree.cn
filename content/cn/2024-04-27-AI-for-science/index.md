---
title: 科研用人工智能
author: Miao Yu
date: '2024-04-27'
slug: AI-for-science
categories: []
tags:
  - sci
---
科研用人工智能或者说AI for Science是最近几年比较火的研究领域，而ChatGPT无疑是给这把火浇了不少的汽油，各路豪杰纷纷下场。站在这么一个前不着村后不着店的时间节点，我大约理了下思路。

人工智能本就是拿来解决实际问题的一种技术工具，本质上遵循现实问题抽象化为概念，概念抽象为符号与运算，符号与运算抽象为二进制计算的三部曲技术框架。现实问题抽象化为概念是科学观察与实验获取的，符号与运算抽象为二进制计算是计算机工程师解决的。因此，人工智能发展的瓶颈很长时间以来都是卡在“概念抽象为符号与运算”这个阶段。这个阶段在不同学科有不同的体现，例如化学信息学就试图把分子物理化学性质变成可计算问题，生物信息学则想沿着中心法则构建从基因到蛋白的完整信息流，环境科学则想通过对环境变量建模讨论污染物或环境因子对人的影响。虽然不同学科诉求不同，但本质上都是想用计算补充“现实问题抽象化为概念”这一阶段实验与观察缺失的部分信息，而我们验证技术工具的方法一般也是拿实验与观察得到的已知理论来评价其计算后的符合度。

那么，究竟是什么让人工智能成为前沿？技术进步，或者说计算能力的进步，也就是三部曲的“符号与运算抽象为二进制计算”部分。而最先获得突破的领域其实是两块：图像处理与自然语言处理。几乎所有的深度学习算法框架都是这两个部分互相学习迭代出来的，而深度学习的出现正是人工智能火热的基石。这两个领域有什么特点呢？数据量大。在工程师解决掉胶片数字化问题后，高质量图片一直源源不断产生，本质上可能已经穷尽了地面上所有存在的物体。而自然语言方面，文本电子化可能也已经把人类高质量知识的排列组合逻辑都过了一遍。一个训练充分的模型在这两个领域里其实直接替代了“概念抽象为符号与运算”这一步，也就是说，模型所知的图片类型或语言排列方式被替换成了特征值，而训练过程就是从图像与文本里提取所有特征值及其合理的排列组合模式。此时，模型本身已经包含了这两个领域里大部分已知与未知信息，但没有具体的问题，模型本身就是个闷葫芦。在这个训练好的模型上进行引导性训练，就可以直接回答具体的问题，这是所谓“迁移学习”的本质。在一组图片种类足够丰富的数据集上训练的分类模型通常已经训练好了提取特征值与其排列组合模式的框架，在新数据集上二次训练收敛速度会很快。而终极的迁移学习模型本身是自己训练自己，自己找到图片或文字里所有可能模式，这个框架恰恰是注意力机制所擅长的。因此，图像处理与自然语言处理模型很快就都抽象成了从高质量数据集里自训练出大模型然后按照实际问题出应用的状态，例如ChatGPT本质就是GPT大模型在对话语境下训练出来的一个应用。

我们现在反过头看三部曲技术框架会发现，人工智能真正的竞争力其实是在信息模式的穷举，也就是前面说的“概念抽象为符号与运算”。当某一学科可训练数据多到一定程度后，利用数据本身就可以训练出一个学科知识模型。而最先突破的图像与语言这两块的大模型恰好提供了模型里知识可视化与自然语言化的模块，这样理论上借助既有图像与语言大模型，我们可以直接把包含现实问题信息的数据送到学科模型里，然后输出那些现实问题里缺失的部分。例如我们去看alphafold 2，这个蛋白结构模型预测的训练数据是已知蛋白质里氨基酸序列、蛋白结构数据、同源蛋白质序列及物理化学约束条件，这些基本就是跟蛋白结构相关所有的信息，当模型识别到了信息里的模式后，新的蛋白序列空间构型就可以较为准确的预测出来。

然而，这个论断前提有两个，一个是学科可训练数据足够多，另一个是有包含现实问题信息的数据，这两个前提在很多学科并不成熟。不同于图像与文字，环境监测数据在很多领域是长期缺失的，此时想训练一种新发现污染物的环境行为模型最靠谱的办法是去找那些物理化学性质接近的污染物做类比；而在另一些领域，虽然分析技术可以采集到大量的数据，但这些数据可能不包括未知科学问题所需的信息，例如新冠初期我就看到有人申请了用头发样本通过机器学习来寻找新冠标记物的项目，这就属于搞笑了，新冠从发病到痊愈或入土也就两周，头发两周也就长几毫米，且不说刮头皮这事患者会不会同意，就算采集到了，头发主要成分也是角蛋白，这玩意新冠病毒能改的可能性几乎为零，不是说机器学习模型不行，而是实在没法在南极找到北极熊。

因此，科研用人工智能最大的瓶颈在于分析技术是否能产生足够的高质量数据。另一个更难，那就是如何把学科内概念抽象为符号与运算，也就是如何训练出一个能有效提取学科已知未知概念的模型。对于第一个问题，当前常见的情况就是样本数远小于测量维度，经常看见一个实验就不到一百个样品，每个样品测几千个蛋白或基因或代谢物，此时想训练出靠谱模型就很难受。一般要么就是把基因蛋白代谢物对应到已知通路里搞个验证，要么就是想办法排除掉无关测量，前者不会发现任何新知识，后者则否定了组学研究的基础。而当数据满足条件后，第二个问题有效的解决方法应该是先想办法把已知通路进行编码，然后用实验数据去更新通路的细节，这个具体如何实现就需要模型设计者的巧思了。

也就是说，科研用人工智能难点不在模型通用性，而在于科学问题的针对性。现在很多原本做人工智能的想把通用人工智能搞到具体学科里，我认为最多做个自然语言或可视化的交互接口，属于外包装，而且这也解决不了具体科学问题。更实际的应用路径是具体学科的研究人员与模型工程师坐下来讨论下如何合理的把学科概念抽象为符号与运算，这里要搞清楚学科里最基础的概念单元及单元本身与交互的限制条件，然后用变形金刚也好，新设计构架也好，想办法在数据充足的条件下自训练出一个靠谱模型。当然，还有一种取巧的方法，那就是把已知学科数据转换成图像或自然语言，然后用图像或自然语言既有的模型构架提取信息然后解决问题，但你得确认这个转换过程不损失信息。

总结一下，现阶段科研用人工智能火爆很大程度依赖图像与自然语言处理方面模型的成功，想复制这个成功到其他领域，要针对性设计概念抽象为符号与运算的自训练模型，否则很可能南辕北辙。
