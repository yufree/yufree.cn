---
title: 世另我
author: Miao Yu
date: '2024-12-21'
slug: another-yufree
categories: []
tags:
  - tech
---
今年愚人节，我搞了个YuGPT，声称用我的资料训练了一个语言模型。当然，那就是一个js脚本，不论你问什么，答案都是固定的。其实我当时是真的计划做一个YuGPT当彩蛋的，但那段时间我过于繁忙（到处钓鱼），而现在外面都冻上了，所以我打算在冬至这天完成这个小项目。

首先，我们现在所处的2024年末可以电子化的人类知识基本都被拿去训练大语言模型了，我们基于口口相传与文字总结出的经验基本到头了。当前最尖端的大语言模型都是推理模型，推理模型的思考深度可以不恰当类比为脑内博弈几轮才给出结论，后面如果我们还打算提取知识，可能要依赖可与外界交互的推理模型才可以，给他们眼睛与手去探索，当然在地球月球还是火星都无所谓。但我不是很关心他们能做到什么程度，现在给的数学证明我也早就看不懂了，我更好奇的是下限，也就是实现一个可以五五开通过图灵测试的模型需要什么样的资源。

这个问题其实也不用我做，苹果公司今年推出的端侧模型就基本界定了一个范围。如果我们想要一个接受过九年义务教育的助理，能响应你预定行程、做文章摘要或是礼貌回复邮件，这个任务难度大概就是1B，也就是十亿参数的语言模型就可以胜任。苹果其实就是在新款手机里内置了这个级别的端侧模型，当然，当任务比较复杂，他就会接入云端大语言模型来完成。那么现在大语言模型参数大概什么水平呢？开源模型里大概就是70B参数到顶，云端的几百B参数量的也到顶了。在之前研究里，很多人单纯认为增大参数量，把深度模型搞深就可以不断突破模型能力，这个思路我认为是没问题的，现在的问题是我们拿不出更多高质量训练资料了，现在很多大语言模型的训练资料其实就是用另外的大语言模型生成的。也就是说，以人类的认知水平，70B参数量基本足够用了，而且是达到大学生水准。

我们也可以用另外一种不恰当的类比来看这件事。如果我要描述一辆车的加速过程，那么我只需要知道两个量，一个是加速度，另一个是车的质量，车的质量我们看作常数，那么事实上加速度这一个参数就足够描述车辆加速过程。对于具体的事，参数量其实通过人类知识的抽象与数学描述会被压缩到很少。那么我们可以去想象一个人类大脑，大概有86B的神经元，我们用一个独立参数去描述一个神经元的话，需要构架的就是一个86B参数量的模型，至于这些独立参数如何互相影响，就如同深度神经网络模型那样，我们最多看个构架跟激活区域，根本没法了解具体的物理化学过程。

在进行具体语言认知任务时，我们只能动用少量参数，因此想构建一个拟人的语言模型，参数量不大可能超过86B。作为类比，没有语言的猴子大概有20B神经元，那么最优化的语言模型参数量就不应该超过50B。那么完全没有语言的蚂蚁有50万个神经元，但一个1000只蚂蚁的小蚁群却有大概5B的神经元在交互，而大型蚁群里能达到几千上万个B的神经元。我们当前智力水平大概可以很轻松看明白一个小蚁穴里的分工协作，但大蚁穴其实已经超了我们可理解的能力了，以后可能推理模型能告诉我们这里面的蕴藏的智慧。生物上的类比也许不恰当，但从自由度或模型复杂度上我们可以猜一个量级。

有意思的是，我们当前搞出来的能用的语言模型参数恰好是十亿参数这个数量级上的。而苹果的端侧模型则进一步告诉我们，大概1B相当于九年义务教育水平，那么3-5B的模型大概就是高中毕业水平，7-13B其实就可以看出一个接受了高等教育的人的认知水平了。当然，我没有去讨论那些MoE啥的模型框架，而且搞成3B或13B这种诡异数字也主要是为了适配训练用的显卡显存，这个类比问题很多。我了解国内很多手机端已经布署到7B这个量级的本地模型了，但这只是下限，估计几年内端侧模型就能超越13B，给人那种2022年年末看到ChatGPT的惊艳感，到时候语言模型可能是以智能代理（agent）的形式出现，可以完成一系列复杂操作，可以类比当年互联网从电脑走向移动端。而且手机的传感器是开放给语言模型的，这其实就给了新高质量数据收集的机会，后面可能还有好戏。

那么电脑端呢？其实本地跑大语言模型已经很成熟了。我十年前就感觉电脑性能过剩了，但那是因为当时我不咋玩游戏，现在也是用掌机多，但现在的电脑跑大语言模型大概处在一个性价比阶段，也就是你肯升级电脑，就能跑更大的模型出更好的结果，还没有性能过剩的问题。我现在个人电脑是21年的MBP M1 Pro，别看是个M芯片，用ollama跑本地模型也是能轻松跑动7B的，甚至可以跑qwen2.5的14B，苹果最新出的丐版mac mini上M4芯片大概也有同等表现。当然，如果我打算部署一个本地大语言模型服务器当家庭管家，那么还得是英伟达的显卡，或者用最新那个开发板。但我现在单纯就是跑着玩，自己电脑上部署的都是14B以下的模型，大概率未来5年我们身边能接触到的端侧模型都是这个量级的，我需要了解下其优缺点。

好了，说回YuGPT，其实当前要实现这样的语言模型有两种方案。第一种方案是微调现有模型，这个我这台电脑大概勉强也能做，但网上有现成的unsloth笔记本，谷歌Colab笔记本的免费资源不用白不用（实际训练用了付费的A100，速度很快）。另一个方案是RAG，也就是让语言模型在回答你的问题前先去你的知识库，例如一堆文本里找到相关片段，然后整合输出，其实Bing那种从网上搜网页然后给汇总就属于一种RAG。我了解最新的RAG技术应该是微软的GraphRAG，可以通过构建知识图谱而不是简单的向量数据库来进行检索，他们也说最近会发布一个LazyGraphRAG来实现更高速的图谱构建检索。我本机装了半天一堆问题，主要是不想调用付费API，改成了本地模型，但遇到了些莫名其妙的问题，因此我打算等LazyGraphRAG放出来后再试。不过，本地端RAG更可能是未来端侧模型的应用路线，模型自动微调这种高端操作估计得等模型架构出现革新后才能普及。另外，本地部署一个属于自己的知识库现在也没啥技术门槛了，但对更多人而言，凑不出这个库的文本可能是更大的问题。

这次我用的是微调，找了博客里面104篇随笔，扔到了Hugging Face上。然后就魔改了unsolth上面的微调笔记本，让其吃进去我的文本，这次我选了qwen2.5的14b的Instruct模型，整个训练过程大概两个小时多点吧（A100可以1小时内搞定，100个epoch），我看了下收敛很慢，很可能因为我没啥固定风格，或者说我的那点思考大语言模型都从其他地方得到了充分训练。不过训练好的模型你如果问“yufree怎么看某某问题”或“于淼怎么看某某问题”，那么这个模型是会给你答案的，微调前模型是不认的。但输出效果吧，我感觉是胡说八道占一半，另外一半倒是有点像，但也就是个玩具。

这里改进点其实挺多的，我其实可以先调用本地端模型来处理原始问题，让其提取里面的观点与回应，这样在训练上好一点。不过其实我也这么做了，但输出的json文件经常中途报错，报错原因一般就是大语言模型忽略我的提示词输出了json以外的内容，所以我就改用了文章直接作为输入。另一个点在于我写文章思路会有跳跃，同一文章会涉及多个主题，可以把训练集切碎一点，这样训练也许效果会好一点。另外训练集也非常少，所以7B模型有些部分可能也没调动。

如果你打算尝试，可以先去装个ollama，然后运行：

> ollama run hf.co/yufree/Qwen2.5-14B-Instruct-bnb-4bit-yufree

你要不习惯命令行下问问题，随便装个前端例如anythingllm，LM studio，Lorachat啥的就能得到类ChatGPT的交互体验了。

到这里我想说的是未来几年，我们将很容易构建出世界另一个我，这对正在经历老龄化的世界不算是坏事。很多人需要陪伴，但又不想让机器陪伴，因此虚拟人格授权未来可能会成为有意思的社会现象。如果一个人记录自己的生活足够多，那么未来是大概率可以实现数字重生的，现在那些所谓的数字重生不过是让照片动一下或者有类似的语音，但未来可能出现的那种授权式虚拟人格也许可以直接模仿某个人的思想与思维过程。作为普通人，这算不上啥好消息，甚至伦理风险都很高，要是虚拟人格唆使犯罪，那么本体就很尴尬了。

也许从我们这一代人开始，个体就很难真正死去了。肉体当然是有生理极限的，但思维方式、观点等是可以通过留下来的语音文字等资料来重构的，这里的前提是要有记录可循。我估计类似蒋介石这种写日记的人是最容易被重生出虚拟人格的，视频博主、作家这些也比较容易，未来可以设计一套题，想数字重生的人可通过反复人机提问来重构虚拟人格，只是不知道有多少人愿意重生，又有多少人期待着另一些人重生。

我是不介意世界上出现另一个我的，但世界也许很介意。
