<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>blog on Miao Yu | 于淼 </title>
    <link>https://yufree.cn/en/</link>
    <description>Recent content in blog on Miao Yu | 于淼 </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Feb 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://yufree.cn/en/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Invitation to Submit Manuscripts for a Special Issue of Chemosphere</title>
      <link>https://yufree.cn/en/2022/07/11/invitation-to-submit-manuscripts-for-a-special-issue-of-chemosphere/</link>
      <pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2022/07/11/invitation-to-submit-manuscripts-for-a-special-issue-of-chemosphere/</guid>
      <description>&lt;p&gt;The prestigious journal &lt;em&gt;Chemosphere&lt;/em&gt; is currently running a special issue entitled &amp;quot; Human Health Effects of Chemical Mixture Exposures&amp;quot;. As we are acting as guest editors for this issue, we would like to welcome contributions from various disciplines. We kindly invite you to consider submitting your full paper to this special issue.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Guest editors&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Prof. Dr. Peng Gao University of Pittsburgh School of Public Health &lt;a href=&#34;mailto:peg47@pitt.edu&#34;&gt;peg47@pitt.edu&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prof. Dr. Hui Peng University of Toronto &lt;a href=&#34;mailto:hui.peng@utoronto.ca&#34;&gt;hui.peng@utoronto.ca&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dr. Miao Yu The Jackson Laboratory &lt;a href=&#34;mailto:miao.yu@jax.org&#34;&gt;miao.yu@jax.org&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Special issue information&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Current environmental chemistry and toxicology studies mainly focus on a single stressor or single group of stressors, which does not reflect the multiple stressors in the dynamic exposome that humans are facing. Usually, human exposures are presented as cocktails with thousands of organic chemicals and dozens of inorganic chemicals being presented. However, the significant relationships and interactions among those stressors in the environment and their holistic human health effects remain unclear. Fortunately, the rapid developments of various techniques provide us with the possibility of revealing these mixture exposures. This Chemosphere special issue aims to provide a platform to dissect the complexity of chemical mixture exposures from experimental, analytical, and computational perspectives.&lt;/p&gt;
&lt;p&gt;Manuscript submission information:
The submission website for this journal is located at &lt;a href=&#34;https://www.editorialmanager.com/chemosphere/default.aspx&#34;&gt;here&lt;/a&gt;. Author guidelines and manuscript submission to Chemosphere can be found &lt;a href=&#34;https://www.elsevier.com/journals/chemosphere/0045-6535/guide-for-authors&#34;&gt;here&lt;/a&gt;. To ensure that your manuscript is correctly submitted to the special issue, please select ‘‘VSI: Exposure of Mixture” when you reach the step of “Article Type” during the submission process.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Keywords&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Human exposure, Chemical mixture, Health effects&lt;/p&gt;
&lt;p&gt;Learn &lt;a href=&#34;https://www.elsevier.com/authors/submit-your-paper/special-issues&#34;&gt;more&lt;/a&gt; about the benefits of publishing in a special issue.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using xcmsrocker on HPC via Singularity</title>
      <link>https://yufree.cn/en/2022/05/26/using-xcmsrocker-on-hpc-via-singularity/</link>
      <pubDate>Thu, 26 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2022/05/26/using-xcmsrocker-on-hpc-via-singularity/</guid>
      <description>&lt;p&gt;Docker should be the most popular container platform. Container distribution via dockerhub makes it easy to provide all-in-one development/data analysis environment for scientist. It&amp;rsquo;s always a good idea to use container on the high performance computing (HPC) cluster to accelerate data processing. Since Docker provides root access to the system they are running on, it&amp;rsquo;s always not allowed to be used on HPC. On the other hand, Singularity is more friendly to scientific research with MPI support, as well as security restriction.&lt;/p&gt;
&lt;p&gt;I released &lt;a href=&#34;https://github.com/yufree/xcmsrocker&#34;&gt;xcmsrocker&lt;/a&gt; image for metabolomics data analysis for a long time and always said that it should be easy to deploy on HPC or cloud computing platform. It&amp;rsquo;s always right for the latter options and you can use docker image on the most popular cloud. However, you will need some extra work for HPC.&lt;/p&gt;
&lt;p&gt;The first issue is to build a Singularity image from a docker image hosted on Docker Hub. You need to load singularity module after login on HPC:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ml singularity
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then pull the xcmsrocker image&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;singularity pull docker://yufree/xcmsrocker:lastest
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now you will find a file with name &amp;lsquo;xcmsrocker_latest.sif&amp;rsquo; in you home folder. If your HPC use slurm for job management, you can use the following job script and save as a file called &amp;ldquo;rstudio-server.job&amp;rdquo;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/sh
#SBATCH --time=05:00:00
#SBATCH --signal=USR2
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=2
#SBATCH --mem=8192
#SBATCH --output=/home/%u/rstudio-server.job.%j

# Create temporary directory to be populated with directories to bind-mount in the container
# where writable file systems are necessary. Adjust path as appropriate for your computing environment.
workdir=$(python -c &#39;import tempfile; print(tempfile.mkdtemp())&#39;)

mkdir -p -m 700 ${workdir}/run ${workdir}/tmp ${workdir}/var/lib/rstudio-server
cat &amp;gt; ${workdir}/database.conf &amp;lt;&amp;lt;END
provider=sqlite
directory=/var/lib/rstudio-server
END

# Set OMP_NUM_THREADS to prevent OpenBLAS (and any other OpenMP-enhanced
# libraries used by R) from spawning more threads than the number of processors
# allocated to the job.
#
# Set R_LIBS_USER to a path specific to rocker/rstudio to avoid conflicts with
# personal libraries from any R installation in the host environment

cat &amp;gt; ${workdir}/rsession.sh &amp;lt;&amp;lt;END
#!/bin/sh
export OMP_NUM_THREADS=${SLURM_JOB_CPUS_PER_NODE}
export R_LIBS_USER=${HOME}/R/xcmsrocker
exec rsession &amp;quot;\${@}&amp;quot;
END

chmod +x ${workdir}/rsession.sh

export SINGULARITY_BIND=&amp;quot;${workdir}/run:/run,${workdir}/tmp:/tmp,${workdir}/database.conf:/etc/rstudio/database.conf,${workdir}/rsession.sh:/etc/rstudio/rsession.sh,${workdir}/var/lib/rstudio-server:/var/lib/rstudio-server&amp;quot;

# Do not suspend idle sessions.
# Alternative to setting session-timeout-minutes=0 in /etc/rstudio/rsession.conf
# https://github.com/rstudio/rstudio/blob/v1.4.1106/src/cpp/server/ServerSessionManager.cpp#L126
export SINGULARITYENV_RSTUDIO_SESSION_TIMEOUT=0

export SINGULARITYENV_USER=$(id -un)
export SINGULARITYENV_PASSWORD=$(openssl rand -base64 15)
# get unused socket per https://unix.stackexchange.com/a/132524
# tiny race condition between the python &amp;amp; singularity commands
readonly PORT=$(python -c &#39;import socket; s=socket.socket(); s.bind((&amp;quot;&amp;quot;, 0)); print(s.getsockname()[1]); s.close()&#39;)
cat 1&amp;gt;&amp;amp;2 &amp;lt;&amp;lt;END
1. SSH tunnel from your workstation using the following command:

   ssh -N -L 8787:${HOSTNAME}:${PORT} ${SINGULARITYENV_USER}@LOGIN-HOST

   and point your web browser to http://localhost:8787

2. log in to RStudio Server using the following credentials:

   user: ${SINGULARITYENV_USER}
   password: ${SINGULARITYENV_PASSWORD}

When done using RStudio Server, terminate the job by:

1. Exit the RStudio Session (&amp;quot;power&amp;quot; button in the top right corner of the RStudio window)
2. Issue the following command on the login node:

      scancel -f ${SLURM_JOB_ID}
END

singularity exec --cleanenv xcmsrocker_latest.sif \
    rserver --www-port ${PORT} \
            --auth-none=0 \
            --auth-pam-helper-path=pam-helper \
            --auth-stay-signed-in-days=30 \
            --auth-timeout-minutes=0 \
            --server-user XXX \
            --rsession-path=/etc/rstudio/rsession.sh
printf &#39;rserver exited&#39; 1&amp;gt;&amp;amp;2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This file is modified from &lt;a href=&#34;https://www.rocker-project.org/use/singularity/&#34;&gt;Rocker&amp;rsquo;s singularity tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here, you need to change &lt;code&gt;--server-user XXX&lt;/code&gt; to the user name for your HPC. For example, my user name to login HPC is &amp;lsquo;yufree&amp;rsquo; and I will set &lt;code&gt;--server-user yufree&lt;/code&gt;. This option will make sure you can login in your RStudio server and the default user don&amp;rsquo;t have access.&lt;/p&gt;
&lt;p&gt;Then submit this job to HPC:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sbatch rstudio-server.job
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then you should see a file with job ID as extension such as &amp;lsquo;rstudio-server.job.xxxxxxx&amp;rsquo; in your HPC home folder. &amp;lsquo;xxxxxxx&amp;rsquo; is your job ID. Then you can check the content in this file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat rstudio-server.job.xxxxxxx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You will find the user name, password and port information on HPC. The user name should be the same as you HPC user name and password will change anytime you submit this job.&lt;/p&gt;
&lt;p&gt;To access RStudio on your local computer, you need to bind your local port to the running HPC port. You need to open a new terminal to establish the SSH tunnel:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh -N -L 8787:[YOUR_PORT_INFORMATION] [HPC_USERNAME]@[HPC domain]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here the port information is from &lt;code&gt;rstudio-server.job.xxxxxxx&lt;/code&gt;. &lt;code&gt;[HPC_USERNAME]@[HPC domain]&lt;/code&gt; is the same with the regular ssh information to login in HPC. This command will forward HPC&amp;rsquo;s port to port 8787 on your local computer. After you open the SSH tunnel, you can access the RStudio from xcmsrocker via your own browser: http://localhost:8787&lt;/p&gt;
&lt;p&gt;Now you can enjoy your xcmsrocker image on HPC. Keep in mind that only the packages supporting parallel computing would get benefits from HPC resources. If the software doesn&amp;rsquo;t support parallel computing, you will need to modify their source code or it will be a waste of time to run them on HPC.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I am looking for a faculty position</title>
      <link>https://yufree.cn/en/2021/09/23/i-am-looking-for-a-faculty-position/</link>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2021/09/23/i-am-looking-for-a-faculty-position/</guid>
      <description>&lt;p&gt;I am looking for a faculty position on earth. It&amp;rsquo;s always right to fit the position with your skill sets. However, after sending a dozen of applications with tailored resume or research statements, I decide to leave my cover letter online with my desired research interests.&lt;/p&gt;
&lt;p&gt;I am trained as an environmental analytical chemist from a state key laboratory under the supervision of Prof. Guibin Jiang in China. Then I worked with Prof. Janusz Pawliszyn in University of Waterloo, Canada for projects about &lt;em&gt;in vivo&lt;/em&gt; SPME based metabolomics data analysis as a PostDoc. After two years’ training, I joined Institute for Exposomic Research at Mount Sinai for environmental exposure related bioinformatics studies and worked with Dr. Lauren Petrick. I have published 37 peer reviewed journal papers with 9 first author or co-first author papers. My publications have more than 800 citations and a h-index of 17. I have two papers selected as journal cover (AC and ES&amp;amp;T letter) and one paper selected as ES&amp;amp;T Letter 2018 best paper. I authored three R packages on CRAN and developed shiny applications for my research. More details can be found in my CV.&lt;/p&gt;
&lt;p&gt;My research interests are the assessment of environmental exposures and impacts on humans through high resoltuion mass spectrometry based metabolomics analysis. I can apply &lt;em&gt;in vivo&lt;/em&gt; SPME technique to capture real-time changes in living organisms. I proposed the concept of “reactomics” based on paired mass distances to retrieve the changes of general chemical relationship in the samples and developed related software and database. Besides, I proposed a concept called “gatekeeper” to explain the influence of multiple exposures or exposome on health outcomes at molecular levels by metabolomics or other omics data. Those techniques and models can be used to understand the health impact of general environmental exposures. I can be either an experimental or bioinformatic scientist. However, I will treat myself as a mass spectrometry guy to solve various environmental related scientific problem by both dry and wet lab skills.&lt;/p&gt;
&lt;p&gt;I hope to continuously develop reactomics tools to investigate the influences of certain exposure and perform gatekeeper discovery for population-based exposure studies. I am planning to introduce machine learning into the biomarker reaction discovery based on reactomics and gatekeeper model for certain diseases. I am willing to collaborate with other researchers for multidisciplinary research projects.&lt;/p&gt;
&lt;p&gt;Feel free to &lt;a href=&#34;mailto:yufree@live.cn&#34;&gt;contact&lt;/a&gt; me if you need extra information. Thank you for your consideration.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Correlation coefficients cutoff to generate network in metabolomics</title>
      <link>https://yufree.cn/en/2021/07/28/correlation-coefficients-cutoff-to-generate-network-in-metabolomics/</link>
      <pubDate>Wed, 28 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2021/07/28/correlation-coefficients-cutoff-to-generate-network-in-metabolomics/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/en/2021/07/28/correlation-coefficients-cutoff-to-generate-network-in-metabolomics/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;One common research purpose in metabolomics is to check the relations among the metabolites. Correlation network is one of the most popular way to show such relations. However, such network will change with different selection of the cutoff of correlation coefficients.&lt;/p&gt;
&lt;p&gt;Let’s check some real world data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(pmd)
library(enviGCMS)
data(spmeinvivo)
# remove redundant peaks
newmet &amp;lt;- globalstd(spmeinvivo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 75 retention time cluster found.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 369 paired masses found&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 5 unique within RT clusters high frequency PMD(s) used for further investigation.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The unique within RT clusters high frequency PMD(s) is(are)  28.03 21.98 44.03 17.03 18.01.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 719 isotopologue(s) related paired mass found.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 492 multi-charger(s) related paired mass found.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 8 retention group(s) have single peaks. 14 23 32 33 54 55 56 75&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 11 group(s) with multiple peaks while no isotope/paired relationship 4 5 7 8 11 41 42 49 68 72 73&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 9 group(s) with multiple peaks with isotope without paired relationship 2 9 22 26 52 62 64 66 70&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 4 group(s) with paired relationship without isotope 1 10 15 18&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 43 group(s) with paired relationship and isotope 3 6 12 13 16 17 19 20 21 24 25 27 28 29 30 31 34 35 36 37 38 39 40 43 44 45 46 47 48 50 51 53 57 58 59 60 61 63 65 67 69 71 74&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 291 std mass found.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metabolites &amp;lt;- getfilter(spmeinvivo,rowindex = newmet$stdmassindex)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Originally we have 1459 peaks. After removal of redundant peaks such as isotope, adducts and Neutral losses by globalstd algorithm, we have 291 peaks as the number of potential metabolites. To check their relations, we will calculate the paired correlation coefficients among their intensities.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metcor &amp;lt;- cor(t(metabolites$data))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check the distribution of correlation coefficients:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(metcor)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/07/28/correlation-coefficients-cutoff-to-generate-network-in-metabolomics/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since correlation coefficients are also associated with a p value, we can also check the distribution of p values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor.test.p &amp;lt;- function(x){
    FUN &amp;lt;- function(x, y) cor.test(x, y)[[&amp;quot;p.value&amp;quot;]]
    z &amp;lt;- outer(
      colnames(x), 
      colnames(x), 
      Vectorize(function(i,j) FUN(x[,i], x[,j]))
    )
    dimnames(z) &amp;lt;- list(colnames(x), colnames(x))
    z
}

pmat &amp;lt;- cor.test.p(t(metabolites$data))
hist(pmat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/07/28/correlation-coefficients-cutoff-to-generate-network-in-metabolomics/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(pmat&amp;lt;0.05)/length(pmat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4145&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;41% original p values are less than 0.05. We can filter the correlation coefficients based on this rule.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metcor2 &amp;lt;- metcor[pmat&amp;lt;0.05]
hist(metcor2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/07/28/correlation-coefficients-cutoff-to-generate-network-in-metabolomics/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;range(abs(metcor2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6664 1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we can find the cutoff is around +/-0.67. However, we didn’t perform FDR control. If we use BH method to correct the p value, we will have a different cutoff.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pmat_adj &amp;lt;- p.adjust(pmat)
metcor3 &amp;lt;- metcor[pmat_adj&amp;lt;0.05]
range(abs(metcor3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9881 1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the cutoff is 0.99. We can display the data as network:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metcor[pmat&amp;gt;=0.05] &amp;lt;- 0
library(igraph)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;igraph&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     decompose, spectrum&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:base&amp;#39;:
## 
##     union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;net &amp;lt;- graph.adjacency(metcor,weighted=TRUE,diag=FALSE,mode = &amp;#39;undirected&amp;#39;)
plot(net,vertex.size=1,edge.width=1,vertex.label=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/07/28/correlation-coefficients-cutoff-to-generate-network-in-metabolomics/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here it seems all metabolites are connected and FDR control will solve this issue.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metcor &amp;lt;- cor(t(metabolites$data))
metcor[pmat_adj&amp;gt;=0.05] &amp;lt;- 0
net &amp;lt;- graph.adjacency(metcor,weighted=TRUE,diag=FALSE,mode = &amp;#39;undirected&amp;#39;)
plot(net,vertex.size=1,edge.width=1,vertex.label=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/07/28/correlation-coefficients-cutoff-to-generate-network-in-metabolomics/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we will see the networks with few large clusters and lots of single metabolites without any association with each other.&lt;/p&gt;
&lt;p&gt;If we didn’t consider the p values, we can also check the networks with different cutoffs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- c()
for (i in seq(0,1,0.1)) {
        metcor &amp;lt;- cor(t(metabolites$data))
        metcor[metcor&amp;lt;i] &amp;lt;- 0
        net &amp;lt;- graph.adjacency(metcor,weighted=TRUE,diag=FALSE,mode = &amp;#39;undirected&amp;#39;)
        # plot(net,vertex.size=1,edge.width=1,vertex.label=&amp;quot;&amp;quot;)
        cn &amp;lt;- components(net)
        # check the numbers of cluster
        n &amp;lt;- c(n,length(table(membership(cn))[table(membership(cn))&amp;gt;1]))
}
plot(seq(0,1,0.1),n,xlab=&amp;#39;cutoff&amp;#39;,ylab = &amp;#39;cluster number&amp;#39;,type = &amp;#39;l&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/07/28/correlation-coefficients-cutoff-to-generate-network-in-metabolomics/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we can see the cluster numbers will firstly increase and then decrease. Let’s check &lt;span class=&#34;math display&#34;&gt;\[0.8,1\]&lt;/span&gt; carefully.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- c()
for (i in seq(0.8,1,0.001)) {
        metcor &amp;lt;- cor(t(metabolites$data))
        metcor[metcor&amp;lt;i] &amp;lt;- 0
        net &amp;lt;- graph.adjacency(metcor,weighted=TRUE,diag=FALSE,mode = &amp;#39;undirected&amp;#39;)
        # plot(net,vertex.size=1,edge.width=1,vertex.label=&amp;quot;&amp;quot;)
        cn &amp;lt;- components(net)
        # check the numbers of cluster
        n &amp;lt;- c(n,length(table(membership(cn))[table(membership(cn))&amp;gt;1]))
}
plot(seq(0.8,1,0.001),n,xlab=&amp;#39;cutoff&amp;#39;,ylab = &amp;#39;cluster number&amp;#39;,type = &amp;#39;l&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/07/28/correlation-coefficients-cutoff-to-generate-network-in-metabolomics/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# display the cutoff
seq(0.8,1,0.001)[which.max(n)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.988&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we find the max number of network clusters has a similar cutoff of p value cutoff with FDR control. However, the computation process is much faster. When the cutoff is small, all metabolites are connected. When the cutoff is large, few metabolites will be covered. In terms of physics, largest number of network clusters means the coverage of largest numbers of connected metabolites with largest clusters separations. I think this should be the fastest way to select cutoff from the real world data.&lt;/p&gt;
&lt;p&gt;Actually, I add a function called `getcf()` into `enet` package to automate find this cutoff of correlation network analysis. Here is the network for our demo data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metcor &amp;lt;- cor(t(metabolites$data))
metcor[metcor&amp;lt;seq(0.8,1,0.001)[which.max(n)]] &amp;lt;- 0
net &amp;lt;- graph.adjacency(metcor,weighted=TRUE,diag=FALSE,mode = &amp;#39;undirected&amp;#39;)
plot(net,vertex.size=1,edge.width=1,vertex.label=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/07/28/correlation-coefficients-cutoff-to-generate-network-in-metabolomics/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The reason to avoid using p values or adjust p values of correlation test is not only the slow speed of computation, but also cutoff selection of p values or adjust p values is determined by the researcher instead of the data themselves. p value cutoff will not help us to find biological functional modules when all the metabolites are connected. In my opinion, each data sets can speak for itself by an automated cutoff selection process and I think the network cluster numbers can just take this job.&lt;/p&gt;
&lt;p&gt;PS. I actually use the same idea to generate PMD metabolites network, which can be treated as another relation among metabolites with chemical meanings.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>reactomics data analysis template within rmwf package</title>
      <link>https://yufree.cn/en/2021/02/09/reactomics-workflow-template-within-rmwf-package/</link>
      <pubDate>Tue, 09 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2021/02/09/reactomics-workflow-template-within-rmwf-package/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/en/2021/02/09/reactomics-workflow-template-within-rmwf-package/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;To make reactomics data analysis more transparent and reproducible, I included one template in rmwf package. You could install the package from Github.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;#39;remotes&amp;#39;)
remotes::install_github(&amp;quot;yufree/rmwf&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you use RStudio, you could try:&lt;/p&gt;
&lt;p&gt;File-New file-R Markdown-from template&lt;/p&gt;
&lt;p&gt;Then select ‘reactomics’ to use template for reactomics analysis. Here is a preview for data analysis of this &lt;a href=&#34;https://www.metabolomicsworkbench.org/data/DRCCMetadata.php?Mode=Study&amp;amp;StudyID=ST000560&#34;&gt;study&lt;/a&gt;:&lt;/p&gt;
&lt;div id=&#34;demo-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Demo data&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;path &amp;lt;- system.file(&amp;quot;demodata/untarget&amp;quot;, package = &amp;quot;rmwf&amp;quot;)
files &amp;lt;- list.files(path,recursive = T,full.names = T)
ST000560pos &amp;lt;- enviGCMS::getmzrtcsv(files[grepl(&amp;#39;ST000560mzrt&amp;#39;,files)])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;remove-the-redundant-peaks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Remove the redundant peaks&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check the paired mass distance relationship
pmd &amp;lt;- pmd::getpaired(ST000560pos)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 56 retention time cluster found.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 826 paired masses found&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 23 unique within RT clusters high frequency PMD(s) used for further investigation.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The unique within RT clusters high frequency PMD(s) is(are)  12 2.02 26.02 28.03 14.02 26.01 54.05 24 9.99 40.03 44.04 2.01 15.01 30.01 27.02 44.03 14.01 21.98 30.05 42.05 29.02 4.03 66.02.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 182 isotopologue(s) related paired mass found.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1145 multi-charger(s) related paired mass found.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pmd::plotpaired(pmd)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/02/09/reactomics-workflow-template-within-rmwf-package/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we could see some common PMDs within the same retention time bins like 21.98Da for the mass differences between [M+Na] and [M+H]. Other PMDs might refer to in-source reaction such as PMD 2.02Da for opening or forming of double bond. Another common kinds of PMDs should the homologous series compounds which could not be separated by the column such as PMD 14.02Da for CH2, PMD 28.03Da for C2H4, PMD 44.03Da for C3H6, and 56.05Da for C4H8, as well as 58.04Da for C3H6O. There are also some PMDs which highly depended on the the samples’ matrix. Anyway, we will check those high frequency PMD considering isotopes, as well as multiple chargers to extract one peak for one potential compound. Such algorithm is called GlobalStd. The advantage of GlobalStd is that no pre-defined paired mass distances list is needed to remove redundant peaks. When a PMD appeared with high frequency in certain samples, it will be treated as potential adducts to be removed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;std &amp;lt;- pmd::getstd(pmd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 4 retention group(s) have single peaks. 52 54 55 56&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10 group(s) with multiple peaks while no isotope/paired relationship 26 30 33 35 45 47 48 49 51 53&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2 group(s) with multiple peaks with isotope without paired relationship 29 32&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 20 group(s) with paired relationship without isotope 10 11 13 14 15 16 18 20 23 27 28 31 36 38 40 42 43 44 46 50&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 20 group(s) with paired relationship and isotope 1 2 3 4 5 6 7 8 9 12 17 19 21 22 24 25 34 37 39 41&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 196 std mass found.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pmd::plotstd(std)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/02/09/reactomics-workflow-template-within-rmwf-package/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, we get 205 peaks for 205 potential compounds. Now we could retain those peaks for reactomics analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# generate new peak list and matrix sample
peakstd &amp;lt;- enviGCMS::getfilter(std,rowindex = std$stdmassindex)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;GlobalStd algorithm was originally designed to retrieve independent peaks by the paired mass distances relationship among features without a predefined adducts or neutral loss list. However, the peaks from the same compounds should also be correlated with each other. Meanwhile, the independent peaks selection might still have peaks from the same compounds when the peaks’ high frequency PMDs are not independent. In this case, the GlobalStd algorithm could set a cutoff to re-check the independent peaks by their relationship with potential PMDs groups and select the base peaks for the clusters of peaks.&lt;/p&gt;
&lt;p&gt;Meanwhile, network analysis could be used for PMDDA workflow to select precursor ion for MS/MS annotation. Such precursor ions was selected by checking the peak with highest intensity of each independent peaks’ high frequency PMD network cluster, which could be treated as pseudo spectra.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extract-high-frequency-pmds&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Extract high frequency PMDs&lt;/h1&gt;
&lt;p&gt;To retrieve the general chemical relationship, we will focus on high frequency PMDs within a certain metabolic profile. If one PMD occur multiple times among peaks from a snapshot of samples, certain reactions or bio-process should be important or occur multiple times compared with rarely PMD, which could be a random differences among compounds. In this case, extraction of high frequency PMDs will refine the investigation on a few active reactions instead of treating each peak individually, which is almost impossible for untargeted analysis.&lt;/p&gt;
&lt;p&gt;Such PMDs frequency analysis should be performed on the data set with the redundant peaks removal. Otherwise, the high frequency PMD among compounds will be immersed by PMD with from isotopes, adducts or other common PMDs from the backgrounds.&lt;/p&gt;
&lt;p&gt;You could define the cutoff of frequency while the default setting using the largest PMD network cluster numbers to determine the cutoff, which try to capture more information. Here we will retrieve high frequency PMDs from the demo data using a larger cutoff to reduce the complexity:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hfp &amp;lt;- pmd::getsda(std,freqcutoff = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 8 groups were found as high frequency PMD group.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0 was found as high frequency PMD. 
## 2.02 was found as high frequency PMD. 
## 12 was found as high frequency PMD. 
## 14.02 was found as high frequency PMD. 
## 24 was found as high frequency PMD. 
## 26.02 was found as high frequency PMD. 
## 28.03 was found as high frequency PMD. 
## 50.01 was found as high frequency PMD.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pmd::plotstdsda(hfp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/02/09/reactomics-workflow-template-within-rmwf-package/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we could find 8 PMDs were selected as high frequency PMDs. PMD 0 Da could be some isomers, PMD 2.02 Da could be reduction reactions, etc. Some PMDs can be the combination of other PMDs, which could be a chain reactions. From the plot you might also identify the homologous series by the retention times relations.&lt;/p&gt;
&lt;p&gt;When you have the lists of high frequency PMDs, you could check the PMDs changes among groups. Here we will quantitatively analysis certain PMD to show the reaction level changes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remove QC sample
hfp2 &amp;lt;- enviGCMS::getfilter(hfp,colindex = !grepl(&amp;#39;QC&amp;#39;,hfp$group$sample_group))
# check pmd 14.02
qreact &amp;lt;- pmd::getreact(hfp2,pmd = 14.02)
qreactsum &amp;lt;- apply(qreact$data,2,sum)
t.test(qreactsum~qreact$group$sample_group)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  qreactsum by qreact$group$sample_group
## t = 2.2, df = 17, p-value = 0.04
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##   2939 95129
## sample estimates:
## mean in group control    mean in group IgAN 
##               1247817               1198783&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,1))
boxplot(qreactsum~qreact$group$sample_group,xlab=&amp;#39;&amp;#39;,ylab = &amp;#39;intensity&amp;#39;, main=&amp;#39;PMD 14.02Da&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/02/09/reactomics-workflow-template-within-rmwf-package/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we could find PMD 14.02Da could be a biomarker reaction for case and control. Meanwhile, paired relationship could be connected into network to show the overall relationship within the samples.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reactomics-network-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reactomics network analysis&lt;/h1&gt;
&lt;p&gt;The relation among those high frequency PMDs peaks could be further checked in two ways by network analysis: one from the correlation analysis and another from the PMD analysis. If we combined them together, reactomics network could be generated to capture the major reaction network within the samples. We will check them step by step.&lt;/p&gt;
&lt;div id=&#34;build-the-correlation-network&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build the correlation network&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(igraph)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;igraph&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     decompose, spectrum&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:base&amp;#39;:
## 
##     union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cutoff &amp;lt;- 0.9
metacor &amp;lt;- stats::cor(t(peakstd$data))
metacor[abs(metacor)&amp;lt;cutoff] &amp;lt;- 0
df &amp;lt;- data.frame(from=rownames(peakstd$data)[which(lower.tri(metacor), arr.ind = T)[, 1]],to=rownames(peakstd$data)[which(lower.tri(metacor), arr.ind = T)[, 2]],cor=metacor[lower.tri(metacor)])
df &amp;lt;- df[abs(df$cor)&amp;gt;0,]
df$direction &amp;lt;- ifelse(df$cor&amp;gt;0,&amp;#39;positive&amp;#39;,&amp;#39;negative&amp;#39;)
net &amp;lt;- igraph::graph_from_data_frame(df,directed = F)
netc &amp;lt;- igraph::components(net)
message(paste(netc$no, &amp;#39;metabolites correlation network clusters found&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 16 metabolites correlation network clusters found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index &amp;lt;- rep(NA,length(rownames(peakstd$data)))
index[match(names(netc$membership),rownames(peakstd$data))] &amp;lt;- netc$membership
message(paste(sum(is.na(index)), &amp;#39;out of&amp;#39;, length(rownames(peakstd$data)), &amp;#39;metabolites have no correlation with other metabolites&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 88 out of 197 metabolites have no correlation with other metabolites&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(net,vertex.label=NA,vertex.size =5,edge.width = 3, main = &amp;#39;Correlation network&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/02/09/reactomics-workflow-template-within-rmwf-package/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we could see the correlation among those peaks as network. 109 peaks have relations with each others and 88 peaks were single.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-the-pmd-network&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build the PMD network&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;peaksda &amp;lt;- pmd::getsda(std,freqcutoff = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 8 groups were found as high frequency PMD group.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0 was found as high frequency PMD. 
## 2.02 was found as high frequency PMD. 
## 12 was found as high frequency PMD. 
## 14.02 was found as high frequency PMD. 
## 24 was found as high frequency PMD. 
## 26.02 was found as high frequency PMD. 
## 28.03 was found as high frequency PMD. 
## 50.01 was found as high frequency PMD.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- peaksda$sda
df$from &amp;lt;- paste0(&amp;#39;M&amp;#39;,round(df$ms1,4),&amp;#39;T&amp;#39;,round(df$rt1,1))
df$to &amp;lt;- paste0(&amp;#39;M&amp;#39;,round(df$ms2,4),&amp;#39;T&amp;#39;,round(df$rt2,1))
net &amp;lt;- graph_from_data_frame(df[,c(&amp;#39;from&amp;#39;,&amp;#39;to&amp;#39;,&amp;#39;diff2&amp;#39;)],directed = F)
netc &amp;lt;- igraph::components(net)
message(paste(netc$no, &amp;#39;metabolites PMD network clusters found&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 15 metabolites PMD network clusters found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index &amp;lt;- rep(NA,length(rownames(peakstd$data)))
index[match(names(netc$membership),rownames(peakstd$data))] &amp;lt;- netc$membership
message(paste(sum(is.na(index)), &amp;#39;out of&amp;#39;, length(rownames(peakstd$data)), &amp;#39;metabolites have no PMD relations with other metabolites&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 143 out of 197 metabolites have no PMD relations with other metabolites&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pal &amp;lt;- grDevices::rainbow(8)
plot(net,vertex.label=NA,vertex.size =5,edge.width = 3,edge.color = pal[as.numeric(as.factor(E(net)$diff2))], main = &amp;#39;PMD network&amp;#39;)
legend(&amp;quot;topright&amp;quot;,bty = &amp;quot;n&amp;quot;,
       legend=unique(E(net)$diff2),
       fill=unique(pal[as.numeric(as.factor(E(net)$diff2))]), border=NA,horiz = F)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/02/09/reactomics-workflow-template-within-rmwf-package/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(E(net)$diff2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 24.00  0.00 50.01  2.02 12.00 28.03 26.02 14.02&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By checking the high frequency PMD relation, we see a similar while different results. Those high frequency PMDs could also be linked to potential reactions such as 0Da for isomers, 2.02Da for double bonds breaking/forming. Such PMDs could reveal the major reactions found among the metabolites. 54 peaks have PMDs relations with each others and 143 peaks were single.&lt;/p&gt;
&lt;p&gt;Here we need to define a frequency cutoff. With the increasing number of high frequency PMDs cutoff, the ions cluster numbers would firstly increase then decrease. At the very beginning, the increasing numbers will include more information because high frequency PMDs always capture real reactions or structures relationships among compounds. Low frequency PMDs will introduce limited information as they might be generated by random differences among ions. In terms of network analysis, when the high frequency PMD cutoff is small, the network clusters will be small. However, when the numbers of network clusters are not increasing any more with more PMDs included, the relationship information among ions will not increase and the cutoff could be automated detected by GlobalStd algorithm. In detail, the algorithm will try to include PMDs one by one starting from the highest frequency PMDs. Meanwhile, the ions cluster numbers were recorded for the generated network among independent peaks and the cutoff will be the PMDs list with the largest number of independent peaks’ network cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-the-pmd-network-with-correlation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build the PMD network with correlation&lt;/h2&gt;
&lt;p&gt;We could combine the PMD relation with correlation together to show the quantitative reactomics networks within the samples. Those metabolites could be quantitatively checked among different samples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;peaksda &amp;lt;- pmd::getsda(std,freqcutoff = 8,corcutoff = 0.6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 8 groups were found as high frequency PMD group.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0 was found as high frequency PMD. 
## 2.02 was found as high frequency PMD. 
## 12 was found as high frequency PMD. 
## 14.02 was found as high frequency PMD. 
## 24 was found as high frequency PMD. 
## 26.02 was found as high frequency PMD. 
## 28.03 was found as high frequency PMD. 
## 50.01 was found as high frequency PMD.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- peaksda$sda
df$from &amp;lt;- paste0(&amp;#39;M&amp;#39;,round(df$ms1,4),&amp;#39;T&amp;#39;,round(df$rt1,1))
df$to &amp;lt;- paste0(&amp;#39;M&amp;#39;,round(df$ms2,4),&amp;#39;T&amp;#39;,round(df$rt2,1))
net &amp;lt;- graph_from_data_frame(df[,c(&amp;#39;from&amp;#39;,&amp;#39;to&amp;#39;,&amp;#39;diff2&amp;#39;)],directed = F)
netc &amp;lt;- igraph::components(net)
message(paste(netc$no, &amp;#39;metabolites quantitative reactomics network clusters found&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10 metabolites quantitative reactomics network clusters found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index &amp;lt;- rep(NA,length(rownames(peakstd$data)))
index[match(names(netc$membership),rownames(peakstd$data))] &amp;lt;- netc$membership
message(paste(sum(is.na(index)), &amp;#39;out of&amp;#39;, length(rownames(peakstd$data)), &amp;#39;metabolites have no PMD&amp;amp;correlation relations with other metabolites&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 162 out of 197 metabolites have no PMD&amp;amp;correlation relations with other metabolites&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;net &amp;lt;- graph_from_data_frame(peaksda$sda,directed = F)
pal &amp;lt;- grDevices::rainbow(21)
plot(net,vertex.label=NA,vertex.size =5,edge.width = 3,edge.color = pal[as.numeric(as.factor(E(net)$diff2))], main = &amp;#39;Quantitative reactomics network&amp;#39;)
legend(&amp;quot;topright&amp;quot;,bty = &amp;quot;n&amp;quot;,
       legend=unique(E(net)$diff2),
       fill=unique(pal[as.numeric(as.factor(E(net)$diff2))]), border=NA,horiz = F)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2021/02/09/reactomics-workflow-template-within-rmwf-package/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MS/MS annotation by paired mass distances analysis</title>
      <link>https://yufree.cn/en/2021/01/17/ms-ms-annotation-by-paired-mass-distances/</link>
      <pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2021/01/17/ms-ms-annotation-by-paired-mass-distances/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/en/2021/01/17/ms-ms-annotation-by-paired-mass-distances/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Last year I make a poster presentation for MS/MS annotation by paired mass distance(PMD) analysis. It’s already been included as &lt;code&gt;pmdanno&lt;/code&gt; function in pmd package. Here I will explain the principle of PMD annotation.&lt;/p&gt;
&lt;p&gt;Firstly, you need a spectra database. Here I use HMDB MS/MS spectra database as an example. Then you will get a list with each compound as element. The list should have a element of spectra with mz and ins, an element of name, an element of prec for precursor ions. I have included this database in rmwf package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remotes::install_github(&amp;#39;yufree/rmwf&amp;#39;)
# remotes::install_github(&amp;#39;yufree/pmd&amp;#39;)
library(rmwf)
data(&amp;quot;qtof&amp;quot;)
str(qtof)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 4
##  $ name   : chr [1:5062] &amp;quot;HMDB0000014&amp;quot; &amp;quot;HMDB0000014&amp;quot; &amp;quot;HMDB0000014&amp;quot; &amp;quot;HMDB0000014&amp;quot; ...
##  $ mz     : num [1:5062] 227 227 227 227 227 ...
##  $ msms   :List of 5062
##   ..$ : num 116
##   ..$ : num [1:3] 5 111 116
##   ..$ : num [1:15] 0.07 16.03 16.1 27.01 42.01 ...
##   ..$ : num [1:15] 0.07 16.03 16.1 27.01 42.01 ...
##   ..$ : num 116
##   ..$ : num [1:3] 5 111 116
##   ..$ : num [1:136] 1.98 2.01 2.01 2.02 2.02 2.02 3.99 3.99 4.03 4.03 ...
##   ..$ : num [1:36] 1 3.99 3.99 5 8.01 ...
##   ..$ : num [1:3] 30 44 74
##   ..$ : num [1:6] 18 18 36 83 101 ...
##   ..$ : num [1:15] 1.98 9.98 11.96 18.01 26.02 ...
##   ..$ : num [1:6] 18 18 36 83 101 ...
##   ..$ : num [1:15] 1.98 9.98 11.96 18.01 26.02 ...
##   ..$ : num [1:3] 30 44 74
##   ..$ : num 1
##   ..$ : num [1:10] 1 10 34 43 44 ...
##   ..$ : num [1:3] 1 18 19
##   ..$ : num [1:45] 0.98 1 9.98 15.01 16.03 ...
##   ..$ : num [1:190] 0.93 0.98 1 1 1.06 2.02 2.02 2.04 2.95 2.95 ...
##   ..$ : num [1:105] 1 1 1.01 1.06 2 2.01 2.02 2.95 2.98 3.01 ...
##   ..$ : num [1:703] 0.03 0.04 0.62 0.93 0.93 0.97 0.99 1 1 1 ...
##   ..$ : num [1:3] 46 71 117
##   ..$ : num [1:36] 2.02 8.01 9.98 12 12 ...
##   ..$ : num [1:21] 0.08 9.04 12 17.03 29.03 ...
##   ..$ : num [1:21] 0.08 9.04 12 17.03 29.03 ...
##   ..$ : num [1:3] 46 71 117
##   ..$ : num [1:36] 2.02 8.01 9.98 12 12 ...
##   ..$ : num 17
##   ..$ : num 27
##   ..$ : num [1:6] 15 27 27 42 42 ...
##   ..$ : num 27
##   ..$ : num [1:6] 15 27 27 42 42 ...
##   ..$ : num 17
##   ..$ : num [1:3] 1.01 59.01 60.02
##   ..$ : num [1:3] 1.01 59.01 60.02
##   ..$ : num [1:6] 18 37.1 55.1 212 249.1 ...
##   ..$ : num 212
##   ..$ : num 212
##   ..$ : num 212
##   ..$ : num 212
##   ..$ : num [1:6] 18 37.1 55.1 212 249.1 ...
##   ..$ : num 132
##   ..$ : num 132
##   ..$ : num [1:3] 27 132 159
##   ..$ : num 132
##   ..$ : num [1:3] 27 132 159
##   ..$ : num 132
##   ..$ : num 17
##   ..$ : num 132
##   ..$ : num 132
##   ..$ : num 132
##   ..$ : num 132
##   ..$ : num [1:3] 1 17 18
##   ..$ : num 9.45
##   ..$ : num 9.45
##   ..$ : num 194
##   ..$ : num [1:3] 55.1 194 249.1
##   ..$ : num [1:3] 55.1 194 249.1
##   ..$ : num 194
##   ..$ : num [1:10] 0.95 17.06 18.01 24.95 42.01 ...
##   ..$ : num [1:15] 0.95 1.01 1.01 2.02 17.06 ...
##   ..$ : num [1:10] 0.95 17.06 18.01 24.95 42.01 ...
##   ..$ : num [1:3] 1 36 37
##   ..$ : num [1:2485] 0.03 0.03 0.03 0.04 0.04 0.04 0.04 0.04 0.04 0.04 ...
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num [1:3] 18 26 44
##   ..$ : num [1:3] 18 26 44
##   ..$ : num 17
##   ..$ : num [1:3] 18 28 46
##   ..$ : num [1:3] 18 28 46
##   ..$ : num 26
##   ..$ : num 26
##   ..$ : num [1:6] 2.02 17.03 25.98 27.99 43.01 ...
##   ..$ : num [1:6] 2.02 17.03 25.98 27.99 43.01 ...
##   ..$ : num [1:3] 1 180 181
##   ..$ : num [1:16836] 0.02 0.03 0.03 0.03 0.04 0.04 0.05 0.05 0.05 0.06 ...
##   ..$ : num [1:15] 6.09 18.01 19.12 20.88 21.91 ...
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num [1:28] 1.01 1.98 8.06 15.94 15.99 ...
##   ..$ : num [1:28] 1.01 1.98 8.06 15.94 15.99 ...
##   ..$ : num 212
##   ..$ : num 212
##   ..$ : num [1:3] 18 225 243
##   ..$ : num [1:3] 18 225 243
##   ..$ : num [1:3] 18 225 243
##   ..$ : num [1:3] 18 225 243
##   ..$ : num 212
##   ..$ : num 212
##   ..$ : num [1:3] 2.02 44.03 46.04
##   ..$ : num [1:3] 2.02 44.03 46.04
##   ..$ : num 16
##   .. [list output truncated]
##  $ msmsraw:List of 5062
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 112 228
##   .. ..$ intensity : num [1:2] 70 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 112 117 228
##   .. ..$ intensity : num [1:3] 100 25.8 50.5
##   ..$ :&amp;#39;data.frame&amp;#39;: 6 obs. of  2 variables:
##   .. ..$ masscharge: num [1:6] 66 93 135 210 226 ...
##   .. ..$ intensity : num [1:6] 15.5 100 15.2 12 14.2 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 6 obs. of  2 variables:
##   .. ..$ masscharge: num [1:6] 66 93 135 210 226 ...
##   .. ..$ intensity : num [1:6] 15.5 100 15.2 12 14.3 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 112 228
##   .. ..$ intensity : num [1:2] 70 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 112 117 228
##   .. ..$ intensity : num [1:3] 100 25.8 50.5
##   ..$ :&amp;#39;data.frame&amp;#39;: 814 obs. of  2 variables:
##   .. ..$ masscharge: num [1:814] 45.2 45.4 45.4 45.8 46 ...
##   .. ..$ intensity : num [1:814] 2.75 2.38 1.81 2.23 2.07 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 890 obs. of  2 variables:
##   .. ..$ masscharge: num [1:890] 44.9 44.9 44.9 45.7 45.7 ...
##   .. ..$ intensity : num [1:890] 0.927 1.514 1.947 0.402 0.34 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 108 138 182
##   .. ..$ intensity : num [1:3] 72.5 100 48.7
##   ..$ :&amp;#39;data.frame&amp;#39;: 7 obs. of  2 variables:
##   .. ..$ masscharge: num [1:7] 65 92.1 120 138.1 148 ...
##   .. ..$ intensity : num [1:7] 12.29 8.8 6.49 7.67 100 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 6 obs. of  2 variables:
##   .. ..$ masscharge: num [1:6] 65 92 110 120 122 ...
##   .. ..$ intensity : num [1:6] 88.3 42.8 11.8 24.6 13.5 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 7 obs. of  2 variables:
##   .. ..$ masscharge: num [1:7] 65 92.1 120 138.1 148 ...
##   .. ..$ intensity : num [1:7] 12.31 8.81 6.51 7.71 100 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 6 obs. of  2 variables:
##   .. ..$ masscharge: num [1:6] 65 92 110 120 122 ...
##   .. ..$ intensity : num [1:6] 88.3 42.8 11.8 24.6 13.5 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 108 138 182
##   .. ..$ intensity : num [1:3] 72.6 100 48.6
##   ..$ :&amp;#39;data.frame&amp;#39;: 6 obs. of  2 variables:
##   .. ..$ masscharge: num [1:6] 166 200 209 243 244 ...
##   .. ..$ intensity : num [1:6] 1.05 3.46 1.31 100 12.76 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 14 obs. of  2 variables:
##   .. ..$ masscharge: num [1:14] 122 156 165 166 167 ...
##   .. ..$ intensity : num [1:14] 7.75 10.08 4.93 30.33 3.05 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 6 obs. of  2 variables:
##   .. ..$ masscharge: num [1:6] 227 228 229 245 246 ...
##   .. ..$ intensity : num [1:6] 59.52 5.77 2.16 100 10.01 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 53 obs. of  2 variables:
##   .. ..$ masscharge: num [1:53] 97 98.1 100 101 105.1 ...
##   .. ..$ intensity : num [1:53] 13.87 1.2 1.36 1.07 8.73 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 72 obs. of  2 variables:
##   .. ..$ masscharge: num [1:72] 79.1 81.1 82 85 91.1 ...
##   .. ..$ intensity : num [1:72] 2.94 2.13 3.41 3.7 6.6 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 26 obs. of  2 variables:
##   .. ..$ masscharge: num [1:26] 91.1 93.1 94.1 97 99 ...
##   .. ..$ intensity : num [1:26] 18.02 8.07 8.51 81.98 9.6 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 228 obs. of  2 variables:
##   .. ..$ masscharge: num [1:228] 95.1 95.1 95.3 96.2 96.9 ...
##   .. ..$ intensity : num [1:228] 11.61 7.12 2.62 8.8 1.87 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 110 156 210 227
##   .. ..$ intensity : num [1:4] 25.43 43.54 9.51 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 10 obs. of  2 variables:
##   .. ..$ masscharge: num [1:10] 83.1 93 95.1 110.1 122.1 ...
##   .. ..$ intensity : num [1:10] 12.8 10.7 11 100 13.1 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 7 obs. of  2 variables:
##   .. ..$ masscharge: num [1:7] 81 93 110 154 163 ...
##   .. ..$ intensity : num [1:7] 14.4 11.1 100 79.3 11 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 7 obs. of  2 variables:
##   .. ..$ masscharge: num [1:7] 81 93 110 154 163 ...
##   .. ..$ intensity : num [1:7] 14.4 11.1 100 79.3 11 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 110 156 210 227
##   .. ..$ intensity : num [1:4] 25.46 43.52 9.52 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 10 obs. of  2 variables:
##   .. ..$ masscharge: num [1:10] 83.1 93 95.1 110.1 122.1 ...
##   .. ..$ intensity : num [1:10] 12.8 10.7 11 100 13.1 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 119 136
##   .. ..$ intensity : num [1:2] 37.4 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 92 107 134
##   .. ..$ intensity : num [1:3] 6.71 26.83 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 8 obs. of  2 variables:
##   .. ..$ masscharge: num [1:8] 64 65 68 90 92 ...
##   .. ..$ intensity : num [1:8] 6.91 37.74 8.31 8.91 74.77 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 92 107 134
##   .. ..$ intensity : num [1:3] 6.69 26.8 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 8 obs. of  2 variables:
##   .. ..$ masscharge: num [1:8] 64 65 68 90 92 ...
##   .. ..$ intensity : num [1:8] 6.91 37.73 8.33 8.95 74.78 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 119 136
##   .. ..$ intensity : num [1:2] 37.4 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 58.1 59.1 118.1
##   .. ..$ intensity : num [1:3] 100 30.7 83
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 58.1 59.1 118.1
##   .. ..$ intensity : num [1:3] 100 30.7 82.9
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 97 134 346
##   .. ..$ intensity : num [1:4] 100 37.9 29.9 66.8
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 348
##   .. ..$ intensity : num [1:2] 20.5 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 348
##   .. ..$ intensity : num [1:2] 83.8 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 348
##   .. ..$ intensity : num [1:2] 20.4 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 348
##   .. ..$ intensity : num [1:2] 83.8 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 97 134 346
##   .. ..$ intensity : num [1:4] 100 37.9 29.8 66.8
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 268
##   .. ..$ intensity : num [1:2] 31.2 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 268
##   .. ..$ intensity : num [1:2] 100 57.1
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 107 134 266
##   .. ..$ intensity : num [1:3] 12.8 100 15.9
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 119 136 268
##   .. ..$ intensity : num [1:3] 7.81 100 10.61
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 107 134 266
##   .. ..$ intensity : num [1:3] 12.8 100 15.9
##   ..$ :&amp;#39;data.frame&amp;#39;: 5 obs. of  2 variables:
##   .. ..$ masscharge: num [1:5] 136 137 268 269 270
##   .. ..$ intensity : num [1:5] 47.69 2.16 100 9.67 1.29
##   ..$ :&amp;#39;data.frame&amp;#39;: 5 obs. of  2 variables:
##   .. ..$ masscharge: num [1:5] 94 119 120 136 137
##   .. ..$ intensity : num [1:5] 1.52 17.18 1.37 100 5.09
##   ..$ :&amp;#39;data.frame&amp;#39;: 6 obs. of  2 variables:
##   .. ..$ masscharge: num [1:6] 119 136 137 268 269 ...
##   .. ..$ intensity : num [1:6] 0.3 100 5.31 68.77 6.91 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 268
##   .. ..$ intensity : num [1:2] 31.2 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 268
##   .. ..$ intensity : num [1:2] 100 57
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 119 136 268
##   .. ..$ intensity : num [1:3] 7.77 100 10.65
##   ..$ :&amp;#39;data.frame&amp;#39;: 242 obs. of  2 variables:
##   .. ..$ masscharge: num [1:242] 46.8 47 47.9 48.7 48.7 ...
##   .. ..$ intensity : num [1:242] 1.127 0.999 0.384 0.973 1.434 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 90.1 99.5 111
##   .. ..$ intensity : num [1:3] 100 14.41 9.51
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 90.1 99.5 111
##   .. ..$ intensity : num [1:3] 100 14.42 9.54
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 136 330 330 330
##   .. ..$ intensity : num [1:4] 33.33 5.51 9.21 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 107 134 328
##   .. ..$ intensity : num [1:4] 26.43 9.91 100 53.85
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 107 134 328
##   .. ..$ intensity : num [1:4] 26.41 9.89 100 53.89
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 136 330 330 330
##   .. ..$ intensity : num [1:4] 33.32 5.47 9.22 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 5 obs. of  2 variables:
##   .. ..$ masscharge: num [1:5] 60.1 85 102.1 103 162.1
##   .. ..$ intensity : num [1:5] 12.9 25.8 20.4 47.1 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 9 obs. of  2 variables:
##   .. ..$ masscharge: num [1:9] 57 58.1 59.1 60.1 61 ...
##   .. ..$ intensity : num [1:9] 5.23 42.66 14.31 39.56 2.91 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 5 obs. of  2 variables:
##   .. ..$ masscharge: num [1:5] 60.1 85 102.1 103 162.1
##   .. ..$ intensity : num [1:5] 13.3 26.1 19.8 47.4 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 485 obs. of  2 variables:
##   .. ..$ masscharge: num [1:485] 46.3 46.5 47.5 47.7 48.8 ...
##   .. ..$ intensity : num [1:485] 0.3 0.343 0.314 0.279 0.414 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 1000 obs. of  2 variables:
##   .. ..$ masscharge: num [1:1000] 45.1 45.2 45.2 45.3 45.6 ...
##   .. ..$ intensity : num [1:1000] 1.61 1.78 4.23 1.78 1.01 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 84.1 130.1
##   .. ..$ intensity : num [1:2] 100 38.1
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 84.1 130.1
##   .. ..$ intensity : num [1:2] 100 38.1
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 85 111 129 173
##   .. ..$ intensity : num [1:4] 100 10.71 26.83 8.71
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 85 111 129 173
##   .. ..$ intensity : num [1:4] 100 10.71 26.79 8.71
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 137 154
##   .. ..$ intensity : num [1:2] 100 95.4
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 91.1 109.1 119 137.1
##   .. ..$ intensity : num [1:4] 14.92 1.38 16.93 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 8 obs. of  2 variables:
##   .. ..$ masscharge: num [1:8] 65 79.1 81.1 91.1 94 ...
##   .. ..$ intensity : num [1:8] 6.071 1.277 2.066 100 0.501 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 11 obs. of  2 variables:
##   .. ..$ masscharge: num [1:11] 41 53 63 65 77 ...
##   .. ..$ intensity : num [1:11] 0.67 0.579 1.187 49.753 0.563 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 16 obs. of  2 variables:
##   .. ..$ masscharge: num [1:16] 39 41 51 53 55 ...
##   .. ..$ intensity : num [1:16] 3.35 2.27 3.34 1.22 0.634 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 9 obs. of  2 variables:
##   .. ..$ masscharge: num [1:9] 55 70 72 73 73.9 ...
##   .. ..$ intensity : num [1:9] 4.8 10.91 11.81 3.3 6.31 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 9 obs. of  2 variables:
##   .. ..$ masscharge: num [1:9] 55 70 72 73 73.9 ...
##   .. ..$ intensity : num [1:9] 4.83 10.89 11.76 3.3 6.26 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 447 obs. of  2 variables:
##   .. ..$ masscharge: num [1:447] 45.3 45.9 46.1 46.6 47 ...
##   .. ..$ intensity : num [1:447] 0.144 0.126 0.12 0.117 0.111 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 286 obs. of  2 variables:
##   .. ..$ masscharge: num [1:286] 45.3 45.8 48.5 48.6 49.4 ...
##   .. ..$ intensity : num [1:286] 11.93 13.92 8.81 10.23 12.22 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 8 obs. of  2 variables:
##   .. ..$ masscharge: num [1:8] 59 76 96.9 116 137.9 ...
##   .. ..$ intensity : num [1:8] 9.45 100 16.33 21.13 29.02 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 67.1 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 100 14.4
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 60.9 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 100 25.5
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 67.1 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 100 14.4
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 60.8 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 100 25.5
##   ..$ :&amp;#39;data.frame&amp;#39;: 14 obs. of  2 variables:
##   .. ..$ masscharge: num [1:14] 85 86.1 87 102.9 111 ...
##   .. ..$ intensity : num [1:14] 30 4.2 40.2 11.7 100 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 14 obs. of  2 variables:
##   .. ..$ masscharge: num [1:14] 85 86.1 87 102.9 111 ...
##   .. ..$ intensity : num [1:14] 30.06 4.24 40.25 11.67 100 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 112 324
##   .. ..$ intensity : num [1:2] 50.7 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 112 324
##   .. ..$ intensity : num [1:2] 100 33.2
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 97 139 322
##   .. ..$ intensity : num [1:4] 100 52.45 7.51 60.36
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 97 139 322
##   .. ..$ intensity : num [1:4] 100 51.85 6.81 58.96
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 97 139 322
##   .. ..$ intensity : num [1:4] 100 52.48 7.46 60.4
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 97 139 322
##   .. ..$ intensity : num [1:4] 100 51.86 6.83 58.94
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 112 324
##   .. ..$ intensity : num [1:2] 50.6 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 112 324
##   .. ..$ intensity : num [1:2] 100 33.3
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 58.1 60.1 104.1
##   .. ..$ intensity : num [1:3] 19.1 38.1 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 58.1 60.1 104.1
##   .. ..$ intensity : num [1:3] 19.1 38.1 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 134 207 223 223
##   .. ..$ intensity : num [1:4] 4.2 13 3 100
##   .. [list output truncated]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This database has included all of the 5062 Q-ToF spectra from 1259 compounds in HMDB. We only considered the peaks larger than 10% of the base peak and calculated all of the paired mass distances within the spectra. For example, for compound HMDB0000014, the MS/MS spectra should be (112.1, 228.1) with intensity (69.97, 100). Then the PMD spectra for annotation should be 116 for this compounds.&lt;/p&gt;
&lt;p&gt;For the PMD annotation, we will also compute the PMDs of input spectra. Then we compare the input PMDs with the database. Here we need three parameters to refine the candidates. The first parameter is ppm for mass accuracy of precursor ions. The second parameter is the range of precursor ions, the default setting should be 1.1 to include M+H or M-H. The third parameter is the pmd length percentage cutoff for annotation. 0.6(default) means 60 percentage of the pmds in your sample could be found in certain compound pmd database. The fourth parameter is the relative intensity cutoff for input spectra for pmd analysis, default 0.1 for 10 % of the base peak.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this is the sepctra of HMDB0034004
file &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;challenge-msms.mgf&amp;quot;, package = &amp;quot;rmwf&amp;quot;)
# pmd msms annotation
anno &amp;lt;- pmd::pmdanno(file,db=qtof)
unique(anno$name)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;HMDB0034004&amp;quot; &amp;quot;HMDB0003217&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;enviGCMS::plotanno(anno)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The score rule for pmd annotation is that the candidates will be ordered according to the overlapped pmd numbers. In this case, if two candidates have 3 and 4 pmd overlapped with the input spectra, the latter one will be the first candidate.&lt;/p&gt;
&lt;p&gt;Such annotation could be used for MS1 annotation. However, without precursor ion to refine the candidates. It’s better to find the M+H or M-H in advance. In this case, the input spectra should be processed by isotope, adducts or neutral loss detection by pmd of 1.006Da, 22.98Da, etc. Then the following step should be the same as MS2 pmd annotation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Retention time alignment for peaks list</title>
      <link>https://yufree.cn/en/2020/12/16/retention-time-alignment-for-peaks-list/</link>
      <pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2020/12/16/retention-time-alignment-for-peaks-list/</guid>
      <description>&lt;p&gt;A regular open source metabolomics workflow could start from the open source format of RAW data. For xcms or other software, algorithm like obiwarp could be used to align the peaks into features. However, some workflows will start from the exported csv files from the instruments. The major issue is that peaks list is not features table and multiple samples should be aligned. Here I will show a native method to align peaks across samples in R considering the mass accuracy and pre-defined retention time shift.&lt;/p&gt;
&lt;p&gt;Firstly, the input object should be a list with elements as peaks list from single samples. It should be a data.frame with retention time, mass to charge ratio and intensities.&lt;/p&gt;
&lt;p&gt;Then we need to assign a sample as the template for alignment. The output of the alignment function should use m/z and rt data from this sample as the features property.&lt;/p&gt;
&lt;p&gt;Now we could align the peaks across samples.  All the samples&#39; peaks list should be aligned with the peaks list of template sample one by one. Here the alignment should consider the ppm of m/z and delta retention time. Each alignment will decrease the numbers of featuers when no peaks could be aligned to certain peaks in the template samples. This is a recursive process considering the aligned features&#39; intensities would be saved and reduced for the next paired alignment.&lt;/p&gt;
&lt;p&gt;When one peak from template sample could be aligned to multiple peaks in other peaks list, you need to define a function to deal with the intensity of feature in other samples. For example, you could use mean/median/sum to generate the features&#39; intensities for the other samples. Since our inputs are peaks lists, no peaks properties such as peak width, peak shape could be checked. It&amp;rsquo;s better to control this when you output the peaks list for single samples.&lt;/p&gt;
&lt;p&gt;If you are familiar with minifrac in xcms, you might find such alignment will actually set the minifrac as 1. In this case, you should perform this alignment on samples from the same group and you should not use this alignment for samples without group information.&lt;/p&gt;
&lt;p&gt;The final output should show the feature&#39; m/z, retention time and intensity across samples. However, this is alignment instead of correction. Such alignment will not correct the shift of retention time larger than certain cutoff, for example, 5s. The advantage of this alignment is that the concept is clear and easy to explain. The aligned peaks should be of high quality. This method could also be used to find the common ions across samples for quality control puporse.&lt;/p&gt;
&lt;p&gt;Here is the code (I will put this function in enviGCMS package later):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-R&#34; data-lang=&#34;R&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# check input to make sure the each peaks list contain &amp;#39;mz&amp;#39;, &amp;#39;rt&amp;#39; and &amp;#39;ins&amp;#39; as m/z, retetion time in seconds and intensity of certain peaks.&lt;/span&gt;
data1 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;read.csv&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sample1.csv&amp;#39;&lt;/span&gt;)
data2 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;read.csv&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sample2.csv&amp;#39;&lt;/span&gt;)
data3 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;read.csv&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sample3.csv&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# generate the list as input&lt;/span&gt;
li &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(data1,data2,data3)
&lt;span style=&#34;color:#75715e&#34;&gt;# define the function to align peaks list&lt;/span&gt;
getretcor &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(list,cs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,ppm&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,deltart&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, FUN){
  nli &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; list[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;cs]
  csd &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; list[[cs]]
  i&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  df1 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; csd
  ins &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; df1&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;ins
  &lt;span style=&#34;color:#a6e22e&#34;&gt;while&lt;/span&gt;(i&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;(nli)){
    df2 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; list[[i]]
    df &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; enviGCMS&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getalign&lt;/span&gt;(df1&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;mz,df2&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;mz,df1&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;rt,df2&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;rt,ppm&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;ppm,deltart&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;deltart)
  mr2 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;paste0&lt;/span&gt;(df2&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;mz,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;@&amp;#39;&lt;/span&gt;,df2&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;rt)
  mrx &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;paste0&lt;/span&gt;(df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;mz2,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;@&amp;#39;&lt;/span&gt;,df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;rt2)
  
  df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;ins2 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; df2&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;ins&lt;span style=&#34;color:#a6e22e&#34;&gt;[match&lt;/span&gt;(mrx,mr2)]
  dfx &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; df[&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;duplicated&lt;/span&gt;(df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;xid),]
  dfx&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;ins2 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;aggregate&lt;/span&gt;(df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;ins2,by&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;xid),FUN)[,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]
  df1 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cbind.data.frame&lt;/span&gt;(mz&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;dfx&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;mz1,rt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;dfx&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;rt1)
  &lt;span style=&#34;color:#a6e22e&#34;&gt;if&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;dim&lt;/span&gt;(ins))&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;){
    insn &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; ins[df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;xid,]
    ins &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cbind.data.frame&lt;/span&gt;(insn[&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;duplicated&lt;/span&gt;(df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;xid),],dfx&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;ins2)
  }else{
    insn &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; ins[df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;xid]
    ins &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cbind.data.frame&lt;/span&gt;(ins1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;insn[&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;duplicated&lt;/span&gt;(df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;xid)],dfx&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;ins2)
  }
  i&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;i&lt;span style=&#34;color:#ae81ff&#34;&gt;+1&lt;/span&gt;
  }
  re &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cbind.data.frame&lt;/span&gt;(df1,ins)
  &lt;span style=&#34;color:#a6e22e&#34;&gt;colnames&lt;/span&gt;(re) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mz&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rt&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#a6e22e&#34;&gt;paste0&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ins&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;(list)))
  &lt;span style=&#34;color:#a6e22e&#34;&gt;return&lt;/span&gt;(re)
}
&lt;span style=&#34;color:#75715e&#34;&gt;# usage&lt;/span&gt;
re &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;getretcor&lt;/span&gt;(list,FUN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mean)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Local Metabolites-diseases database based on HMDB</title>
      <link>https://yufree.cn/en/2020/12/01/metabolites-diseases-database-based-on-hmdb/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2020/12/01/metabolites-diseases-database-based-on-hmdb/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/en/2020/12/01/metabolites-diseases-database-based-on-hmdb/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Human Metabolites Database(HMDB) is a great tool to access human related metabolites or small molecular. You could access metabolites’ information about disease, which is important for public health related studies. However, it’s not easy to search multiple metabolites at the same time. Here I will show a demo to locally access multiple metabolites’ disease information.&lt;/p&gt;
&lt;div id=&#34;step-1-annotate-the-mz-with-hmdb-id&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: Annotate the m/z with HMDB ID&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rmwf)
library(enviGCMS)
data(&amp;quot;mzrt&amp;quot;)
data(&amp;quot;hr&amp;quot;)
re &amp;lt;- enviGCMS::getms1anno(pmd = c(&amp;#39;Na-H&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;Na&amp;#39;), mz = mzrt$mz[1:10], ppm = 5, db=hr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## No retention time information!
## No retention time information!
## No retention time information!&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Only check [M+H]+
name &amp;lt;- unique(re[[2]]$name)
# from name to HMDB ID
HMDBID &amp;lt;- webchem::cts_convert(name,&amp;#39;name&amp;#39;,&amp;#39;human metabolome database&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Querying Diethanolamine.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## OK (HTTP 200).
## Querying 2-Piperidinone. OK (HTTP 200).
## Querying S-Ethyl thioacetate. OK (HTTP 200).
##  Not found. Returning NA.
## Querying S-Methyl propanethioate. OK (HTTP 200).
##  Not found. Returning NA.
## Querying N-Nitroso-pyrrolidine. OK (HTTP 200).
##  Not found. Returning NA.
## Querying 3-(Methylthio)propanal. OK (HTTP 200).
## Querying 3-Mercapto-2-butanone. OK (HTTP 200).
## Querying Gyromitrin. OK (HTTP 200).
## Querying 1-Pyrrolidinecarboxaldehyde. OK (HTTP 200).
## Querying (Methylthio)acetone. OK (HTTP 200).
## Querying 2,5-Dihydro-2,4-dimethyloxazole. OK (HTTP 200).
##  Not found. Returning NA.
## Querying 4-Mercapto-2-butanone. OK (HTTP 200).
## Querying (2R)-2-Hydroxy-2-methylbutanenitrile. OK (HTTP 200).
##  Not found. Returning NA.
## Querying cis-3-Chloroacrylic acid. OK (HTTP 200).
##  Not found. Returning NA.
## Querying trans-3-Chloroacrylic acid. OK (HTTP 200).
##  Not found. Returning NA.
## Querying Allyl methyl sulfoxide. OK (HTTP 200).
##  Not found. Returning NA.
## Querying Neurine. OK (HTTP 200).
##  Not found. Returning NA.
## Querying 2-Hydroxy-2-methylbutanenitrile. OK (HTTP 200).
## Querying N-Methylpyrrolidin-2-one. OK (HTTP 200).
##  Not found. Returning NA.
## Querying Tetrahydrothiophene-1-oxide. OK (HTTP 200).
##  Not found. Returning NA.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HID &amp;lt;- unlist(HMDBID)[!is.na(unlist(HMDBID))]
# Check the results
HID&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  Diethanolamine                  2-Piperidinone 
##                   &amp;quot;HMDB0004437&amp;quot;                   &amp;quot;HMDB0011749&amp;quot; 
##          3-(Methylthio)propanal           3-Mercapto-2-butanone 
##                   &amp;quot;HMDB0031857&amp;quot;                   &amp;quot;HMDB0031982&amp;quot; 
##                      Gyromitrin     1-Pyrrolidinecarboxaldehyde 
##                   &amp;quot;HMDB0033952&amp;quot;                   &amp;quot;HMDB0034587&amp;quot; 
##             (Methylthio)acetone           4-Mercapto-2-butanone 
##                   &amp;quot;HMDB0040170&amp;quot;                   &amp;quot;HMDB0041015&amp;quot; 
## 2-Hydroxy-2-methylbutanenitrile 
##                   &amp;quot;HMDB0060309&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-build-local-hmdb-disease-database-and-search-id&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: Build local HMDB disease database and search ID&lt;/h2&gt;
&lt;p&gt;Here is the python code to extract disease related information:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from io import StringIO
from lxml import etree
import csv
def hmdbpextract(name, file):
  ns = {&amp;#39;hmdb&amp;#39;: &amp;#39;http://www.hmdb.ca&amp;#39;}
  context = etree.iterparse(name, tag=&amp;#39;{http://www.hmdb.ca}metabolite&amp;#39;)
  csvfile = open(file, &amp;#39;w&amp;#39;)
  fieldnames = [&amp;#39;accession&amp;#39;, &amp;#39;dname&amp;#39;]
  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
  writer.writeheader()
  for event, elem in context:

    accession = elem.xpath(&amp;#39;hmdb:accession/text()&amp;#39;, namespaces=ns)[0]
    try:
        dname = elem.xpath(&amp;#39;hmdb:diseases/hmdb:disease/hmdb:name/text()&amp;#39;, namespaces=ns)
    except:
        dname = &amp;#39;NA&amp;#39;
    writer.writerow({&amp;#39;accession&amp;#39;: accession, &amp;#39;dname&amp;#39;: dname})
    # It&amp;#39;s safe to call clear() here because no descendants will be
    # accessed
    elem.clear()
# Also eliminate now-empty references from the root node to elem
    for ancestor in elem.xpath(&amp;#39;ancestor-or-self::*&amp;#39;):
        while ancestor.getprevious() is not None:
            del ancestor.getparent()[0]
  del context
  return;
hmdbpextract(&amp;#39;hmdb_metabolites.xml&amp;#39;,&amp;#39;hmdbd.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you download HMDB xml file, you could get csv file with disease related information with above code.&lt;/p&gt;
&lt;p&gt;However, we need to make this csv file friendly for batch search.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hmdb &amp;lt;- read.csv(&amp;#39;hmdbd.csv&amp;#39;)
hmdb2 &amp;lt;- t(apply(hmdb,1,function(x) gsub( &amp;quot;^\\[|]$&amp;quot;, &amp;quot;&amp;quot;, as.character(x))))
hmdb2 &amp;lt;- as.data.frame(hmdb2)
colnames(hmdb2) &amp;lt;- colnames(hmdb)

disease &amp;lt;- hmdb2[,c(&amp;quot;accession&amp;quot;,&amp;quot;dname&amp;quot;)]

g &amp;lt;- strsplit(as.character(hmdb2$dname), &amp;quot;,&amp;quot;)
dname &amp;lt;- data.frame(accession = rep(hmdb2$accession, lapply(g, length)), dname = unlist(g))
dname$dname &amp;lt;- gsub( &amp;#39;^ &amp;#39;, &amp;quot;&amp;quot;, as.character(dname$dname))
dname$dname &amp;lt;- gsub( &amp;quot;^&amp;#39;|&amp;#39;$&amp;quot;, &amp;quot;&amp;quot;, as.character(dname$dname))
dname$dname &amp;lt;- gsub( &amp;#39;^&amp;quot;|&amp;quot;$&amp;#39;, &amp;quot;&amp;quot;, as.character(dname$dname))

write.csv(dname,&amp;#39;hmdbdname.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This part need to download HMDB database and extract the disease information. I know it’s too much for user and I already did this for you.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# download HMDB disease database
db &amp;lt;- read.csv(&amp;#39;https://raw.githubusercontent.com/yufree/expodb/master/hmdb/hmdbdname.csv&amp;#39;)
alld &amp;lt;- db[db$accession%in%HID,]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-search-hmdb-id-for-disease-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: Search HMDB ID for disease information&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alld&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         X   accession                            dname
## 6908 6908 HMDB0011749                   Ovarian cancer
## 6909 6909 HMDB0011749                  Crohn&amp;#39;s disease
## 6910 6910 HMDB0011749               Ulcerative colitis
## 6911 6911 HMDB0011749                Colorectal cancer
## 7625 7625 HMDB0031857 Nonalcoholic fatty liver disease&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we could find two compounds might relate to five disease.&lt;/p&gt;
&lt;p&gt;This is a very simple example from m/z to disease and you could hack the code to find new world.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Independent peaks selection algorithms</title>
      <link>https://yufree.cn/en/2020/08/12/independent-peaks-selection-algorithms/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2020/08/12/independent-peaks-selection-algorithms/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In mass spectrometry based untargeted analysis, raw data from instrument contain peaks level information. However, we actually care about compounds level information. For target analysis, quantitative and qualitative ions could stand for the target compounds. However, in untargeted analysis, full scan mode will collect all the charged compounds’ ions. One compound could generate multiple ions such as adducts, neutral loss, multiple charged ions, isotopougue and/or fragmental ions and those peaks are highly correlated with each other. If we will perform the statistical analysis at compounds level, those highly correlated ions will disturb the independent assumption of those statistical analysis. In this case, we need algorithm to remove such redundant peaks or select pseudo targeted ions for unknown compounds. Ideally, selection of molecular ion will make the following analysis much easier.&lt;/p&gt;
&lt;p&gt;To detect such redundant peaks, two relationships could be used: paired mass distance (PMD) and paired correlation. I developed GlobalStd algorithm to remove such redundant peaks based on PMD relationship. However, I actually added the function to use correlation along with PMD in this algorithm. To make the details clear, I also added functions to extract independent peaks based on correlation, as well as the ions clusters extraction function. The ions clusters could be treated the pseudo spectra detection algorithm. In this case, we could not only extract the independent ions for compounds level analysis, but also grab the pseudo specturm of this compounds for annotation purpose.&lt;/p&gt;
&lt;p&gt;Here I will demo how to perform such analysis based on PMD package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# use dev version of PMD package
# remotes::install_github(&amp;#39;yufree/pmd&amp;#39;)
library(pmd)
library(enviGCMS)
# load demo data
data(spmeinvivo)
# perform GlobalStd algorithm
list &amp;lt;- globalstd(spmeinvivo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 75 retention time cluster found.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 380 paired masses found&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 9 unique within RT clusters high frequency PMD(s) used for further investigation.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 719 isotopologue(s) related paired mass found.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 492 multi-charger(s) related paired mass found.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 8 retention group(s) have single peaks. 14 23 32 33 54 55 56 75&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 11 group(s) with multiple peaks while no isotope/paired relationship 4 5 7 8 11 41 42 49 68 72 73&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 9 group(s) with multiple peaks with isotope without paired relationship 2 9 22 26 52 62 64 66 70&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 4 group(s) with paired relationship without isotope 1 10 15 18&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 43 group(s) with paired relationship and isotope 3 6 12 13 16 17 19 20 21 24 25 27 28 29 30 31 34 35 36 37 38 39 40 43 44 45 46 47 48 50 51 53 57 58 59 60 61 63 65 67 69 71 74&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 297 std mass found.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## PMD frequency cutoff is 6 by PMD network analysis with largest network average distance 5.99 .&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 57 groups were found as high frequency PMD group.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0 was found as high frequency PMD. 
## 1.98 was found as high frequency PMD. 
## 2.01 was found as high frequency PMD. 
## 2.02 was found as high frequency PMD. 
## 6.97 was found as high frequency PMD. 
## 11.96 was found as high frequency PMD. 
## 12 was found as high frequency PMD. 
## 12.04 was found as high frequency PMD. 
## 13.98 was found as high frequency PMD. 
## 14.02 was found as high frequency PMD. 
## 14.05 was found as high frequency PMD. 
## 15.99 was found as high frequency PMD. 
## 16.03 was found as high frequency PMD. 
## 19.04 was found as high frequency PMD. 
## 28.03 was found as high frequency PMD. 
## 30.05 was found as high frequency PMD. 
## 31.99 was found as high frequency PMD. 
## 37.02 was found as high frequency PMD. 
## 42.05 was found as high frequency PMD. 
## 48.04 was found as high frequency PMD. 
## 48.98 was found as high frequency PMD. 
## 49.02 was found as high frequency PMD. 
## 54.05 was found as high frequency PMD. 
## 56.06 was found as high frequency PMD. 
## 56.1 was found as high frequency PMD. 
## 58.04 was found as high frequency PMD. 
## 58.08 was found as high frequency PMD. 
## 58.11 was found as high frequency PMD. 
## 63.96 was found as high frequency PMD. 
## 66.05 was found as high frequency PMD. 
## 68.06 was found as high frequency PMD. 
## 70.04 was found as high frequency PMD. 
## 70.08 was found as high frequency PMD. 
## 74.02 was found as high frequency PMD. 
## 80.03 was found as high frequency PMD. 
## 82.08 was found as high frequency PMD. 
## 88.05 was found as high frequency PMD. 
## 91.1 was found as high frequency PMD. 
## 93.12 was found as high frequency PMD. 
## 96.09 was found as high frequency PMD. 
## 101.05 was found as high frequency PMD. 
## 108.13 was found as high frequency PMD. 
## 110.11 was found as high frequency PMD. 
## 112.16 was found as high frequency PMD. 
## 116.08 was found as high frequency PMD. 
## 122.15 was found as high frequency PMD. 
## 124.16 was found as high frequency PMD. 
## 126.14 was found as high frequency PMD. 
## 148.04 was found as high frequency PMD. 
## 150.2 was found as high frequency PMD. 
## 173.18 was found as high frequency PMD. 
## 191.08 was found as high frequency PMD. 
## 191.15 was found as high frequency PMD. 
## 192.19 was found as high frequency PMD. 
## 194.2 was found as high frequency PMD. 
## 267.25 was found as high frequency PMD. 
## 325.3 was found as high frequency PMD.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get new list with independent peaks
listi &amp;lt;- getfilter(list,rowindex = list$stdmassindex)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we have 297 peaks compared with original 1459 peaks. If we removed the right redundant peaks, the PCA score plot will not change too much with a much smaller data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot the comparision
par(mfrow = c(1,2),mar = c(4,4,2,1)+0.1)
plotpca(list$data,lv = as.numeric(as.factor(list$group)),main = &amp;quot;all peaks&amp;quot;)
plotpca(listi$data,lv = as.numeric(as.factor(listi$group)),main = &amp;quot; independent peaks&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-08-12-independent-peaks-selection-algorithms_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The GlobalStd algorithm with default setting will not use intensity data. However, if we could use intensity data to refine the result, we could select the base peaks of each pseudo spectrum. After consideration of correlation relationship, we could further select the base independent peaks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;base &amp;lt;- getcluster(list)
# get the new list with independent base peaks
listb &amp;lt;- getfilter(list,rowindex = base$stdmassindex2)
# get the new list with reduced independent base peaks
basei &amp;lt;- getcluster(list,corcutoff = 0.9)
listib &amp;lt;- getfilter(list,rowindex = basei$stdmassindex2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we didn’t use PMD, we could also detect the correlation cluster within narrow retention time window as pseudo spectra. I also supplied function to select the correlation independent peaks and base correlation independent peaks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ci &amp;lt;- getcorcluster(spmeinvivo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 75 retention time cluster found.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get the new list with correlation independent peaks
listci &amp;lt;- getfilter(list,rowindex = ci$stdmassindex)
# get the new list with base correlation independent peaks
listcib &amp;lt;- getfilter(list,rowindex = ci$stdmassindex2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could compare the compare reduced result using PCA similarity factor. A good peak selection algorithm could show a high PCA similarity factor compared with original data set while retain the minmized number of peaks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(2,3),mar = c(4,4,2,1)+0.1)
plotpca(listi$data,lv = as.numeric(as.factor(list$group)),main = paste(sum(list$stdmassindex),&amp;quot;independent peaks&amp;quot;))
plotpca(listb$data,lv = as.numeric(as.factor(list$group)),main = paste(sum(base$stdmassindex2),&amp;quot;independent base peaks&amp;quot;))
plotpca(listib$data,lv = as.numeric(as.factor(list$group)),main = paste(sum(basei$stdmassindex2),&amp;quot;reduced independent base peaks&amp;quot;))
plotpca(listci$data,lv = as.numeric(as.factor(list$group)),main = paste(sum(ci$stdmassindex),&amp;quot;peaks without correlationship&amp;quot;))
plotpca(listcib$data,lv = as.numeric(as.factor(list$group)),main = paste(sum(ci$stdmassindex2),&amp;quot;base peaks without correlationship&amp;quot;))
plotpca(list$data,lv = as.numeric(as.factor(list$group)),main = paste(nrow(list$data),&amp;quot;all peaks&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-08-12-independent-peaks-selection-algorithms_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, five peaks selection algorithms are fine to stand for the original peaks. However, the independent base peaks retain the most information with relative low numbers of peaks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SI for ASMS 2020 Reboot Reactomics Presentation</title>
      <link>https://yufree.cn/en/2020/06/07/si-for-asms-2020-reboot-presentation/</link>
      <pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2020/06/07/si-for-asms-2020-reboot-presentation/</guid>
      <description>&lt;p&gt;I will be in the Q&amp;amp;A session for ASMS 2020 Reboot MOB pm: Exposomics, Toxicology, and Health Outcomes tomorrow. 20 minutes might not be enough to cover all the details of &amp;ldquo;Reactomics: using mass spectrometry as chemical reaction detector&amp;rdquo;. Here I list some useful resources for this presentation and I will update the Q&amp;amp;A after the online session in this post.&lt;/p&gt;
&lt;h3 id=&#34;resources&#34;&gt;Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/-mT3HcVygHE&#34;&gt;Video&lt;/a&gt;. If you didn&amp;rsquo;t register for ASMS 2020 Reboot, here is the link to unlisted video on YouTube.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://yufree.github.io/presentation/reactomics/pres-asms.html&#34;&gt;Slides&lt;/a&gt;. This is the slides for ASMS 2020 Reboot. Press &amp;ldquo;P&amp;rdquo; and you will see the notes for each slide with details. Another full version of reactomics presentation for one hour presentation could be found &lt;a href=&#34;http://yufree.github.io/presentation/reactomics/pres&#34;&gt;here&lt;/a&gt;. I will not update the conference presentation while I will add new contents for the full version of reactomics presentation whenever I have new results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/855148v2&#34;&gt;Reactomics Preprint on BioRxiv&lt;/a&gt;. This preprint contain the same contents as shown in the presentation. I will update the manuscript later in this week for some changes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.rstudio.com/web/packages/pmd/index.html&#34;&gt;pmd package&lt;/a&gt;. This is the software for reactomics. It&amp;rsquo;s on CRAN and you could check this &lt;a href=&#34;https://yufree.github.io/pmd/&#34;&gt;site&lt;/a&gt; for the most updated version with new functions, updated pmd annotation database and &lt;a href=&#34;https://yufree.github.io/pmd/articles/globalstd.html&#34;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/30661584/&#34;&gt;pmd paper&lt;/a&gt;. This is the first paper on pmd package to introduce structure/reaction directed analysis and I developed this idea into reactomics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://yufree.shinyapps.io/pmdapp/&#34;&gt;GlobalStd Shiny App&lt;/a&gt;. This is online application to reduce peaks list into independent peaks by GlobalStd algorithm. This app try to remove redundancy peaks such as co-eluted peaks, multi-chargers, adducts, neutral loss, isotopologues, and fragments ions in the peak list by pmd frequency analysis. Then you will have a smaller while independent peaks list (usually less that 20% of original peaks numbers) csv file for reactomics analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://yufree.shinyapps.io/pmdnet&#34;&gt;pmd network analysis Shiny App&lt;/a&gt;. This is online application to perform pmd network analysis. It tries to find all the possible metabolites of certain input mass to charge ratio and pmd values. You could reproduce the pumpkin TBBPA study results as shown in the presentation. Both online app could be run locally when you install pmd package. &lt;code&gt;runPMD()&lt;/code&gt; could start the GlobalStd Shiny App and &lt;code&gt;runPMDnet()&lt;/code&gt; could start the pmd network analysis Shiny App.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/18qDbjy1PYuLZgOOlbSgzkpFBQnc3k-H5XhQPHjszfHA/edit?usp=sharing&#34;&gt;PMDDA Workflow Poster&lt;/a&gt;. This is the PMDDA workflow poster for ASMS 2020 Reboot. PMDDA workflow tries to produce MS1 level independent peaks (GlobalStd algorithm) for MS2 pseudo targeted analysis and make annotation based on PMD of MS2 spectra.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://hub.docker.com/r/yufree/xcmsrocker/&#34;&gt;xcmsrocker image&lt;/a&gt;. This is the Rocker image for metabolomics data analysis. If you are familiar with docker image, this image include the same data analysis environment with all of the R based software used in my current lab for reproducible research. It also contain PMDDA workflow data analysis template as part of &lt;a href=&#34;https://github.com/yufree/rmwf&#34;&gt;rmwf package&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://bookdown.org/yufree/Metabolomics/&#34;&gt;meta-workflow online book&lt;/a&gt;. This is a online book for maintained by me. You could find answers for most of the metabolomics data analysis related questions. It&amp;rsquo;s open source to anyone who wish to contribute. Any PR is welcomed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/yufree/mdaw&#34;&gt;Metabolomics Data Analysis Workshop Slides&lt;/a&gt;. I have organized this Workshop in University of Waterloo and University of California, Irvine. The slides will be updated according to the most updated research. It&amp;rsquo;s a good start for the beginner of metabolomics data analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;qa&#34;&gt;Q&amp;amp;A&lt;/h3&gt;
&lt;p&gt;There are one question for the resources of the software and I listed above. Just share some interesting data:&lt;/p&gt;
&lt;p&gt;On Monday, there were 620 presentations with 364 questions. However, without time limitation and questions from the session chair, 415 presentations got 0 question and the hottest talk got 13 questions. In my session, no more than 40 people were online for Q&amp;amp;A, which looks poor compared with offline meeting.&lt;/p&gt;
&lt;p&gt;However, those numbers coin with the paper citation records. Few important paper dominated the citations of certain journal. Anyway, popularity means nothing about science and communication will help us something new.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Daily check for coronavirus data</title>
      <link>https://yufree.cn/en/2020/04/17/daily-check-for-coronavirus-data/</link>
      <pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2020/04/17/daily-check-for-coronavirus-data/</guid>
      <description>


&lt;p&gt;As employee of health system, every week I will check our lab for leaks or instrumental issues. Meanwhile, I am using the following code to check daily increasing cases in US state-county level every morning.&lt;/p&gt;
&lt;p&gt;I think three signals are crucial for community level data and every community and people should decide their own timeline for normal live to survive in this pandemic and potential recession in the following months or years:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Positive rate of testing is decreasing to ~1%. In this case, the testing ability should be enough to screen all the potential infected people.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Daily increasing hospitialized number is less than the sum of dischared patients and dead people. In this case, the medical resources should reach the peak.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Daily death numbers reached the peak and decrease for two weeks. In this case, the susceptible individuals in the community shoud be either infected or isolated and we could start to consider the cease of lockdown of local community.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;state&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;STATE&lt;/h1&gt;
&lt;p&gt;Data source is New York Times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

file &amp;lt;- read.csv(&amp;#39;https://github.com/nytimes/covid-19-data/raw/master/us-states.csv&amp;#39;,stringsAsFactors = F)
file$date &amp;lt;- as.Date(file$date)

# Check States with max case larger than 10000 and more than 100 deaths
df &amp;lt;- file%&amp;gt;%
    group_by(state)%&amp;gt;%
    mutate(change=c(0,diff(cases)),change2=c(0,diff(deaths)))%&amp;gt;%
    filter(max(cases)&amp;gt;10000 &amp;amp; max(deaths)&amp;gt;100)%&amp;gt;%
    ungroup() 

df %&amp;gt;%
    ggplot(aes(x=date,y=change,fill=state)) +
    geom_point() +
    geom_smooth() +
    facet_wrap(~state,scales = &amp;#39;free&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-04-17-daily-check-for-coronavirus-data_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;%
    ggplot(aes(x=date,y=change2,fill=state)) +
    geom_point() +
    geom_smooth() +
    facet_wrap(~state,scales = &amp;#39;free&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-04-17-daily-check-for-coronavirus-data_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;county&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;COUNTY&lt;/h1&gt;
&lt;p&gt;Data source is New York Times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;file &amp;lt;- read.csv(&amp;#39;https://github.com/nytimes/covid-19-data/raw/master/us-counties.csv&amp;#39;,stringsAsFactors = F)
file$date &amp;lt;- as.Date(file$date)

library(tidyverse)
# Check Counties with max case larger than 10000 and more than 100 deaths
df &amp;lt;- file%&amp;gt;%
    group_by(county,state)%&amp;gt;%
    mutate(CaseChange=c(0,diff(cases)),DeathChange=c(0,diff(deaths)))%&amp;gt;%
    filter(max(cases)&amp;gt;10000 &amp;amp; max(deaths)&amp;gt;100)%&amp;gt;%
    ungroup() 

df %&amp;gt;%
    ggplot(aes(x=date,y=CaseChange,fill=state)) +
    geom_point() +
    geom_smooth() +
    facet_wrap(~county+state,scales = &amp;#39;free&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-04-17-daily-check-for-coronavirus-data_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;%
    ggplot(aes(x=date,y=DeathChange,fill=state)) +
    geom_point() +
    geom_smooth() +
    facet_wrap(~county+state,scales = &amp;#39;free&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-04-17-daily-check-for-coronavirus-data_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## NYC zipcode
## covid19nyc &amp;lt;- read.csv(&amp;#39;https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;test&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Test&lt;/h1&gt;
&lt;p&gt;Data source is &lt;a href=&#34;https://covidtracking.com&#34;&gt;covidtracking.com&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;daily &amp;lt;- read.csv(&amp;#39;https://covidtracking.com/api/v1/states/daily.csv&amp;#39;,stringsAsFactors = T,header = T)
daily$date &amp;lt;- as.Date(daily$dateChecked)

dd &amp;lt;- daily %&amp;gt;%
    group_by(state)%&amp;gt;%
    mutate(posrateChange=positiveIncrease/totalTestResultsIncrease,recin = c(0,-diff(recovered)))%&amp;gt;%
    ungroup()
# pos rate changes
dd %&amp;gt;%
    filter(state==&amp;#39;NY&amp;#39;)%&amp;gt;%
    ggplot(aes(x=date,y=posrateChange)) +
    geom_point() +
    geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-04-17-daily-check-for-coronavirus-data_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# hospitialized/death/recover daily increase in NY
dd %&amp;gt;%
    filter(state==&amp;#39;NY&amp;#39;)%&amp;gt;%
    tidyr::pivot_longer(col=c(&amp;#39;hospitalizedIncrease&amp;#39;,&amp;#39;deathIncrease&amp;#39;,&amp;#39;recin&amp;#39;),names_to = &amp;#39;condition&amp;#39;,values_to = &amp;#39;count&amp;#39;) %&amp;gt;%
    ggplot(aes(x=date,y=count,color=condition)) +
    geom_point() +
    geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-04-17-daily-check-for-coronavirus-data_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# hospitialized/death/recover daily increase in States with more than 10000 positive cases
dd %&amp;gt;%
    group_by(state) %&amp;gt;%
    filter(positive&amp;gt;10000) %&amp;gt;%
    ungroup() %&amp;gt;%
    tidyr::pivot_longer(col=c(&amp;#39;hospitalizedIncrease&amp;#39;,&amp;#39;deathIncrease&amp;#39;,&amp;#39;recin&amp;#39;),names_to = &amp;#39;condition&amp;#39;,values_to = &amp;#39;count&amp;#39;) %&amp;gt;%
    ggplot(aes(x=date,y=count,fill=condition)) +
    geom_point(aes(col=condition)) +
    geom_smooth()+
    facet_wrap(~state,scales = &amp;#39;free&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-04-17-daily-check-for-coronavirus-data_files/figure-html/unnamed-chunk-3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Those numbers are real people.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Run order effect for metabolomics studies</title>
      <link>https://yufree.cn/en/2019/11/27/run-order-effect-for-metabolomics-studies/</link>
      <pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2019/11/27/run-order-effect-for-metabolomics-studies/</guid>
      <description>&lt;p&gt;For a regular XC-MS based metabolomics workflow, the injection sequence should be carefully designed. Chromatograph column will always change and the first and last samples would show a shift of baseline. Such shifts would be monotone increasing or decreasing. In this case, we need some pooled QC samples to dirty the column at the very beginning of sequence. However, how many pooled QC samples will give us a stable baseline?&lt;/p&gt;
&lt;p&gt;To solve this issue, I defined a Pooled QC Stable Index(PQSI) in my enviGCMS package. Instead of checking the TIC of one sample, I will check the stability of each peak one by one. Here is a demo:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/presentation/figure/pooledQC.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As shown in above figure, for one peak repeated analyzed in one sequence, the intensity would become stable in long term. In math, the slope of every n(5 is the default number) samples along the run order would become 0. Then we could define the percentage of stable peaks as PQSI. Such index would be a value between 0 and 1. The higher of such index, more peaks within the QC would be affected by run order effect. You could use such function to check the QC samples to see if run order effects would influence the samples at the beginning of sequences. I include this &lt;code&gt;getpqsi&lt;/code&gt; function in enviGCMS package and here is the demo:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r}&#34; data-lang=&#34;{r}&#34;&gt;library(enviGCMS)
data(list)
order &amp;lt;- 1:12
# n means how many points to build a linear regression model
n = 5
idx &amp;lt;- getpqsi(list$data,order,n = n)
plot(idx~order[-(1:(n-1))],pch=19)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/enviGCMS/articles/PooledQC_files/figure-html/unnamed-chunk-6-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, we could see at 5th sample, 30% peaks show correlation with the run order. However, ever since 6th sample, the run order effects could be ignore.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Use msconvert in Linux or Mac</title>
      <link>https://yufree.cn/en/2019/10/15/use-msconvert-in-linux-or-mac/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2019/10/15/use-msconvert-in-linux-or-mac/</guid>
      <description>&lt;p&gt;One of the major issue for metabolomics data analysis is that user have to use Window to convert vendor data into open source format like mzML or mzXML. Though msConvert could be run under Linux or Mac, it will not support vendor format. It&amp;rsquo;s really annoying to set up virtual machine for data format issue. Another software supporting vendor format is &lt;a href=&#34;https://www.reifycs.com/AbfConverter/index.html&#34;&gt;Abf Converter&lt;/a&gt;, which also only support Windows and their output is for MS-DIAL.&lt;/p&gt;
&lt;p&gt;Recently I noticed that a docker &lt;a href=&#34;https://hub.docker.com/r/chambm/pwiz-skyline-i-agree-to-the-vendor-licenses&#34;&gt;image&lt;/a&gt; is created to covert vendor data by &lt;a href=&#34;http://proteowizard.sourceforge.net/download.html&#34;&gt;ProteoWizard&lt;/a&gt;. It only has 3.9GB, which is much easier to use under Linux. Just pull it from DockerHub:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker pull chambm/pwiz-skyline-i-agree-to-the-vendor-licenses
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The usage is quite clear  You could use the following code to convert all the files in a folder:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker run -it --rm -e WINEDEBUG&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;-all -v /your/data/path/:/data chambm/pwiz-skyline-i-agree-to-the-vendor-licenses wine msconvert /data/*.RAW
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There is small problem for Agilent files: &lt;code&gt;.d&lt;/code&gt; format is actually a folder. If you use previous code, you will get Then a &lt;code&gt;for&lt;/code&gt; trick could solve this issue:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; f in &lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;basename /your/path/*.*&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt;
  docker run -it --rm -e WINEDEBUG&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;-all -v /your/path:/data chambm/pwiz-skyline-i-agree-to-the-vendor-licenses wine msconvert /data/$f
&lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then you will see the converted mzML in the same directory of your raw data folder.&lt;/p&gt;
&lt;p&gt;There are three vendor formats not supporting by this image: ABI T2D, Bruker FID/YEP and Waters UNIFI.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Updates on 2023-04-23&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Errors for Thermos Fisher Raw files&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Try &lt;a href=&#34;https://github.com/compomics/ThermoRawFileParser&#34;&gt;ThermoRawFileParser&lt;/a&gt; and they also shared a docker image:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -i -t -v /home/user/raw:/data_input quay.io/biocontainers/thermorawfileparser:&amp;lt;tag&amp;gt; ThermoRawFileParser.sh --help
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;tag&gt; could be &lt;em&gt;1.4.2&amp;ndash;ha8f3691_0&lt;/em&gt; and you could check here for &lt;a href=&#34;https://quay.io/repository/biocontainers/thermorawfileparser?tab=tags&#34;&gt;details&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Singularity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For msconvert, you need to build the singularity image through Dockerhub:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;singularity pull docker://chambm/pwiz-skyline-i-agree-to-the-vendor-licenses
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then use the following code to convert the file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;singularity exec --cleanenv -B ~:/mywineprefix --writable-tmpfs pwiz-skyline-i-agree-to-the-vendor-licenses_latest.sif mywine msconvert --32 --zlib --filter &amp;quot;peakPicking true 1-&amp;quot; --filter &amp;quot;zeroSamples removeExtra&amp;quot; &#39;test.RAW&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Or you might choose ThermoRawFileParser:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;singularity pull docker://globusgenomics/thermorawfileparser
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then use the following code to convert files in certain folder:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;singularity exec --cleanenv thermorawfileparser_latest.sif ThermoRawFileParser.sh -d PATH_OF_RAW_FILES
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Metabolites network by PMD analysis</title>
      <link>https://yufree.cn/en/2019/07/26/metabolites-network-by-pmd-analysis/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2019/07/26/metabolites-network-by-pmd-analysis/</guid>
      <description>&lt;p&gt;Recently, my colleagues from Chinese Academy of Sciences published a &lt;a href=&#34;https://pubs.acs.org/doi/10.1021/acs.est.9b02122&#34;&gt;paper&lt;/a&gt; about pumpkin TBBPA metabolites. I am one of the co-authors since I wrote the code to screen bronimated compounds. Later I wrote a formal function &lt;a href=&#34;http://yufree.github.io/enviGCMS/reference/findohc.html&#34;&gt;findohc&lt;/a&gt; in enviGCMS package to screen those compounds. The code design is slightly different with the original &lt;a href=&#34;https://pubs.acs.org/doi/abs/10.1021/acs.est.6b03294&#34;&gt;paper&lt;/a&gt; while it works fine. However, today my topic will not be about screen metabolites by mass defect clusters. I want to show you how to build a metabolites network by known PMDs.&lt;/p&gt;
&lt;p&gt;The original question is simple and straightforward: it&amp;rsquo;s much easier to find metabolites for parent compounds while it&amp;rsquo;s not easy to find the metabolites of metabolites. In the recently published paper, my colleague showed the following figure:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/presentation/figure/TBBPAmet.jpeg&#34; alt=&#34;TBBPA metabolites&#34;&gt;&lt;/p&gt;
&lt;p&gt;I agree with this plot while it obviously that some metabolites are from other metabolites instead of parent compounds. For example, TBBPA MG could be the parent compound of TBBPA MMG. It&amp;rsquo;s biologically meaningful to have those connected reactions. Then I suddenly realized my PMD analysis should also worked.&lt;/p&gt;
&lt;p&gt;For pollution metabolism, Phase I and Phase II reactions are always there since both drug and pollution are treated equally as xenobiotic for living system. In this case, we actually know the corresponding PMDs. Here I selected five PMDs: &amp;ldquo;+Br/-H&amp;rdquo; for debromination process, &amp;ldquo;+6C10H5O&amp;rdquo; for Glycosylation, &amp;ldquo;+3C2H3O&amp;rdquo; for Malonylation, &amp;ldquo;+C2H&amp;rdquo; for Methylation and &amp;ldquo;+O&amp;rdquo; for Hydroxylation. I put atom number in front of atom character to make a difference with chemical formula. By writing a function called &lt;a href=&#34;http://yufree.github.io/pmd/reference/getchain.html&#34;&gt;getchain&lt;/a&gt; to recursive search ions with those PMDs relationships in pmd package and filtered by pair-wise correlation coefficients larger than 0.6, I get this figure:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/presentation/figure/TBBPA.png&#34; alt=&#34;TBBPA&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here we could see metabolites network for TBBPA. It actually found lots of intermediate compounds from the raw data. Assuming those five reactions could stand for the major metabolism pathway in pumpkin, they could already produce a network structure. We could find TBBPA is not the center of the metabolism and Glycosylation build a bridge for more metabolites. The red dash line parts included some compounds we missed in the published paper. It&amp;rsquo;s surprising to find some Phase II reaction happened again and again. Since this work is already published, I didn&amp;rsquo;t expect another publication for this work. However, I believe this script would be useful for community. In this case, I included peaks list data from this study in enviGCMS package. Here is the code to reproduce the network analysis for TBBPA metabolites and please cite the papers for &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0003267018313047?via%3Dihub&#34;&gt;pmd&lt;/a&gt; and &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0039914016309298&#34;&gt;enviGCMS&lt;/a&gt; package, as well as the &lt;a href=&#34;https://pubs.acs.org/doi/10.1021/acs.est.9b02122&#34;&gt;TBBPA&lt;/a&gt; study if you use those functions:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-R&#34; data-lang=&#34;R&#34;&gt;remotes&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;install_github&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;yufree/enviGCMS&amp;#39;&lt;/span&gt;)
remotes&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;install_github&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;yufree/pmd&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;install.packages&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;igraph&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;RColorBrewer&amp;#39;&lt;/span&gt;))
&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(enviGCMS)
&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(pmd)
&lt;span style=&#34;color:#75715e&#34;&gt;# load the data&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;data&lt;/span&gt;(TBBPA)
&lt;span style=&#34;color:#75715e&#34;&gt;# extract brominated compounds&lt;/span&gt;
x &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;findohc&lt;/span&gt;(TBBPA)
brcomp &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;getfilter&lt;/span&gt;(x,rowindex &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;mz &lt;span style=&#34;color:#f92672&#34;&gt;%in%&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;ohc&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;mz)
pmd &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; brcomp&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;data,mz &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; brcomp&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;mz, rt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; brcomp&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;rt, group &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; brcomp&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;group)
&lt;span style=&#34;color:#75715e&#34;&gt;# build the network&lt;/span&gt;
df &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;getchain&lt;/span&gt;(pmd,diff &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;77.91&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;162.05&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;86&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;14.02&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;15.99&lt;/span&gt;),&lt;span style=&#34;color:#ae81ff&#34;&gt;542.7446&lt;/span&gt;)
df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;sdac&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;ms1 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;sdac&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;ms1,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;sdac&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;ms2 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;round&lt;/span&gt;(df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;sdac&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;ms2,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(igraph)
&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(RColorBrewer)
pal &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;brewer.pal&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accent&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# only use pairs with correlation coefficience larger than 0.6&lt;/span&gt;
net &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;graph_from_data_frame&lt;/span&gt;(df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;sdac&lt;span style=&#34;color:#a6e22e&#34;&gt;[abs&lt;/span&gt;(df&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;sdac&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;cor)&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.6&lt;/span&gt;,],directed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; F)
&lt;span style=&#34;color:#75715e&#34;&gt;# show the network&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;plot&lt;/span&gt;(net,vertex.size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;,edge.color &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pal&lt;span style=&#34;color:#a6e22e&#34;&gt;[as.numeric&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;as.factor&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;E&lt;/span&gt;(net)&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;diff2))],edge.width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;legend&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topright&amp;#34;&lt;/span&gt;,bty &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;n&amp;#34;&lt;/span&gt;, 
       legend&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;+Br/-H&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;+6C10H5O(Glycosylation)&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;+3C2H3O(Malonylation)&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;+C2H(Methylation)&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;+O(Hydroxylation)&amp;#39;&lt;/span&gt;),
       fill&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;unique&lt;/span&gt;(pal&lt;span style=&#34;color:#a6e22e&#34;&gt;[as.numeric&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;as.factor&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;E&lt;/span&gt;(net)&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;diff2))]), border&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;NA&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Besides, I will attend American Chemical Society Fall 2019 National Meeting &amp;amp; Expo in San Diego, CA next month and give a oral presentation about structure/reaction directed analysis for environmental studies. If you will be there and want to do some &lt;del&gt;stupid&lt;/del&gt; cool things for untarget analysis, feel free to contact me.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quantum effect for chemical reaction</title>
      <link>https://yufree.cn/en/2019/03/10/quantum-effect-for-paired-mass-distances/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2019/03/10/quantum-effect-for-paired-mass-distances/</guid>
      <description>&lt;p&gt;Due to mass defect, the high resolution mass spectrum could be used for qualitative analysis. However, I am curious about the distribution of the decimal part of real compounds&#39; exact mass. If compounds are generated randomly, the decimal part of exact mass should be uniform distributed between 0 and 1. However, we all know compounds should follow chemical property to form new compounds. In this case, let&amp;rsquo;s see what happened for certain compounds database.&lt;/p&gt;
&lt;p&gt;Again, we use HMDB to make a demo. This is the decimal part of real compounds&#39; exact mass for 13251 unique exact mass in HMDB.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2019-03-10-quantum-effect-for-paired-mass-distances_files/hmdbmzr.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As you could see, the distribution is not uniform distribution. However, it&amp;rsquo;s kind of continuous between 0 and 1. Then I wonder what happened if I checked the paired mass distances(PMD) among those unique exact mass and check the distribution of the decimal part of PMD. After one cup of tea, my 2014 laptop give me this result:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2019-03-10-quantum-effect-for-paired-mass-distances_files/hmdbpmd.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Well. Compared with compounds, pmd&amp;rsquo;s distribution show a much clear pattern with incontinuity around 0.7. It seems that pmd could be used to check if certain reactions are real or unknown. Furthermore, pmd could stand for chemical reaction, such distribution imply a selection of reaction.&lt;/p&gt;
&lt;p&gt;Well, let&amp;rsquo;s zoom in the distribution between 0 and 0.1:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2019-03-10-quantum-effect-for-paired-mass-distances_files/comp.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Hmmmm&amp;hellip;it seems kind of quantum effect could be found in both compound and reaction level. Such quantum effect might be used for qualitative analysis or outlier detection.&lt;/p&gt;
&lt;p&gt;I hope you could still follow this interesting phenomenon. Anyway, if not, I will attend ASMS 2019 this summer. Unfortunately I didn&amp;rsquo;t get the opportunity for oral presentation but poster presentation.&lt;/p&gt;
&lt;p&gt;My topic would be &lt;strong&gt;Reacomics for LC-MS based untargeted analysis&lt;/strong&gt;. I will show more interesting results there.&lt;/p&gt;
&lt;p&gt;See you in Atlanta!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Biomarker compounds? reactions!</title>
      <link>https://yufree.cn/en/2018/12/11/biomarker-compounds-reactions/</link>
      <pubDate>Tue, 11 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2018/12/11/biomarker-compounds-reactions/</guid>
      <description>&lt;p&gt;When we talk about biomarker in metabolomics, most of people refer to certain compound. When this &amp;lsquo;magic&amp;rsquo; compound show different concentrations between control and treatment group, we could find some evidence of diseases or disorders. In a regular workflow, chemist detect compounds from mass spectrumetry peaks and biologist found the the connections between compounds. That is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;peaks -&amp;gt; compounds -&amp;gt; connections among compounds&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Analytical chemist spent too much energy to make annotations for peaks, pack them into a compound list and send the list to biologist. Interesting, biologist would also spend a lot of time to explain why those compounds could appear at the same time. Well, let&amp;rsquo;s have a rest and visit the museum.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/7/7d/A_Sunday_on_La_Grande_Jatte%2C_Georges_Seurat%2C_1884.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;How to explain a painting? I am not an artist. However, if I see a painting above, I am not really care about what is the name of all the person in the scene. I do care the relationship among those people. You could see couples, friends, family members &amp;hellip; Wait a moment, how could you find those relationships?&lt;/p&gt;
&lt;p&gt;The answer is distance. People could be grouped by paired distances. Similar thing happened in metabolomics: when we collect peaks from mass spectrumetry, we actually capture a snapshot of all compounds with their paired chemical reaction relationship. For example, when we see two compounds show a mass distance of 16, those two compounds might be involved in xenbiotics Phase I reactions such as hydrolysis, reduction and oxidation. If a lot of paired compounds show this relationship, we could use the sum of their intensity to stand for certain reactions for further discussion.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/presentation/figure/srda.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, mass spectrum was used as a tool to measure reactions instead of compounds. If certain paired mass distances show a high frequency in one data set, we might treat those bunch of compounds as representative of certain reactions. Then we could go back to check what happened of those &amp;lsquo;reactions&amp;rsquo; between different groups. Yes, here we could skip the annotation and directly make quantitative analysis at reactions level. This is concept of biomaker reactions.&lt;/p&gt;
&lt;p&gt;Actually I just published this concept as reaction directed analysis on &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0003267018313047&#34;&gt;ACA&lt;/a&gt;. Last week, the corresponding R package pmd was on &lt;a href=&#34;https://cran.r-project.org/web/packages/pmd/index.html&#34;&gt;CRAN&lt;/a&gt;. I put demo data along with a shiny application in this package. Also I wrote a &lt;a href=&#34;https://yufree.github.io/pmd/&#34;&gt;website&lt;/a&gt; to show the &lt;a href=&#34;https://yufree.github.io/pmd/articles/globalstd.html&#34;&gt;usage&lt;/a&gt; of this package. You could connect this analysis with xcms package.&lt;/p&gt;
&lt;p&gt;To perform the reaction directed analysis, two step is necessary. The first step is find independent peaks from the peaks list by removing adducts, isotopes and neutral loss. In pmd package, I developed &lt;code&gt;GlobalStd&lt;/code&gt; algorithm to rule out those peaks in an untargeted way without pre-defined list. The second step is perform reaction directed analysis based on paired mass distance based on frequency analysis. This package is still under active development and more features would come soon.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/presentation/figure/toc.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Next time when you consider biomarker, recall the painting and reactions. Then use your mass spectrometry as reaction discoverer.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Peaks -&amp;gt; &lt;del&gt;Compounds&lt;/del&gt; -&amp;gt; Relationship among compounds&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Online tools for chemical research</title>
      <link>https://yufree.cn/en/2018/10/14/online-tools-for-chemical-research/</link>
      <pubDate>Sun, 14 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2018/10/14/online-tools-for-chemical-research/</guid>
      <description>&lt;p&gt;I am tired of searching compounds online. So far, more than 140 Millons substances have been registered for CAS registry number. However, it doesn&amp;rsquo;t mean CAS has covered all the compounds. In fact, at least three identifiers have been used to let someone find the compounds&#39; information online.&lt;/p&gt;
&lt;p&gt;The first one is CAS registry number. This number is quiet useful to buy standards. However, it&amp;rsquo;s not free to get access to the full list of CAS registry number. Furthormore, this number is for substance, which is different from compounds. For example,  &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4702940/&#34;&gt;PubChem&lt;/a&gt; show the SID and CID for the same compounds. Since substances&#39; names or numbers have been used for a long time, some mistakes might happen when we could not tell apart different compounds from the same substances.&lt;/p&gt;
&lt;p&gt;The second identifier is Simplified Molecular-Input Line-Entry System (SMILES). This identifier could contain the structure information. However, since the standards are not well established, one compound could be linked to multiple SMILES. Such property has been improved by so-called canonical SMILES. However, this identifier still could not be trusted when you want to use them to get other identifier. SMILES is good to generate the molecular structure. However, we all knew 2D structure could not reflact the 3D structure and some software like MOPAC could be used to find the best structure if you want to do some Quantitative structure–activity relationship based prediction.&lt;/p&gt;
&lt;p&gt;The third one is from IUPAC. Of course, it&amp;rsquo;s not IUPAC name because the name has same issue like SMILES. However, IUPAC has trying to generate InChI for compounds with structure information. They also developed InChIKey by hashes on InChI. InChIKey didn&amp;rsquo;t cover all of the chemical substances and user could generate the InChIKey themselves when they have InChI. So the problem is that if such key could be generated locally, some mistakes or missing of layers in InChI would still give different keys to the same compounds.&lt;/p&gt;
&lt;p&gt;Those three indentifers are not perfect. This is the major issue when you use convertor for them such as &lt;a href=&#34;http://cts.fiehnlab.ucdavis.edu/batch&#34;&gt;Chemical Translation Services from UCDavis&lt;/a&gt;. You could always find multiple hits from single entry. I know there are lots of indentifers such as PubChem CID and ChemSpider ID. However, they all have own coverages. Some conversions have no U turn.&lt;/p&gt;
&lt;p&gt;Anyway this post is not about those issues. I just want to list some databases which might be helpful for researchers doing non-targeted annotation analysis.&lt;/p&gt;
&lt;h2 id=&#34;compounds-database&#34;&gt;Compounds database&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://pubchem.ncbi.nlm.nih.gov/&#34;&gt;PubChem&lt;/a&gt; is an open chemistry database at the National Institutes of Health (NIH).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.chemspider.com/&#34;&gt;Chemspider&lt;/a&gt; is a free chemical structure database providing fast text and structure search access to over 67 million structures from hundreds of data sources.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ebi.ac.uk/chebi/&#34;&gt;ChEBI&lt;/a&gt; is a freely available dictionary of molecular entities focused on ‘small’ chemical compounds.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cas.org/support/documentation/chemical-substances/faqs&#34;&gt;CAS numbers&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.echemportal.org/echemportal/substancesearch/substancesearchlink.action&#34;&gt;eChemPortal&lt;/a&gt;  provides free public access to information on properties of chemicals. You could find physical chemical Properties, ecotoxicity, environmental fate and behaviour
toxicity for certain compounds&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;compounds-database-on-certain-topic&#34;&gt;Compounds database on certain topic&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.metabolomicsworkbench.org/databases/refmet/index.php&#34;&gt;RefMet&lt;/a&gt; A Reference list of Metabolite names.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://comptox.epa.gov/dashboard&#34;&gt;CompTox&lt;/a&gt; compounds, exposure and toxicity database. &lt;a href=&#34;https://www.epa.gov/chemical-research/downloadable-computational-toxicology-data&#34;&gt;Here&lt;/a&gt; is related data. This is for environmental study.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.t3db.ca/&#34;&gt;T3DB&lt;/a&gt; is a unique bioinformatics resource that combines detailed toxin data with comprehensive toxin target information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.hmdb.ca/&#34;&gt;HMDB&lt;/a&gt; is a freely available electronic database containing detailed information about small molecule metabolites found in the human body.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.lipidmaps.org/&#34;&gt;Lipid Maps&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://jcggdb.jp/rcmg/glycodb/Ms_ResultSearch&#34;&gt;GMDB&lt;/a&gt; a multistage tandem mass spectral database using a variety of structurally defined glycans.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.genome.jp/kegg/compound/&#34;&gt;KEGG&lt;/a&gt; is a collection of small molecules, biopolymers, and other chemical substances that are relevant to biological systems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.drugbank.ca/releases/latest&#34;&gt;Drugbank&lt;/a&gt; is a unique bioinformatics and cheminformatics resource that combines detailed drug data with comprehensive drug target information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://foodb.ca/&#34;&gt;FooDB&lt;/a&gt; is the world’s largest and most comprehensive resource on food constituents, chemistry and biology.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://lmdb.ca&#34;&gt;LMDB&lt;/a&gt; is a freely available electronic database containing detailed information about small molecule metabolites found in different livestock species.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://zinc15.docking.org/&#34;&gt;Zinc&lt;/a&gt; is a free database of commercially-available compounds for virtual screening. You could use this database to search commercial information for certain compound&amp;rsquo;s standards.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://mastersearch.chemexper.com/&#34;&gt;chemexper&lt;/a&gt; is a commercial website for commercially-available compounds.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ms-database-with-annotation&#34;&gt;MS Database with annotation&lt;/h2&gt;
&lt;h3 id=&#34;msms&#34;&gt;MS/MS&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://mona.fiehnlab.ucdavis.edu/&#34;&gt;MoNA&lt;/a&gt; Platform to collect all other open source database&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.massbank.jp/?lang=en&#34;&gt;MassBank&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://gnps.ucsd.edu/ProteoSAFe/static/gnps-splash.jsp&#34;&gt;GNPS&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://spectra.psc.riken.jp/&#34;&gt;ReSpect&lt;/a&gt;: phytochemicals&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://metlin.scripps.edu/&#34;&gt;Metlin&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://fiehnlab.ucdavis.edu/projects/LipidBlast&#34;&gt;LipidBlast&lt;/a&gt;: in silico prediction&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.mzcloud.org/&#34;&gt;MZcloud&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nist.gov/srd/nist-standard-reference-database-1a-v17&#34;&gt;NIST&lt;/a&gt;: Not free&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ms&#34;&gt;MS&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://fiehnlab.ucdavis.edu/projects/binbase-setup&#34;&gt;Fiehn Lab&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nist.gov/srd/nist-standard-reference-database-1a-v17&#34;&gt;NIST&lt;/a&gt;: No free&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;online-tools&#34;&gt;Online tools&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://molview.org&#34;&gt;Molview&lt;/a&gt; to show a 3D structure from 2D structure. The 3D structure source is from &lt;a href=&#34;https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-3-4&#34;&gt;PubChem&lt;/a&gt; or &lt;a href=&#34;https://www.mn-am.com/products/corina&#34;&gt;Corina&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://chimpsky.uwaterloo.ca/mol2chemfig/&#34;&gt;mol2chemfig&lt;/a&gt; could convert the smiles string into a figure using in LaTeX.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://molgen.de&#34;&gt;molgen&lt;/a&gt; generating all structures (connectivity isomers, constitutions) that correspond to a given molecular formula, with optional further restrictions, e.g. presence or absence of particular substructures.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.moleculardescriptors.eu/resources/resources.htm&#34;&gt;QSPR MD tools list&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.envipat.eawag.ch/index.php&#34;&gt;Isotope&lt;/a&gt; pattern prediction&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.chemcalc.org/mf_finder/mfFinder_em_new&#34;&gt;mfFinder&lt;/a&gt; predict formula based on accurate mass&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will appreciate it if you could comment with other useful while available online tools related to non-targeted analysis in this post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Metabolomics data analysis workshop slides @ UWaterloo</title>
      <link>https://yufree.cn/en/2018/07/13/metabolomics-data-analysis-workshop-slides-uwaterloo/</link>
      <pubDate>Fri, 13 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2018/07/13/metabolomics-data-analysis-workshop-slides-uwaterloo/</guid>
      <description>&lt;p&gt;I am leaving Prof. Pawliszyn&amp;rsquo;s research group for Icahn School of Medicine at Mount Sinai(ISMMS) in September. During the past two week, I made 171 slides for a workshop about metabolomics data analsysis for our group in University of Waterloo. One target is training new group member and another target is to set up a reproducible data analysis system to facilitate further data analysis.&lt;/p&gt;
&lt;p&gt;I divided this workshop into five different parts: the first part is about a general introduction about metabolomics; the second part is related to statistical analysis in metabolomics; the third part is focused on batch effects; the fourth part is annotation and the last part is a demo for the whole workflow. I made the presentation in five days and lasted for about 7 hours in total. Now I realized the presentation was a physical labor when you had to talk and answer questions in more than one hour at dinner-time.&lt;/p&gt;
&lt;p&gt;As I promised, here is the slides for the workshop:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://yufree.github.io/presentation/metabolomics/introduction#1&#34;&gt;Introduction&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://yufree.github.io/presentation/metabolomics/StatisticalAnalysis#1&#34;&gt;Statistical Analysis&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://yufree.github.io/presentation/metabolomics/BatchCorrection#1&#34;&gt;Batch Correction&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://yufree.github.io/presentation/metabolomics/Annotation#1&#34;&gt;Annotation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://yufree.github.io/presentation/metabolomics/demo#1&#34;&gt;Demo&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Those slides are wroten by &lt;a href=&#34;https://github.com/yihui/xaringan&#34;&gt;xaringan&lt;/a&gt;, which means they support full screen (press f) and notes (press p). The source file could be found &lt;a href=&#34;https://github.com/yufree/presentation&#34;&gt;here&lt;/a&gt; under subfolder &amp;lsquo;metabolomics&amp;rsquo;. You might find amusing images or GIF at the beginning of each slides and I found such way could really help to relax both the reporter and the audiences (I stole this idea and some GIF from &lt;a href=&#34;https://yihui.name/en/2017/05/gifs/&#34;&gt;Yihui&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Since it&amp;rsquo;s already public availiable, you could also use them as your wishes. No Guarantee. I will appreciate if you could send some feedbacks or enhencements for those slides. Those slides would be updated with &lt;a href=&#34;https://yufree.cn/metaworkflow/&#34;&gt;Meta-Workflow&lt;/a&gt; which you could also comment on the source &lt;a href=&#34;https://github.com/yufree/metaworkflow&#34;&gt;file&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Enjoy your Friday 13rd!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Play with HMDB datasets: Part II</title>
      <link>https://yufree.cn/en/2018/04/22/play-with-hmdb-datasets-part-ii/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2018/04/22/play-with-hmdb-datasets-part-ii/</guid>
      <description>&lt;p&gt;Our HMDB data have 114066 metabolites with 13 properties such as hmdb ID, monisotopic_molecular_weight, iupac_name, name, chemical_formula, cas_registry_number, smiles, kingdom, direct_parent, super_class, class, sub_class, molecular_framework. Let&amp;rsquo;s make some explore analysis:&lt;/p&gt;
&lt;h2 id=&#34;ratio-between-monisotopic-molecular-weight-and-smiles&#34;&gt;Ratio between monisotopic molecular weight and smiles&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(tidyverse)
hmdb &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span style=&#34;color:#a6e22e&#34;&gt;summarise_all&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;funs&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;n_distinct&lt;/span&gt;(.)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 13
##   accession monisotopic_molecular_weig… iupac_name   name chemical_formula
##       &amp;lt;int&amp;gt;                       &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;            &amp;lt;int&amp;gt;
## 1    114066                       13429     113720 114066            11762
## # ... with 8 more variables: cas_registry_number &amp;lt;int&amp;gt;, smiles &amp;lt;int&amp;gt;,
## #   kingdom &amp;lt;int&amp;gt;, direct_parent &amp;lt;int&amp;gt;, super_class &amp;lt;int&amp;gt;, class &amp;lt;int&amp;gt;,
## #   sub_class &amp;lt;int&amp;gt;, molecular_framework &amp;lt;int&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We have 13429 monisotopic molecular weight, 11762 chemical formula, 113900 smiles among the 114066 metabolites, which means for each monisotopic molecular weight, you could find ~8.5 metabolites. Different smiles means different structures and this is the main reason why we need MS/MS data. However, we could check the ratio in details.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(tidyverse)
hmdb &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span style=&#34;color:#a6e22e&#34;&gt;group_by&lt;/span&gt;(super_class) &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span style=&#34;color:#a6e22e&#34;&gt;summarize&lt;/span&gt;(ratio &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;unique&lt;/span&gt;(monisotopic_molecular_weight))&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;unique&lt;/span&gt;(smiles))) &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span style=&#34;color:#a6e22e&#34;&gt;ungroup&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;## # A tibble: 28 x 2
##    super_class                                ratio
##    &amp;lt;chr&amp;gt;                                      &amp;lt;dbl&amp;gt;
##  1 Acetylides                                1.00  
##  2 Alkaloids and derivatives                 0.761 
##  3 Benzenoids                                0.572 
##  4 Homogeneous metal compounds               0.980 
##  5 Homogeneous non-metal compounds           0.972 
##  6 Hydrocarbon derivatives                   0.800 
##  7 Hydrocarbons                              0.456 
##  8 Inorganic compounds                       1.00  
##  9 Lignans, neolignans and related compounds 0.669 
## 10 Lipids and lipid-like molecules           0.0604
## # ... with 18 more rows
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Well, it&amp;rsquo;s not a uniform distribution. For lipid, the ratio is about 0.06 and MS/MS analysis is required. However, for acetylides, homogeneous metal compounds, homogeneous non-metal compounds, organic polymers, organonitrogen compounds, organohalogen compounds, organic nitrogen compounds, hydrocarbon derivatives, inorganic compounds, mixed metal/non-metal compounds, organic compounds, organic salts, organophosphorus compounds, and organometallic compounds, the ratio is above 0.8. I think it&amp;rsquo;s fine to use MS data to make annotation for those compounds. For other groups with ratio range from 0.23 to 0.8, it&amp;rsquo;s hard to say. Basically, if you removed lipid from your samples in the pretreatment, it&amp;rsquo;s fine to use MS data to check the data. However, when you find lipid, take care and make confirmation by MS/MS.&lt;/p&gt;
&lt;h2 id=&#34;mass-distribution&#34;&gt;Mass distribution&lt;/h2&gt;
&lt;p&gt;The mass distribution of all the metabolites is quite different from comment mass spectrum database.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;hist&lt;/span&gt;(hmdb&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;monisotopic_molecular_weight,breaks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.cn/images/unnamed-chunk-6-1.png&#34; alt=&#34;plot of chunk unnamed-chunk-6&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;hist&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;unique&lt;/span&gt;(hmdb&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;monisotopic_molecular_weight,breaks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.cn/images/unnamed-chunk-6-2.png&#34; alt=&#34;plot of chunk unnamed-chunk-6&#34;&gt;&lt;/p&gt;
&lt;p&gt;However, we could separate the data set into lipid group and other compounds and check again.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;lipid &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; hmdb[hmdb&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;super_class &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Lipids and lipid-like molecules&amp;#39;&lt;/span&gt;,]
other &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; hmdb[&lt;span style=&#34;color:#f92672&#34;&gt;!&lt;/span&gt;hmdb&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;super_class &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Lipids and lipid-like molecules&amp;#39;&lt;/span&gt;,]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;91391 compounds are classified into Lipids and lipid-like molecules and dominate 80% of the metabolites in HMDB database. Their mass distribution is here:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;hist&lt;/span&gt;(lipid&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;monisotopic_molecular_weight,breaks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.cn/images/unnamed-chunk-8-1.png&#34; alt=&#34;plot of chunk unnamed-chunk-8&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;hist&lt;/span&gt;(other&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;monisotopic_molecular_weight,breaks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.cn/images/unnamed-chunk-8-2.png&#34; alt=&#34;plot of chunk unnamed-chunk-8&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;hist&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;unique&lt;/span&gt;(lipid&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;monisotopic_molecular_weight),breaks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.cn/images/unnamed-chunk-8-3.png&#34; alt=&#34;plot of chunk unnamed-chunk-8&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;hist&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;unique&lt;/span&gt;(other&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;monisotopic_molecular_weight),breaks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.cn/images/unnamed-chunk-8-4.png&#34; alt=&#34;plot of chunk unnamed-chunk-8&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, I have to say the 0-500 are mainly non-lipid metabolites while 500-1000 are dominated by lipids.&lt;/p&gt;
&lt;h2 id=&#34;mass-defect-analysis&#34;&gt;Mass defect analysis&lt;/h2&gt;
&lt;p&gt;For lipid compounds, we expected a mass defect with certain pattern. Here we use -CH2- as an example.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# lipid&lt;/span&gt;
mdalipid &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; enviGCMS&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getmassdefect&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;unique&lt;/span&gt;(lipid&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;monisotopic_molecular_weight)&lt;span style=&#34;color:#a6e22e&#34;&gt;[unique&lt;/span&gt;(lipid&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;monisotopic_molecular_weight)&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;0.9988&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.cn/images/unnamed-chunk-9-1.png&#34; alt=&#34;plot of chunk unnamed-chunk-9&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# other&lt;/span&gt;
mdaother &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; enviGCMS&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getmassdefect&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;unique&lt;/span&gt;(other&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;monisotopic_molecular_weight)&lt;span style=&#34;color:#a6e22e&#34;&gt;[unique&lt;/span&gt;(other&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;monisotopic_molecular_weight)&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;0.9988&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.cn/images/unnamed-chunk-9-2.png&#34; alt=&#34;plot of chunk unnamed-chunk-9&#34;&gt;&lt;/p&gt;
&lt;p&gt;We could find the distribution of mass defect and m/z show different profiles between lipid metabolites and other metabolites. We could use more units to build a model between compounds&#39; class and multiple mass defects. In this case, we might spread one-dimension of data(m/z) into multiple divisions. Then a simple machine learning could give us the answers. I think such way would be a better option compared with build models with thousands of molecular descriptors. When you detected your signal, you know nothing about the structures and such QSPR model would only be useful when you have few candidates.&lt;/p&gt;
&lt;h2 id=&#34;home-message&#34;&gt;Home message&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If you don&amp;rsquo;t perform lipidomics, MS would be enough for annotation. Of course, you still need high resolution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;By using multiple mass defect analysis, we might build a model to class unknown compounds.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Play with HMDB datasets: Part I</title>
      <link>https://yufree.cn/en/2018/04/11/play-with-hmdb-datasets/</link>
      <pubDate>Wed, 11 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2018/04/11/play-with-hmdb-datasets/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The Human Metabolome Database is always used to make annotation for metabolomics studies. However, I am not sure which version of hmdb used in some packages like xMSannotator or MAIT. Then I suddenly realized I could directly grab the most updated version from their website and make annotation with those data. When you have a hammer, you always want to hit a nail.&lt;/p&gt;
&lt;p&gt;The most updated &lt;a href=&#34;http://www.hmdb.ca/downloads&#34;&gt;version&lt;/a&gt; is from 2018-04-10 in xml formats. Well, since I deal with the rss xml in the text mining &lt;a href=&#34;https://yufree.cn/en/2017/07/07/text-mining/&#34;&gt;post&lt;/a&gt;, I think such file is not a big deal. However, when I de-compressed the file, I found the size of the raw singal xml file is almost 3G, similar to the whole human genomics. However, I checked and found lots of the information for each compounds are references and I think data clean should be down.&lt;/p&gt;
&lt;p&gt;My Macbook Pro is a begger version with 8GB memory and 3G would make it impossible for R to process the data. Actually I try R and the procudure would run forever. Here is the R code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(xml2)
library(stringr)
hmdb &amp;lt;- read_xml(&amp;#39;hmdb_metabolites.xml&amp;#39;)%&amp;gt;%
as_list()
getremove &amp;lt;- function(list){
  list$synthesis_reference &amp;lt;- NULL
  list$general_references &amp;lt;- NULL
  list$version &amp;lt;- NULL
  list$creation_date &amp;lt;- NULL
  list$update_date &amp;lt;- NULL
  list$secondary_accessions &amp;lt;- NULL
  list$description &amp;lt;- NULL
  list$synonyms &amp;lt;- NULL
  list$traditional_iupac &amp;lt;- NULL
  list$spectra &amp;lt;- NULL
  list$predicted_properties &amp;lt;- NULL
  list$taxonomy$alternative_parents &amp;lt;- NULL
  list$taxonomy$substituents &amp;lt;- NULL
  list$taxonomy$description&amp;lt;- NULL
  list$normal_concentrations &amp;lt;- NULL
  list$abnormal_concentrations &amp;lt;- NULL
  list$diseases &amp;lt;- NULL
  list$ontology &amp;lt;- NULL
  list$direct_parent &amp;lt;- list$taxonomy$direct_parent
  list$kingdom &amp;lt;- list$taxonomy$kingdom
  list$super_class &amp;lt;- list$taxonomy$super_class
  list$class &amp;lt;- list$taxonomy$class
  list$sub_class &amp;lt;- list$taxonomy$sub_class
  list$molecular_framework &amp;lt;- list$taxomomy$molecular_framework
  list$taxonomy &amp;lt;- NULL
  list$protein_associations &amp;lt;- NULL
  list$biofluid_locations &amp;lt;- NULL
  list$pathways &amp;lt;- NULL
  list$tissue_locations &amp;lt;- NULL
  list$experimental_properties &amp;lt;- NULL
  return(list)
}
subhmdb &amp;lt;- lapply(hmdb,getremove)
id = unlist(sapply(subhmdb, &amp;quot;[[&amp;quot;, &amp;quot;accession&amp;quot;))
name = unlist(sapply(subhmdb, &amp;quot;[[&amp;quot;, &amp;quot;name&amp;quot;))
hmdbclass = sapply(subhmdb, &amp;quot;[[&amp;quot;, &amp;quot;class&amp;quot;)
hmdbclass2 = sapply(hmdbclass, unlist)
class &amp;lt;- as.character(hmdbclass2)
hmdbsubclass = sapply(subhmdb, &amp;quot;[[&amp;quot;, &amp;quot;sub_class&amp;quot;)
hmdbsubclass2 = sapply(hmdbsubclass, unlist)
subclass &amp;lt;- as.character(hmdbsubclass2)
hmdbMW = sapply(subhmdb, &amp;quot;[[&amp;quot;, &amp;quot;monisotopic_molecular_weight&amp;quot;)
hmdbMW2 = sapply(hmdbMW, unlist)
MW &amp;lt;- as.numeric(as.character(hmdbMW2))
table &amp;lt;- cbind.data.frame(id = id,name = name,class = class,subclass = subclass, MW = MW)
write.csv(table,file = &amp;#39;hmdb.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I thought I could use &lt;code&gt;bash&lt;/code&gt; to process such xml. Although I finally get the files via &lt;code&gt;xmlstarlet&lt;/code&gt;, I think the waiting time is too long.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;xmlstarlet sel -N hmdb=http://www.hmdb.ca -T -t -m //hmdb:metabolite -v &amp;quot;concat(//hmdb:metabolite//hmdb:accession,&amp;#39;,&amp;#39;,//hmdb:metabolite//hmdb:monisotopic_molecular_weight,&amp;#39;,&amp;#39;,//hmdb:metabolite//hmdb:iupac_name,&amp;#39;,&amp;#39;,//hmdb:metabolite//hmdb:name,&amp;#39;,&amp;#39;,//hmdb:metabolite//hmdb:chemical_formula,&amp;#39;,&amp;#39;,//hmdb:metabolite//hmdb:cas_registry_number,&amp;#39;,&amp;#39;,//hmdb:metabolite//hmdb:smiles,&amp;#39;,&amp;#39;,//hmdb:metabolite//hmdb:kingdom,&amp;#39;,&amp;#39;,//hmdb:metabolite//hmdb:direct_parent,&amp;#39;,&amp;#39;,//hmdb:metabolite//hmdb:taxonomy//hmdb:super_class,&amp;#39;,&amp;#39;,//hmdb:metabolite//hmdb:taxonomy//hmdb:class,&amp;#39;,&amp;#39;,//hmdb:metabolite//hmdb:taxonomy//hmdb:sub_class, &amp;#39;,&amp;#39;,//hmdb:metabolite//hmdb:taxonomy//hmdb:molecular_framework)&amp;quot; -n hmdb_metabolites.xml &amp;gt; hmdb.csv&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I think it’s time for &lt;code&gt;python&lt;/code&gt;. I didn’t use python too much. After struglling with some issues between &lt;code&gt;python 2&lt;/code&gt; and &lt;code&gt;python 3&lt;/code&gt; , finally I make a dirty code as follows:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# delete the xmlns=&amp;quot;http://www.hmdb.ca&amp;quot; in the second line of xml space
from lxml import etree
import csv
xml = &amp;#39;hmdb.xml&amp;#39;

context = etree.iterparse(xml, tag=&amp;#39;metabolite&amp;#39;)

csvfile = open(&amp;#39;hmdb.csv&amp;#39;, &amp;#39;w&amp;#39;)
fieldnames = [&amp;#39;accession&amp;#39;, &amp;#39;monisotopic_molecular_weight&amp;#39;, &amp;#39;iupac_name&amp;#39;, &amp;#39;name&amp;#39;, &amp;#39;chemical_formula&amp;#39;, &amp;#39;cas_registry_number&amp;#39;, &amp;#39;smiles&amp;#39;, &amp;#39;kingdom&amp;#39;, &amp;#39;direct_parent&amp;#39;, &amp;#39;super_class&amp;#39;, &amp;#39;class&amp;#39;, &amp;#39;sub_class&amp;#39;, &amp;#39;molecular_framework&amp;#39;]
writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
writer.writeheader()

for event, elem in context:

    accession = elem.xpath(&amp;#39;accession/text()&amp;#39;)[0]
    try:
        monisotopic_molecular_weight = elem.xpath(&amp;#39;monisotopic_molecular_weight/text()&amp;#39;)[0]
    except:
        monisotopic_molecular_weight = &amp;#39;NA&amp;#39;
    try:
        iupac_name = elem.xpath(&amp;#39;iupac_name/text()&amp;#39;)[0].encode(&amp;#39;utf-8&amp;#39;)
    except:
        iupac_name = &amp;#39;NA&amp;#39;
    name = elem.xpath(&amp;#39;name/text()&amp;#39;)[0].encode(&amp;#39;utf-8&amp;#39;)
    try:
        chemical_formula = elem.xpath(&amp;#39;chemical_formula/text()&amp;#39;)[0]
    except:
        chemical_formula = &amp;#39;NA&amp;#39;

    try:
        cas_registry_number = elem.xpath(&amp;#39;cas_registry_number/text()&amp;#39;)[0]
    except:
        cas_registry_number = &amp;#39;NA&amp;#39;
    try:
        smiles = elem.xpath(&amp;#39;smiles/text()&amp;#39;)[0]
    except:
        smiles = &amp;#39;NA&amp;#39;
    try:
        kingdom = elem.xpath(&amp;#39;taxonomy/kingdom/text()&amp;#39;)[0]
    except:
        kingdom = &amp;#39;NA&amp;#39;
    try:
        direct_parent = elem.xpath(&amp;#39;taxonomy/direct_parent/text()&amp;#39;)[0]
    except:
        direct_parent = &amp;#39;NA&amp;#39;
    try:
        super_class = elem.xpath(&amp;#39;taxonomy/super_class/text()&amp;#39;)[0]
    except:
        super_class = &amp;#39;NA&amp;#39;
    try:
        classorg = elem.xpath(&amp;#39;taxonomy/class/text()&amp;#39;)[0]
    except:
        classorg = &amp;#39;NA&amp;#39;
    try:
        sub_class = elem.xpath(&amp;#39;taxonomy/sub_class/text()&amp;#39;)[0]
    except:
        sub_class = &amp;#39;NA&amp;#39;
    try:
        molecular_framework = elem.xpath(&amp;#39;taxonomy/molecular_framework/text()&amp;#39;)[0]
    except:
        molecular_framework = &amp;#39;NA&amp;#39;

    writer.writerow({&amp;#39;accession&amp;#39;: accession, &amp;#39;monisotopic_molecular_weight&amp;#39;: monisotopic_molecular_weight, &amp;#39;iupac_name&amp;#39;: iupac_name, &amp;#39;name&amp;#39;: name, &amp;#39;chemical_formula&amp;#39;: chemical_formula, &amp;#39;cas_registry_number&amp;#39;: cas_registry_number, &amp;#39;smiles&amp;#39;: smiles, &amp;#39;kingdom&amp;#39;: kingdom, &amp;#39;direct_parent&amp;#39;: direct_parent, &amp;#39;super_class&amp;#39;: super_class, &amp;#39;class&amp;#39;: classorg, &amp;#39;sub_class&amp;#39;: sub_class, &amp;#39;molecular_framework&amp;#39;: molecular_framework})
    # It&amp;#39;s safe to call clear() here because no descendants will be
    # accessed
    elem.clear()
# Also eliminate now-empty references from the root node to elem
    for ancestor in elem.xpath(&amp;#39;ancestor-or-self::*&amp;#39;):
        while ancestor.getprevious() is not None:
            del ancestor.getparent()[0]
del context&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After 134.2s, it’s done. The size of csv file is around 50.5MB.&lt;/p&gt;
&lt;p&gt;I will not show the csv files because if you really want the results, you’d better to run this code on the furture version of HMDB. If you don’t know how to use python, just learn it! If you already know one programming language, you could learn another high-level language in few minutes to run a script.&lt;/p&gt;
&lt;p&gt;I have to mention that the devolopers of MS-DIAL always released an updated MS/MS spectrum database &lt;a href=&#34;http://prime.psc.riken.jp/Metabolomics_Software/MS-DIAL/index.html&#34;&gt;online&lt;/a&gt;. I wonder how many people spend time on those data. If you could find the source of the raw data, run them from the very beginning and you would know how complex to produce a realiable database.&lt;/p&gt;
&lt;p&gt;OK, part I only cover the data clean part. I might show some data mining for those data in next posts. The take home message is that tools would not change the results while they could change the time to get the results. Learn something new and it would help you someday.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Downverse workflow for research</title>
      <link>https://yufree.cn/en/2018/03/13/workflow-for-r-based-research-environment/</link>
      <pubDate>Tue, 13 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2018/03/13/workflow-for-r-based-research-environment/</guid>
      <description>&lt;p&gt;Firstly, I really appreciate Yihui&amp;rsquo;s &lt;a href=&#34;https://yihui.name/en/2018/03/miao-yu-postdoc/&#34;&gt;post&lt;/a&gt; about me.&lt;/p&gt;
&lt;p&gt;Today I want to share some tips with people in academia about how to reduce repeated works in research. You might treat this as an extension for &lt;a href=&#34;http://jtleek.com/&#34;&gt;Jeek Leek&lt;/a&gt;&amp;rsquo;s excellent book: &lt;a href=&#34;https://leanpub.com/modernscientist&#34;&gt;How to be a mordern scientist&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;Which kind of works could be treated as repeated works in research? A lot! However, unless you experiments are all &lt;em&gt;in silico&lt;/em&gt;, I just want to focused on the parts after you finished your experiment. Repeated works in data analysis, writing and presentation should be considered.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;Doing the same thing multiple times would waste your time. Also such behavior would introduce variances to reproduce your results. If we could not find a robot, a better choice is using codes. An important principle in research is that what you could do should be totally reproducible by other people, and yourself. When you do something related to research, make sure other people could always repeat your operation from the recipe you left. You should always treat yourself as &amp;lsquo;other people&amp;rsquo; and such codes or text files would help you reduce the future energy.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Use Graphical User Interface(GUI)?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nooooooooo! No one would remember whether certain buttons have been clicked or not. However, you could also use macro to track your operations if possible. Make sure such macro could be shared without certain requirements about expensive software. If you have to use GUI, try to avoid the interactive operations which could affect your results.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Version control?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Yes! Always use version control for your files and add comments about changes. Use words which could be understood by human beings. Otherwise prepare a code book for yourself. Trust me, you need such code book rather than any other people. &lt;a href=&#34;https://git-scm.com/&#34;&gt;Git&lt;/a&gt; and &lt;a href=&#34;https://github.com/&#34;&gt;github&lt;/a&gt; would be a good start.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RSS?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Yes! Try to create your own rss list and keep track of new papers in passive way. Always start with abstract and end with a note links to original paper. Organize the notes as a book related to your research topic. When a new paper appeared as a note in your knowledge system or books, you could finally use such paper. Avoid guiding by the authors and keep your own system updated as I did for metabolomics &lt;a href=&#34;https://bookdown.org/yufree/Metabolomics/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plot/Process similar data with the same setting?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Try to organize scripts in one function and use &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;rmarkdown&lt;/a&gt; to make reproducible reports about it. Next time you only need to call that functions as I did &lt;a href=&#34;https://github.com/yufree/democode&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make it clear?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Use &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; style or verse with &lt;a href=&#34;https://yihui.name/tinytex/&#34;&gt;tinytex&lt;/a&gt; support. Try to keep style stable and other people with similar style would figure out what you said from your codes in one second.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Market your ideas?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Write journal papers with &lt;a href=&#34;https://github.com/rstudio/rticles&#34;&gt;rticles&lt;/a&gt; and show them at conferences with &lt;a href=&#34;https://github.com/yihui/xaringan&#34;&gt;xaringan&lt;/a&gt; and/or &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;shiny&lt;/a&gt;. All you need to learn is &lt;a href=&#34;https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet&#34;&gt;markdown&lt;/a&gt; and you could graduate in five minutes at most.&lt;/p&gt;
&lt;p&gt;Try to share source files about your ideas as a github repo and this would help you spread your ideas as &lt;a href=&#34;https://en.wikipedia.org/wiki/Meme&#34;&gt;meme&lt;/a&gt;. Also you should consider pre-print sever such as &lt;a href=&#34;https://arxiv.org/&#34;&gt;arxiv&lt;/a&gt;, &lt;a href=&#34;https://www.biorxiv.org/&#34;&gt;bioRxiv&lt;/a&gt; and &lt;a href=&#34;https://chemrxiv.org/&#34;&gt;chemrxiv&lt;/a&gt; to share your ideas in Physics, Life science and Chemistry, respectively.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SNS your ideas?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Write blog posts with &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;blogdown&lt;/a&gt; in plain text and share them on twitter/linkedin/researchgate. Actually, one of the purposes of my &lt;a href=&#34;https://yufree.cn&#34;&gt;website&lt;/a&gt; is share my ideas.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Similar topic of a bunch of functions?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pack the functions into a package and make detailed documents and publish with &lt;a href=&#34;https://github.com/r-lib/pkgdown&#34;&gt;pkgdown&lt;/a&gt; as I did in &lt;a href=&#34;https://cran.rstudio.com/web/packages/enviGCMS/index.html&#34;&gt;enviGCMS&lt;/a&gt; package.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Similar topic of your ideas?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Write a book online with &lt;a href=&#34;https://bookdown.org/yihui/bookdown/&#34;&gt;bookdown&lt;/a&gt; as I did for metabolomics &lt;a href=&#34;https://bookdown.org/yufree/Metabolomics/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reproduce the whole environment for data analysis?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Use &lt;a href=&#34;https://www.docker.com/&#34;&gt;docker&lt;/a&gt; or &lt;a href=&#34;https://hub.docker.com/u/rocker/&#34;&gt;rocker&lt;/a&gt; image to pack all but least dependence or software in a Linux image and share them on &lt;a href=&#34;https://hub.docker.com&#34;&gt;dockerhub&lt;/a&gt;. You could also distribute your raw data and scripts with your docker image and show the links on your publications. In this case, anyone could validate your results with the same setting. I also did &lt;a href=&#34;https://hub.docker.com/r/yufree/xcmsrocker/&#34;&gt;one&lt;/a&gt; for metabolomics study.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Share the data?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Try &lt;a href=&#34;https://figshare.com/&#34;&gt;figshare&lt;/a&gt; or &lt;a href=&#34;https://osf.io&#34;&gt;Open Science Framework&lt;/a&gt; to share your data or projects. I still suggested to use docker image to avoid potential issues.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Literature management?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zotero.org/&#34;&gt;Zotero&lt;/a&gt; or &lt;a href=&#34;https://github.com/ropensci/fulltext&#34;&gt;fulltext&lt;/a&gt;. However, as I shown &lt;a href=&#34;https://yufree.cn/en/2018/01/12/get-rid-of-bibliography/&#34;&gt;here&lt;/a&gt;, all you need for literature management is &lt;a href=&#34;https://www.doi.org/&#34;&gt;DOI&lt;/a&gt;. Just build your own knowledge system and organize literature according to your topic. You would benefit from such system by reducing a lot of time to align literature into the sections of your papers since you have already done this at the very beginning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I can&amp;rsquo;t remember those tips&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Well, I prepare a &lt;code&gt;downverse&lt;/code&gt; rocker &lt;a href=&#34;https://hub.docker.com/r/yufree/downverse/&#34;&gt;image&lt;/a&gt; with those software installed and you could try. Other websites where you could always find excellent tools for research is &lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt; and &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;Rstudio&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thanks again, Yihui! Actually you developed most of the packages.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pittcon 2018</title>
      <link>https://yufree.cn/en/2018/02/27/see-you-pittcon-2018/</link>
      <pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2018/02/27/see-you-pittcon-2018/</guid>
      <description>&lt;p&gt;This is the first time I attend Pittcon. The past two days I saw a lot of old friends and some big fish on analytical chemistry. However, this post is mainly for my two presentations on Feb. 28th, 2018 in case someone miss the take-home message either by the early presentaion time or my communication skills.&lt;/p&gt;
&lt;h2 id=&#34;online-batch-correction-and-interactive-data-visualization-for-gclc-ms-data&#34;&gt;Online batch correction and interactive data visualization for GC/LC-MS data&lt;/h2&gt;
&lt;p&gt;This oral presentaiton are mainly focused on the online application &lt;a href=&#34;https://yufreecas.shinyapps.io/xcmsplus/&#34;&gt;XcmsPlus&lt;/a&gt;. This app is powered by &lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;shiny&lt;/a&gt;. The first part about batch correction methods are actually surpassed by &lt;a href=&#34;http://idrb.zju.edu.cn/noreva/&#34;&gt;NOREVA&lt;/a&gt;, which have more methods to be used. However, I am not intended to add more correction methods but focused on the simulation of batch effects and the evaluation of correction methods based on statistical properties about GC/LC-MS data.&lt;/p&gt;
&lt;p&gt;By simulation with real data, I found most of the batch correction methods are fine in most cases. However, not so much improvement before and after correction. Acutally, batch correction would add false positive while keep more true positive. Such effects should be considered before applying any batch correction methods.&lt;/p&gt;
&lt;p&gt;As for methods selection, I found in most cases, surrogated variable analysis or independant surrogated variable analysis outperform other correction methods. Also we could use quantitative analysis to show the influences from experimental design and batch effects.&lt;/p&gt;
&lt;p&gt;Another topic about this presentaiton is about the visulization of peak list data. Such feature is powered by &lt;a href=&#34;https://rstudio.github.io/DT/&#34;&gt;DT&lt;/a&gt; package. You could always select, scale and download part of your peak list data through your eyes. This function is simple while useful for most of the metabolomics starters.&lt;/p&gt;
&lt;p&gt;My slides could be found via &lt;a href=&#34;https://docs.google.com/presentation/d/11nEtnaYR5KGE2uufv5sgw0Qw70G7ZAYd7AnCdMUKZa4/edit?usp=sharing&#34;&gt;google doc&lt;/a&gt; and the source code of XcmsPlus could be found via &lt;a href=&#34;https://github.com/yufree/xcmsplus&#34;&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;See you at 205C between 8:50 am and 9:10 am tomorrow!&lt;/p&gt;
&lt;h2 id=&#34;tissue-storage-affects-global-metabolic-profiling-in-comparision-to-in-vivo-microsampling-approach&#34;&gt;Tissue storage affects global metabolic: Profiling in comparision to in vivo microsampling approach&lt;/h2&gt;
&lt;p&gt;This poster presentation are works done by Anna and me. The original experiment is done by Vincent, Anna and me. We found the tissue storage would strongly change the pathway analysis results for in vivo study and hydrolysis process should be considered. Solid phase microextration could capture more lipophilic and short live compounds in in vivo studies.&lt;/p&gt;
&lt;p&gt;This is a fundamental research for non-targeted analyticla methods and the results might give us a hint about the complex during in vivo metabolomics studies.&lt;/p&gt;
&lt;p&gt;See you at poster zone between 1:00pm and 3:00pm.&lt;/p&gt;
&lt;h2 id=&#34;flyer-about-spme-courses&#34;&gt;Flyer about SPME courses&lt;/h2&gt;
&lt;p&gt;Our SPME courses are canceled just before this conference. So Prof. Pawliszyn asked all the group members to present the flyer about two days SPME course in Waterloo between April 26th and 27th. Here is the flyer and you could contact Nikita for more infomation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/images/pittcon2018flyer.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;hire-me&#34;&gt;Hire me&lt;/h2&gt;
&lt;p&gt;My PostDoc contract would end at the end of July, 2018. I want to find a new PostDoc or research scientist position in U.S. because my girlfriend is here and travels between U.S. and Canada are really not so convenient for me. If you are interested in me or my research, you could find everything including referee and cv on this website and contact me &lt;a href=&#34;mailto:42@yufree.cn&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Use docker to package your metabolomics study</title>
      <link>https://yufree.cn/en/2018/01/17/use-docker-to-package-your-metabolomics-study/</link>
      <pubDate>Wed, 17 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2018/01/17/use-docker-to-package-your-metabolomics-study/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;One of the most annoying thing during data analysis is that the technique barrier between the authors and the users. For example, you could share your raw data, script and reports online while other people just can’t reproduce your results. Actually, the development environment matters.&lt;/p&gt;
&lt;p&gt;If you have developed some software, you must know that we just can’t build an application from the very beginning and dependencies are always needed. Some of them could be distributed with the system software, while the others must be installed yourself. In Windows, you should see some dll files missing in some cases. In UNIX-like system, some missing packages are always handled by &lt;code&gt;apt-get&lt;/code&gt; or &lt;code&gt;yum&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When you use high level languish like R, the R packages could be handled by &lt;code&gt;install.packages&lt;/code&gt;. For R packages, &lt;a href=&#34;http://rstudio.github.io/packrat/?version=1.1.383&amp;amp;mode=desktop&#34;&gt;packrat&lt;/a&gt; package could be used to build a isolated dependency management system. However, some R packages need other system level packages to run. For example, &lt;code&gt;netcdf&lt;/code&gt; is needed to import cdf files for &lt;code&gt;mzR&lt;/code&gt; packages in metabolomics studies. However, if you don’t know this package should be installed outside R, you might blame the package and turn to GUI software, which is hard to be reproduced by other people.&lt;/p&gt;
&lt;p&gt;In fact, when you want to reproduce others’ data analysis, such development environment should be kept constant. Otherwise, it’s hard to know the origin of reproducible issues: the analysis itself or other hidden technique differences. Also, by comparing the differences in the results among different developed environment, we could know the influences of developed environments.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; could make such analysis work. They could pack all the developed environments in a minimized size and make the analysis shaerable as a whole. If you are familiar with online consistent integration tools like &lt;a href=&#34;https://travis-ci.org/&#34;&gt;Travis-CI&lt;/a&gt; or &lt;a href=&#34;https://circleci.com/&#34;&gt;CircleCI&lt;/a&gt; to test your software, you might find they actually install a mini OS to trigger the test.&lt;/p&gt;
&lt;p&gt;In Docker, you could get such environmental directly and distribute them as a image. Such image could be installed in any computer and furthermore on cloud. You could just rent a cloud server to make data analysis and the annoying OS installation could be replaced by a docker image with everything done. The major difference between docker container and virtual machine image is that docker image do not rely on a guest OS, which reduce the resources for extra useless software. More technique details could be found on their official websites.&lt;/p&gt;
&lt;p&gt;I am always not the first one to find the potential of such tools. Biocuductor has built such Docker for certain types of the data analysis &lt;a href=&#34;https://www.bioconductor.org/help/docker/&#34;&gt;here&lt;/a&gt;. Carl Boettiger and Dirk Eddelbuettel wrote a nice &lt;a href=&#34;https://arxiv.org/abs/1710.03675&#34;&gt;piece&lt;/a&gt; on arXiv to introduce Rocker for R developer. My way is that the combination of those ideas.&lt;/p&gt;
&lt;p&gt;For the metabolomics images on Bioconductor, only packages hosted on Bioconductor were contained. Some packages for metabolomics data analysis are not released on CRAN or Bioconductor while got published as part of the papers. I actually collected them on github. Another trends is that tidyverse data analysis is really popular and the metabolomics images should start in a tidyverse way. Thus I started to build my docker images for metabolomics studies &lt;a href=&#34;https://github.com/yufree/xcmsrocker&#34;&gt;here&lt;/a&gt;. Now you could use the following command to get my data analysis environment after you &lt;a href=&#34;https://www.docker.com/docker-mac&#34;&gt;install&lt;/a&gt; the docker:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;docker pull yufree/xcmsrocker:latest&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Then all you need to do is run this command:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;docker run –rm -p 8787:8787 yufree/xcmsrocker&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And open your favourite browser open &lt;a href=&#34;http://localhost:8787&#34; class=&#34;uri&#34;&gt;http://localhost:8787&lt;/a&gt; to start your data analysis. The default user and password are both rstudio.&lt;/p&gt;
&lt;p&gt;Three major reasons to use docker for metabolomics are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Everyone could reproduce your results in exactly the same development environment you find the reported results&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You could write your papers in Rtsudio with the help of &lt;code&gt;rticles&lt;/code&gt; packages. The journal templates for American Chemistry Society are added by me (you could always send me the feedback when you are in trouble) and you could write your paper in Rmarkdown way which could also be associated with the data. Such docker with your paper manuscripts and raw data is much easy for reviewers or readers to follow your idea&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The total size of the Docker is merely 1G including a tiny LaTex distribution TinyTex provided by &lt;a href=&#34;https://yihui.name&#34;&gt;Yihui Xie&lt;/a&gt;. You could make it portable on Dropbox or directly pull the image from the &lt;a href=&#34;https://hub.docker.com/&#34;&gt;Docker Hub&lt;/a&gt; anytime you wanted. In fact, they also support &lt;a href=&#34;https://docs.docker.com/engine/swarm/&#34;&gt;swarm mode&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the most updated image, you could find the packages powered by xcms or apLCMS workflow or metaboanalystR which behind the metaboanalyst server. Such image might help you to get the data analysis tools I used in half an hour and deploy it on your computer. Then you might only need to copy and paste to run your metabolomics data in a Rstudio server.&lt;/p&gt;
&lt;p&gt;Hopefully, we could share such image for research to reduce the time and pain on reproduce the same data analysis environment in the near future.&lt;/p&gt;
&lt;p&gt;If you want to learn more about R based docker, check this &lt;a href=&#34;http://ropenscilabs.github.io/r-docker-tutorial/&#34;&gt;tutorial&lt;/a&gt; from &lt;a href=&#34;https://ropensci.org&#34;&gt;rOpenSci&lt;/a&gt;. For extra packages, you could check &lt;a href=&#34;https://bookdown.org/yufree/Metabolomics/&#34;&gt;Meta-Workflow&lt;/a&gt; and/or report an issue to this github &lt;a href=&#34;https://github.com/yufree/xcmsrocker/issues&#34;&gt;repo&lt;/a&gt;, then I will add new features for this image.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Get rid of bibliography with R</title>
      <link>https://yufree.cn/en/2018/01/12/get-rid-of-bibliography/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2018/01/12/get-rid-of-bibliography/</guid>
      <description>


&lt;p&gt;Change citation and bibliography format for papers are somewhat waste of time when we are not really care about the contents about how to order the authors, journal and issues. To be honest, with the popularity of HTML support on most of the journal, citation in the paper could always be linked to the original webpage via DOI. In those cases, extra bibliography just make the papers longer and require extra efforts to covert the items of interests into the original paper webpages.&lt;/p&gt;
&lt;p&gt;Another issue is the authors might need to learn zotero, endnote or mendeley to format the papers. For Tex user, BibTeX should be learned. We need to learn the differences between citation key, citation format and bibliography to make it work. Yes, make those concepts clear would help us to collaborate with the journal easier. However, we actually make such thing much simpler.&lt;/p&gt;
&lt;div id=&#34;goal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal&lt;/h2&gt;
&lt;p&gt;When you write the paper, JUST need to know the DOI of the paper needed to be cited. Put the DOI in the right place and that’s it. All the other things should be handled by a small script.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method&lt;/h2&gt;
&lt;p&gt;Firstly, I need a fast copy support for literature management software such as Zotero. All I need to do is the creation of a &lt;a href=&#34;http://citationstyles.org/&#34;&gt;CSL files&lt;/a&gt; with citation format as DOI. Furthermore, make the DOI into a HTTP link. Here is &lt;a href=&#34;https://github.com/yufree/democode/blob/master/doi/doi.csl&#34;&gt;it&lt;/a&gt;. Such CSL could output a bibliography with DOI links only and you could always use the links to find the original papers.&lt;/p&gt;
&lt;p&gt;For zotero, download or copy&amp;amp;save this CSL file and then install this file according to the following two screenshot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;\images\doi1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;\images\doi2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then you could directly drag your items( hold the shift to use citation instead of bibliography) in the local library to any text editor. Of course you need to ensure your literature information contain a DOI. In this step, your manuscript would contain the DOI links to the cited papers. I think we could stop here while you might not like it since the bibliography are still needed for the print version of your paper.&lt;/p&gt;
&lt;p&gt;OK. The following step is to process the text file with DOI and output a text file with bibliography. Here is the script:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;#39;stringr&amp;#39;)
library(&amp;#39;readr&amp;#39;)
library(&amp;#39;rcrossref&amp;#39;)

doiref &amp;lt;- function(path, style = &amp;#39;apa&amp;#39;){
  mystring &amp;lt;- readr::read_file(path)
  doi &amp;lt;- unlist(stringr::str_extract_all(mystring, , &amp;quot;\\b10\\.(\\d+\\.*)+[\\/](([^\\s\\.])+\\.*)+\\b&amp;quot;))
  doi &amp;lt;- unique(doi)
  ref &amp;lt;- vector()
  for (i in 1:length(doi)){
        temp &amp;lt;- try(rcrossref::cr_cn(dois = doi[i], format = &amp;quot;text&amp;quot;, style = style), T)
        ref &amp;lt;- c(ref,temp)
  }
  readr::write_lines(ref, path = &amp;#39;bibliography.txt&amp;#39;)
  return(ref)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Also I prepared a test files &lt;a href=&#34;https://github.com/yufree/democode/blob/master/doi/test.txt&#34;&gt;here&lt;/a&gt; and the output should be &lt;a href=&#34;https://github.com/yufree/democode/blob/master/doi/bibliography.txt&#34;&gt;this&lt;/a&gt;. The usage is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;#39;stringr&amp;#39;)
library(&amp;#39;readr&amp;#39;)
library(&amp;#39;rcrossref&amp;#39;)
doiref(path = &amp;#39;you/text/file/path&amp;#39;, style = &amp;#39;journal/you/like&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might find a file with name ‘bibliography.txt’ under your workdir. Just change the style for specific journal you like anytime you REALLY need such bibliography. Here is the name &lt;a href=&#34;https://www.zotero.org/styles&#34;&gt;list&lt;/a&gt; you might need to search.&lt;/p&gt;
&lt;p&gt;In general, such script should work under any kind of pure text file format such as md, Rmd, tex, and so on. DOI would relax you from different markup languages and focused on the contents only no matter which software you used. Hopefully, such feature could be finally accepted by the journals. We might reduce quite a lot of efforts on bibliography and enjoy a green life.&lt;/p&gt;
&lt;p&gt;When we have the links online, why we still need that shortcut references lines in a modern WWW world?&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Follow-up based open source journal</title>
      <link>https://yufree.cn/en/2017/12/17/blockchain-based-journal/</link>
      <pubDate>Sun, 17 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2017/12/17/blockchain-based-journal/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Dose peer reviews really fit into this era? I have been a reviewer for few times and recently such idea come into my minds. My answer would be NO.&lt;/p&gt;
&lt;p&gt;Almost all of the studies are from N to N+1 instead of 0 to 1. That’s why we need peer reviews to strength new findings and hypothesis. However, peer reviews actually happens everyday before and after the paper get published. Most of the research group would have some kinds of journal talk or literature reading to introduce new study for other group members or train the skills for presentation. As a critical readers, every time I could hear some drawbacks in published papers. To be honest, I don’t think reviewers could cover everything and such critical post-published comments only exists within research group. No feedback, no follow-up. This wasted a lot of efforts.&lt;/p&gt;
&lt;p&gt;Peer reviews actually block the academia communication in an inefficient way. The popular of pre-prints servers have shown that the critical review could happen before get published and studies could also gain influences without journal based peer-review. Generally, the credits of one paper are always independent of the journal and depend the inner insights of the authors, contributions to certain discipline and the follow-up studies. If the peer reviews could only gain the accumulated impact factors on one’s CV instead of the citations, such methods might need alter way to reflect the importance of their studies.&lt;/p&gt;
&lt;p&gt;Citation or follow up relationship might be a natural way for development in academia. The development of certain theory could always be summarized by a timeline with some brunches. Why not journals? For each journal, we could build a follow-up relationship based on the co-relationship among the papers. The base paper could be a textbook and all of the papers are some kinds of the brunches of certain book section. No need to get published with traditional peer review. Authors just need to select the right chapter or the following paper under that topic, put their paper under the very last paper they agree all the augments and wait for the follow-up. Only the positive feedback could be the follow-up, otherwise a brunch might appears. Other studies just need to choose the papers they like to vote up to follow up, so actually we could get enough high-quality reviewers for each studies. All of the efforts on one work could get paid and credits by their readers.&lt;/p&gt;
&lt;p&gt;Such knowledge structure could be pruned by peers’ comments and only the one with the most of the follow-up studies could become the mainstream. Such journal would be easy for the graduates and undergraduates to find the newest viewpoints instead of get buried by tons of papers and chaos reviews.&lt;/p&gt;
&lt;p&gt;One paper could follow up different topics. If some topics have few people to work on, they might not gain credits by peers while gain credits by following up another textbook in other disciplines. It would be easy for researchers to make cross-discipline studies and invade or develop new research topics.&lt;/p&gt;
&lt;p&gt;Actually, open source software always fit this no peer review scheme. Platform like CRAN only check the technique details and code quality and only the packages with real interesting or useful functions could be popular and get credits. Still one could gain credits for only one package instead of a list of packages with no users. Also the one submit issues or bug report could get some credits by a update or follow-up of that software. I mean, such idea works well in open source software development, why not use this way to build a follow-up style journal.&lt;/p&gt;
&lt;p&gt;All we need are a server to host all of the studies and researchers would build the journal themselves like Wikipedia. However, we might find too much brunches under certain topics than Wikipedia and that’s the fun of research, isn’t it?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Real high-throughput for LC/GC-MS</title>
      <link>https://yufree.cn/en/2017/07/09/real-high-throughput-for-lc-gc-ms/</link>
      <pubDate>Sun, 09 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2017/07/09/real-high-throughput-for-lc-gc-ms/</guid>
      <description>


&lt;p&gt;If someone want to know whether some compounds exist in certain samples. He always need to make pretreatment for that samples and make them into a little vial for analysis in sophisticated instruments like mass spectrum. If you want to analysis many components in one sample, you also need some separation methods like gas/liquid chromatography. How about analysis multiple samples and multiple compounds in a single run?&lt;/p&gt;
&lt;div id=&#34;pseudo-high-throughput&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pseudo high-throughput&lt;/h2&gt;
&lt;p&gt;If you use LC/GC-MS to analysis samples, all the efforts for high-throughput would be limited at the injection step. You could arrange 96-well plate for analysis. But wait, ONE BY ONE. If some short-live compound could survive in the pre-treatment, they would disappear in the auto-sampler.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;back-to-mass-spectrum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Back to Mass Spectrum&lt;/h2&gt;
&lt;p&gt;Actually, similar issue happened when we perform Multiple Reaction Monitoring(MRM) to collect the intensities from different ions. If the detector could only measure one ion at one time, mass spectrum simply use a high frequency scan like 50ms or 20ms for one ion and re-construct the time profile by smooth the points into a line. To my knowledge, 15 points would fit a bell curve well for one peak with smooth. OK, if the peak width is 15s, for each ion we only need 1 scan per second as shown below. OK, that means if our instrument could reach 10ms per scan per ion, we could monitor 100 ions at the same time.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2017-07-09-real-high-throughput-for-mass-spectrum_files/figure-html/sim-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then how about full scan, some detectors like orbitrap and tof-tof could monitor a full scan for all ions at almost the same time. Common high resolution mass spectrum could collect more than 10 spectrum per second. If the compounds could show peaks’ width larger than 15s, we could actually collect 10 spectrum from different samples, which is the real high-throughput.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;real-high-throughput&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Real high-throughput&lt;/h2&gt;
&lt;p&gt;All we need to do is to synchronize the injection and mass spectrum scan. I have three options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple columns with single channel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;\images\htoption1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this solution, we injected 6 samples at the same time. The column should be the same. Then when the samples reach MS part, they were arranged into one sequence. All we need to do is to ensure every six full scan on the mass spectrum could meet six identity sample from the LC/GC part. Then MS could collect and rebuilt six samples’ retention time - mass profile and output six data set for those samples.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple columns with Multiple channels&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;\images\htoption2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this way, we need some controls on pumps to on/off when one channel’s sample get into MS for full scan. Or we could have cells to guide the samples into the slit before the lens. This option is better to avoid the cross contamination. However, we need re-design the MS ion source for multiple channels.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single column with single channel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this way, you do not need to modify your current instruments. All you need to do is the injection of the samples by sequence without considering the full separation in former sample. However, you need a lot of efforts to deconvolution. Since it’s the same column, the separation for most ions should be same. You could build a model to capture such patterns and separate the samples by those patterns. In such way, the batch effects could be minimized. However, the requirements for data mining are maximization. I like this way.&lt;/p&gt;
&lt;p&gt;I think in the near future we would find the real high-throughput LC/GC-MS. Those devices would short the analysis time between sample collection and data acquisition. MS-based scientists could reach more interesting findings with REAL high-throughput.&lt;/p&gt;
&lt;p&gt;Happy explore!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Text mining for top academic journals</title>
      <link>https://yufree.cn/en/2017/07/07/text-mining/</link>
      <pubDate>Fri, 07 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2017/07/07/text-mining/</guid>
      <description>


&lt;p&gt;Text mining is often used to find spatiotemporal trends in news, government report and user generated contents in SNS websites. We could make sentiment analysis to find the real opinions of netizens. Also we could track the popularity of certain phases and find the connections among them. Such technique might also be useful for scientists or researchers to read papers.&lt;/p&gt;
&lt;p&gt;Scientists or researchers’ daily life is immersed by a lot of literature. Most of the them are only focused on limited area in certain subjects. However, a modern scientist should always know what had happened in all of the other subjects. Some techniques used in other research might inspire your research. The only problem is that you need a lot time reading top general journals like Science, Nature and PNAS. Wait, we actually do not need to know the technique details and all we want to know is the patterns in those journals. Well, text mining would help.&lt;/p&gt;
&lt;p&gt;The main advantage for text mining in academic journals is that academic papers always share same structures in one journals. Public academic databases such as PubMed or Google scholar could always show you the structured records for papers such as journal, authors, title, published dates and even abstracts. We could directly fetch those data and save in database. I developed scifetch package to get those data from PubMed. This package would support Google scholar, bing academic and baidu xueshu in the future. Actually I support PubMed in the first version because they have a user-friendly API and I could connect such pipe with xml2 package in a tidyverse way. Besides, easyPubMed package is also a good package to extract such data from PubMed.&lt;/p&gt;
&lt;div id=&#34;data-collection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data collection&lt;/h2&gt;
&lt;p&gt;Here I collected the information from all the papers published in SNP i.e. science, nature and PNAS in the past three years as xml format and clean them into a dataframe for further text mining. The search grammar could be find from &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/books/NBK3827/&#34;&gt;NCBI websites&lt;/a&gt; and a cheat sheet here.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/images/cheatsheetpubmed.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are 26559 papers and I will use such data for text mining. PubMed has a limitation for 10000 records per query. So we need to fetch the data multiple times.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 26,557 x 7
##                          journal
##                            &amp;lt;chr&amp;gt;
##  1                        Nature
##  2                        Nature
##  3 Proc. Natl. Acad. Sci. U.S.A.
##  4 Proc. Natl. Acad. Sci. U.S.A.
##  5                       Science
##  6                       Science
##  7                       Science
##  8                       Science
##  9                       Science
## 10                       Science
## # ... with 26,547 more rows, and 6 more variables: title &amp;lt;chr&amp;gt;,
## #   date &amp;lt;date&amp;gt;, abstract &amp;lt;chr&amp;gt;, line &amp;lt;int&amp;gt;, time &amp;lt;dttm&amp;gt;, month &amp;lt;date&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;description-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Description statistics&lt;/h2&gt;
&lt;p&gt;Firstly, let’s see the papers by journal:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/sum-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then we could check the high frequency terms in the title and abstract of these papers.&lt;/p&gt;
&lt;div id=&#34;title&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Title&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/wordtitle-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As top journals, one of the most obvious features is that they all need correction and reply. With high influences, those journals would be the best place to discuss the leading edge techniques and findings. However, Science likes U.S., glance and paleoanthropology more while Nature and PNAS like tumor to be used in the titles.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/wordabs-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, when we focused on the abstracts, something interesting happens:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;They all focused on tumor while Nature use ‘tumour’ as a journal from U.K.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Nature’s title and abstracts look similar while Science’s title always use different terms compared with their abstracts. Maybe Science’s authors like to be clickbaits.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;PNAS’s authors use a lot of abbreviation in their abstracts.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Those top journals all like tumor, behavior and modeling and now you know how to pick up a topic to be published.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;temporal-trends&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Temporal trends&lt;/h2&gt;
&lt;p&gt;Here we use logistic regression to examine whether the frequency of each word is increasing or decreasing over time. Every term will then have a growth rate associated with it.&lt;/p&gt;
&lt;div id=&#34;title-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Title&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/titletemp-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://yufree.cn/en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/titletemp-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we could find some trending terms like CRISPR, gut, and corrigendum are ‘promising’. However, some topics like Ebola, Hiv and cell differentiation would leave us. Another interesting trending is that the names of certain subjects is disappearing in those top journals like biology, chemistry, neuroscience, ecology and policy. Maybe most titles would like to focus on specific topics or certain problems.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Now let’s review the temporal trends of terms in abstracts during the past three years by months.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/abstemp-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://yufree.cn/en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/abstemp-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s hard to find a clear pattern in growing words in abstracts. Maybe the abstracts focused more on technique details. However, shrinking words like viral, gene expression and pathway showed clear trends. Meanwhile, we could find some words like suggests, previously and production are discarded by the top scientist.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-among-words&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relationship among words&lt;/h2&gt;
&lt;p&gt;N-gram analysis could be used to find a meaningful terms in those papers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/bigramt-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/images/biabs.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, climate change, transcription factor, stem cell and cancer would always be the favorite bigrams in the titles of top journals. For the abstracts, cell related topics such as function, protein and expression are always preferred. Anyway, life science is always the center of trending sciences.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;topic-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Topic modeling&lt;/h2&gt;
&lt;p&gt;Relationships among words could show us some trending. However, we could employ topic modeling to explore the topics as a bunch of words in the abstracts of those top journals.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/images/tm.png&#34; /&gt;
This topic model showed topics like climate change, virus infection, brain science and social science are other important research topics other than life science.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sentiment analysis&lt;/h2&gt;
&lt;p&gt;Text mining could also be used to find the sentiment behind those journal papers or the customs using certain words.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2017-07-07-text-mining-for-trends-in-top-journal_files/figure-html/SA-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, I think the afinn word list for sentiment analysis is not suitable for scientific literature. Some words is actually neutral in academic journals. If someone could develop a specific word list for scientists, we might find something ignored by the writing lessons on campus.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Here, I demo some basic text mining results for top academic journals. Just like Google trends might predict the popularity of flu, text mining for academic journal might be a good tool to reveal unknown patterns or trends in certain subjects or top journals. Besides, we could also find unique usages of some words and some tones behind the journal. Besides, such methods might work better than impact factor or H index as the evaluation tools for certain researcher, journal or institute in a dynamic view. The most attractive thing is that every scientist could use this tools through open databases and find their own answers. This is the best benefits from information era.&lt;/p&gt;
&lt;p&gt;You might read this excellent on-line &lt;a href=&#34;http://tidytextmining.com/&#34;&gt;book&lt;/a&gt; and David Robinson’s &lt;a href=&#34;http://varianceexplained.org/&#34;&gt;blog&lt;/a&gt; to make more findings.&lt;/p&gt;
&lt;p&gt;All the R code for this post could be found &lt;a href=&#34;https://github.com/yufree/yufree.cn/blob/master/content/en/2017-07-07-text-mining-for-trends-in-top-journal.Rmd&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Selection of open source software platform for metabolomics or lipidomics</title>
      <link>https://yufree.cn/en/2017/06/12/selection-of-software-platform-for-metabolomics-or-lipidomics/</link>
      <pubDate>Mon, 12 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2017/06/12/selection-of-software-platform-for-metabolomics-or-lipidomics/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Recently I reviewed some platforms for metabolomics or lipidomics and tried to find out which one is currently available to my demands. I knew some instrumental company supplied software or solutions for metabolomics and lipidomics. I just disliked them since they concealed too many details about data processing which made omics studies as a magic box like Artificial Neural Network. It gains nothing for scientist to get insights from the data and you might only follow the workflow defined by the company. I read some papers using those kind of software and felt the authors know little about what they performed. Data analysis for metabolomics or lipidomics is a systems engineering. If someone really want to use this tools, just choose a open source platform and inspect the code when you needed. Otherwise, your work could be replaced by AI someday. It’s not a joke. Here is a summary for the selection of mass spectrum based metabolomics or lipidomics software platform.&lt;/p&gt;
&lt;div id=&#34;principles-for-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Principles for selection&lt;/h2&gt;
&lt;p&gt;A decent solution should have functions covering the following required functions or features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Open source: as I said before, researcher could benefit a lot from open source community. Based on your experiences, Java, matlab, python and R would be your choices. I liked R most while python might dominate everything in the further.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data input/output: this work could be done by msconvert from &lt;a href=&#34;http://proteowizard.sourceforge.net/tools.shtml&#34;&gt;proteowizard&lt;/a&gt;. I preferred to install msconvert directly on the instruments and convert the vendor files into mzML or mzXML files to perform further analysis. I just dislike Windows. On the other hand, some software from the instruments could output CDF files, which is also nice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Turn the Raw data into mass-retention time matrix: single data file from mass spectrum would always contain matadata and profile data. The former container has records for that data and the latter were always full scans of mass spectrum indexed by retention time. Then you need to map such data into a matrix because the following peaks picking and alignments always based on algorithms for matrix.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Peaks picking: you also need functions to pick up the peaks from mass-retention time matrix. Functions like centWave or isotope-based algorithms would help to find the peaks. Sometimes you need to bin the matrix first before peak picking and such requirements should be carefully checked. Anyway you could just use one function to perform peaks picking in most software. The output of this step would be a peak list with intensity, mass and retention time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Retention time correction: when you process more than one data file such as technique replicates, the drift of retention time would make the final peak list contained replicated peaks. Someone use certain peaks to corrected the data, while I prefer some global similarity analysis based algorithm to undertake this task. However, some software might add one step to group the peaks before and/or after retention time correction. Anyway, the output of this step is the peak list from one group.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Batch effects correction: this is another issue from analysis procedure and batch effects would also bury signals from noise. Researchers from analytical chemistry always use random experiment design and pooled sample to check if the batch effects exist. However, I think the most important things were correction. Just treat a series of samples suffered batch effects and use some statistical analysis to correct them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Statistical analysis: when you get the peaks list, you could perform the following supervised or non-supervised analysis to find bio-markers or data pattern. I think &lt;a href=&#34;http://www.metaboanalyst.ca/&#34;&gt;Metaboanalyst&lt;/a&gt; could satisfy most researchers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Annotation or identification: the peaks annotation is also important. &lt;a href=&#34;www.hmdb.ca&#34;&gt;HMDB&lt;/a&gt; and &lt;a href=&#34;http://www.lipidmaps.org/&#34;&gt;LIPID MAPS&lt;/a&gt; would be the basic version for mapping peaks to compounds. In most cases, MS/MS data would be used. I think most of MS/MS dataset could be separated handled by &lt;a href=&#34;https://gnps.ucsd.edu/ProteoSAFe/static/gnps-splash.jsp&#34;&gt;GNPS&lt;/a&gt; or &lt;a href=&#34;https://metlin.scripps.edu&#34;&gt;Metlin&lt;/a&gt; and predicted by &lt;a href=&#34;http://www.csi-fingerid.org/&#34;&gt;FingerID&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pathway analysis: sometimes we also need to know the relationship among compounds and pathway analysis is always performed for that purpose.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;platform&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Platform&lt;/h2&gt;
&lt;div id=&#34;xcms-online&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;XCMS online&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://xcmsonline.scripps.edu/landing_page.php?pgcontent=mainPage&#34;&gt;XCMS online&lt;/a&gt; is hosted by Scripps Institute. If your datasets are not large, XCMS online would be the best option for you. Recently they updated the online version to support more functions for systems biology. They use metlin and iso metlin to annotate the MS/MS data. Pathway analysis is also supported. Besides, to accelerate the process, xcms online employed stream (windows only). You could use stream to connect your instrument workstation to their server and process the data along with the data acquisition automate. They also developed apps for xcms online, but I think apps for slack would be even cooler to control the data processing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prime&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;PRIMe&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://prime.psc.riken.jp/Metabolomics_Software/&#34;&gt;PRIMe&lt;/a&gt; is from RIKEN and UC Davis. It supports mzML and major MS vendor formats. They defined own file format ABF and eco-system for omics studies. The software are updated almost everyday. You could use MS-DIAL for untargeted analysis and MRMOROBS for targeted analysis. For annotation, they developed MS-FINDER and they also developed statistic tools with excel. This platform could replaced the dear software from company and well prepared for MS/MS data analysis and lipidomics. They are open source, work on Windows and also could run within mathmamtics. However, they don’t cover pathway analysis. Another feature is they always show the most recently spectral records from public repositories. You could always get the updated MSP spectra files for your own data analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;openms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;OpenMS&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.openms.de/&#34;&gt;OpenMS&lt;/a&gt; is another good platform for mass spectrum data analysis developed with C++. You could use them as plugin of &lt;a href=&#34;https://www.knime.org/&#34;&gt;KNIME&lt;/a&gt;. I suggest anyone who want to be a data scientist to get familiar with platform like KNIME because they supplied various API for different programme language, which is easy to use and show every steps for others. Also TOPPView in OpenMS could be the best software to visualize the MS data. You could always use the metabolomics workflow to train starter about details in data processing. pyOpenMS and OpenSWATH are also used in this platform. If you want to turn into industry, this platform fit you best because you might get a clear idea about solution and workflow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mzmine-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;MZmine 2&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://mzmine.github.io/&#34;&gt;MZmine 2&lt;/a&gt; has three version developed on Java platform and the lastest version is included into &lt;a href=&#34;https://msdk.github.io/&#34;&gt;MSDK&lt;/a&gt;. Similar function could be found from MZmine 2 as shown in XCMS online. However, MZmine 2 do not have pathway analysis. You could use metaboanalyst for that purpose. Actually, you could go into MSDK to find similar function supplied by &lt;a href=&#34;http://www.proteosuite.org&#34;&gt;ProteoSuite&lt;/a&gt; and &lt;a href=&#34;https://www.openchrom.net/&#34;&gt;Openchrom&lt;/a&gt;. If you are a experienced coder for Java, you should start here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;xcms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;XCMS&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://bioconductor.org/packages/release/bioc/html/xcms.html&#34;&gt;xcms&lt;/a&gt; is different from xcms online while they might share the same code. I used it almost every data to run local metabolomics data analysis. Recently, they will change their version to xcms 3 with major update for object class. Their data format would integrate into the MSnbase package and the parameters would be easy to set up for each step. Normally, I will use msconvert-IPO-xcms-xMSannotator-metaboanalyst as workflow to process the offline data. It could accelerate the process by parallel processing. However, if you are not familiar with R, you would better to choose some software above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;emory-mahpic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Emory MaHPIC&lt;/h3&gt;
&lt;p&gt;This platform is composed by several R packages from Emory University including &lt;a href=&#34;https://sourceforge.net/projects/aplcms/&#34;&gt;apLCMS&lt;/a&gt; to collect the data, &lt;a href=&#34;https://sourceforge.net/projects/xmsanalyzer/&#34;&gt;xMSanalyzer&lt;/a&gt; to handle automated pipeline for large-scale, non-targeted metabolomics data, &lt;a href=&#34;https://sourceforge.net/projects/xmsannotator/&#34;&gt;xMSannotator&lt;/a&gt; for annotation of LC-MS data and &lt;a href=&#34;https://code.google.com/archive/p/atcg/wikis/mummichog_for_metabolomics.wiki&#34;&gt;Mummichog&lt;/a&gt; for pathway and network analysis for high-throughput metabolomics. This platform would be preferred by someone from environmental science to study exposome. I always use xMSannotator to annotate the LC-MS data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;others&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://genomics-pubs.princeton.edu/mzroll/index.php?show=index&#34;&gt;MAVEN&lt;/a&gt; from Princeton University&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://vpr.colostate.edu/pmf/wp-content/uploads/sites/23/2015/06/Broeckling_2014.pdf&#34;&gt;RAMclustR&lt;/a&gt; from Colorado State University&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.bioconductor.org/packages/release/bioc/html/MAIT.html&#34;&gt;MAIT&lt;/a&gt; based on xcms&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/yufree/enviGCMS&#34;&gt;enviGCMS&lt;/a&gt; from me&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.ebi.ac.uk/metabolights/&#34;&gt;Metabolights&lt;/a&gt; for sharing data&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selection&lt;/h2&gt;
&lt;p&gt;All of the platform above could be used for metabolomics and lipidomics. However, best selection would be based on your programming skills and the popularity in your research area. Every tool need training before analysis your data and you could choose one randomly and be focused on the source code for one day. If you feel you could handle it, just use it. Otherwise select another one. Enjoy and fight with the data.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data analysis for Desorption Electrospray Ionization</title>
      <link>https://yufree.cn/en/2017/05/11/data-analysis-for-desorption-electrospray-ionization/</link>
      <pubDate>Thu, 11 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2017/05/11/data-analysis-for-desorption-electrospray-ionization/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Desorption Electrospray Ionization (DESI) is known for on-site mass spectrum analysis. For example, you could use DESI-MS to get the distribution of certain ions on the surface or cross section of sample. Without chromatograph or related separation process, the mass spectrum is actually the average intensities of all the mass during the sampling time. Recently I am thinking how to process such data via xcms.&lt;/p&gt;
&lt;p&gt;Supposing we have data from 1 min sampling and we want to get the mass spectrum. For mass spectrum, 1 min usually means more than 100 full scan. The basic idea to process such data is directly binning the mass and average the intensity. However, I found &lt;code&gt;group.mzClust&lt;/code&gt; function and &lt;code&gt;MSW&lt;/code&gt; method are designed for this purpose.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(msdata)
mzdatapath &amp;lt;- system.file(&amp;quot;fticr&amp;quot;, package = &amp;quot;msdata&amp;quot;)
mzdatafiles &amp;lt;- list.files(mzdatapath, recursive = TRUE, full.names = TRUE)

xs &amp;lt;- xcmsSet(method=&amp;quot;MSW&amp;quot;, files=mzdatafiles, scales=c(1,7),
              SNR.method=&amp;#39;data.mean&amp;#39; , winSize.noise=500,
               peakThr=80000,  amp.Th=0.005)
xsg &amp;lt;- group.mzClust(xs)
xsg &amp;lt;- fillPeaks.MSW(xsg)
r &amp;lt;- groupval(xsg,&amp;#39;medret&amp;#39;,&amp;#39;into&amp;#39;)
z &amp;lt;- as.data.frame(groups(xsg))
file &amp;lt;- cbind(z$mz,r)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You could save the data as csv file. Then you could perform further analysis such as peak picking or PCA analysis. If your data could be organized for spatial analysis or imaging, all the data has been ready and you could draw them by one for loop or just use &lt;code&gt;animation&lt;/code&gt; package for a gif. I used this trick for my last group meeting as PhD students.&lt;/p&gt;
&lt;p&gt;PS: the new design of xcms 3 is much more friendly to process such data via &lt;code&gt;XCMSnExp&lt;/code&gt; object and the parameters design is very friendly to users.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using xcms offline for metabolomics study</title>
      <link>https://yufree.cn/en/2017/05/02/using-xcms-offline-for-metabolomics-study/</link>
      <pubDate>Tue, 02 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2017/05/02/using-xcms-offline-for-metabolomics-study/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;XCMS online is preferred for its convenience, especially with Stream. However, the storage is limited and you need to wait for some time to process your data. Actually, almost all of the functions online could be processed offline on local computer. Here I will show you some tips about using xcms package locally in R.&lt;/p&gt;
&lt;div id=&#34;optimized-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimized Parameters&lt;/h2&gt;
&lt;p&gt;Most of the users like xcms online because they have optimized parameters for different instruments and you could directly choose them. Those parameters are related to peaks extraction, grouping, retention time correction and fill missing peaks. Authors of xcms online has published &lt;a href=&#34;http://www.nature.com/nprot/journal/v7/n3/fig_tab/nprot.2011.454_T1.html&#34;&gt;paper&lt;/a&gt; and show the table of suggested parameters. Thus in the local version, you could directly use them. If you still feel hard, I write a function &lt;code&gt;getdata&lt;/code&gt; in the &lt;code&gt;enviGCMS&lt;/code&gt; package. You could install it from Github (CRAN version has not been updated):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;#39;yufree/enviGCMS&amp;#39;)
# we need parallel computing
library(enviGCMS)
library(BiocParallel)
library(xcms)
# you need faahKO package for demo
cdfpath &amp;lt;- system.file(&amp;quot;cdf&amp;quot;, package = &amp;quot;faahKO&amp;quot;)
# directly input path and you could get xcmsSet object
xset &amp;lt;- getdata(cdfpath, pmethod = &amp;#39;hplcqtof&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;getdata&lt;/code&gt; could directly perform peaks extraction, grouping, retention time correction and fill missing peaks and return the &lt;code&gt;xcmsSet&lt;/code&gt; object for further analysis.&lt;/p&gt;
&lt;p&gt;However, I suggest use &lt;code&gt;IPO&lt;/code&gt; package to optimize the parameters for certain instrumental. Here is the R script for optimizing. You need to be patient because such process usually take half day. After finding the parameters for your instrumental, you could use those parameters for the following studies. Here is the R script to optimize parameters for certain instrumental:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# path and files
# use pool qc or blank for this optimization
mzdatapath &amp;lt;- system.file(&amp;quot;cdf&amp;quot;,package = &amp;quot;faahKO&amp;quot;)
mzdatafiles &amp;lt;- list.files(mzdatapath, recursive = TRUE, full.names=TRUE)
library(IPO)
# use centwave if you use obitrap
peakpickingParameters &amp;lt;- getDefaultXcmsSetStartingParams(&amp;#39;matchedFilter&amp;#39;)
#setting levels for min_peakwidth to 10 and 20 (hence 15 is the center point)
peakpickingParameters$min_peakwidth &amp;lt;- c(10,20) 
peakpickingParameters$max_peakwidth &amp;lt;- c(26,42)
#setting only one value for ppm therefore this parameter is not optimized
peakpickingParameters$ppm &amp;lt;- 20 
resultPeakpicking &amp;lt;- 
  optimizeXcmsSet(files = mzdatafiles[6:9], 
                  params = peakpickingParameters, 
                  nSlaves = 4, 
                  subdir = &amp;#39;rsmDirectory&amp;#39;)

optimizedXcmsSetObject &amp;lt;- resultPeakpicking$best_settings$xset

retcorGroupParameters &amp;lt;- getDefaultRetGroupStartingParams()
retcorGroupParameters$profStep &amp;lt;- 1
resultRetcorGroup &amp;lt;-
  optimizeRetGroup(xset = optimizedXcmsSetObject, 
                   params = retcorGroupParameters, 
                   nSlaves = 4, 
                   subdir = &amp;quot;rsmDirectory&amp;quot;)


writeRScript(resultPeakpicking$best_settings$parameters, 
             resultRetcorGroup$best_settings, 
             nSlaves=12)
# https://github.com/rietho/IPO/blob/master/vignettes/IPO.Rmd&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Statistical analysis&lt;/h2&gt;
&lt;p&gt;Actually, the statistival methods in xcms online are limited compared with Metaboanalyst. In last post, I have shown how to install Metaboanalyst locally. Here, I also supply a function in &lt;code&gt;enviGCMS&lt;/code&gt; to directly get the csv file to be uploaded to Metaboanalyst. You need to show a xcmsSet object and the name for the file:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this xcmsSet object could be directly get from getdata function
getupload(xset,name = &amp;#39;peaklist&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;eic-and-boxplot-for-peaks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;EIC and Boxplot for peaks&lt;/h2&gt;
&lt;p&gt;If you like the report from xcms online, you could also get them with the figures. I also write a function called &lt;code&gt;plote&lt;/code&gt; in &lt;code&gt;enviGCMS&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# you also need the name for subdir of EIC and Boxplot, you might also change the test method for the diffreport
plote(xset,name = &amp;#39;test&amp;#39;,test = &amp;#39;t&amp;#39;, nonpara = &amp;#39;y&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of the function has been documented. I might update the CRAN version in the near future.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;waters-q-tof-mass-lock-issue&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Waters Q-ToF mass lock issue&lt;/h2&gt;
&lt;p&gt;If you use Waters Q-ToF, you might be confused by data conversion. I suggest you use the most updated msconvert to convert RAW folder into mzxml, which you could input the lock mass(older version miss this function). However, such data still have gap, you might use the &lt;code&gt;lockMassFreq = T&lt;/code&gt; in xcms to imput such gap to get more peaks. Such parameters could be transfer in &lt;code&gt;getdata&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xset &amp;lt;- getdata(path,lockMassFreq = T)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;annotation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Annotation&lt;/h2&gt;
&lt;p&gt;For the annotation part, I suggest using &lt;code&gt;xMSannotator&lt;/code&gt; package. You could install it from my github repo since the author didn’t use github:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# You might need to install the following packages before installing this package
install.packages(&amp;#39;data.table&amp;#39;)
install.packages(&amp;#39;digest&amp;#39;)
source(&amp;quot;http://bioconductor.org/biocLite.R&amp;quot;)
biocLite(&amp;quot;SSOAP&amp;quot;)
biocLite(&amp;quot;KEGGREST&amp;quot;)
biocLite(&amp;quot;pcaMethods&amp;quot;)
biocLite(&amp;quot;Rdisop&amp;quot;)
biocLite(&amp;quot;GO.db&amp;quot;)
biocLite(&amp;quot;matrixStats&amp;quot;)
biocLite(&amp;#39;WGCNA&amp;#39;)
devtools::install_github(&amp;quot;yufree/xMSannotator&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other functions&lt;/h2&gt;
&lt;p&gt;I have writed some other functions in &lt;code&gt;enviGCMS&lt;/code&gt; package and you could explore them. You might find some Easter Eggs. Also I will documented them as vignette in the future.&lt;/p&gt;
&lt;p&gt;This post and the post before is about finding the peaks and performing statistical analysis for metabolomics. In the next post, I will show you some tips about annotation based on &lt;code&gt;xMSannotator&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;If you have other issues about metabolomics data analysis, you could comment here and I’d like to discuss them. Also you could sent email to &lt;a href=&#34;mailto:slack@yufree.cn&#34; class=&#34;email&#34;&gt;slack@yufree.cn&lt;/a&gt; to get invitation for a slack group about metabolomics data analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tips for local installation of MetaboAnalyst on Windows</title>
      <link>https://yufree.cn/en/2017/03/29/metaboanalyst/</link>
      <pubDate>Wed, 29 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2017/03/29/metaboanalyst/</guid>
      <description>&lt;p&gt;I am running Windows 7 to perform metabolomics data analysis(mainly for mscovert). Recently I found MetaboAnalyst could be installed locally. Since some group members really care about their data safety, I just installed MetaboAnalyst on one of group computers. Here is some tips for it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Windows 7 is currently not supported by Metaboanalyst, so I use virtualbox to install a 64-bit Ubuntu 16.10.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For Ubuntu, you need to install a few packages to support both the R and Java environment, also some packages. You might follow the script in bash:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install libnetcdf-dev graphviz libxml2-dev libcairo2-dev default-jdk r-base-dev 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;You also need to install some packages from either CRAN or Bioconductor&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install Rserver in bash to get rid of configure of R&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get isntall r-cran-rserve
R
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Use the following code to install packages in R:&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;install.packages&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ellipse&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;scatterplot3d&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pls&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;caret&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;lattice&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Cairo&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;randomForest&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;e1071&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gplots&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;som&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;xtable&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;RColorBrewer&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pheatmap&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;igraph&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;RJSONIO&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;caTools&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ROCR&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pROC&amp;#34;&lt;/span&gt;))
&lt;span style=&#34;color:#a6e22e&#34;&gt;source&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://bioconductor.org/biocLite.R&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;biocLite&lt;/span&gt;()
&lt;span style=&#34;color:#a6e22e&#34;&gt;biocLite&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;xcms&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;impute&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pcaMethods&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;siggenes&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;globaltest&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GlobalAncova&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Rgraphviz&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;KEGGgraph&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;preprocessCore&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;genefilter&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;SSPA&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sva&amp;#34;&lt;/span&gt;))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;If you want to install Rstudio on 64-bit Ubuntu, you need the following steps:
&lt;ul&gt;
&lt;li&gt;Download &amp;ldquo;libgstreamer plugin&amp;rdquo; from &lt;a href=&#34;https://packages.debian.org/jessie/amd64/libgstreamer-plugins-base0.10-0/download&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Download &amp;ldquo;libgstreamer&amp;rdquo; from &lt;a href=&#34;https://packages.debian.org/jessie/amd64/libgstreamer0.10-0/download&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install two packages above&lt;/li&gt;
&lt;li&gt;Install the following packages&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install libjpeg62
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;MetaboAnalyst is actually a java-based web application (also, R based). You need java environment and use Tomcat or Glassfish to host the *.war file on server (Linux or Mac OS). Then you only need to access it by browser, just like what you did online.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install Glassfish. I tried Tomcat and the deploy always failed and I suggest to use Glassfish following the guide(you might need to set up user and password) and upload the *.war file by a web interface at http://localhost:4848&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;wget download.java.net/glassfish/4.1.1/release/glassfish-4.1.1.zip
apt-get install unzip
unzip glassfish-4.0.zip -d /opt
cd /opt/glassfish/bin
./asadmin start-domain
./asadmin enable-secure-admin
./asadmin restart-domain
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/war.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run the Rserve in bash:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;R CMD Rserve
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;After the installation of MetaboAnalyst on Glassfish, make a port transfer to ensure you could access the MetaboAnalyst on browsers of windows. You need to know the local IP address of both your host and virtual machine(VM).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your host address is the IP for the connection between host and VM. Use &lt;code&gt;ipconfig /all&lt;/code&gt; to get it&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/hostip.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your VM address could be found by &lt;code&gt;connection information&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/vmip.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set up the NAT port transfer to ensure you could access MetaboAnalyst on VM from host browser&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/porttrans.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Save a bookmark for the url(in my case: http://192.168.56.1:8080/MetaboAnalyst/ ) Open the virtualbox all the time at the background&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/ip.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enjoy local access (while not updated) to MetaboAnalyst&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Every time you restart your computer, input this in bash to start the MetaboAnalyst:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;R CMD Rserve
cd /opt/glassfish/bin
./asadmin start-domain
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;For the other thing, just follow the official guide &lt;a href=&#34;http://www.metaboanalyst.ca/faces/home.xhtml&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Statistical uncertainty of Isotope Ratio</title>
      <link>https://yufree.cn/en/2017/01/15/sd-isotope-ratio/</link>
      <pubDate>Sun, 15 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2017/01/15/sd-isotope-ratio/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In Analytical Chemistry, the measurements of isotope ratios are commons. However, I found the uncertainty of ratios are always shown in the format of standard deviation of independant vairiable, which is inappropriate in statistic. You accually measure at least two values to get one measurement.&lt;/p&gt;
&lt;p&gt;In fact, if you want to use the differences of isotope ratios as a measurement for certain process, you need to accept the assumption that the intensities of different isotopes are independant. Then we could make the Taylor series expansion of the ratio x/y around the mean of x and y:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{split} 
\frac{x}{y} \approx \frac{x}{y}\Big|_{\mu_x,\mu_y}&amp;amp;+(x-\mu_x)\frac{\partial}{\partial x}\Big(\frac{x}{y}\Big)\Big|_{\mu_x,\mu_y}+(y-\mu_y)\frac{\partial}{\partial y}\Big(\frac{x}{y}\Big)\Big|_{\mu_x,\mu_y}\\&amp;amp;+\frac{1}{2}(x-\mu_x)^2\frac{\partial^2}{\partial x^2}\Big(\frac{x}{y}\Big)\Big|_{\mu_x,\mu_y}+\frac{1}{2}(y-\mu_y)^2\frac{\partial^2}{\partial y^2}\Big(\frac{x}{y}\Big)\Big|_{\mu_x,\mu_y}+(x-\mu_x)(y-\mu_y)\frac{\partial^2}{\partial x \partial y}\Big(\frac{x}{y}\Big)\Big|_{\mu_x,\mu_y}\\&amp;amp;+\mathcal{O}\Big(\Big((x-\mu_x)\frac{\partial}{\partial x}+(y-\mu_y)\frac{\partial}{\partial y}\Big)^3\Big(\frac{x}{y}\Big)\Big)
\end{split}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The expectation of the ratio is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}[r] = \mathbb{E}\Big[\frac{\bar x}{\bar y}\Big] = \frac{\mu_x}{\mu_y} + Var(\bar y)\frac{\mu_x}{\mu_y^3} - \frac{Cov(\bar x,\bar y)}{\mu_y^2} \approx \frac{\mu_x}{\mu_y} + \frac{1}{n}\Big(Var(y)\frac{\mu_x}{\mu_y^3} - \frac{Cov(x,y)}{\mu_y^2}\Big)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The variance of the ratio is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{split}
Var(r) &amp;amp;= Var\Big( \frac{\bar x}{\bar y} \Big) = \mathbb{E}\Big[\Big(\frac{\bar x}{\bar y} - \mathbb{E}\Big[\frac{\bar x}{\bar y}\Big]\Big)^2\Big] \\&amp;amp;\approx \mathbb{E}\Big[\Big(\frac{\bar x}{\bar y} - \frac{\mu_x}{\mu_y}\Big)^2\Big]\\&amp;amp;\approx \mathbb{E}\Big[\Big((\bar x-\mu_x)\frac{\partial}{\partial \bar x}\Big(\frac{\bar x}{\bar y}\Big)\Big|_{\mu_x,\mu_y} + (\bar y - \mu_y)\frac{\partial}{\partial \bar y}\Big(\frac{\bar x}{\bar y}\Big)\Big|_{\mu_x,\mu_y}\Big)^2\Big]\\&amp;amp;\approx\frac{Var(\bar x)}{\mu^2_y} + \frac{\mu^2_x Var(\bar y)}{\mu^4_y} - \frac{2\mu_x Cov(\bar x, \bar y)}{\mu^3_y}\\&amp;amp;\approx\frac{1}{n}\Big(\frac{Var(x)}{\mu^2_y} + \frac{\mu_x^2 Var(y)}{\mu^4_y} - \frac{2\mu_x Cov(x,y)}{\mu^3_y}\Big)
\end{split}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Such values could be used as the uncertainty of the isotope ratios instead of the standard deviation of the ratios themselves.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evaluation and reduction of the analytical uncertainties in GC-MS analysis using a boundary regression model</title>
      <link>https://yufree.cn/en/2016/11/29/my-third-paper/</link>
      <pubDate>Tue, 29 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2016/11/29/my-third-paper/</guid>
      <description>&lt;p&gt;This &lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S0039914016309298&#34;&gt;paper&lt;/a&gt; received opposite comments from reviewers. One rejected and the other recommanded. Anyway, this is just the beginning of this kind of data analysis for mass spectrum. Also this work was the basis of one chapter in my thesis.&lt;/p&gt;
&lt;p&gt;In this work, I wanted to access and reduce the uncertainties in the whole procedure of environmental analysis. In regular analysis, we would use pure standards to optimized the analysis method and recovery and RSD were commonly used for quality control analysis. My concerns are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Uncertainties were hard to be found with standards in advance. When you injected a dirty samples, you instruments would be polluted after you see the results. Furthormore, when you found your targeted compounds were influenced by something from the matrices, you have to start the analysis from the beginning with new methods. So, I wounder if we could access some common properties during the analysis before we analysis the samples. Then I used visualization methods to show the Uncertainties in the raw data from GC-MS.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Another issue is that how to escape the influnces from the uncertainties found in the visualization methods. My solution was that building a boundary regression models to seperate the &amp;ldquo;clean&amp;rdquo; zone from the &amp;ldquo;dirty&amp;rdquo; zone in the raw data. By this model, we would get a better sensitivity by choosing right ions regardless of the matrices or pretreatment.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I am always wondering whether different pretreatments would show similar results for certain matrix and compounds. From this paper, my answer is almost yes. Certain pretreatments would remove something we do not like or harmful to the instruments. However, such influnces might be pointless and can&amp;rsquo;t be detected on mass spectrum. In GC-MS, the co-elute influnces are hard to affect the your target compounds at the same retention time and the same massed. Only the rising baseline is important and we could get rid of it by the boundary model. Then the only thing we need to consider is the pollution of the instruments.&lt;/p&gt;
&lt;p&gt;Meanwhile, I need to say such model might not be suitable for high-resolution mass spectrum. However, this idea could be used to improve the analytical methods for some compounds, especially for PBDEs. Also this paper supplied some basic data for environmental analysis. As a rule of thumb, you might know:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When you rise 1 degrees centigrade, the &amp;lsquo;dirty&amp;rsquo; zone&amp;rsquo;s boundary would rise about 2 unit mass in the worst matrix and pretreatment. Always try to choose heavier ions for qualitative and quantitative analysis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here is the graphical summary for the whole methods and I think more patterns could be mined from the data of GC-MS:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/blogcn/figure/MorF.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Also I developed a package to perform this kind of analysis in R. Check &lt;a href=&#34;https://github.com/yufree/enviGCMS&#34;&gt;here&lt;/a&gt;. This package has been published on &lt;a href=&#34;https://cran.r-project.org/web/packages/enviGCMS/index.html&#34;&gt;CRAN&lt;/a&gt; and you could install and load it by:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&#39;enviGCMS&#39;)
library(&#39;enviGCMS&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You might find &lt;strong&gt;Easter Eggs&lt;/strong&gt; in this package.&lt;/p&gt;
&lt;p&gt;If you have questions about this paper, comment here and I will reply as soon as possible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Use Chinese in RStudio Beamer Slides</title>
      <link>https://yufree.cn/en/2016/09/19/beamer-in-chinese/</link>
      <pubDate>Mon, 19 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2016/09/19/beamer-in-chinese/</guid>
      <description>&lt;p&gt;&lt;strong&gt;RStudio&lt;/strong&gt; is an excellent IDE for R. However, using Chinese in default setting of Rmd to output a PDF document is always annoying. Well, the source is &lt;strong&gt;tex&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RStudio&lt;/strong&gt; uses &lt;strong&gt;knitr&lt;/strong&gt; to covert the Rmd document into md document. Then it uses &lt;strong&gt;Pandoc&lt;/strong&gt; to convert the md document into tex document. Then they actually use &lt;strong&gt;tex&lt;/strong&gt; engine such as pdflatex or xelatex to get PDF document.&lt;/p&gt;
&lt;p&gt;Why Chinese would not display? This issue happens at the last step. By default, some templates such as beamer in &lt;strong&gt;RStudio&lt;/strong&gt; use pdflatex. However, you might need CJK package. However you would need to use CJK environment to display Chinese. I don&amp;rsquo;t think it is a good way and you need to write ugly documents.&lt;/p&gt;
&lt;p&gt;xeCJK package would be preferred because you only need to set up the font for your Chinese and you will get the output. However, such configuration need you use xelatex to compile you documents.&lt;/p&gt;
&lt;p&gt;For the beamer template in &lt;strong&gt;RStudio&lt;/strong&gt;, they use pdflatex. So the first way to show Chinese is telling &lt;strong&gt;Pandoc&lt;/strong&gt; to use xelatex other than pdflatex. You could set up such command in the yaml.&lt;/p&gt;
&lt;p&gt;The second issue is the font. When you use xelatex(actually xeCJK package), you need to set the font for CJK charactors such as Chinese. Maybe you could try the following yaml to use a font without sources.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
title: &amp;quot;中文测试&amp;quot;
author: &amp;quot;Yufree&amp;quot;
date: &amp;quot;2016年9月19日&amp;quot;
CJKmainfont: FandolFang
output:
  beamer_presentation:
    latex_engine: xelatex
---
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Not everyone knows how to find the right name of a font. However, the updated ctex package solved such problem. They use some default setting to avoid the font issue. All you need to do is use the ctex package for your tex template.&lt;/p&gt;
&lt;p&gt;We might also use yaml:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
title: &amp;quot;中文测试&amp;quot;
author: &amp;quot;Yufree&amp;quot;
date: &amp;quot;2016年9月19日&amp;quot;
header-includes:
  - \usepackage{ctex}
output: 
  beamer_presentation:
    latex_engine: xelatex
---
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;OK, now you would see Chinese in your Beamer PDF slides.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Three solutions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use CJK packages along with pdflatex (Not recommanded, only for Guru from 20 century)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set you font yourself in the yaml with xelatex (for Geek)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use the ctex package in your yaml (for everyone)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For Chinese in the figure and pdf, check &lt;a href=&#34;http://yufree.cn/blog/2014/07/21/rmd-to-pdf.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Basic idea behind cluster analysis</title>
      <link>https://yufree.cn/en/2016/09/11/basic-idea-cluster/</link>
      <pubDate>Sun, 11 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2016/09/11/basic-idea-cluster/</guid>
      <description>&lt;p&gt;After we got a lot of samples and analyzed the concentrations of many compounds in them, we may ask about the relationship between the samples. You might have the sampling information such as the date and the position and you could use boxplot or violin plot to explore the relationships among those categorical variables. However, you could also use the data to find some potential relationship.&lt;/p&gt;
&lt;p&gt;But how? if two samples&#39; data were almost the same, we might think those samples were from the same potential group. On the other hand, how do we define the &amp;ldquo;same&amp;rdquo; in the data?&lt;/p&gt;
&lt;p&gt;Cluster analysis told us that just define a &amp;ldquo;distances&amp;rdquo; to measure the similarity between samples. Mathematically, such distances would be shown in many different manners such as the sum of the absolute values of the differences between samples.&lt;/p&gt;
&lt;p&gt;For example, we analyzed the amounts of compound A, B and C in two samples and get the results:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Compounds(ng)&lt;/th&gt;
&lt;th&gt;A&lt;/th&gt;
&lt;th&gt;B&lt;/th&gt;
&lt;th&gt;C&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sample 1&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sample 2&lt;/td&gt;
&lt;td&gt;54&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The distance could be:&lt;/p&gt;
&lt;p&gt;$$
distance = |10-54|+|13-23|+|21-16| = 59
$$&lt;/p&gt;
&lt;p&gt;Also you could use the sum of squares or other way to stand for the similarity. After you defined a &amp;ldquo;distance&amp;rdquo;, you could get the distances between all of pairs for your samples. If two samples&#39; distance was the smallest, put them together as one group. Then calculate the distances again to combine the small group into big group until all of the samples were include in one group. Then draw a dendrogram for those process.&lt;/p&gt;
&lt;p&gt;The following issue is that how to cluster samples? You might set a cut-off and directly get the group from the dendrogram. However, sometimes you were ordered to cluster the samples into certain numbers of groups such as three. In such situation, you need K means cluster analysis.&lt;/p&gt;
&lt;p&gt;The basic idea behind the K means is that generate three virtual samples and calculate the distances between those three virtual samples and all of the other samples. There would be three values for each samples. Choose the smallest values and class that sample into this group. Then your samples were classified into three groups. You need to calculate the center of those three groups and get three new virtual samples. Repeat such process until the group members unchanged and you get your samples classified.&lt;/p&gt;
&lt;p&gt;OK, the basic idea behind the cluster analysis could be summarized as define the distances, set your cut-off and find the group. By this way, you might show potential relationships among samples.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Basic idea behind principal components analysis</title>
      <link>https://yufree.cn/en/2016/08/31/basic-idea-pca/</link>
      <pubDate>Wed, 31 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2016/08/31/basic-idea-pca/</guid>
      <description>&lt;p&gt;For environmental scientist, data analysis might be the only way to show your ability when you get the data from observation. I found many students even researcher showed their data in a bad way and many data analysis pattern just came from certain one paper. However, data analysis methods always have their scopes and some methods might just not suit your cases.&lt;/p&gt;
&lt;p&gt;Thanks to data analysis software, you need not to calculate some values by hand. But to make their usage clear, you need to know the basic idea. I will show some basic ideas behind certain method in a few posts. The first one is principal components analysis(PCA).&lt;/p&gt;
&lt;p&gt;In most cases, PCA is used as an exploratory data analysis(EDA) method. In most of those most cases, PCA is just served as visualization method. I mean, when I need to visualize some high-dimension data, I would use PCA.&lt;/p&gt;
&lt;p&gt;So, the basic idea behind PCA is compression. When you have 100 samples with concentrations of certain compound, you could plot the concentrations with samples&#39; ID. However, if you have 100 compounds to be analyzed, it would by hard to show the relationship between the samples. Actually, you need to show a matrix with sample and compounds (100 * 100 with the concentrations filled into the matrix) in an informal way.&lt;/p&gt;
&lt;p&gt;The PCA would say: OK, guys, I could convert your data into only 100 * 2 matrix with the loss of information minimized. Yeah, that is what the mathematical guys or computer programmer do. You just run the command of PCA. The new two &amp;ldquo;compounds&amp;rdquo; might have the cor-relationship between the original 100 compounds and retain the variances between them. After such projection, you would see the compressed relationship between the 100 samples. If some samples&#39; data are similar, they would be projected together in new two &amp;ldquo;compounds&amp;rdquo; plot. That is why PCA could be used for cluster and the new &amp;ldquo;compounds&amp;rdquo; could be referred as principal components(PCs).&lt;/p&gt;
&lt;p&gt;However, you might ask why only two new compounds could finished such task. I have to say, two PCs are just good for visualization. In most cases, we need to collect PCs standing for more than 80% variances in our data if you want to recovery the data with PCs. If each compound have no relationship between each other, the PCs are still those 100 compounds. So you have found a property of the PCs: PCs are orthogonal between each other.&lt;/p&gt;
&lt;p&gt;Another issue is how to find the relationship between the compounds. We could use PCA to find the relationship between samples. However, we could also extract the influences of the compounds on certain PCs. You might find many compounds showed the same loading on the first PC. That means the concentrations pattern between the compounds are looked similar. So PCA could also be used to explore the relationship between the compounds.&lt;/p&gt;
&lt;p&gt;OK, next time you might recall PCA when you need it instead of other paper showed them.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Metabolomics workflow in Rstudio</title>
      <link>https://yufree.cn/en/2016/08/21/meta-workflow/</link>
      <pubDate>Sun, 21 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2016/08/21/meta-workflow/</guid>
      <description>&lt;p&gt;I have moved to Canada for about three weeks. Now I am a PostDoc in University of Waterloo. I will handle two projects about &lt;strong&gt;in silico&lt;/strong&gt; studies in analytical chemistry. Well, I treated them as another data and modeling-driven interdisciplinary studies.&lt;/p&gt;
&lt;p&gt;The first step is building the data analysis envrionment for group members. Since I could set down such envrionment on a super computer with RAM 128 GB, I preferred to use R and xcms for metabolomics data analysis.&lt;/p&gt;
&lt;p&gt;For a well-trained analytical chemist, software or programming related stuff is always something agonizing. However, for degree or promotion, researchers have to learn related contents. xcms online is well-designed metabolomics data analysis tool for user with limited coding experiences. Actually, the earlier online version might come from xcms package for R.&lt;/p&gt;
&lt;p&gt;If you know the more details of data processing, you might get more insights for the data. Understanding the each steps might cost you whole day. But I also want to show them in my way. Such process would be helpful if you want to make further development to answer your scientific problems.&lt;/p&gt;
&lt;p&gt;Here is the &lt;a href=&#34;http://yufree.cn/metaworkflow/&#34;&gt;workflow&lt;/a&gt; in Rstudio and a brife &lt;a href=&#34;http://yufree.cn/notes/xcms.html&#34;&gt;version&lt;/a&gt; in Chinese.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Structure Prediction of Methyoxy-polybrominated diphenyls ethers (MeO-PBDEs) through GC-MS analysis of their corresponding PBDEs</title>
      <link>https://yufree.cn/en/2016/02/11/my-second-paper/</link>
      <pubDate>Thu, 11 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2016/02/11/my-second-paper/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is a paper with many rejection and comments. It was finally published by &lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S0039914016300455&#34;&gt;Talanta&lt;/a&gt; with DOI 10.1016/j.talanta.2016.01.047. One &lt;a href=&#34;http://www.smithsonianmag.com/smart-news/half-academic-studies-are-never-read-more-three-people-180950222/?no-ist&#34;&gt;study&lt;/a&gt; said the average read times of an academic paper was no more than 3. In my case, at least 11 reviewers had read this paper before published and I thanked all of them though some of them really misunderstand my idea.&lt;/p&gt;
&lt;p&gt;The basic idea before the structure prediction is that the combination of two qualitative methods. Usually, we use full scan of mass spectrum to get some rules about the structure such as the position of the substitute group of certain compound. Meanwhile, the retention time of seperation process also showed us some information about the compound. For unknown MeO-PBDEs, mass spectrum could tell us the position of MeO- group while can not show us the position of the Br atoms. Chromatography could show us the Br atoms position of PBDEs while not MeO-PBDE. what I should do is that building a model to connect those two information sources to get the structure of unknown MeO-PBDEs in certain samples.&lt;/p&gt;
&lt;p&gt;But how? I collected 32 MeO-PBDEs and corresponding PBDEs and get the retention time of those pairs under the same analysis condition. I found we could use those data to make a connection between the information from mass spectrum and chromatography. The basic model is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
RT_{MeO-PBDEs} = RT_{PBBDEs} + Group position
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For different positions, the mass spectrum could show a constant. For example, if BDE-47’s retention time is 21.753 min, the ortho- substitute MeO-BDE-47 would show a retention time of 25.648min. The differences of those retention time pairs is a constant around 4. We use regression analysis to get an estimation of such constants. Then when we get a potential peak of MeO-PBDEs. Mass spectrum would tell us the mass, the numbers of Br and the position of MeO- group. Then we just test the standards of potential PBDEs and use the models to check the position of Br atoms. The trick is that the standards of PBDEs were 209 and 837 for MeO-PBDEs. We use small numbers standards to cover large unknown standards(no available standards) . Another thick is that we could use multiple dislike columns to build such models and then the estimation would be much accurate.&lt;/p&gt;
&lt;p&gt;This is just a try. I used this method to get three unknown structures of MeO-PBDEs. However, the most important part is that we should try to summarize the data from different analysis method to build a much stronger model. In many studies, scientists use many independent analysis method to explain one problem. I think this is also a model and when we build them, the left could be thrown to computer or automation
. We human should do smart things!&lt;/p&gt;
&lt;p&gt;If you have questions about this paper, comment here and I will reply as soon as possible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HPLC 2015 Beijing</title>
      <link>https://yufree.cn/en/2015/09/30/hplc-2015/</link>
      <pubDate>Wed, 30 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2015/09/30/hplc-2015/</guid>
      <description>&lt;p&gt;Last week 43rd International Symposium on High Performance Liquid Phase Separations and Related Techniques(HPLC 2015 Beijing) was held at the Beijing International Conference Center in Beijing, China. For my tutor was the chair of this conference, I stayed there for three days and made a poster presentation. Here is some tips from the HPLC 2015 Beijing.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;NMR is your mother, MS is your love and the LC is your superhero.&amp;rdquo; Prof. Peter Schoenmakers said.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The hairstyle of Prof. Jonothan Sweedler is impressive and I don&amp;rsquo;t know if he had played punk.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Though Prof. Robert Kennedy has rejected my paper before, I admit he is handsome.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Girls from South Korea were really beauty. In HPLC 2017 Jeju we might see them again.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2D and 3D LC were really popular. However, I found they were limited to a few applications. Yeah, they showed a fantastic column efficiency but in practice they somewhat like the art of butchering dragons while no dragons for them. In environmental analysis, I think new complex matrix effect in various samples might be a dragon for them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Superficially Porous Particles are interesting and attractive. Better choice for start up group.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Modeling in HPLC is really native and ignore the development of novel methods in computer science or data science.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mass spectrum is the best spouse for (U)HPLC. However, omics treat the features or profiles more than certain compounds.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some groups have noticed the data mining of hyphenated method data and I think such issue is the best application for data science.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Chemical modification of certain materials or nanomatreials to gain the selectivity are just permutation and combination. However, they could publish good papers&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Oral presentation for PI is very important. Some Chinese PI need to learn how to make presentation and if English is not good, try to list it on slides.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Young scientists are the future of HPLC and I am really appreciate the workshop for starters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I bet you only care the beauty and handsome in this post.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;See you next HPLC(well, I always need financial support)!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/blogcn/figure/hplc2015.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;p.s. Finally I could vote on the &lt;a href=&#34;http://stackoverflow.com/users/3083491/yufree&#34;&gt;stackoverflow&lt;/a&gt;!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Data Analysis Similarity between Microarray and GC-MS</title>
      <link>https://yufree.cn/en/2015/09/11/microarry-vs-ms/</link>
      <pubDate>Fri, 11 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2015/09/11/microarry-vs-ms/</guid>
      <description>&lt;p&gt;I have finished &lt;a href=&#34;https://courses.edx.org/courses/HarvardX/PH525x/1T2014/info&#34;&gt;Data Analysis for Genomics(HarvardX-PH525x)&lt;/a&gt; by Prof. Rafael A Irizarry and Dr. Michael I Love for more than a year until recently I realised the data analysis similarity between microarray and Gas chromatography–mass spectrometry(GC-MS).&lt;/p&gt;
&lt;p&gt;When we talked about data analysis of microarray, we use different genes or probes as the rows and different samples as the columns. The responses are fluorescence signals.&lt;/p&gt;
&lt;p&gt;When we talked about data analysis of microarray, we use different m/z as the rows and different retention times as the columns. The responses are count signals.&lt;/p&gt;
&lt;p&gt;Interesting, the Total Ion Chromatorgraphy(TIC) is widely used in GC-MS while heatmap in microarray. How about show the heatmap of GC-MS and TIC of heatmap.&lt;/p&gt;
&lt;p&gt;Wait, we couldn&amp;rsquo;t do a thing without meanings. Why use TIC in GC-MS? Because we always think one compound would show at certain retention time. However, under EI source or hard ionization, one compound could show many m/z responses. In environmental analysis, the matrix effect might also show responses. Then we got the meanings: the heatmap of GC-MS would show a visualization of matrix effect.&lt;/p&gt;
&lt;p&gt;How about TIC in microarray? I don&amp;rsquo;t think such plot has meanings because there is no time dependences in the samples of microarray.&lt;/p&gt;
&lt;p&gt;But when the data could be shown in heatmap, we might employ some noise reduction methods to ease the matrix effect. The following two heatmaps were a native &amp;ldquo;before and after&amp;rdquo; results processed by some microarray data analysis methods. Yeah, now I think it is OK to use such method to reduce the matrix effect in environmental samples.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/blogcn/figure/h2585bg.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/blogcn/figure/h2585diffgcms.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Wait, my paper is writing. And I will show the details of such method soon(maybe or might be).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to deal with the less than obvious?</title>
      <link>https://yufree.cn/en/2015/09/05/loq/</link>
      <pubDate>Sat, 05 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2015/09/05/loq/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In environmental analysis, we often target a lot of compounds to get a detailed profile of certain pollution. However, due to different environmental process, not all of the compounds could be detected by our analysis method. For example, when I analysed PBDEs in 100 sludge samples, only a few congeners such as BDE-47, BDE-99, BDE-209 could be found in all of them. The other congerners such as BDE-28, BDE-17, BDE-153 could only be found in about 80% of them. Also some congerners such as BDE-77, BDE-53, BDE-17 were got in only 50% of them. Thus, we faced a problem: how to perform analysis such as summary or hypothesis testing or regression when some values were not detected by our analysis method.&lt;/p&gt;
&lt;div id=&#34;summary-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary Statistics&lt;/h2&gt;
&lt;p&gt;Often a junior PhD student would be told to just use half of the LOQ(limit of quantitative) to substitute the N.D. of certain compounds. However, such method is not recommended for publishing papers. Such method would change the distribution of the concentrations. But what is the raw distribution of the environmental data?&lt;/p&gt;
&lt;p&gt;Recently Prof. Ronald A. Hites from Indiana University published a paper about such issue and found such log-normally distribution could be really fit well for PCBs in about 2900 samples.For a log-normally distributed data, the geometric means are the unbiased estimation of the original median. So both of the geometric means and the median could be used to make a summary for the data with less than obvious.&lt;/p&gt;
&lt;p&gt;That paper published on ES&amp;amp;T letter also showed a interesting method to find the LOQ as follows:&lt;/p&gt;
&lt;p&gt;Assuming that the PCB concentrations are detected with a decreasing probability below their LOQ and that this probability is not zero. That means:&lt;/p&gt;
&lt;p&gt;For x &amp;gt; LOQ:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(x) = 1 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For x &amp;lt; LOQ:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(x) = \frac{1}{LOQ - x + 1} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Such &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; means a weight for the concentrations we get from our samples. The PDF(probability density function) of concentrations would be a log-normally distribution as discussed before. So the actually PDF of concentration distribution we got would be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[pdf(x) = a*exp(\frac{-(x-\mu)^2}{2*\sigma})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[PDF(x) = p(x) * pdf(x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, none of the &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(LOQ\)&lt;/span&gt; were known. Thus we could use our data and the model to fit those parameters.&lt;/p&gt;
&lt;p&gt;Prof. Hites found that such method could be robust even for small data and interesting, such fitted LOQ would be much higher than we get from the analysis method. So our LOQ might not be sensitivity as we thought.&lt;/p&gt;
&lt;p&gt;My comment is that maybe the assumption of &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; just get the nature of the trace analysis. If we insist the analysis method is sensitivity, then maybe the LOQ we fitted is just another environmental threshold which implied something.&lt;/p&gt;
&lt;p&gt;However, we could use such method to simulate the less than obvious for our data. In fact, MLE(maximum-likelihood estimation) and some robust method could be used to impute our data. Then we need to do some inferences about our data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-testing-and-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hypothesis Testing and Regression&lt;/h2&gt;
&lt;p&gt;In most cases, we could not directly use an imputed data for hypothesis testing and regression. However, some statistical methods could help us to find the answer.&lt;/p&gt;
&lt;p&gt;When the missing values is less than 20%, Kendall’s robust line fit could be used. Such method get the pairwise slope of all of the data with missing values zero or imputed data. Then the median of the slope were used to fit the data.&lt;/p&gt;
&lt;p&gt;When the missing values is less than 50% and more than 20%. Tobit regression and logistic regression could be used. For Tobit regression, we use MLE instead of OLS(ordinary least squares) to fit the parameters. For logistic regression, we could treat the detected samples as 1 and non-detected samples as 0 to make analysis. However, such method could only show little information form concentration distribution. Anyway only one or two high concentration might also show little information.&lt;/p&gt;
&lt;p&gt;When little compounds detected, only the logistic regression and contingency table analysis with Chi-square test would be performed.&lt;/p&gt;
&lt;p&gt;When you want to discuss the relationship between two group of samples with imputed data, only the robust method such as Kendall’s &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; and Spearman’s &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; could be used.&lt;/p&gt;
&lt;p&gt;Ok, the core of this post is “Less thans are valuable data.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;1.Hites, R. A. A Statistical Approach for Left-Censored Data: Distributions of Atmospheric Polychlorinated Biphenyl Concentrations near the Great Lakes as a Case Study. Environmental Science &amp;amp; Technology Letters 150831163839007 (2015). &lt;a href=&#34;doi:10.1021/acs.estlett.5b00223&#34; class=&#34;uri&#34;&gt;doi:10.1021/acs.estlett.5b00223&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2.Helsel, D. R. Less than obvious - statistical treatment of data below the detection limit. Environ. Sci. Technol. 24, 1766–1774 (1990).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ISOTOPES 2015</title>
      <link>https://yufree.cn/en/2015/06/30/isotopes-2015/</link>
      <pubDate>Tue, 30 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2015/06/30/isotopes-2015/</guid>
      <description>&lt;p&gt;I went to Israel at the late June for Isotopes 2015 conference. This is my first time trip abroad and it is my pleasure to visit Jerusalem and make friends with scholars from all over the world. This is a quick note of some important tips about isotopes 2015.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Thank you, all of the organisers of the conference!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Thank you, all of the participants and I learned a lot!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I have made the web application &lt;a href=&#34;https://yufree.shinyapps.io/MIRtools/&#34;&gt;avaliable&lt;/a&gt; on my web site and just try it!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Thank you, Dr. Martin Elsner. Your comments are very important for me.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The old city of Jerusalem is definitly holyland for the three major Abrahamic religions and show a very different view compared with old city such as Beijing and Xi&amp;rsquo;an in China.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Food is delicious and finally I get used to the fork and knife!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I am the only one from China for this conference but it is an amazing coincidence to meet Xi Wei, who also get the Bachelor&amp;rsquo;s degree from Shandong University.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dead sea and Masada should never be missing if you go to Israel.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Thank you, All my new friends! Though I can&amp;rsquo;t spell all of your names, leave message via &lt;a href=&#34;mailto://yufree@live.cn&#34;&gt;Email&lt;/a&gt; or guestbook on this website if you come to China for a visit. You are bound to be received warmly here!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Thank you, Dr. Anil Modak. I use your photo as a summary.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/blogcn/figure/Isotopes2015.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to SP-ICP-MS</title>
      <link>https://yufree.cn/en/2015/01/03/intro-sp-icp-ms/</link>
      <pubDate>Sat, 03 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2015/01/03/intro-sp-icp-ms/</guid>
      <description>&lt;p&gt;Single particle Inductively coupled plasma mass spectrometry(SP-ICP-MS) is a novol method for detect, quantify, and characterize nanoparticles. Its theoretical basis was founded by Dr. Degueldre et al. at 2004. Here I would like to show a much more simplified version of this theory and show you how to perform a SP-ICP-MS analysis.&lt;/p&gt;
&lt;h2 id=&#34;single-particle&#34;&gt;Single Particle&lt;/h2&gt;
&lt;p&gt;The core concept in SP-ICP-MS is single particle. How to understand single particle? Single means every scan time on MS would get no more than one particle&amp;rsquo;s signals. So how to get such signal? two ways: limit the amounts in certain scan time(dwell time) get into the MS or limit the dwell time to cut the particle flows into real pieces. We could write them in the following equation:&lt;/p&gt;
&lt;p&gt;$$\frac{F \cdot C \cdot V \cdot t}{m} \leq 1$$&lt;/p&gt;
&lt;p&gt;C stands for the concentration of certain particles, V stands for the flow rate of the injection, t stands for the dwell time and m stand for the mass of one particles. F means the measurable fraction of the injected particles. When the instrument could satisfy the equation, the collected data could be used for SP-ICP-MS. When you scan the real data, you might see a lot of low signals which is the same with blank data. Those signals could be used as a background and when such signals occupy a large percentage of your data, you data could certainly be used for SP-ICP-MS and you might need a long time to collect enough data for a distribution.&lt;/p&gt;
&lt;h2 id=&#34;get-the-diameter-distribution-of-certain-standard-particles&#34;&gt;Get the diameter distribution of certain standard particles&lt;/h2&gt;
&lt;p&gt;OK, when you know the source of such data for SP-ICP-MS, we should prepare some standards of nano particles, for example, NIST 60nm silver nano particles. We should treat such diameter as an average of the whole particles and get the mass of single particles by the following equations:&lt;/p&gt;
&lt;p&gt;$$m = \frac{4}{3} \pi (\frac{d}{2})^3 \rho $$&lt;/p&gt;
&lt;p&gt;When we fixed the F(typically 0.01~0.05), the dwell time and the flow rate, we could get a max concentration of your standards for SP-ICP-MS analysis by the first equation. In most of the litratures, the ICP-MS currently used could not get the signals when the sliver nano particle&amp;rsquo;s diameter less than about 20nm. In such state, the data look like the data collected from blank. You must know for different element the detection limit of diameter is different. Generally speaking, when the atomic weight is heavior, we could get a lower detectable diameter of such pure element. However, for such nano particles have more than one elements, you need a correction.&lt;/p&gt;
&lt;p&gt;OK, with a proper concentration for SP-ICP-MS, we could get a standard data for SP-ICP-MS analysis(fixed time such as 1 min) and a blank data for noise. Then we could do the following analysis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;subset the singal bigger than the average noise as partical signal data&lt;/li&gt;
&lt;li&gt;the signal data should minus the average noise&lt;/li&gt;
&lt;li&gt;treat the average signal data as the average diameter of nano particle and get the relationship(r) between the signal data(S) and the particle mass(M) as S = r(M)&lt;/li&gt;
&lt;li&gt;use such relationship to convert each signal into particle mass and now you could get the mass distribution of nano particle&lt;/li&gt;
&lt;li&gt;use the second equation to change the mass into diameter and you will get the diameter distribution of standard nano particle.&lt;/li&gt;
&lt;li&gt;Since you could fix the dwell time and flow rate, you could also get the F for your analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here when you perform a SP-ICP-MS analysis for standards, the most important parameters you get is the relationship between the signal and the mass. Also the F could be helpful when you want to perform a quantitative analysis.&lt;/p&gt;
&lt;h2 id=&#34;get-the-diameter-distribution-of-certain-particles-in-samples&#34;&gt;Get the diameter distribution of certain particles in samples&lt;/h2&gt;
&lt;p&gt;When you get a samples, I suggest you perform a normal quantitative analysis for your target element to get the concertration. Then calculate according to the first equation and dilute your sample to meet the requirment for SP-ICP-MS. When you get the sample data, perform the following analysis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;subset the singal bigger than the average noise as partical signal data&lt;/li&gt;
&lt;li&gt;the signal data should minus the average noise&lt;/li&gt;
&lt;li&gt;use the relationship between the signal and the mass to get the mass and diameter distribution of the partical in your sample&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OK, if you want to get the partical number concertration, treat each signal bigger than the average noise as one particle and count the numbers of signal in certain time you will get the partical number concertration with the help of F.&lt;/p&gt;
&lt;h2 id=&#34;some-issues&#34;&gt;Some issues&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the distribution of particles are often log-normal while the ions show poisson distribution which could be used to find if the samples have nano particles&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;try to use a larger particles to get the relationship between the signal and the mass because small particles show no signal&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TEM could be used to confirm the distribution of your result from SP-ICP-MS&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;application&#34;&gt;Application&lt;/h2&gt;
&lt;p&gt;I wrote a web application for SP-ICP-MS, you could upload your standard and sample data and then get the diameter distribution of the samples. Click &lt;a href=&#34;https://yufree.shinyapps.io/spicpms/demo.Rmd&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Degueldre, C., P. -Y. Favarger, and C. Bitea. “Zirconia Colloid Analysis by Single Particle Inductively Coupled Plasma–mass Spectrometry.” Analytica Chimica Acta 518, no. 1–2 (August 2, 2004): 137–42. doi:10.1016/j.aca.2004.04.015.&lt;/li&gt;
&lt;li&gt;Kiss, L. B., J. Söderlund, G. A. Niklasson, and C. G. Granqvist. “New Approach to the Origin of Lognormal Size Distributions of Nanoparticles.” Nanotechnology 10, no. 1 (March 1, 1999): 25. doi:10.1088/0957-4484/10/1/006.&lt;/li&gt;
&lt;li&gt;Laborda, Francisco, Javier Jiménez-Lamana, Eduardo Bolea, and Juan R. Castillo. “Selective Identification, Characterization and Determination of Dissolved silver(I) and Silver Nanoparticles Based on Single Particle Detection by Inductively Coupled Plasma Mass Spectrometry.” Journal of Analytical Atomic Spectrometry 26, no. 7 (July 1, 2011): 1362–71. doi:10.1039/C0JA00098A.&lt;/li&gt;
&lt;li&gt;Lee, Sungyun, Xiangyu Bi, Robert B. Reed, James F. Ranville, Pierre Herckes, and Paul Westerhoff. “Nanoparticle Size Detection Limits by Single Particle ICP-MS for 40 Elements.” Environmental Science &amp;amp; Technology 48, no. 17 (2014): 10291–300. doi:10.1021/es502422v.&lt;/li&gt;
&lt;li&gt;Mitrano, Denise M., Emily K. Lesher, Anthony Bednar, Jon Monserud, Christopher P. Higgins, and James F. Ranville. “Detecting Nanoparticulate Silver Using Single-Particle Inductively Coupled Plasma–mass Spectrometry.” Environmental Toxicology and Chemistry 31, no. 1 (2012): 115–21. doi:10.1002/etc.719.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Using recommender systems to fill the missing data</title>
      <link>https://yufree.cn/en/2014/11/26/recommender-system/</link>
      <pubDate>Wed, 26 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2014/11/26/recommender-system/</guid>
      <description>&lt;p&gt;Recommender systems were often used to show the candidates such as books, movies and music for certain user. If you bought something at online shop, you might get a spam email listing something the shop recommended and there must be a recommender systems for you. You might get confused by such system: how does they know what you would buy?&lt;/p&gt;
&lt;p&gt;If you know basic concepts about machine learning, you might say: surely there would be a supervised learning algorithm for this problem. Yes, when you have huge data, you might train your algorithm with rating as your output and items as your input. But those data would always be sparse. There would be thousands of books and thousands of users but the input and the output would not be a normal distribution and should be some groups connected with each other. In another words, the books and the users data would have their own structures. If you like novels, most of your books would be novels and the references of your rating on a dictionary would be missing. However, we found another person with the similar rating as you on novels and a lot of rating on dictionaries. So your reasonable rating on a dictionary would be similar to that guy. On the other hand, the novel and the dictionary would have some inner features and those features could be used to group the books.&lt;/p&gt;
&lt;p&gt;OK, here the original problems had been convert to the cluster problems on the input and the output. After the cluster, we should train a model to make a connection with the items&#39; groups and users&#39; groups. Here an algorithm with the update of both cluster on input and output at the same time would be reasonable.&lt;/p&gt;
&lt;p&gt;Firstly, the original data would show a matrix structure(M) with rows standing for the books, columns standing for the users and ratings in each cell. To make the algorithm run without error, we need a mask matrix to identify the missing data(R, 0 for missing and 1 for rating). So M.*R would show a matrix without missing data. Our task was finding two matrix(T and U). T should show the features of terms and U should show the corresponding features of the users. U * T should give us a matrix(N) like M and we could use min(N-M) as cost function to train our data. When we get the final N, we could fill the gaps with reasonable data. That is, we get the your ratings on the books you never read and order the rating. OK, we would send you an e-mail with the high rating books.&lt;/p&gt;
&lt;p&gt;You might say why we must get two unknown matrix U and T, why not just show me the N. N has no reasonable algorithms to get directly while U and T could be get with a optimization algorithms. We give the U a random initial numbers and get a optimized T, then we use such T to get the U, run in a circle and finally we could get those two. Meanwhile we could use U and T separately to explore the inner groups for books or describe our potential users. In fact, we could use collaborative filtering to get the U and T at the same time.&lt;/p&gt;
&lt;p&gt;This idea comes from machine learning but I think we could use such method to get missing data for environmental research. For example, we get huge data about the concentrations of 100 compounds for 100 cities while not all of the cities could analysis those 100 compounds: some analysis 60 and some analysis 80. Then such recommander system could be used to show missing data and the group information for both compounds and cities. But I doubt if traditional environmental scientist could understand such machine learning ideas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Display Chinese/Japanese/Korean in PDF from Rmd in RStudio</title>
      <link>https://yufree.cn/en/2014/07/21/rmd-to-pdf/</link>
      <pubDate>Mon, 21 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2014/07/21/rmd-to-pdf/</guid>
      <description>&lt;p&gt;New RStudio shows many useful features to make dynamic documents. We could write Rmd and output word document and PDF. However, when I try to write Chinese in Rmd and convert it into PDF, the Chinese characters is missing. @Yihui added a new feature &lt;code&gt;fig.showtext&lt;/code&gt; which allow us to show Chinese in the plot. Still, the Chinese words in the content are missing. I refer to a lot of posts and find the only way might be using the Rnw to write plain tex document. But I just want to use Rmd!&lt;/p&gt;
&lt;p&gt;Then I review the PDF generation process and find the key is the md to tex. Rmarkdown use pandoc as the converter and pandoc just use some template. If we want to show Chinese, we need to hack the template and add the support of Chinese. Just go the &amp;ldquo;R/i686-pc-linux-gnu-library/3.1/rmarkdown/rmd/latex&amp;rdquo; where is the template located and add \usepackage{xeCJK}  and \setCJKmainfont{youfont}  before \begin{document}. Youfont stand for the character font installed in your computer which is used to show in the PDF. Remember to use xetex to process the tex or you may try CJK solution. OK, save the hack and now we could use Chinese both in the main text and the plot. More information could be found &lt;a href=&#34;https://github.com/yihui/knitr/issues/799&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The same way could be applied for Japanese and Korean. Just add the package support for your language.&lt;/p&gt;
&lt;h1 id=&#34;update-20140722&#34;&gt;update 20140722&lt;/h1&gt;
&lt;p&gt;@Yixuan write a &lt;a href=&#34;http://statr.me/2014/07/showtext-with-knitr/&#34;&gt;post&lt;/a&gt; on the usage of showtext package in knitr. Also @Yihui showed adding the packages in the header.tex and modified the yaml of the Rmd as the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
output:
  pdf_document:
    includes:
      in_header: header.tex
---
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;will also solve the font problems. You may also use the tex code in the Rmd and pandoc will complie the code like in the Rnw. Thanks a lot, @Yihui and @Yixuan!&lt;/p&gt;
&lt;h1 id=&#34;update-20150119&#34;&gt;update 20150119&lt;/h1&gt;
&lt;p&gt;Chinese issue has been solved by the ctex templete in rticles package(on CRAN now). You might just use the following code to install the package.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# install pandoc first
install.packages(c(&#39;rmarkdown&#39;,&#39;rticles&#39;))
rmarkdown::draft(&amp;quot;MyCtexArticle.Rmd&amp;quot;, template = &amp;quot;ctex&amp;quot;, package = &amp;quot;rticles&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;updata-20160919&#34;&gt;updata 20160919&lt;/h1&gt;
&lt;p&gt;Aftet the update of ctex, use yaml as shown in this &lt;a href=&#34;http://yufree.cn/blog/2016/09/19/beamer-in-chinese.html&#34;&gt;post&lt;/a&gt; would directly show the Chinese/Japanese/Korean:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
title: &amp;quot;中文／にほんご／韓國語&amp;quot;
author: &amp;quot;Yufree&amp;quot;
date: &amp;quot;2016年9月19日&amp;quot;
header-includes:
  - \usepackage{ctex}
output: 
  pdf_document:
    latex_engine: xelatex
---
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;if you preferred xeCJK package to handle this issue, use CJKmainfont: [fontname] in yaml at the top level to set your font.&lt;/p&gt;
&lt;p&gt;You might find Rmd templates for Chinese/Janpanese/Korean &lt;a href=&#34;https://github.com/yufree/democode/tree/master/cjk&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using rcdk package for QSPR</title>
      <link>https://yufree.cn/en/2014/05/30/qspr-rcdk/</link>
      <pubDate>Fri, 30 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2014/05/30/qspr-rcdk/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;code&gt;R&lt;/code&gt; is a handy tool for modeling, so there must be some packages for Quantitative structure–activity relationship(QSPR) in Chemistry. Recently I wanted to summarize some papers for a presentation, then I found rcdk package, which is a useful tool for QSPR. However, little posts about this topic for beginner. As an absolutely beginner, I make this post as a note for the whole process and add comments for someone else to reproduce a QSPR model by their own.&lt;/p&gt;
&lt;p&gt;First,you need the following knowledge:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Chemistry Development Kit(CDK) is powerful, we will use their function to read in the molecular and calculate some descriptor via &lt;code&gt;rcdk&lt;/code&gt; package and no need to install CDK and their function has been included in the package&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Just &lt;code&gt;install.packages(c(&#39;rJava&#39;,&#39;rcdk&#39;))&lt;/code&gt; is enough for a R user&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Copy the smiles files or the sdf files to your work directory or just use a demo data &lt;code&gt;bpdata&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Then we begin&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;library(rcdk)
data(bpdata)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first column is the SMILES vector of the molecular structures so we use parse.smiles to read it&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mols &amp;lt;- parse.smiles(bpdata[, 1])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So rcdk will convert the smiles into a java object this object could be used to get the milecular descriptor. Also you need to know the class of mols is a list. rcdk has five ctegories of descriptor we think the topological descriptor may relate to the Boiling Point in this tests.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;desc.names &amp;lt;- get.desc.names(&amp;quot;topological&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we get the names of those topological descriptors.Up to now, we know the structures and the descriptors’ name. Then we will get the descriptors.&lt;/p&gt;
&lt;p&gt;For the list we need to use lapply or sapply to apply the descriptors caculator function with the descriptors’ name in the desc.names.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data &amp;lt;- lapply(mols, eval.desc, desc.names)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is also a list so we need to unlist and make a dataframe for QSPR model. Also we need to give the column a name.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df &amp;lt;- data.frame(matrix(unlist(data), nrow = 277, byrow = T))
colnames(df) &amp;lt;- colnames(data[[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to remove some desciptors unchanged&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df2 &amp;lt;- df[, which(!apply(df, 2, sd) == 0)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK, here we get a data frame contained the descriptors we needed. Then we will use those &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; to make a &lt;span class=&#34;math inline&#34;&gt;\(f(X) = Y\)&lt;/span&gt;, which is a QSPR model.&lt;/p&gt;
&lt;p&gt;Define the response&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Y &amp;lt;- bpdata[, 2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Get a train set and a test set&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;train &amp;lt;- sample(1:277, 200)
traindataX &amp;lt;- df2[train, ]
traindataY &amp;lt;- Y[train]
testdataX &amp;lt;- df2[-train, ]
testdataY &amp;lt;- Y[-train]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Build a regression model with the leaps packages&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(&amp;quot;leaps&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define a function to predict the test data&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;predict.regsubsets = function(object, newdata, id, ...) {
form = as.formula(~.)
mat = model.matrix(form, newdata)
coefi = coef(object, id)
xvars = names(coefi)
mat[, xvars] %*% coefi}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A 5-fold cross-validation&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;k = 5
folds = sample(1:k, nrow(traindataX), replace = TRUE)
cv.errors = matrix(NA, k, 50, dimnames = list(NULL, paste(1:50)))

for (j in 1:k) {
best.fit = regsubsets(y = traindataY[folds != j],
                      x = traindataX[folds != j, ],
                      nvmax = 50, 
                      really.big = T, 
                      method = &amp;quot;forward&amp;quot;)
                      for (i in 1:50) {pred = predict.regsubsets(best.fit, traindataX[folds == j, ], id = i);
                                      cv.errors[j, i] = mean((traindataY[folds == j] - pred)^2)
                                      }
                }
mean.cv.errors = apply(cv.errors, 2, mean)
which.min(mean.cv.errors)

#38

reg.fwd = regsubsets(x = traindataX,
                     y = traindataY,
                     nvmax = 44, 
                     really.big = T, 
                     method = &amp;quot;forward&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We got a model with many variables on the train dataset, and now we could see the results on the test dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;val.errors &amp;lt;- rep(NA, 50)
for (i in 1:50) {
        pred = predict.regsubsets(reg.fwd, testdataX, id = i)
        val.errors[i] = mean((testdataY - pred)^2)
        }
which.min(val.errors)
#41&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So if you want to see the names of selected variables for prediction the selected QSPR model, you may use &lt;code&gt;coef&lt;/code&gt; to get what you want. For example, we show the coef of the selected 15 variables models by the following code.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;coef(reg.fwd, 15)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Intercept) khs.dCH2 khs.sOH khs.dO HybRatio VP.2&lt;/p&gt;
&lt;p&gt;310.981 -69.776 70.007 38.068 -61.738 58.648&lt;/p&gt;
&lt;p&gt;SPC.4 SPC.6 SC.4 VC.3 ATSm5 ATSc2&lt;/p&gt;
&lt;p&gt;-4.957 -3.223 -75.819 -42.528 1.890 35.852&lt;/p&gt;
&lt;p&gt;ATSc4 khs.aaS khs.sI C4SP3&lt;/p&gt;
&lt;p&gt;31.831 0.000 -8.693 10.937&lt;/p&gt;
&lt;p&gt;From the results I find the topological descriptors might be not good for the prediction for that no clear overfit were found in train and test dataset.&lt;/p&gt;
&lt;p&gt;OK, this is the whole process for using rcdk package for QSPR. Just modify the input, you will get what you want.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A very brief intro of shiny</title>
      <link>https://yufree.cn/en/2014/03/23/intro-of-shiny/</link>
      <pubDate>Sun, 23 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2014/03/23/intro-of-shiny/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;If you are a starter of shiny as me, this post may help your know some principle of shiny.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;what-is-shiny&#34;&gt;What is shiny?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;R based&lt;/li&gt;
&lt;li&gt;View as webpage hosted on servers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;give a request, show the result&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;backgroud-needed&#34;&gt;Backgroud needed&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Almost R only&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;why-shiny&#34;&gt;Why shiny?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Entirely extensible&lt;/li&gt;
&lt;li&gt;Display your data in an interactive way&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;how-shiny-works&#34;&gt;How shiny works?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Reactive Programming&lt;/p&gt;
&lt;p&gt;When the input changes, the R code will run again to get a refresh. Shiny makes the connections among R, input and the the output on your screen by some communication tags. So you can just focused on R code and let shiny do the other things.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;server.R&lt;/p&gt;
&lt;p&gt;You need write your R code here to tell shiny run what when the inputs change. The R code need a input and output values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ui.R&lt;/p&gt;
&lt;p&gt;You need tell the app user where is the input/output box, which values of the input should be transfer to R code and which values of the output should be display on the webpage. You can also custome your UI of app here if you have writen some basic html pages.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Render function in server.R&lt;/p&gt;
&lt;p&gt;Tell shiny the output values used in the app&amp;rsquo;s webpage. This function need to be re-ran when the input changes, so the render function must be assighed to a output values while contains the input values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RUN&lt;/p&gt;
&lt;p&gt;When you finish your server.R and ui.R, just &lt;code&gt;runApp()&lt;/code&gt; and you will get the great webpage.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;how-to-share-your-apps&#34;&gt;How to share your apps?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Push your shiny code on the github or make a zip file, the user can run it locally use the &lt;code&gt;runUrl&lt;/code&gt;,&lt;code&gt;runGitHub&lt;/code&gt; or &lt;code&gt;runGist&lt;/code&gt; in the shiny package.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can also build up a server with www connection and put your app there.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;learn-more&#34;&gt;Learn more?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.rstudio.com/shiny/lessons/Intro/&#34;&gt;Lessons&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://rstudio.github.io/shiny/tutorial/&#34;&gt;Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Mean, Variance and Something about Sample</title>
      <link>https://yufree.cn/en/2013/12/07/mean-and-variance/</link>
      <pubDate>Sat, 07 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2013/12/07/mean-and-variance/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;OK，I must clear the concepts of those terms because this is the third time I review my notes.&lt;/p&gt;
&lt;div id=&#34;description-of-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Description of Data&lt;/h2&gt;
&lt;p&gt;If one wants to descript a collection of data, the Mode, Median and Mean are the first three terms to be learned. They are used to show the whole data in one parameter. And yes, one value is enough and the most important property of a dataset is the center. So where do the center come from? Here, our formula could be written as a distance between a parameter and the every values in the dataset.&lt;/p&gt;
&lt;p&gt;So the Mode could be written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
\sum_{}^{}(M_{para} - x_{i})^0 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And the Median could be written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{}^{}(M_{para} - x_{i})^1 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then the Mean could be written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{}^{}(M_{para} - x_{i})^2 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Eh, I think the solution of the formula will show the Mean which make the distance smallest and yes, just make a diff. So here the description of the data is finished or just begin. Change the uppercase as you like. The origin idea came from &lt;a href=&#34;http://www.johnmyleswhite.com/notebook/2013/03/22/modes-medians-and-means-an-unifying-perspective/&#34;&gt;John Myles White&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mean&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mean&lt;/h2&gt;
&lt;p&gt;Here we get Mean to show the center of dataset. And mean of discrete random variable X could be wright in the following formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E[X] = \sum_x xp(x) 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For a continuous random variable,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E[X] = \int_{-\infty}^\infty t f(t)dt 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Mean tell us the center of the distribution because we just use the smallest distance. Now we want to known the spread of the data. So we use variance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variance&lt;/h2&gt;
&lt;p&gt;Here, we need to talk about moment in another view. The mean or the expected values is the first raw moment and the second central moment is variance. The &lt;a href=&#34;http://en.wikipedia.org/wiki/Moment_(mathematics)&#34;&gt;moment&lt;/a&gt; is another discription method of the distribution. Here we only use the formula of variance:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Var(X) = E[(X - \mu)^2] 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And the variance could be written as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Var(X) = E[X^2] - E[X]^2 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So how to understand the variance in probability?&lt;/p&gt;
&lt;div id=&#34;chebyshevs-inequality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chebyshev’s inequality&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we found the variance actually show the probaliblity of a observation in a distribution. That is just a description of spread of the distribution.&lt;/p&gt;
&lt;p&gt;Ok, we talk enough about the distribution itself. Next we will see the sample.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sample&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sample&lt;/h2&gt;
&lt;p&gt;The independent and identically distributed random variables are the default model for random samples. Under iid, we could use the probaliblity to give out the description of sample. So if the values of variables is uncorrelated, then the variance of the sum is the sum of the variances.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Var\left(\sum_{i=1}^n X_i \right) = \sum_{i=1}^n Var(X_i) 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we will get the variance of the mean of the sample:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
Var(\bar X) &amp;amp; = Var \left( \frac{1}{n}\sum_{i=1}^n X_i \right)\\ 
    &amp;amp; =  \frac{1}{n^2} Var\left(\sum_{i=1}^n X_i \right)\\ 
    &amp;amp; =  \frac{1}{n^2} \sum_{i=1}^n Var(X_i) \\ 
    &amp;amp; =  \frac{1}{n^2} \times n\sigma^2 \\ 
    &amp;amp; =  \frac{\sigma^2}{n} 
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We also get the standard error of the sample mean: &lt;span class=&#34;math inline&#34;&gt;\(\sigma/\sqrt{n}\)&lt;/span&gt;,the sample mean has to be less variable than a single observation, therefore its standard deviation is divided.&lt;/p&gt;
&lt;p&gt;For the sample mean which is the unbiased estimator of the population, nothing could be discussed. But for the sample variance need more attentions:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S^2 =   \frac{\sum_{i=1}^n (X_i - \bar X)^2}{n-1} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We proof it:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
E\left[\sum_{i=1}^n (X_i - \bar X)^2\right] &amp;amp; = \sum_{i=1}^n E\left[X_i^2\right] - n E\left[\bar X^2\right] \\ \\
    &amp;amp; = \sum_{i=1}^n \left\{Var(X_i) + \mu^2\right\} - n \left\{Var(\bar X) + \mu^2\right\} \\ \\
    &amp;amp; = \sum_{i=1}^n \left\{\sigma^2 + \mu^2\right\} - n \left\{\sigma^2 / n + \mu^2\right\} \\ \\
    &amp;amp; = n \sigma^2 + n \mu ^ 2 - \sigma^2 - n \mu^2 \\ \\
    &amp;amp; = (n - 1) \sigma^2
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the estimator &lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is unbiased, the calculation of sample variance &lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt; involves dividing by &lt;span class=&#34;math inline&#34;&gt;\(n-1\)&lt;/span&gt;. And &lt;span class=&#34;math inline&#34;&gt;\(S / \sqrt{n}\)&lt;/span&gt; is called the sample standard error of the mean.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mean-again&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mean Again&lt;/h2&gt;
&lt;p&gt;If I want to descript a collection of samples, the mean is esay to get. But if we want to know the population behind the samples, we will use S as a description of the spread. But if we only care the mean, the &lt;span class=&#34;math inline&#34;&gt;\(S / \sqrt{n}\)&lt;/span&gt; will show the description of the error when we calculate the mean. When the n is large enough, we will get a precise mean with small error. But this error have no effect of S, which is the description of the spread behind the sample. If you want an interval estimation of the sample mean instead of a point estimation, you may need to consider that t-value multiply the standard error.&lt;/p&gt;
&lt;p&gt;Ok, finally I sort out those terms. I cite the work done by &lt;a href=&#34;https://github.com/bcaffo/Caffo-Coursera&#34;&gt;Professor Brian Caffo&lt;/a&gt;, thank you very much.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Metabolites of 2,4,4′-Tribrominated Diphenyl Ether (BDE-28) in Pumpkin after In Vivo and In Vitro Exposure</title>
      <link>https://yufree.cn/en/2013/12/04/my-first-paper/</link>
      <pubDate>Wed, 04 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2013/12/04/my-first-paper/</guid>
      <description>&lt;p&gt;This is the first paper written by me which was just published on &lt;em&gt;Environmental Science &amp;amp; Technology&lt;/em&gt;, pp 13494–13501, Volume 47, Issue 23, 2013. You could access the full text either by &lt;a href=&#34;http://pubs.acs.org/doi/abs/10.1021/es404144p&#34;&gt;ACS&amp;rsquo;s website&lt;/a&gt; or just send &lt;a href=&#34;mailto:liujy@rcees.ac.cn&#34;&gt;Dr. Liu&lt;/a&gt; or &lt;a href=&#34;mailto:yufreecas@gmail.com&#34;&gt;me&lt;/a&gt; an email.&lt;/p&gt;
&lt;p&gt;This work focused on the metabolites of BDE-28 in plant. The main finding is that after the exposure, BDE-28 could be transformed to OH-diBDEs, OH-triBDEs and MeO-triBDEs. Without the standards of metabolites, the MeO-triBDEs were identified as para-MeO-triBDE by thumb rules. This is interesting because we found all of the literature about the MeO- metabolites of PBDEs are para-MeO-PBDEs. Though &lt;em&gt;Science&lt;/em&gt; has published a &lt;a href=&#34;http://www.sciencemag.org/content/307/5711/917.short&#34;&gt;paper&lt;/a&gt; in 2005 which show some Ortho- MeO-PBDEs has a nature source, our finding reveal that the metabolites of PBDEs may also have some features to show their sources. However, there are little works reporting the MeO-PBDEs as metabolites, so more proof is needed.&lt;/p&gt;
&lt;p&gt;If you have questions about this paper, comment here and I will reply as soon as possible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Halogen Bond for separation</title>
      <link>https://yufree.cn/en/2013/10/10/halogen-bond/</link>
      <pubDate>Thu, 10 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2013/10/10/halogen-bond/</guid>
      <description>&lt;p&gt;As shown in &lt;a href=&#34;http://en.wikipedia.org/wiki/Halogen_bond&#34;&gt;wikipedia&lt;/a&gt;, the Halogen bond is the non-covalent interaction that occurs between a halogen atom (Lewis acid) and a Lewis base. The Hydrogen bond could be seen as a special case of halogen bond — they all need electron-donor while the hydrogen may also need electron-receptor. In fact, some Organic Halogen Compounds such as PCBs, PBDEs,PFCs,SCCPs&amp;hellip; were hard to analysis for they have so many congeners. Most of time, the capillary columns were used to separate those compounds. However, when we use the capillary columns for a long time, the basic principles of the separation were still the interaction between the packing material and our target compounds, also some other parameters such as temperature and flow rate of carrier gas. The separation only need a delta instead of so much theory. But if we figure out some principle, we could design a column to separate interesting things. I think the halogen bond is interesting for those bond has a direction and make use of those direction we could achieve a high resolution. Well, I admit it is just an idea and some real separation processes may have used those interactions. We could also use hundreds of molecular descriptor to predict the retention action, but I am still want to figure out the most important, maybe simple rules behind those phenomenon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Causal Inference with Confounding Factors</title>
      <link>https://yufree.cn/en/2013/09/02/causal-inference/</link>
      <pubDate>Mon, 02 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2013/09/02/causal-inference/</guid>
      <description>&lt;p&gt;Recently I read some articles about causal interpretations. I was told that the correlation doesn&amp;rsquo;t imply causation the first time I learned it. Also the Simpson&amp;rsquo;s paradox show that some confounding factors may destroy the conclusion based on the whole. Maybe when someone wanted to use the cor relationship to do the causal inference, the first problem is the evaluation of the effect of confounding factors.&lt;/p&gt;
&lt;p&gt;Here, we must know the confounding factors are not the interaction between the variables. If our goal is to find the causal effect between X and Y, the confounding factor is a jerk U who could affect both the X and Y. So most of time, a third man, Z could be invited to solve this problem. Z will have a strong relationship with X while no relationship with Y. OK, the next thing is change Z to observe X and Y. In case the effect of Z must work under X to give something to Y, we could access the changes trend of Y when the Z changes. I mean, U is hidden but Z is in the light, so if Z could change Y via X, the relationship between X and Y will still be obviously. Meanwhile, if the X is a confounding factor to Y, the relationship between X and Y will be much weaker than no change of Z. Most of time the role of Z is taken by randomization. For example, the smoking and cancer has a strong relationship, but the brands of cigarette may not have a relationship with cancer. Also we know that the favor of brand is different among smokers. Here we control one brands within a group with different tastes of brands. Then the result will show the relationship between the smoking and cancer. After comparing with a random survey, a clear relationship may be shown. But as you know those result has a limit ion of brand.&lt;/p&gt;
&lt;p&gt;I think this problem may be solved by another method: Z must have an effect on X while no effect on Y. What if affect both X and Y? well, you may find the Z might be another U. So why not just use random numbers? I mean if strong relationship or causal relationship between X and Y, random Z will not change it. So we could simulate a lot of relationship variables to find the changes of r-squared between X and Y. On the other hand, we could use a lot of suspicious U to find their effect on X and Y. I doubt that there is a distribution of confounding factors&#39; effect and there also exist a impossible space of the r-squared. Using computer, we might simulate those distribution.&lt;/p&gt;
&lt;p&gt;Back to the Simpson&amp;rsquo;s paradox, we might find the survey is strong effected by U, we might improve the result by a random sampling or design an experiment. Anyway, the data must be randomized to draw the conclusion. An experiment with the control variable must be designed with other variables have average 0 effect. But when we got a census data, some data analysis processes might be applied to make you focuse on the relationship you care.&lt;/p&gt;
&lt;p&gt;The causal inference could be done both by well- experiment and math analysis, I believe.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Box Plot v.s. Violin Plot</title>
      <link>https://yufree.cn/en/2013/08/15/boxplot-vs-violinplot/</link>
      <pubDate>Thu, 15 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2013/08/15/boxplot-vs-violinplot/</guid>
      <description>&lt;p&gt;In a seminar I introduced the violin plot and showed the following figure(this example comes from the help document):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(vioplot)
library(sm)
par(mfrow = c(1, 2))
mu &amp;lt;- 2
si &amp;lt;- 0.6
bimodal &amp;lt;- c(rnorm(1000, -mu, si), rnorm(1000, mu, si))
uniform &amp;lt;- runif(1000, -4, 4)
normal &amp;lt;- rnorm(2000, 0, 3)
vioplot(bimodal, uniform, normal)
boxplot(bimodal, uniform, normal)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/vs.png&#34; alt=&#34;plot of chunk vs&#34;&gt;&lt;/p&gt;
&lt;p&gt;So obviously, the violin plot can show more information than box plot. When we perform an exploratory analysis, nothing about the samples could be known. So the distribution of the samples can not be assumed to a normal distribution and usually when you get a big data, the normal distribution will show some out liars in box plot. Referring to the paper by Hintze, J. L. and R. D. Nelson (1998), the violin plot combines the box plot and the density trace, so it seems that the box plot may give the place to the violin plot and I said this in the seminar from a viewpoint of environmental science. But after the seminar, I really doubt that no environmental scientists use this plot. Of course, the violin plot is young comparing with the box plot introduced by Tukey(1977), but there also exist some reasons which stop the spread of violin plot. Here I list it as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the violin plot can&amp;rsquo;t show a better curve with small samples. In Hintze&amp;rsquo;s paper, he thought a smooth curve with at least 30 observations. But the box plot may stand for a smaller observations. Also the bandwidth need to be chosed carefully.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the modification box plot could show the number of observations in the groups using the var width while the violin plot couldn&amp;rsquo;t. When we make some comparison between different groups, the violin plot will hide this information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Another problem is the notch in the box plot to compare the median. In the violin plot, we get a better understanding of distribution of violin plot but less with comparisone with &amp;lsquo;strong evidence&amp;rsquo;(Chambers et al., 1983, p. 62).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Those were nitpick reasons but I think if someone just want to show the violin plot instead of box plot, he need to know the details. Nowadays, it is easy to use new concepts to confusing the readers, we need more thoughts about the nature. Here is a example: there are numbers people who thought the box plot show the mean&amp;hellip;&lt;/p&gt;
&lt;p&gt;Further Reading&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Violin_plot&#34;&gt;Wiki pedia&lt;/a&gt;: you need to read the further readings and the references to know more details about violin plot. I recommend ggplot2 to show the violin plot, it is beautiful anyway.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The Log Transformation of Concentration Data</title>
      <link>https://yufree.cn/en/2013/06/19/statistics-tips/</link>
      <pubDate>Wed, 19 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2013/06/19/statistics-tips/</guid>
      <description>&lt;p&gt;Recently, I read Part I of Song S. Qian&amp;rsquo;s book - &lt;em&gt;Environmental and Ecological Statistics with R&lt;/em&gt; and find some interesting tips about statistics in environmental science. Here I will discuss the log transformation of concentration.&lt;/p&gt;
&lt;p&gt;In this book, the author said that the statistical distribution of concentration data of certain compounds need a log transformation to show a bell curve. I don&amp;rsquo;t know if this conclusion comes from certain theory or just depends on the observation. But usually, when we sample from a certain place for the concentration of POPs, most of the sample will show n.d. and most of time we will use half of the MDL as the concentration in case the N.A. make some troubles. So at a lower concentration, namely the MDL, you will get a lot of samples with the same values. We all know that this situation means an overlay of log-normal distribution and a uniform distribution. So when we have such samples, we need to use our knowledge to find out the right way to summary the data. Also the author said the independence is much more important than the distribution. We need to take care of any factor when we perform sampling.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SNS Websites in China</title>
      <link>https://yufree.cn/en/2013/06/08/cloned-websites/</link>
      <pubDate>Sat, 08 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2013/06/08/cloned-websites/</guid>
      <description>&lt;p&gt;If you find a Chinese blog, you may confused by so many unknown icons at the share buttons. You may want to know the daily life of Chinese and only to find the Chinese on the Facebook are also lived in you country. Here, I think you need to know something about the network in China.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Great Firewall&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;He who does not reach the Great Wall is not a man. In fact, if you see this blog, the author himself has reach the Great Wall, also the Great Firewall. You&amp;rsquo;d better refer to &lt;a href=&#34;http://en.wikipedia.org/wiki/Internet_censorship_in_the_People%27s_Republic_of_China#Technical_implementation&#34;&gt;the wikipedia pages&lt;/a&gt; for more details. So when you came to China, you may need to disappear from most of you familiar SNS websites. They have good severs but we have no connections for them.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Substitution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of causes, we need SNS! So smart person will make the firewall as a nature advantage to make a local substitution of some famous websites you know. In fact, if you talk to a young about twitter, he will think we have weibo and most of them even think twitter is a copy of weibo. We don&amp;rsquo;t care much about the copyright, so we like to copy and share without any limitations. I mean, if some websites can&amp;rsquo;t get accessed while the substitution can satisfy our need, we don&amp;rsquo;t care where it comes from. Also, I will tell you it is not easy to copy SNS due to different cultures and some codes may not copy the ideas but think from their own thinking. But I admit some ideas may just a copy but they must know the games of our Chinese minds. If you think we share universal value and I will tell you the universal value can&amp;rsquo;t make money for our IT companies and they need to make a living, also the universal value is so vague that anyone could use it as an excuse.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why and Why not&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Why some SNS website is hard to get popular even they are not banned?&lt;/p&gt;
&lt;p&gt;Why not some Chinese substitutions get popular in your country?&lt;/p&gt;
&lt;p&gt;Everyone need a net but not everyone need a internet. If I want to find something interesting to share, I will share it with the one who may also think it is interesting. So you want free and no limitation and I think it is OK. But even if the GFW stopped, you may still find hard to make your products popular in China and even your products get popular, you will find it is totally a different thing comparing with your exception. I mean the internet is just a tools and it need a long time to get anyone OK to play this games with the same rules. When you know both rules, you will find the communication is really an important things and not anyone likes to communicate with strangers. They might do this once but not always. Just remember the Babel!&lt;/p&gt;
&lt;p&gt;Anyway, I am against any limitation of internet and if you want to know the real life of the young in China, you may learn Chinese and read their life by some SNS websites. Never believe in the newspapers and I can read both and most of the time, they just want to tell you a story, not the real thing. Here I list some substitutions of your websites and I think with more communication, we could know each other better.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Facebook in China : renren.com&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Twitter in China: weibo.com&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Google in China: baidu.com&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Gmail in China: mail.163.com&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Quora in China: zhihu.com&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Pinterest in China: huaban.com&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Tumblr in China: lofter.com&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Hacker News in China: news.dbanotes.net&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;amozon in China: z.cn or jd.com&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;ebay in China: taobao.com&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you want to know more, just comment.&lt;/p&gt;
&lt;p&gt;======&lt;/p&gt;
&lt;p&gt;Ok, I think I need to give a way to help you visit Facebook or some other banned websites here in China. You need to search for goagent which is a software help you to build a free proxy for you only host on google&amp;rsquo;s severs. Here is the &lt;a href=&#34;http://freenuts.com/how-to-install-and-use-goagent/&#34;&gt;how to&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Brief Introduction of Nomograph</title>
      <link>https://yufree.cn/en/2013/06/03/nomograph-intro/</link>
      <pubDate>Mon, 03 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2013/06/03/nomograph-intro/</guid>
      <description>&lt;p&gt;Before I know something about the geometry, the mathematics equals algebra. Algebra is really an art of abstract while the geometry will bring you back to real life. In other word, maths doesn&amp;rsquo;t always mean an accurate calculation with each steps clearly. Don&amp;rsquo;t panic, I mean sometimes we will get the answer just like a magic. The nomogragh is a typically skill to get an answer of equations without analytic solution.&lt;/p&gt;
&lt;p&gt;The first time I heard that the equation could have no analytic solution I think it unbelievable, but you know that the answers is always coming with the problem which you may heard the first time. In fact, sometimes the problems will not be noticed until someone find answers of them. For example, before Jobs told you the effect of retina display, you may never find you screen grainy. So we could get the answers of almost any equation by so many approximation skills. When we talk about the computer, most of the time we are using the their ability of approximation, in other words, the core of the computer is their algorithms which could be coded. As far as I can see, the algorithms could be coded but not be created by the computer itself so if some algorithms were hard to coded, the computer can only get the answer from iteration. But we all know the technology of word recognize and we only need few points to find a word by machine learning. So what if solve a equation by the same idea, I mean we needn&amp;rsquo;t every step return a certain number and we just use some geometry skills. Yes, the answer is nomograph.&lt;/p&gt;
&lt;p&gt;If you could get the idea of how to find the questions, all you should do is just Google the keywords. I mean, the nomograph is a simple way to give out an answer to a complex equation while all you need to do is just draw some lines and find the numbers on the scales. If this idea could be contained in some algorithms of machine learning, we may get an increase of calculation speed. I mean the base layer of some algorithms could be a solution from nomograph. I don&amp;rsquo;t know whether or not this algorithms show a better performance, it is just an idea when I found the geometry could be used to solve some complex equations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Data</title>
      <link>https://yufree.cn/en/2013/05/27/my-data/</link>
      <pubDate>Mon, 27 May 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2013/05/27/my-data/</guid>
      <description>&lt;p&gt;After a travel to Henan province, I found my removable disk is broken. I don&amp;rsquo;t know when it happened but it just happened. Two days ago, Dropbox pushed an email to remind me of using it, then I find it is hard to record the data  in dropbox from my memory. I got so many data that I can&amp;rsquo;t find one when I need it, so I just got more data from the web. This situation turn over after I use evernote but there exists a lot of data to be classified into my notebook. Well, I&amp;rsquo;d like to rebuild it from bases and I set some rules to organised my data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use folder and sub folder and two levels are enough. If the classification and numerals levels made the user confusing, you know which type of data should be contained.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Always process you data when you got them. Movies or documentary should be turned into messages of you notebook instead of showing them for others. Just keep the one you wanted to scan for more than three times, not the new one.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Achieve the data into your removable devices or sync them via email, evernote, dropbox, Google drive or skydrive
. Not all of them, just choose the one you need.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maybe searching the data from your own database will release your brain, but your must remember some in case you forget you have got some data about one topic.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A cache folder named temp will help a lot if your try to collect you data into this folder and clear it before you finish one day&amp;rsquo;s work.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Get away from your computer as soon as you finished your work, it is boring when you just hang out on the web and we use computer, not vice versa.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Diskgenius will help you recover your data from a disk error, keep a copy in case.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The Rules of Ignoring</title>
      <link>https://yufree.cn/en/2013/05/24/rules-of-ignoring/</link>
      <pubDate>Fri, 24 May 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2013/05/24/rules-of-ignoring/</guid>
      <description>&lt;p&gt;If I&amp;rsquo;d like to keep on writing this pages for such a long time, the new readers will find it&amp;rsquo;s hard to figure out something about me by so many articles. In fact, if I find a guy&amp;rsquo;s post interesting, I will try to read the other posts in his blog. But most of the time, I am confused by so many tags and classification and I will drop. So for most of the viewers as you, this posts will be just ignored. As you read this line, you should type Ctrl + w to end this dull post.&lt;/p&gt;
&lt;p&gt;Anyway, so many articles were made by us as a result of information age and most of the time, we are talking to ourselves. When I wanted to open this blog in English, a friend told me that no one will read your posts and this was just mean-less. If I could find the data about every post, he will be right so I just remove some codes to track the visits from you(OK, I keep some anyway). I am not doing a silly thing and I know one day even myself will forget this post but I just want to leave some messages for a stranger. Everyday on the web you could get into another stranger&amp;rsquo;s life, find the happiness and unhappiness from his twitter or Facebook, then you just leave. This felling is wired because it just like peeping. But you must know when you put something on the web, finally there will be a stranger find it out. But don&amp;rsquo;t panic, you are not the one they care and they are just curious about unknown.&lt;/p&gt;
&lt;p&gt;In fact, we could communicate with each other so convenient that we could just find another to instead of certain one. We are actually getting far away from each other. Recently I went out by train, no one talked with their neighbor but see their cellphone. I know on the cellphone they may just see some twitters from the their friends or the news from the websites but when I was young, getting on a train would be an adventure with so many unknown. They are just at hand and I don&amp;rsquo;t know why somebody just want to keep an eye for some news. The news themselves just stand for a way to satisfy your empty feelings and if you are not feeling empty, you need nothing. So just try to ignore something.&lt;/p&gt;
&lt;p&gt;You must face the fact that few strangers will really care about you, they just want to release themselves. Meanwhile, you need to pay attention to someone who really care about you, they will be your friends. If you feel empty, just ignoring the messages from the same empty persons. You could find something interesting finally. Just try to grasp something eternal instead of fresh, life is eventually dull but we should not make this worse. Don&amp;rsquo;t try to figure out how much you will pay for something because the value is mean-less vs your feelings. For communication, we invent standards but your life will not actually change much with some numbers. Yes, they will bring you happiness feelings but what a pity! Yes, I am not trying to teach you, I am teaching myself. But as a bonus, if you see this in 24st May, 2033, try to contact me by email and tell me the weather of your city, I will send you a check of my salary of May,2033, if I have a job and alive(only one!). Yes, this post will be a debt. Well, I think I have finished my buzz and hope you could find some clues in this post.&lt;/p&gt;
&lt;p&gt;So the rules of ignoring is ignoring your empty feelings, that&amp;rsquo;s all I want to say. So when you see this para, just ignore the word before. I have learned a rule to ignore the first 13%~15% of the contents of a person for former posts are not so good, so just ignore this post.&lt;/p&gt;
&lt;p&gt;Yes, I need to say something about the content in this blog. I will share some ideas with you but not recording my life and mind, so there will be only few articles like this one.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Cipher Based on Similar Pictures</title>
      <link>https://yufree.cn/en/2013/05/23/pic-cipher/</link>
      <pubDate>Thu, 23 May 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2013/05/23/pic-cipher/</guid>
      <description>&lt;p&gt;After I finished the computational photography on the &lt;a href=&#34;https://www.coursera.org/&#34;&gt;coursera&lt;/a&gt;, I finally know some basic principles about the digital pictures. The peer-viewed assignment which focused on the epsilon photography is very interesting. In fact, I know nothing about this term before but when I take some photos, I found it awesome to perform something interesting. Recently, Google plus showed a new feature named auto awesome, which is a good application of epsilon photography; but I want to talk about using similar photos to hide some information.&lt;/p&gt;
&lt;h1 id=&#34;principles&#34;&gt;Principles&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Digital photos are made using pixels which look like a matrix of cells;&lt;/li&gt;
&lt;li&gt;Each cells might have 3 numbers which stand for Red, Green, Blue(yes, RGB color model), for a UINT-8 format photos, each numbers stand for the intensity of corresponding color: 0 for totally black, 255 for totally white;&lt;/li&gt;
&lt;li&gt;Our PCs or Macs relay on the numbers to display the right color for us;&lt;/li&gt;
&lt;li&gt;Similar photos usually show most of the pixels same with few changed or most of the pixels changed with the same patterns, so their differences could be used to hide information.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;method&#34;&gt;Method&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;For similar photos, you could get 3 channels to calculate the differences;&lt;/li&gt;
&lt;li&gt;The differences could get by a derivation of corresponding pixels by rows or columns;&lt;/li&gt;
&lt;li&gt;Differences must have the same meaning in a UINT-8 so you will get a differences plot;&lt;/li&gt;
&lt;li&gt;Well, you could use this to plot the words you want.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;discussion&#34;&gt;Discussion&lt;/h1&gt;
&lt;p&gt;Form and content are in fact the same thing, all you need to do is that finding a meaning for each. Maybe they could show in the same way. Everything could be given a meaning when you find one which could be approved by others. But I don&amp;rsquo;t think it is a good idea to give all the phenomenon a tag or meaning, it is silly and meaningless. Life is not easy and we need no more troubles on purpose. Anyway, I just like to use the patterns which could have different meanings on different points of views to make up something and it is really awesome.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Well, I have written what I want to say and the conclusion is the impression in your mind right now.&lt;/p&gt;
&lt;h1 id=&#34;links&#34;&gt;Links&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach&#34;&gt;GEB&lt;/a&gt;: you must know I have read this, isn&amp;rsquo;t it?&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://compphotography.wordpress.com/&#34;&gt;Computational Photography&lt;/a&gt;: this class is from GA Tech if you want to know the details of what I said.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.gamemastertips.com/cipher/cipher.htm&#34;&gt;Cipher&lt;/a&gt;: this game will teach you a lot.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>HELLO WORLD!</title>
      <link>https://yufree.cn/en/2013/05/20/hello-world/</link>
      <pubDate>Mon, 20 May 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2013/05/20/hello-world/</guid>
      <description>&lt;p&gt;&lt;em&gt;italic&lt;/em&gt;   &lt;strong&gt;bold&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;italic&lt;/em&gt;   &lt;strong&gt;bold&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;header-1&#34;&gt;Header 1&lt;/h1&gt;
&lt;h2 id=&#34;header-2&#34;&gt;Header 2&lt;/h2&gt;
&lt;h3 id=&#34;header-3&#34;&gt;Header 3&lt;/h3&gt;
&lt;h4 id=&#34;header-4&#34;&gt;Header 4&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Item 1&lt;/li&gt;
&lt;li&gt;Item 2&lt;/li&gt;
&lt;li&gt;Item 3
&lt;ul&gt;
&lt;li&gt;Item 3a&lt;/li&gt;
&lt;li&gt;Item 3b&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;http://example.com&#34;&gt;http://example.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://example.com&#34;&gt;linked phrase&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/fish.jpg&#34; alt=&#34;icon&#34;&gt;&lt;/p&gt;
&lt;p&gt;A friend once said:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It&amp;rsquo;s always better to give
than to receive.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
