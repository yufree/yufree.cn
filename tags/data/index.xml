<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data on Miao Yu | 于淼 </title>
    <link>https://yufree.cn/tags/data/</link>
    <description>Recent content in data on Miao Yu | 于淼 </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Jan 2023 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://yufree.cn/tags/data/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>自定义大学排行榜</title>
      <link>https://yufree.cn/cn/2023/01/17/custom-ranking-us/</link>
      <pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2023/01/17/custom-ranking-us/</guid>
      <description>&lt;h2 id=&#34;比赛介绍&#34;&gt;比赛介绍&lt;/h2&gt;
&lt;p&gt;这是和鲸社区与 COS 统计之都联合举办的云讲堂实战系列活动第一场，本活动旨在从实际问题出发训练某综合性数据分析技能，优秀选手将通过统计之都&lt;a href=&#34;https://space.bilibili.com/22035559/channel/collectiondetail?sid=70922&amp;amp;ctype=0&#34;&gt;云讲堂&lt;/a&gt;报告展示自己的成果。第一场的实际任务是自定义大学排行榜，主要训练指标生成、可视化与可重复性报告的撰写等技能。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cosx.org/&#34;&gt;COS 统计之都&lt;/a&gt; 是一个旨在推广与应用统计学知识的网站和社区。由谢益辉创办，现任理事会主席为常象宇，现由世界各地的众多志愿者共同管理维护。&lt;/p&gt;
&lt;h2 id=&#34;背景介绍&#34;&gt;&lt;strong&gt;背景介绍&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;对数据科学社区而言，生成综合指标进行排名其实是经常会遇到的问题，例如什么样的文章要靠前推荐？什么商品更适合某类客户需求？而在研究工作中，很多时候也需要设计综合指标来辅助决策。例如，空气质量指数就是一个涵盖六种污染物浓度分指数的综合指标，而其公布的数值其实是对六个指标取最大值，也就是说，如果你看到空气质量指数同样是100的两个城市，一个可能是因为颗粒物超标，另一个可能是臭氧超标，这里的设计原则是木桶原理，取最差的那一个。社会科学中很多指标的生成往往也暗含了不同的选择标准或指标权重，例如消费者物价指数CPI的计算就会涉及多种商品的价格加权，美国CPI里食品比重不到8%而法国则在15%左右。&lt;strong&gt;这些加权或设计原理往往需要相关专业知识来背书，因此在常规的数据科学训练里很少被提及，但实战中却又特别需要&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;要想训练综合指标的设计与生成，最简单的方法就是通过真实数据来研究一个通用主题。大学排行就是这样一个适合你练手的项目&lt;/strong&gt;，任何一所大学都能找到自己排名靠前的某个排行榜来作为招生的宣传，考生与家长也经常喜欢用排名来印证自己的选择是合理的。同时，好或者不好其实是一个很难定义的东西，国际上较为公认的大学排行有四个：QS、USNews、泰晤士报还有上海软科的世界大学排名，他们一般会构建一些量化子指标分别打分，最后算总和进行排名。&lt;/p&gt;
&lt;p&gt;但对个体而言，其实看重的点并不一样，例如大多数排名会算论文发表量，但如果学生不喜欢做学术，带这个指标的排名其实对他意义不大；从纯结果看，学生毕业后 10 年的年均收入对于大多数打工人可能很重要，但排名里可能完全不考虑。此外，大学排行里其实也掺杂了很多商业或可优化因素，例如泰晤士报排名很多年来都是牛津排第一，而 USNews 排名美国大学时会考虑录取率，这就搞的很多有钱的私校在申请季大量投广告来拉低录取率。&lt;/p&gt;
&lt;p&gt;在奥巴马时代，美国政府意识到排名的乱象已经形成了误导，于是教育部就出钱收集了大量的大学相关数据免费公开到网上，这样感兴趣的家长和学生可以根据自己对未来的规划来排名申请大学。这个项目运行到现在不说是大获成功吧，也可以说门可罗雀。虽然数据每年都更新且全部透明公开带 API 跟文档，但利用率其实不高。 因此，可以利用这部分数据来训练综合指标的设计与生成，打造一个你认为合理的美国大学排行榜。&lt;/p&gt;
&lt;p&gt;首先，这是个竞赛，是个有奖竞赛，获奖者会受邀到统计之都云讲堂做在线报告。&lt;/p&gt;
&lt;p&gt;其次，这个竞赛需要的数据量偏大，但和鲸社区提供了计算资源，可以直接在线进行数据分析。&lt;/p&gt;
&lt;p&gt;再次，这个竞赛可以让你了解下美国教育系统，特别是联邦学生贷款的现状，可以从中发现一些问题，如果参赛者有兴趣，可以将其投稿给专业期刊作为自己的学术成果发表或作为研究项目列入简历。&lt;/p&gt;
&lt;p&gt;这里给一个简单思路示例。知名数据分析网站&lt;a href=&#34;https://fivethirtyeight.com/&#34;&gt;538&lt;/a&gt;就针对这部分数据进行过&lt;a href=&#34;https://fivethirtyeight.com/features/the-economic-guide-to-picking-a-college-major/&#34;&gt;分析并撰写了报告&lt;/a&gt;，通过毕业后收入对不同专业进行了排名，发现很多有意思的结果：从收入看STEM（科学、技术、工程、数学）这个传统认为收入比较高的专业里S，也就是科学收入并不高；心理学很流行但其实工资并不高；精算行业收入很高，但失业率也很高等，这些结果都有助于你去构造一个大学排名指标，而且这篇文章的代码在GitHub上&lt;a href=&#34;https://github.com/fivethirtyeight/data/tree/master/college-majors&#34;&gt;有&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据读取示例&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;如果用R可使用&lt;a href=&#34;https://www.btskinner.io/rscorecard/index.html&#34;&gt;rscorecard&lt;/a&gt;包来通过API读取数据&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-R&#34; data-lang=&#34;R&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## 在 https://api.data.gov/signup/ 注册API key&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;sc_key&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;YOUR_API_KEY_HERE&amp;#39;&lt;/span&gt;)
df &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sc_init&lt;/span&gt;() &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#a6e22e&#34;&gt;sc_filter&lt;/span&gt;(region &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, ccbasic &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;21&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;22&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;23&lt;/span&gt;), locale &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;41&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;43&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#a6e22e&#34;&gt;sc_select&lt;/span&gt;(unitid, instnm, stabbr) &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#a6e22e&#34;&gt;sc_year&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;latest&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#a6e22e&#34;&gt;sc_get&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;时程安排&#34;&gt;&lt;strong&gt;时程安排&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;报名 ddl：1 月 27 号 23:59&lt;/li&gt;
&lt;li&gt;报告提交ddl：2 月 10 号 23:59&lt;/li&gt;
&lt;li&gt;作业汇报交流会：2 月 18 号 19:00（腾讯会议号请关注统计之都公众号或和鲸活动页面更新）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;奖项设置&#34;&gt;奖项设置&lt;/h2&gt;
&lt;p&gt;TOP1：优秀奖。将获得 800 元稿酬、电子证书、鲸奇徽章。
TOP2-4：创意奖。将获得 200 元稿酬、电子证书。
其他有效提交：参与奖。将获得《现代科研指北》纸质书 1 本、电子证书。&lt;/p&gt;
&lt;h2 id=&#34;报告要求&#34;&gt;&lt;strong&gt;报告要求&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;这个竞赛没有标准答案与指标，你需要提交的是在线的 Jupyter Notebook 项目，需要包含从数据清洗到指标生成及可视化的全过程且包括理由。也就是说，你需要提交的是一份具有可读性的&lt;strong&gt;可重复性报告&lt;/strong&gt;，要说明指标设计的原因、有效性、灵敏度及延展性，并生成相关图表或可视化面板。&lt;/p&gt;
&lt;p&gt;当然，你也可以只针对一个或几个指标进行可视化与深入讨论，结合新闻与相关研究撰写挖掘其中隐含的例如歧视、冷热门专业、学费/收益比等现象，给出你认为值得关注的结论。&lt;/p&gt;
&lt;p&gt;祝玩得愉快！&lt;/p&gt;
&lt;h2 id=&#34;面向人群&#34;&gt;&lt;strong&gt;面向人群&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;想要提高数据分析实战能力的学生、老师等研究人员&lt;/li&gt;
&lt;li&gt;对指标设计感兴趣的数据分析从业者&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;主办方&#34;&gt;主办方&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;COS 统计之都&lt;/strong&gt;：专业、人本、正直的统计学服务平台。
&lt;strong&gt;主页&lt;/strong&gt;：https://cosx.org/&lt;/p&gt;
&lt;p&gt;活动链接：&lt;a href=&#34;https://www.heywhale.com/home/activity/detail/63b7ecd6555e5f7e505374af&#34;&gt;https://www.heywhale.com/home/activity/detail/63b7ecd6555e5f7e505374af&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>解卷积</title>
      <link>https://yufree.cn/cn/2022/05/21/deconvolution/</link>
      <pubDate>Sat, 21 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2022/05/21/deconvolution/</guid>
      <description>&lt;p&gt;解卷积本来是指反解卷积，但在色谱质谱联用里特指从多物质混合的色谱图里分离出单独物质的色谱。不用说，这跟卷积原始概念已经八杆子打不着了，就是个学科小领域里造词或者填词赋予新含义的案例。要搞清楚这个问题，得从卷积说起。&lt;/p&gt;
&lt;p&gt;卷积跟加减乘除一样，就是一种函数运算方法，定义上就是两个函数经过一通操作变成第三个函数。这个一通操作里涉及两个要点：1）要对其中一个函数要进行反转位移之后2）对两个函数的乘积求和或求积分。这个反转与位移发生在积分的方向上，先卷后积。&lt;/p&gt;
&lt;p&gt;估计很多人看到这个定义会感觉莫名其妙，因为这个定义似乎没啥实际意义，凭啥要卷？又凭啥要积？这就涉及具体学科知识了。不过数学上倒也有个方便的解释，那就是函数卷积的傅立叶变换是函数傅立叶变换的乘积。傅立叶变换可以理解为一种时域与频域的转换，这样时域上卷积就是频域乘积，进行卷积运算相当于在另一个域里做乘积，傅立叶变换是可逆的，只要你能在一个域里找到乘积的物理意义，那么卷积运算就不难理解了。同时这也给解卷积提供了思路，既然可以搞成乘积形式，那就可以对原始数据做变换求卷积就简单多了。&lt;/p&gt;
&lt;p&gt;可能有人又要问了，为啥我要对一个函数做变换？大概率还是因为计算上的方便，例如取对数后乘法就变加法了。又或者让你求正弦函数，你要是不做个泰勒展开那就只能查表了，别忘了我们基于逻辑门的计算机CPU也就能做个加法运算，其余数学运算都要还原成二进制逻辑运算才能进行。至于说傅立叶变换，可以理解为把一个线性函数转为不同频率正弦或余弦函数的叠加，这样做一个好处在于三角函数的导数与积分还是三角函数，然后关注相位变化就可以了。所以说我们并不是吃饱了撑的搞出一些数学变换或运算方法，很多时候是因为进行这些运算或变换能解决实际问题。不过，这是我学了高数很久之后才在实践中意识到的，感觉现在数学教育还是过于抽象了。&lt;/p&gt;
&lt;p&gt;不过卷积也可以用幂级数乘法来理解，两个幂级数相乘后同幂次的系数怎么算？打比方 &lt;code&gt;$x^2+2x+1$&lt;/code&gt; 与 &lt;code&gt;$x+1$&lt;/code&gt; 求乘积的一次方系数，这就要是前者的一次与后者的零次乘积与前者零次与后者一次的和，也就是3，计算对齐过程就是个卷积。其实在概率论里，两个独立变量的和的概率密度函数就是这两个独立变量的卷积，例如两个骰子掷出某个点数和的概率就是两个独立变量概率密度函数的卷积，当然我们固定了和，所以两个变量其实求卷积时并不独立了。到这里其实我们大概能感受到，卷积一定是要在两个函数自变量的特定和上的一个属性。&lt;/p&gt;
&lt;p&gt;到这里为止，我们只知道了卷积运算因为具备一些特性，可能会是一种有实际意义的运算，但实际中为啥要用还是还是要看具体问题。在信号处理里，我们固定信号输入函数跟这个信号自身在时间上的衰减函数，他们的卷积就是时间上输出的信号，这个信号既包含了输入的信号，也累积叠加了衰减效果，这个应该不难理解，时间点x上我们看到的输出信号就应该是前面t = 0时刻信号衰减到t=x时的信号累积上t=1时刻信号衰减到t=x的信号，以此类推。所以我们看到的输出本来就是个卷积后的结果。&lt;/p&gt;
&lt;p&gt;这里积不难理解，但卷在哪？卷在那个衰减函数上，因为你实际用的是输入t=0去乘了衰减函数t=x的数，如果我们把这两个函数对齐，就需要先对衰减函数在积分的时间轴t上做镜像反转，然后平移到t=x处对齐，这就是卷的过程。此处卷积连接的输入与衰减函数也是通过时间和恒定来连接的，也就是如果你能构筑两个函数，其自变量和在某个维度上是恒定的，那么他们就适合在这个维度上做卷积运算且应该有具体的物理意义，例如某种累积效应。不过卷积是翻转加平移的，如果只平移不反转，在乘积上求和或者积分也可能有意义，例如求相关性。这里的意义是需要看需求的，构建运算不难，但跟物理世界相联系是需要下功夫理解的。&lt;/p&gt;
&lt;p&gt;上面那个是一维卷积，如果输入输出都是个二维图像，那么卷积运算就相当于对图像做滤镜，跟输入做卷积的就是所谓的卷积核或滤镜。这个卷积核在卷积神经网络里其实就对应了图像的某个局部特征，或符合某种模式后可以穿过滤镜形成有效信号。当卷积核是3*3的矩阵，那么当图像是出现某种模式时，训练神经网络过程会找到这个对应的卷积核。一张图片可能有很多特征，那么也就可以训练出很多卷积核，当进行图片分类预测时，我们可以训练出一个基于卷积核的全连接层来输出预测分类。当然这里激活层池化层等技术或模型构架就不考虑了，但卷积运算确实是卷积神经网络的一个核心，专门用来抽数据中的特征。至于说为啥是卷积运算而不是其他运算，其实基于其他运算的也有，但本质上要处理的基本问题是图像抽特征的方法，否则1400万像素的图片你要训练的参数就是个天文数字，但训练几万个卷积核就容易多了，可以理解为一种降维的思路。&lt;/p&gt;
&lt;p&gt;说白了卷积运算就是一种特殊滤波器或放大器，但具体到分析化学里，问题就不一样了。分析化学的光谱分析里也涉及卷积，具体说就是我们仪器上测到的信号是真实信号跟某些卷积核做卷积出来的，当然这不是说噪音，而是诸如衍射过程这种仪器来源的有固定模式的信号，此时解卷积其实就是个模式识别过程。到这里卷积的概念还算是符合原有定义的，但具体到学科小领域就完全变味了。&lt;/p&gt;
&lt;p&gt;质谱色谱联用测到的数据是一个三维向量：一个维度是时间，一个维度是荷质比，一个维度是响应。早期可视化手段基本就是直接把荷质比这个维度上的所有响应求和形成总离子流图。但因为存在共流出现象，这种图上你数出来的色谱峰数目跟物质数目是对不上的，很多单独的峰其实是多个物质在不同荷质比上的响应叠加出来的，这导致总离子流图的峰形长得怎么说呢，比较畸形。&lt;/p&gt;
&lt;p&gt;时间来到1974年，Biller 跟 Biemann 发表了一篇题为 &lt;em&gt;Reconstructed mass spectra, a novel approach for the utilization of gas chromatograph&amp;mdash;mass spectrometer data&lt;/em&gt; 的论文，提出了一种重构出单独质谱峰的算法，这个算法受限于年代，非常简单，就是提出用每个扫描循环离子响应最大值来分离不同物质，然后合并对应的扫描得到比较干净的谱图。然后到了1976年，Dromey 等人搞出了一个非常复杂的利用相似峰形建模提取独立物质质谱的方法，这个方法效果不错，但因为计算上太复杂基本属于原汤化原食，就这批人在用。等到了1992年，Colby在JASMS上发表了题为&lt;em&gt;Spectral deconvolution for overlapping GC/MS components&lt;/em&gt;的文章，其实这篇文章的核心在于对1974年的方法进行改进，具体来说就是每个扫描循环里面只找十个峰，然后合并谱图重构出总离子流图，进而提高质谱分离的分辨率。因为这活是90年代做的，94年就有人将其程序化了，但很不幸同时也就把解卷积这个词给带到这个小领域里了。&lt;/p&gt;
&lt;p&gt;不过这里我们很清楚此处的信号根本就没有卷积，解卷积也无从谈起，但因为Colby这篇文章，后面三十年大家就都默认了解卷积等同于重构质谱峰的意义了。不过在蛋白质质谱领域，解卷积还有另外一个概念，就是把多电荷峰反解回分子量，此处最流行的是一种利用最大似然度求解的maxent算法，同样也跟卷积的原始概念毫无关系。在中文环境里，研究人员还有仪器厂商的销售尤其喜欢这个词，因为听上去高大上显专业，但我估计他们中的大多数可能都不知道这个概念本身其实是用错了，单纯以讹传讹，三人成虎。&lt;/p&gt;
&lt;p&gt;但更可悲的是其实错不错都不重要了，很多一线科研人员的数学功底基本上都归零了，傅立叶变换、卷积、二重积分、delta方法等概念全都谁教的还给谁了。论文里的数据分析都是对着其他论文照猫画虎，经常是驴唇不对马嘴，要是继续这么自己造词搞小圈子，那跟八股文写茴字也没啥区别了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>漫谈 base R 与 ggplot2 </title>
      <link>https://yufree.cn/cn/2021/12/18/base-r-ggplot2/</link>
      <pubDate>Sat, 18 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/12/18/base-r-ggplot2/</guid>
      <description>


&lt;p&gt;这几天心里颇不宁静。纽约的单日新冠确诊数又出了新纪录，工作邮箱里好几封通知群聚感染的邮件，很多还是完全免疫甚至打了加强针的。感觉新变种可以自立山头看作新病毒了，而面世仅一年的疫苗似乎特异性已经不那么乐观了。不过更强的传染性倒不见得有更强的症状，但大概率这病毒是打算跟人类长期共存了，这就有点抗生素与致病菌的剧本了。我理解国内清零政策很大程度是因为放开后医疗系统会崩溃，不过长期清零对于一种不断变异增强传染性的病毒而言是否具有可持续性我想已经在被论证了，同样完全躺平现在看也不是什么好的应对策略，解决问题不能非此即彼，总是要根据实际情况动脑子的。说到这我就来填一个很早就想填的坑，是关于R绘图系统的。&lt;/p&gt;
&lt;p&gt;不知道从什么时候开始，tidyverse 跟六角贴纸开始满天飞了，在我看来工具是用来解决问题而不是创造问题的，围绕工具形成的文化现象或类似价值观的东西属于副产品，但推广价值观这类副产品显然要比推广软件容易，因此到一定程度总会出现一些反客为主的声音。我对此不置可否，只是想说工具一定要能解决问题才有意义，在解决问题上，将问题清楚描述出来要比直接问某某工具咋做某某图更重要。&lt;/p&gt;
&lt;p&gt;我接触 R 是在十年前，那个时候提到 R 的绘图系统，更多指的是 base R 与 grid/Lattice 这两套系统，前者强调的是最高程度的自定义自由度而后者侧重的则是一个函数给出想要的图形。显然，前者更适合原始意义的绘图比较适合开发者而后者更适合具体应用场景的用户，后者的一个显著优势是默认出图就足够漂亮而显著缺点则是自定义比较费劲。用户从来都是最难伺候的，总有用户既想要最大程度的默认好看又需要一定程度的自定义（说的就是科研狗），此时 ggplot 就出现了。&lt;/p&gt;
&lt;p&gt;ggplot 里面的 gg 代表的是图形语法，本来 ggplot 就是 Hadley 理论转实践的尝试，原始的包在08年就不更新了，现在 CRAN 上也没有了，后面 ggplot2 里的那个2就是致敬最原始的版本 ggplot 。ggplot2 最显著的优点就是用图形语法结合了 base R 与 grid 系统的理念，默认足够好看，自定义掌握了一套通用层层叠加语法后也很容易上手。不过，我也得替 grid/Lattice 说句话，他们也是支持自定义的，Lattice 到今天也处于活跃开发状态，并且跟 R 一起发布。不过他们的自定义显然没有上升到价值观层面，跟 base R 的逻辑更像，支持公式化的数据表达，支持对象化操作，也可以通过 &lt;code&gt;update&lt;/code&gt; 这个函数来有限调节自定义的细节。不过有句老话说“革命不彻底就是彻底不革命”，用户从来不会过多理会开发历史，基本都是颜狗。ggplot2 也是依赖 grid 包来构建的，包括了所有 Lattice 的优点但学起来更容易，所以很快用户就倒向了这套绘图系统。不过，我们现在常看到的 base R 与 ggplot 的对比很大程度是从可视化结果上来的，但别忘了 ggolot 也是 R 语言的产物，理论上研究清楚了 grid 包也可以搞出类似的东西，真正的区别是用户的使用逻辑。&lt;/p&gt;
&lt;p&gt;base R 的使用逻辑更像是白纸画图，从坐标轴到图像都是可以随意自定义的，与之对应的就是需要用户了解一大堆底层命令。base R需要用户对可视化有很具体的了解，例如先用纸笔草图画出雏形，然后通过排列组合基本的作图元素重现在绘图区里。举个最简单的例子，想在条形图上加个误差线，就需要分解为具体的两部分：1）误差线长度与起点坐标，2）然后从起点坐标平行y轴画一条线段，线段末端可以把原有箭头末端里箭头的角度从锐角改成90度垂直。然后我们就能看到误差线了。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;iris&amp;quot;)
mean &amp;lt;- by(iris$Sepal.Length,iris$Species,mean)
sd &amp;lt;- by(iris$Sepal.Length,iris$Species,sd)
temp &amp;lt;- barplot(mean,ylim=c(0,max(mean)+2*max(sd)))
arrows(temp,mean+sd, temp, mean-sd, angle=90, code=3, length=0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/12/18/base-r-ggplot2/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这个简单到不能再简单的功能偏巧默认的条形图 &lt;code&gt;barplot&lt;/code&gt; 里没有，函数文档里那个误差线又不是平头的，所以很多用户马上认为 base R 做不了。单就这个例子而言，其实条形图本来就不应该有误差线，真正需要表示不确定性范围应该用箱形图或提琴图来展示真实分布，但你要是去读学术论文会发现时至今日还是有大量图用条形图加误差线，有的还要加星号表示显著差异。这些从作图角度而言完全就是缝合怪，但始作俑者后人不断。很多用户只知道最终要看到什么样的图，但完全不关心图背后的统计学意义，然后就到处问哪个函数能做这样的图，追求形似。这是我认为现在还需要学 base R 作图的一个重要原因，那就是用户一定要有把图形元素拆解回原始数据的能力，知道自己再干什么而不是盲目追求炫酷，这太浮躁。&lt;/p&gt;
&lt;p&gt;在 ggplot 这套系统里，图片实际被拆解成了三个部分：数据、映射与可视化方式。数据最好是一个数据框，映射解决变量对应在图片上的维度例如哪个是x？哪个是y？哪个分组用颜色/大小/形状来表示，可视化方式就是指定出图的具体形式。其实这个逻辑在 base R 里也是成立的，只不过最后这个不同可视化方式会对应不同函数，然后里面参数一大堆名字还不一样。ggplot则是把这些都规范到一种形式里去了，而且可以通过加号这个函数层层叠加可视化方式，而各自可视化方式内部也可以重新进行坐标映射。此外，ggplot事实上也支持在作图过程中执行一定的计算，例如平滑或者汇总，这类计算都归到&lt;code&gt;stat_&lt;/code&gt;系列的函数里了。如果打算自定义某个维度，可以统一用 &lt;code&gt;scale_XXX_manual&lt;/code&gt; 来进行修改，这里&lt;code&gt;XXX&lt;/code&gt;对应的是你映射的维度，例如 &lt;code&gt;scale_fill_manual&lt;/code&gt; 对应的就是自定义填充颜色。注意，这里是映射后的图片里的维度，而映射则一般都是在可视化方式的 &lt;code&gt;aes&lt;/code&gt; 里定义的，用来把数据中的某个维度指定到图片里的维度里。此外，ggplot 自然也支持根据分组信息的多图可视化，这里就是&lt;code&gt;facet_grid&lt;/code&gt;函数来统一管理，这里 ggplot 的价值观是每一张图都要尽可能清楚展示数据，且一幅图讲一个故事，然后通过坐标对其进行比较。也就是说，ggplot里画双坐标轴这种图就不太推荐，虽然也可以自定义后来个形似，但其实双坐标轴图在可视化方式里面的地位大概跟饼图或3D柱形图差不多，属于人厌狗嫌那个组的。所有的双坐标轴图理论上都可以并应该拆成两幅图来描述两件不同的事，如果两件事相同，那么双坐标轴总可以转为单坐标轴。&lt;/p&gt;
&lt;p&gt;从我个人经验而言，如果你掌握了 base R 作图，转到 ggplot 作图非常轻松，会省掉一大堆需要自定义的东西，统一用他们的函数体系来画就行。反之，如果掌握了 ggplot 的语法，学 base R 应该也很轻松，因为base R绘图函数里的映射与可视化方式也都有现成的包或函数，只是可能参数名字乱一些。说 ggplot 对新手友好，一大部分指的是默认美观，还有一部分原因就是从设计上就阻止了很多类似3D柱形图或双坐标轴的存在。但搞笑的是在爆栈网上从来不缺人去问这类图如何在ggplot体系里实现，也不缺大神给出解决方法。因此，我一度认为限制一定的自由度其实也对培养良好的可视化习惯有帮助，但现实却是很多用户既没学到ggplot里的可视化原则，又产生了对 base R 莫名其妙的优越感，一抬手就是管道化教条式的层层叠加代码，一行代码能做到的非学习牛津大学贝利学院优秀毕业生、大英帝国爵级司令勋章获得者、大不列颠及北爱尔兰联合王国内阁常任秘书汉弗莱·阿普比的语言风格去卡形式，这就距离解决问题比较遥远了。&lt;/p&gt;
&lt;p&gt;其实 base R 与 ggplot 之争的背后存在一个代码风格统一的问题，如果选 ggplot ，确实代码更容易读，但问题是很多初学者的水平大概就在复制代码改变量名的水平，完全不知道代码的意义，这样层层堆出来的代码甚至会重复定义，也没啥可读性。究其原因，代码风格应该是初学者到了中级水平才应该考虑的问题，初学者最应该了解的是可视化的逻辑，然后结合自己具体的问题去练习并累积经验。不过，很多初学者都是excel打底的，脑子里全是哪个对话框画什么样的图这种思维，这种情况不论是转 base R 还是 ggplot ，他们脑子里的预期都是一个函数出图解决问题，根本不关心图是如何画出来的，这就很容易变成教条化的用户，记住步骤但不知道原因。实话说很多基于ggplot体系的作图包本质上就是把需要用户自定义的部分自己强制定义一遍然后就上线了，这种包完全迎合了某些领域的独特可视化品味，后面跟了一批教条化的用户，这样的默认可用的软件包我觉得并不利于数据分析人员理解自己的数据。&lt;/p&gt;
&lt;p&gt;但凡学用编程语言进行可视化，起码是要知道自己在做什么的。看到一张漂亮的图，可以尝试分析图片元素，然后尝试自己将其组合起来。有这种想法后，base R 也好，ggplot 也好，学明白一个就基本也会另一个了，甚至说迁移到 python 的 matplotlib 或其他交互式作图系统都不困难。但如果搞不清楚原理，那么换一个软件就只能继续到网上复制现成的代码。我倒不是鄙视从网上复制现成的代码，毕竟这事我也没少做，但总要有个学习的过程才能掌握。&lt;/p&gt;
&lt;p&gt;软件优劣之争在我看来很多都是鸡同鸭讲，很多比较都是在特殊应用场景下才有明显区别。但每个人的最终应用场景毕竟是不同的，解决问题的意义显著大于跟工具分高下的意义。显然编程语言绘图的能力范围更多受限于使用者的能力而非工具本身，因此这样的工具优劣争论还是少一些吧，争到最后大概率都成了人身攻击。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>辛普森悖论</title>
      <link>https://yufree.cn/cn/2021/08/27/simpson-paradox/</link>
      <pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/08/27/simpson-paradox/</guid>
      <description>&lt;p&gt;曾经有两本书伴我度过了无数漫漫长夜，毕竟每次鼓起勇气看不超过10页就会睡过去，一本是&lt;a href=&#34;https://yufree.cn/cn/2020/09/01/metaphor/&#34;&gt;前面&lt;/a&gt;写过的 &lt;strong&gt;metaphors we live by&lt;/strong&gt;，另一本就是《悖论简史》。这种书的一大特色就是读的时候如果不带脑子看不懂，带脑子就头疼，但可气的是写的还挺有趣。&lt;/p&gt;
&lt;p&gt;基础科研的很多突破都是来自于悖论或者说反例，不过这里的悖论属于理论悖论，大概率是理论本身有问题，需要新的理论来解释观察与实验。还有很多悖论属于错觉，本身不是悖论仅仅因为解释上的片面出现，其实魔术就可以化为这一类，很多魔术手法展示的现象完全是违背常理的，但了解手法后就会发现其实是利用了一些惯性思维产生的错误解释。真正的悖论是语义学上的，例如“这句话是错的”就是一个语义悖论，如果认为这句话是错的那就应该是对的，但如果认为这句话是对的其又描述了一个自己是错误的判断，这种带有自指的悖论属于无解。其实数学领域的第三次危机本质上也要通过语义学划定语义解释范围来凑合解决，这属于逻辑自身的漏洞。&lt;/p&gt;
&lt;p&gt;辛普森悖论属于某种程度上的错觉悖论。其本质就是说存在一种分组方法，让&lt;code&gt;$\frac{A_1+B_1}{C_1+D_1} &amp;gt; \frac{A_2+B_2}{C_2+D_2}$&lt;/code&gt;，然后&lt;code&gt;$\frac{A_1}{C_1}&amp;lt;\frac{A_2}{C_2}$&lt;/code&gt; 并且 &lt;code&gt;$\frac{B_1}{D_1}&amp;lt;\frac{B_2}{D_2}$&lt;/code&gt;，乍看之下会感觉莫名其妙，因为数学上找这么一组数太简单了（睡不着觉别数羊，就去构造辛普森悖论，比数羊效果好多了）。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$\frac{1+3}{5+4} = \frac{4}{9} &amp;gt; \frac{4}{10} = \frac{2+2}{8+2}$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$\frac{1}{5}&amp;lt;\frac{1}{4}$&lt;/code&gt; 并且 &lt;code&gt;$\frac{3}{4}&amp;lt;\frac{1}{1}$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;从数学角度看完全不存在悖论，因为&lt;code&gt;$\frac{A}{B}+\frac{C}{D} \neq \frac{A+B}{C+D}$&lt;/code&gt;，所以&lt;code&gt;$\frac{A}{B}$&lt;/code&gt;与&lt;code&gt;$\frac{C}{D}$&lt;/code&gt;的数值比较关系无法传递到&lt;code&gt;$\frac{A+B}{C+D}$&lt;/code&gt; 的比较里。&lt;/p&gt;
&lt;p&gt;但我们要加个语境就完全不同了。例如，这里我们把上面的数扩大十倍，某种化学品暴露组一共90人患病40人，对照组一共100人患病40人，此时研究人员会得出暴露组发病率比对照组高的结论。然而，如果暴露组里有50名男性发病10人，对照组80名男性发病20人，我们会发现男性对照组发病率高于暴露组；而同时女性40人里发病30人，对照组里20人发病20人，还是暴露组低于对照组。&lt;/p&gt;
&lt;p&gt;同样的数据，如果环境科学家看到会说这是一种致病污染物需要禁止而药厂则认为这是一种对不同性别都有效的预防性药物，两边成果都足以发表业内很好的杂志上也都说得通，但解读出的含义完全相反。由于人体本身就有回归到正常的现象，确实存在一些病吃药七天恢复不吃药一星期恢复，所以很多我们研究发现的效应如果不能强到药到病除而需要通过不断细分数据来发现效应，那么大概率就会存在辛普森悖论。&lt;/p&gt;
&lt;p&gt;但问题现在的很多病的新药或常见保健品就是这个表现水平，徘徊在吃不死人的底线之上通过安慰剂效应或信仰来起效果，这就很尴尬了。保健品大多数吃了虽然没用但也不会有害，跟食物差不多效果但价钱就不一样了。而且如果一个人得了慢性病，大概率本来身体状态就是起起伏伏，此时你吃保健品就会形成一个错觉：身体状态好就会认为保健品起效了而不好则会认为没吃够或者哪天忘了吃了。保健品本身就起到了信仰的效果，功劳都是它的罪过都是自己的，有这份心什么成不了？&lt;/p&gt;
&lt;p&gt;其实我跟安慰剂效应有很深的渊源，小学每年都有体测，那时候我跑步不行，就开始动歪脑筋，反正体测又不尿检不如用兴奋剂来提高成绩。但问题我家哪有兴奋剂啊，回去一通翻箱倒柜发现一桶咖啡，我之前也没喝过心想这玩意也算兴奋剂吧？结果冲了一碗就去体测，当时感觉甜甜的还挺好喝，效果也还不错，成绩有明显提升。这事过去快一年等到下一次体测来的时候我又想起这玩意了，这次又翻出来仔细读标签才发现是“咖啡伴侣”但伴侣两个字很不好认，也就是喝的是植脂末跟奶粉，这次成绩就崩了。好多年后我才喝到真正的咖啡，说实话，还不如咖啡伴侣好喝。&lt;/p&gt;
&lt;p&gt;后来跟父母说起这事就奇怪，家里又没人喝咖啡为啥要搞一罐咖啡伴侣？答案也不难猜，这是过年走亲戚送来送去留下来的礼物，他们买的时候估计也当成咖啡了，包装洋气而且比真咖啡便宜多了。这里能促进成绩的其实只是服用了兴奋剂的信念而非兴奋剂，也就是当一件事决定因素心理影响更大时，药物的药效反而成了玄学了。当然重申一下，体测别动歪脑筋，人本身的潜力要远大于外界刺激，平时加强锻炼才是正道。否则，凭运气赚来的，早晚都要凭实力输出去，这就是所谓的回归现象。&lt;/p&gt;
&lt;p&gt;接着说辛普森悖论，这里我们已经看到，数学上比较整体比值与局部比值是毫无意义的，各种情况都会出现。然而，如果我们给数字赋予含义，那么就会出现不同专业基于不同立场给出的完全相反但又都解释得通的结论。也就是说，从绝对的数学计算上，这就是个鸡同鸭讲毫无意义的比较，但赋予背景后，现实中又确实存在明确的问题，例如前面说的那种化学品究竟是应该推广还是禁用？这个问题咋解决？&lt;/p&gt;
&lt;p&gt;此时我们就不得不进入因果推断的领域了，我们必须对化学品、性别及疾病这三者关系建模，这也算某种三体问题了，考虑方向其实一共就23种关系：&lt;/p&gt;
&lt;h2 id=&#34;三者都没关系&#34;&gt;三者都没关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品 性别 疾病&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三者里只有一组两两关系&#34;&gt;三者里只有一组两两关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品-&amp;gt;性别 疾病&lt;/li&gt;
&lt;li&gt;化学品&amp;lt;-性别 疾病&lt;/li&gt;
&lt;li&gt;化学品 性别-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;化学品 性别&amp;lt;-疾病&lt;/li&gt;
&lt;li&gt;性别 化学品-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;性别 化学品&amp;lt;-疾病&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三者里有两组两两关系&#34;&gt;三者里有两组两两关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品-&amp;gt;性别-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;化学品-&amp;gt;性别&amp;lt;-疾病&lt;/li&gt;
&lt;li&gt;化学品&amp;lt;-性别-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;化学品&amp;lt;-性别&amp;lt;-疾病&lt;/li&gt;
&lt;li&gt;性别-&amp;gt;化学品-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;性别-&amp;gt;化学品&amp;lt;-疾病&lt;/li&gt;
&lt;li&gt;性别&amp;lt;-化学品-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;性别&amp;lt;-化学品&amp;lt;-疾病&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三者里有三组两两关系&#34;&gt;三者里有三组两两关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品&amp;lt;-性别&amp;lt;-疾病&amp;lt;-化学品&lt;/li&gt;
&lt;li&gt;化学品&amp;lt;-性别&amp;lt;-疾病-&amp;gt;化学品&lt;/li&gt;
&lt;li&gt;化学品&amp;lt;-性别-&amp;gt;疾病&amp;lt;-化学品&lt;/li&gt;
&lt;li&gt;化学品&amp;lt;-性别-&amp;gt;疾病-&amp;gt;化学品&lt;/li&gt;
&lt;li&gt;化学品-&amp;gt;性别&amp;lt;-疾病&amp;lt;-化学品&lt;/li&gt;
&lt;li&gt;化学品-&amp;gt;性别&amp;lt;-疾病-&amp;gt;化学品&lt;/li&gt;
&lt;li&gt;化学品-&amp;gt;性别-&amp;gt;疾病&amp;lt;-化学品&lt;/li&gt;
&lt;li&gt;化学品-&amp;gt;性别-&amp;gt;疾病-&amp;gt;化学品&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们的专业知识或者语义本身就可以缩小待检验的模型，例如性别是先天的，所以凡是有化学品决定性别的都可以排除掉，疾病也不可能决定性别跟化学品，此时我们就剩下六种模型了：&lt;/p&gt;
&lt;h2 id=&#34;三者都没关系-1&#34;&gt;三者都没关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品 性别 疾病&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三者里只有一组两两关系-1&#34;&gt;三者里只有一组两两关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品&amp;lt;-性别 疾病&lt;/li&gt;
&lt;li&gt;化学品 性别-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;性别 化学品-&amp;gt;疾病&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三者里有两组两两关系-1&#34;&gt;三者里有两组两两关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品&amp;lt;-性别-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;性别-&amp;gt;化学品-&amp;gt;疾病&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三者里有三组两两关系-1&#34;&gt;三者里有三组两两关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品&amp;lt;-性别-&amp;gt;疾病&amp;lt;-化学品&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里假如我们发现性别确实会同时影响化学品跟疾病，那么就必须考虑控制性别后的净效应。假如性别只影响化学品不影响疾病（例如女性喜欢用化妆品暴露量更大），那么那么我们则不需要控制性别可以直接考察化学品对疾病的影响。如果性别也不影响化学品，那么也可以直接考察化学品对疾病的影响。这就是三种化学品跟疾病有关系的模型。如果化学品跟疾病本就没关系，甚至性别也跟疾病没关系，例如这是一种细菌性传染病，那从一开始研究就没有因果关系支持，做出的结果就属于玄学了。但一定不要直接排除掉这些可能，不论常识还是专业知识都排除不了就需要逐一考察。&lt;/p&gt;
&lt;p&gt;通过分析这三种模型，我们会发现要首先检验性别跟疾病是不是有关系，确定了这一条，后面就知道是不是要考虑按性别分组了。此时辛普森悖论从实际意义上就解决了，靠的其实还是我们自己赋予的实际语义与专业知识，但别忘了专业知识可能本身就是错的，实际语义可能存在二义性，这些才是搞出悖论的根基。&lt;/p&gt;
&lt;p&gt;不过如果我们看回原始版辛普森悖论，里面的三体是性别、学院及录取率，要解决这个问题就还是要把23种可能性全列出来然后排除掉学院决定性别、录取率决定性别这些语义跟常识上不存在的可能性，然后就可以知道需要检验的是什么了，但这里可以看出辛普森原版悖论可排出的可能性要比我的例子少，因此需要检验的模型就更多，跟性别有关决定录取率的模型有关的有五个。我们来看下其中两个有三组两两关系的模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;性别-&amp;gt;学院&amp;lt;-录取率&amp;lt;-性别&lt;/li&gt;
&lt;li&gt;性别-&amp;gt;学院-&amp;gt;录取率&amp;lt;-性别&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;学院决定录取率还是录取率决定学院这个是不太好说的，因为这两者可能并不是谁决定谁的，可能被学校同时控制，例如学校会决定学院规模与最低录取率之类，此时就不是三体问题，因为另一个变量学校又出现了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;性别-&amp;gt;学院&amp;lt;-学校-&amp;gt;录取率&amp;lt;-性别&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此时因果图出现一个对撞结构，你要是需要控制学院就开了后门无法正确估计性别的作用，此时就不应控制任何变量直接看两者关系。但真实世界哪有这么简单，学校这个因素通常我们根本观察不到，所以也不好假设其与学院的关系，如果学院可以影响学校决策，此时就还是要控制学院。也就是说辛普森悖论是否有解其实完全要依赖实际存在的因果关系。&lt;/p&gt;
&lt;p&gt;Judea Pear 曾经写过一份辛普森悖论的技术&lt;a href=&#34;http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf&#34;&gt;报告&lt;/a&gt;，里面给出了一种无限层级的因果图，他起了名叫做辛普森悖论机。在那张图里，如果你顺次控制混杂因素与对撞因素，那么原因与结果就会反复出现需要控制变量与不需要控制变量的情况。也就是说，想解决悖论，重要的是解决背后的因果图，不同语境逻辑或因果图下对其他变量控制与否是可以完全不同的。我们最终只能接受一个相对语境下的正确答案，绝对语境下那个数学问题其实并没有意义。&lt;/p&gt;
&lt;p&gt;也就是说，辛普森悖论其实暗示了我们不同自洽逻辑下可以出现不唯一的正确解，而这个正确与否取决于你对其背后因果关系模型的假设，而这种假设可以不唯一。好消息是自然科学中检验假设很大一部分可以通过实验来证实或证伪，但坏消息却是我们日常交流用的语言与语义可能天然就无法将因果关系描述清晰，能量化的数值无物理意义而有物理意义的数值无法量化，这就导致我们只能看到模型下的真实而无法验证模型本身。这部分内容在科学哲学里就涉及实在论与非实在论了，也是催眠利器。&lt;/p&gt;
&lt;p&gt;或许我们所谓解决悖论的方法不过就是引入一套模型屏蔽掉会产生悖论的讨论，但这反而说明了悖论的无解与我们自然语义天然存在漏洞。至于说不同学科根据自己学科利益来报道结果倒也不用太担心，这类弱效应的成果最多搞出一堆保健品与智商税产品，基本上无害。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>平行世界的统计推断</title>
      <link>https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/</link>
      <pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/</guid>
      <description>


&lt;p&gt;最近重新看了下之前对p值的笔记，突然对零假设充满了陌生感。在p值的语境里，当我们看到数据D在零假设下发生的概率低就会做出数据D不支持零假设的判断，这是一个条件概率等价替换的问题：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$p(H0|D) = p(D|H0)$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这个当然是有问题的，我真正关心的是零假设是否成立而不应该是数据出现在零假设下概率。而这在假设检验的设计中转化成了零假设成立下出现观察数据的概率，这里最大的不对劲在于数据D不支持零假设依然无法判断零假设是否成立。对于实验科学，我们能收集数据D，但检验零假设似乎没什么道理，零假设是来自于随机过程，我们真正关心的从来都是产生差异的过程，现在却要用随机过程来检验产生差异与否，总感觉哪里不对。&lt;/p&gt;
&lt;p&gt;不过重读了丁鹏老师的&lt;a href=&#34;https://cosx.org/2019/05/recheck-the-lady-tasting-tea/&#34;&gt;文章&lt;/a&gt;后，我意识到当年Fisher搞出的统计推断的背景的女士品茶实验其实是穷举所有可能，因为品茶结果是离散的，所以穷举空间是有限的，此时发生各种情况的概率是离散的，这也就形成了Fisher精确检验的基础。这里面容易被忽视的点在于品茶实验中品茶顺序与结果是无关的，也就是形成正确答案的顺序与答案无关，也就是说如果数据D是真理，那么随机化过程不影响真理对错。&lt;/p&gt;
&lt;p&gt;那么回到前面的问题，Fisher这个p值设计实际上巧妙的规避了&lt;code&gt;$p(H0|D)$&lt;/code&gt;与&lt;code&gt;$p(D|H0)$&lt;/code&gt;的区别，因为在这个离散概率的语境下，空假设提供的实际是一个虚拟的有限平行宇宙，当某个假设成立时，数据一定会支持假设而不成立时数据在有限平行宇宙里只会以很低的概率出现，&lt;code&gt;$p(H0|D)$&lt;/code&gt;与&lt;code&gt;$p(D|H0)$&lt;/code&gt;的讨论在这里就没意义了，虚拟的有限平行宇宙实际是给出了所有可能性空间。也就是说，产生差异这件事被零假设转化成了虚拟的有限平行宇宙，其中有一个宇宙产生了差异，那么这个宇宙下待检验的假设就可以被认为是证实的。&lt;/p&gt;
&lt;p&gt;换句话说，Fisher的零假设是包含了所有假设在内的假设宇宙，在某些宇宙里出现某些数据是正常且合理的，但这些宇宙相比所有假设宇宙的空间非常小，那么在这些宇宙里数据背后对应的规律应该就是真实的。如果假设宇宙本来就不多（当然这个只会在离散结果的条件下出现），那么事实上就形成了统计功效不足的问题。前面我意识到的不对劲其实是误会，因为我脑子里还放着与零假设对应存在的备择假设这个东西，所以我会纠结是不是检验错了。但事实上，备择假设本就是零假设的一部分，其独特性是通过在可能性空间的低概率来表征的。从这个逻辑上看，Fisher的精确检验并不存在现在普遍使用的零假设备择假设这套体系里的问题，而且只要转化一下，Fisher精确检验一样可以用在更广的领域。&lt;/p&gt;
&lt;p&gt;这里最大的问题在于可能性空间的低概率跟数据存在规律性需要是一个东西，这也是Fisher精确检验的核心。现在很多对p值的质疑可能来自于对可能性空间认识的偏误，对于很多人而言，可能性空间是数学意义上的无限空间，但对实验结果而言，在一定观测精度下可能性空间其实是有限的而观测的数据又是一定存在于这个空间之内的，这就导致概率低下进而我们会认为规律存在。举个例子，如果观察我开门这个动作100次，那么我每次都用右手这个事件对比&lt;code&gt;$2^{100}$&lt;/code&gt;这个可能性空间而言概率很低但存在，这件事可能让观察者得出我是右利手的规律性结论。这里统计推断只负责告诉你概率，怎么解释是观察者自己决定的事，p值0.05就是个方便决策的阈值但统计学更关心概率怎么计算。&lt;/p&gt;
&lt;p&gt;也就是说现在对p值的质疑是集中在后面决策步骤上的，但这个决策标准其实本来应该各个学科根据自己的学科规律可能性空间来制定，不能简单把锅甩给统计学家，毕竟他们对不同学科规律可能性空间其实并不了解。很多时候重复性不好的本质是所谓规律性在可能性空间里并不稀有，随机过程就会发生，这个时候应该做的是对规律性给出更严格的定性定量要求与描述，还有种可能就是本来就是伪规律，是噪音被当成了规律，天知道科研人员为了混饭吃会不会把其实不稀有的偶发事件当成规律来报道，这时候应该被质疑的其实应该是实验者而不是统计学决策工具。&lt;/p&gt;
&lt;p&gt;我不太清楚后面是怎么把p值从离散分布推广到0到1之间的连续均匀分布的，但现在我倒是有兴趣看下p值本身在平行宇宙里的分布了。如果有规律的事实在实验限定的空间里发生，其p值的分布应该会与随机过程产生的p值不一样。这里我不打算采用多次随机抽样，因为此时分布事实上是已知的，此时进行随机实验其实是在假设分布存在且成立的条件下判断事实。相反，我会随机生成一组数据但保留这一组数据当成既成事实，但随机化分组过程来检验p值的分布，此时应该更符合事实存在后对假设的判断这个思路，这个应该更贴近Fisher精确检验的思想。这里我们考虑三种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;真实差异固定&lt;/li&gt;
&lt;li&gt;完全是随机数&lt;/li&gt;
&lt;li&gt;固定的真实差异加上随机数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第一种情况是规律完全成立；第二种是完全无规律；第三种是可观察或可测量的数据。生成三组数据后我们对其分组（简单二分）进行10000次随机化操作，然后进行t检验，记录并观察p值。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
# 真实差异
x &amp;lt;- c(rep(100,260),rep(200,260))
# 随机差异
xr &amp;lt;- rnorm(520)
# 考虑误差的真实差异
xm &amp;lt;- c(rep(100,260),rep(200,260))+rnorm(520)
p &amp;lt;- pr &amp;lt;- pm &amp;lt;- c()
for(i in 1:10000){
        # 随机化分组
        g &amp;lt;- factor(sample(c(1,2),520,replace = T))
        p[i] &amp;lt;- t.test(x~g)$p.value
        pr[i] &amp;lt;- t.test(xr~g)$p.value
        pm[i] &amp;lt;- t.test(xm~g)$p.value
}
# 探索p值分布
sum(p&amp;lt;0.05)/length(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0481&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(p&amp;lt;0.5)/length(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5084&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(p&amp;lt;0.9)/length(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8978&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(pr&amp;lt;0.05)/length(pr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0514&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(pr&amp;lt;0.5)/length(pr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4994&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(pr&amp;lt;0.9)/length(pr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9014&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(pm&amp;lt;0.05)/length(pm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0493&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(pm&amp;lt;0.5)/length(pm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5023&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(pm&amp;lt;0.9)/length(pm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8991&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(3,2))
hist(p,breaks = 20)
hist(p,breaks = 100)
hist(pr,breaks = 20)
hist(pr,breaks = 100)
hist(pm,breaks = 20)
hist(pm,breaks = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这个结果非常有意思，第一个能看到的现象是如果数据本身存在规律性，那么p值的分布是一个离散分布。这个分布介于0到1之间，越接近0的部分越密集，越接近1的的部分越稀疏，但是如果计算小于0.05，0.5，0.9的比例情况，会发现这种稀疏分布依旧符合均匀分布的概率分布特征。如果数据不存在规律性，那么p值的分布就是很均匀的。如果数据混合了规律性与噪音，依然会显示出这种离散分布特征。下面我用qq图来观察下这个分布跟均匀分布的区别：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
ref &amp;lt;- runif(10000)
par(mfrow=c(1,1))
qqplot(ref,p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqplot(ref,pr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqplot(ref,pm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;可以看到，如果数据本身存在规律性，其随机化分组后的p值虽然跟均匀分布很接近，但qq图上确实会表现出前密后舒的螺旋延伸状态。&lt;/p&gt;
&lt;p&gt;我虽然不清楚统计学上有没有对这个p值分布的研究，如果没有我先管它叫 MY Distribution ，谁让我名字缩写就是MY，这个“我的分布”可能对实验学科非常有意义。&lt;/p&gt;
&lt;p&gt;这里为了区别我再做一个仿真，这次我不是对分组随机而是对采样随机：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
# 固定分组
g &amp;lt;- factor(c(rep(1,260),rep(2,260)))
p &amp;lt;- c()
for(i in 1:10000){
        # 随机化采样
        x &amp;lt;-  sample(c(rep(100,260),rep(200,260))+rnorm(520),520)
        p[i] &amp;lt;- t.test(x~g)$p.value
}
# 探索p值分布
sum(p&amp;lt;0.05)/length(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0406&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(p&amp;lt;0.5)/length(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5344&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(p&amp;lt;0.9)/length(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9356&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(p,breaks = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(p,breaks = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqplot(ref,p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这里我们同样能看到这种蛇形走位，而且似乎随机化样品看到的趋势更明显。但同样的，这里我的模拟逻辑还是保持数据不变，只是随机化过程。&lt;/p&gt;
&lt;p&gt;实验学科已经被多重检验问题困扰了很久了，通常演示p值分布很多人是喜欢从一个已知分布里反复抽样形成差异，此时的p值分布是有偏的：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
pvalue &amp;lt;- NULL
for (i in 1:10000){
  a &amp;lt;- rnorm(10,1)
  b &amp;lt;- a+1
  c &amp;lt;- t.test(a,b)
  pvalue[i] &amp;lt;- c$p.value
}
# 探索p值分布
hist(pvalue,breaks = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(pvalue,breaks = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;然后因为随机抽样得到的是p值均匀分布，所以很多错误发现率的控制算法都是在想办法区分这两种分布。现在常用的BH矫正、Q值法或者Bonferroni法都依赖检验数量与前面说的分布差异来决定判断标准或者控制整体错误率。这个看上去很合理，因为你检验多了出现随机相关的可能性就是高了。然而这个过程又很不合理，因为我们测量时有时候并不知道测量的维度是不是跟分组有关系，只是顺手测了，这类测量浪费了大量的统计功效。这也是传统实验学科跟组学实验学科经常扯皮的地方，传统做一对一的控制实验，如果测到了一个有意义的信号就可以发表，但技术进步后，因为我同时测了其他其实无意义的信号，那些有意义的信号被掩盖在随机相关里了。前面所说的多重检验问题的处理思想通常是保留强信号，这样本身有规律的弱信号就被自动忽略了。也许我们可以认为效应比较弱的信号不如效应比较强的信号，但只要存在规律性，作为研究人员就一定要去搞清楚是咋回事而不是用统计学工具给自己做挡箭牌。&lt;/p&gt;
&lt;p&gt;这样，前面那个p值分布的意义就很明确了：如果规律会造成数据异质性而我们的分组过程就是试图发现这种规律性，那么不可避免的会在p值分布上造成离散分布的状态。相反，随机相关则不会呈现出这种p值的离散分布而是均匀分布。这个均匀与离散分布的差异如果能用一个统计量来描述，那么我们事实上就能根据这个统计量（暂且命名为统计量MY）区别出真实规律与随机相关。我现在能想到的构建方法非常原始，就是对直方图概率密度曲线的0.9到1这一段找最大值与最小值，如果比值超过一个阈值就认为有规律性，否则就认为是随机数据。但应该有更数学化的构建方法。&lt;/p&gt;
&lt;p&gt;在这个语境下，我们就不用搞这些p值的阈值矫正了，直接对每一次假设检验进行分组随机化模拟过程，然后生成p值分布。如果其MY值表示为离散均匀分布，那么这一组假设检验的规律性就是有保证的，如果指示为均匀分布，那么这组数据本身就可以判定为无法检测规律而排除。这样我们可以对数据在进行统计推断前做一个规律性测试，只有通过了规律性测试的数据才值得进行统计推断。而且，只要单次统计推断给出小于0.05的p值，我们就可以直接相信，因为那些可能出现随机相关的数据已经被我们排除掉了。&lt;/p&gt;
&lt;p&gt;不过，到这里我的知识水平已经到头了，因为这个MY值如何构建我是不知道的，现阶段我能想到的解决方案一个是直接用眼看，一个是动用图像识别的机器学习算法识别这类图像，也就是让机器看。不过这个思路应该问题不大，说白了就是利用模拟探索产生真实数据的可能性空间或平行宇宙而不是利用分布产生仿真数据，后者其实是先定在了某一种宇宙之中，这里基本延续了Fisher精确检验的思路，计算上也是可行的。&lt;/p&gt;
&lt;p&gt;其实我也不是凭空想思考这个问题，前两天中午有个报告里演讲者提到了Fisher精确检验跟因果分析的关心，我当时忙活着做午饭没听明白她到底讲了啥，但Fisher精确检验的思想确实听明白了。然后吃着午饭就想到了这个随机化分组是不是影响p值的问题，一开始我用的是直方图的默认输出，结果发现p值总是均匀分布感到很丧气，但因为在模拟中本来是1000次多输了一个0就打算看看更精细的直方图，这才看到了这个被掩盖的p值离散分布。我应该不是第一个看到这个分布的，但将其用到替换多重检验的错误率控制上应该是个比较有前景的应用，欢迎读者来给我拍砖，要是没人写过论文（不大可能）也可以拿去写论文，我知道的已经都写出来了，欢迎引用或合作。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>暴露组学中的数据挑战</title>
      <link>https://yufree.cn/cn/2021/03/27/exposome-challenge/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/03/27/exposome-challenge/</guid>
      <description>&lt;p&gt;最近做了次闭门分享，是关于暴露组学中存在的数据挑战，也借机整理了下暴露组学的资料，这里也留个底。&lt;/p&gt;
&lt;p&gt;暴露组学认为生物表型或疾病是环境与基因在时间尺度上交互作用的结果，也就是：&lt;/p&gt;
&lt;p&gt;$$P(t) = G(t) * E(t)$$&lt;/p&gt;
&lt;p&gt;而暴露组学更多关注环境影响及其与基因的交互作用，毕竟单纯的基因影响已经被基因组那边搞得差不多了。&lt;/p&gt;
&lt;p&gt;暴露组学(exposome)最早是2005年C. P. Wild 在期刊 Cancer Epidemiol Biomarkers Prev 的&lt;a href=&#34;(https://cebp.aacrjournals.org/content/14/8/1847.short)&#34;&gt;社论&lt;/a&gt;上提出的。2010年，Rappaport 与 Smith 在 Science 上发表了题为 Environment and Disease Risks 的展望&lt;a href=&#34;https://science.sciencemag.org/content/330/6003/460&#34;&gt;文章&lt;/a&gt;，认为暴露不应限制在直接接触到的化学物质，也要考虑更广义的暴露，例如微生物暴露与生活压力等。&lt;/p&gt;
&lt;p&gt;2015年，美国环保署举办了 &lt;a href=&#34;https://sites.google.com/site/nontargetedanalysisworkshop/&#34;&gt;Non-Targeted Analysis Workshop&lt;/a&gt; 来讨论环境与生物介质中外源化合物的标准筛选方法、标准品制备与谱数据库的开发，后来演变成了涉及来自学术界、政府、公司近30家实验室的 ENTACT(EPA&amp;rsquo;s non-targeted analysis collaborative trial) 项目。ENTACT 项目的参与单位包括八家政府机构 (California Dept. of Public Health, California Dept. of Toxic Substances Control, Eawag, EPA, NIST, Pacific Northwest National Laboratory, Research Centre for Toxic Compounds in the Environment, US Geological Survey)，五家公司（AB Sciex, Agilent, Leco, Thermo, Waters）与十五家学术机构（Colorado School of Mines, Cornell Univ., Duke Univ., Emory Univ., Florida International Univ., Icahn School of Medicine at Mt. Sinai, North Carolina State Univ., San Diego State Univ., Scripps Research Institute, Univ. of Alberta, Univ. of Birmingham, Univ. of California at Davis, Univ. of Florida, Univ. of Washington, WI State Laboratory of Hygiene）。在环境领域要想做 NTA 最好去这些地方，因为参与 ENTACT 项目的机构定期会测盲样进行方法比对，基本可以接触到 NTA 最顶尖的技术，相信以后相关环境标准也会脱胎于这个项目。欧洲也有个类似的项目叫做 &lt;a href=&#34;https://ec.europa.eu/programmes/horizon2020/en&#34;&gt;Horizon 2020&lt;/a&gt;，国内目前还是野生游击队状态，不论学术界还是业界大都在炒概念，落地案例有限。&lt;/p&gt;
&lt;p&gt;除了化学污染物，暴露组学也会涉及到社会科学的研究方法。2016年，Global Burden of Disease (GBD) 项目估计全球59.9%的死亡来自各类外部风险，16%的&lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/29056410/&#34;&gt;全球死亡&lt;/a&gt;来自于水、大气、土壤污染，其造成的健康相关开支每年约4.6万亿美金（16%的全球经济产出）。基于双胞胎的&lt;a href=&#34;https://www.nature.com/articles/s41588-018-0313-7&#34;&gt;研&lt;/a&gt;&lt;a href=&#34;https://www.nature.com/articles/ng.3285&#34;&gt;究&lt;/a&gt;也发现遗传因素大概能解释49%的人类特质，剩下的部分就可能来自各类广义上的暴露。&lt;/p&gt;
&lt;p&gt;基于化学污染物的暴露组学在方法学方面主要借鉴全基因组关联分析研究（GWAS）发展为&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0010746&#34;&gt;全暴露组关联分析研究（EWAS）&lt;/a&gt;。但广义的暴露组学涉及的学科非常广，不同学科背景的研究人员可能会使用完全不同的研究思路，整合这些跨学科知识是非常困难的，学科术语墙造成的交流障碍及精细化专业分工导致的研究人员的视野狭隘经常让合作举步维艰。最简单的例子就是当你真的系统性发现某种暴露的意义是远大于另一种暴露时，这对研究重要性比较弱的那部分科研人员的事业发展是毁灭性的，经济或者利益集团在某种程度上已经阻碍了综合性科学问题的探索。&lt;/p&gt;
&lt;p&gt;暴露组学核心科学问题有两个：窗口期与组学。因为健康相关的暴露问题大都是慢性且长期的暴露，需要根据时间尺度变化来推导影响，遗传因素当然也会有时序表达问题但更为保守些，环境因素变化影响可能更大，很多暴露的影响存在窗口期，例如人们孕前或幼儿时期的暴露可能敏感度更高。目前在数据分析方法上，时间序列分析可用来研究被观察者时间尺度上的变化，显著性差异分析可以在实验设计中研究被观察者在单暴露因素下的变化，所谓窗口期，就是找出时间序列分析中被观察者在单暴露因素下的变化时间段。分布滞后模型 Distributed Lag Models (DLM)常被用来讨论时序数据中相关系数与回归因子在时间尺度上的变化。&lt;/p&gt;
&lt;p&gt;而组学涉及更多的是多暴露问题，也就是暴露组学研究并不只关注一种暴露而是系统性关注多种暴露及其相互影响，这点提高了研究的复杂度。现在多暴露数据来源大体可以分为三类：问卷、生物样品及环境样品。问卷数据可以是流行病学调查报告、基于行为的人群画像、基于邮编的社会经济地位或医院的电子病例还有心理学量表；生物样品可以是人的血样、尿样、粪便、头发、牙齿、指甲、汗液等；环境样品则可以是室内灰尘饮用水这类比较个人化的样品分析，也可以是遥感数据、环境监测或被动采样技术下拿到的区域数据流，还可以是更大尺度上的气候变化模型的预测值。这里单人单样本单时间点的暴露组维度可以上万，毕竟就算描述一个小分子，我们能给出的分子描述符也可以成千上万，暴露组涉及成千上万的小分子与各类其他指标，这里降维是必须要做的，不然单是描述暴露组都成问题。不过暴露组并不像遗传信息那样比较稳定，暴露组的动态变化是一定存在的，不同疾病的相关暴露组是不一样的，提高动态数据中信噪比的难度不小。当前的数据分析思路就是通过构建整体影响指标来指代不同污染物的综合加权影响，但又要保证可以回溯出单一污染物的影响。用统计学语言来说就是构建潜在变量，计算不同暴露在该变量上的投影。说到这里可能你会认为不就是因子分析，但我们能拿到的真实数据并不总是连续的，有些还存在严重的缺失问题，对此加权分位数加和回归 Weighted Quantile Sum (WQS) regression 提供了一种思路。&lt;/p&gt;
&lt;p&gt;从数据挑战上看，除了关键的术语壁垒问题，另一个挑战就是高质量的数据采集与管理，这看上去像是技术问题但所有洗过数据的人都知道其中存在多少莫名其妙的问题，这里行业内一定要同一标准，否则大量资源会被浪费在高噪音数据中。高维数据处理其实可以借鉴机器学习的一些思路与方法，但一定要先理解实际问题，因为现在仪器能采集的信号实在太多，最好的降维就是利用专业知识排除掉噪音。缺失值与宏模型训练也是一个挑战，一个人的暴露组数据几乎一定是不全的，你可能只有A的血样与环境样品而有B的尿样与调查问卷，这里很有可能A跟B的数据都包含了足够预测某种疾病的信息，这里就需要训练一个模型的模型来处理暴露组不全的预测问题。此外，暴露组与基因组给出的结论可能在传统的病理学研究看来是离经叛道甚至不合逻辑的，如何跟传统学科对接也是要处理的问题。当然，前面的讨论还都是建立在有数据的前提之下，暴露组还面临个人隐私泄露相关的医学伦理问题，恐怕只能依赖密码学的发展提供工具。&lt;/p&gt;
&lt;p&gt;在当前的阶段，讨论暴露组学更多还是在厘清科学问题并搭建方法与模型的阶段，虽然研究目标很明确但涉及问题过于复杂。我现在其实在为回国找工作，问了一圈高校的环境学院，能单独配高分辨质谱做相关研究的地方一只手就能数过来，现在已经基本放弃去环境学院打算找医学院或交叉学科研究机构了。不过这个方向我还是很看好的，越是复杂的问题，解决起来才有挑战性，如果以后继续科研这条路，我会设计一个上万人的队列研究，追踪一批人至少十年收集一组可重复进行各类研究的样品与数据，希望能得出些许靠谱点的结论。只希望技术发展再快一点，把追踪的成本降到合理范围，用高质量数据回答科学问题而不是像现在这样天天证（chao）明（zuo）概念。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>jupyter与容器技术</title>
      <link>https://yufree.cn/cn/2021/01/28/container/</link>
      <pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/01/28/container/</guid>
      <description>&lt;p&gt;我们现在评价科研成果，经常喜欢用当前的共识这个表述，但前沿领域的共识不见得是对的。更严格说，所有涉及观点站队问题的讨论都不属于科学讨论，面对质疑经过合格训练的科学家都不会简单采信某一方观点而是要去看原始数据或在自己的实验室重现，让事实说话要比空对空有意义的多。不过重复实验在现代社会能不能拿到经费是存疑的，在当前经费获取的方式下，重复研究别人的成果是费力不讨好的，更多人重复他人实验是为了基于此进行进一步研究，如果重复不出有些课题组会尝试其他路径，有些就去联系原作者询问实验细节。&lt;/p&gt;
&lt;p&gt;我博士阶段末期就收到过关于一组植物愈伤组织实验重复的询问，当时我特意去重复了实验还发了操作过程照片过去，结果那边还是重复不出来。我到现在也搞不清楚是哪里的问题，因为我重复后结果跟发表的论文是一样的，但对方咋做的就不知道了，而且他那边其实对我的体系进行了本地化修改，暴露物也换掉了，但在我看来应该不影响结果。因为这段往事让我意识到可重复性里面可能包含了质疑者与原作者都没意识到但很重要的步骤，而这个步骤直接导致了重现性不好，这就是真实科研或者说实验学科会遇到的问题。好比一份菜谱给出来，两个厨子炒出了两份口味不同的菜，如果菜谱符合要求，那么一定是存在菜谱外的东西影响了结果。这种情况在化学实验里不常见，因为化学实验体系通常比较简单，但生物实验就非常常见了，同样成分的培养基经常一个能养活细胞，另一个养啥死啥，这也是为什么生物论文实验里用的材料是要标注厂家信息的，因为很多实验对材料的要求只能用厂家品控来保证重复性。&lt;/p&gt;
&lt;p&gt;实验的可重复性眼下可控性里操作空间还是有的，但眼下实验完成后的数据处理与共享部分如果透明化，那么会极大挤压学术不端的空间。虽然同样一组数据在不同的分析方法或统计模型下可能得到完全不同的结论，但只要你能把数据如何分析的过程及原始数据共享出来，那么也是可接受的。打比方要做一组高维数据的机理与预测模型，你用随机森林选出一组重要变量并基于此构建了逻辑通顺的机理模型，隔天用同样的数据别人用线性混合模型又选出一组变量也构建了逻辑通顺的机理模型，但这两个模型降维上用了完全不同的策略与假设，导致两组模型虽然都说得通但都可能只解释了真相的一部分，这个也属于常见现象。&lt;/p&gt;
&lt;p&gt;当前期刊一般都会要求上传原始数据，但这是不够的，博士阶段我读了一篇论文看到了里面一种计算方法很有意思，但他们给的是数学公式，我是在matlab上重现后才能去验证。这个操作对于实验学科的人而言门槛过高，绝大多数实验学科研究人员的数据分析水平不会超过调用别人写好函数处理数据的阶段，指望自己写需要自学很多东西，这虽然应该是合格研究人员应具备的素质，但难度还是有的。我现在读很多论文可以感到明显的割裂感，就是实验部分与数据分析是分工来做的，这就导致很多描述非常不准，例如简单利用p值来说明结果而意识不到p值本身的问题，很多数据分析方法描述非常奇怪，明明一两句就能说清楚但却自己定义了一大堆东西来绕弯，很明显对分析方法原理没搞清楚。&lt;/p&gt;
&lt;p&gt;这属于无效协作，实验方与数据分析方想按照分工原理来提高效率，但最后展示出来的则是一团乱麻。一篇论文一定要有一个人能同时理顺实验与数据分析的所有步骤，这看上去很不合理但没办法，从我跟单位里所谓专业统计分析人员打交道的体验来看，那种强调要把数据做成他们那种行是样品列是特征的标准格式的要求是荒谬的，真实数据要转化到那种标准格式是要进行大量假设的，这部分实验方通常不了解，数据处理方又不管，最后给出的结果常常导致实验方无法验证而数据处理方又对数据中普遍存在的异常值与确实值大为恼火。在我看来根本就不能割裂开实验与数据分析，这两步需要同一个人来做，而且职业做实验的一定会被自动化仪器取代而职业处理标准数据分析的也一定会被自动化软件取代，唯独互相连接的人是无法被技术取代的。&lt;/p&gt;
&lt;p&gt;当前的解决方案对于实验人员而言就是保留完整的数据分析脚本，这个本来很正常的需求因为图形化商业软件的流行而被认为很不友好。说句不好听的，这就是被图形界面给惯坏了而忘了科研中对重复性的要求，而且大多数专业图形界面的数据分析软件其实也会记录操作步骤，你的每一步点击都会在一份记录中被保留，所以数据分析步骤与图形界面并不矛盾。不过从实际数据分析角度，如何给出脚本确实是个技术问题而jupyter项目则在一定程度上解决了这个问题。&lt;/p&gt;
&lt;p&gt;在介绍Jupyter项目之前，我想说如果你的数据分析完全依赖 R 与 Python，那么自带 RStudio 服务器版的 Rocker 镜像配合 Rmarkdown 文档就已经可以实现数据分析的完全可重现性了，甚至 Rmarkdown 文档本身就可以作为完整数据分析步骤的良好载体而附加在论文附件里来保证可重复性。当然如果你懂一点R包开发，把分析方法作为模版嵌到一个R包里也是没问题的。今天想说 jupyter 项目，纯粹是因为我最近考古发现现在 jupyter 已经从 Python 的轻量级在线开发环境成长为多语言支持的平台了，其部署上也非常容易。当然，其对学术写作生态的支持对比 Rmarkdown 的生态还是弱了非常多，更偏探索，当然依赖pandoc的核心都可以互相转换，但感觉学术界，特别是实验学科目前对R的接受度更高，毕竟学术数据分析所需要的统计工具 R 基本都有现成的，Python 在这方面虽然机器学习的包更全，但实际研究里需要的工具就不完整。R 包的社区里存在大量即懂专业知识又懂统计分析的人，会给出很多研究人员直接用的函数，当然很多开发者并不太在意效率问题。Python的社区里也有科研人员，但实验学科的不多，整体社区偏软件工程偏通用计算问题。不过理想状态是两种语言都掌握，一种做到开发级，另一种做到应用级对于绝大多数科研数据分析问题就都能处理了。&lt;/p&gt;
&lt;p&gt;Jupyter 项目最开始就是专门为 python 设计的，后来逐渐发展壮大，可以支持更多的语言，前提是你要把对应的核装上保证交互通信畅通。这里顺道也捋一捋 Python 的安装问题，现在人装软件都是全家桶，你单装一个 Python后面一样有大量的依赖问题，因此大多数都是去装一个anaconda，这是基于Python的数据处理和科学计算平台发行版，但其实也支持R。可以把 anaconda 理解为 TeX Live 之于 TeX 排版系统的关系，作为发行版，基本要囊括编程语言本身、集成开发环境（IDE）、常用包及包的管理器。例如，TeX live里就会打包排版引擎、参考文献处理引擎、文档格式转换、字体、常用宏包、包管理器 tlmgr、文档编辑器 TeXworks 等几乎所有你可能用到的工具与文档。anaconda 里你可以装 Jupyter notebook作为IDE，也可以装PyCharm作为IDE，甚至可以装RStudio，其包管理用的是 conda，用法上类似 Python 的 pip 安装，但 conda 不仅仅支持python，你是可以用 conda 来装 R 包的，而且其解决依赖问题也比较智能（pip其实也可以做到）。不过，因为 anaconda 本身也是个公司，有付费产品，免费产品界面也有点花哨，所以很多有洁癖的人会选 miniconda 这种精简版。&lt;/p&gt;
&lt;p&gt;Jupyter 经典版就是交互式笔记本，也是最早得以流行的核心功能，用户可以在代码块里写代码，然后运行代码块直接看到结果。熟悉 R 的会发现这跟 knitr 的功能类似，不同点在于 knitr 对于代码块的控制更多，侧重一次编译出带结果的成品文档，而Jupyter notebook 侧重实时输出结果或再现结果，更接近 REPL 但不算是个好的开发工具。在实际写文档时，相信多数人会选择调用包里的函数而不是现写一个，所以 Jupyter notebook 在交互要求高时也还算不错。虽然大多数人用 ipython 作为解释器，但通过 IRkernel 这个R包并初始化后其代码块也可以执行 R。在 RStudio 里，也可以创建类似的 R Notebook，并用 reticulate 包来使用 python。Jupyter 笔记本是完全采用网页界面形式进行交互（其实 RStudio 也是），笔记本的下一代产品是 JupyterLab ，这个界面更接近一个全功能的IDE了，也是基于 notebook的，可以装各种插件来提高效率，所以现在上手可以直接从 JupyterLab 开始。&lt;/p&gt;
&lt;p&gt;Jupyter项目中最吸引我的是Jupyter hub，前面的在线笔记本是一个人用的，Jupyter hub可以将笔记本发布到网上供多人使用。在R里面我们做网络应用一般用Shiny，后台跑的是R，如果借助Jupyter hub，我们也可以把一个交互式应用放到网上，这里可以用带有Jupyter hub的docker镜像进行快速部署，也可以借助k8s部署到集群上。如果你只打算在一台服务器上做一个轻量级的在线应用或分享一个笔记本，用户不超过100人，可以直接用The Littlest JupyterHub 来部署，这个应该是个很好的教学工具平台，不过R里也有learnr包作为对比。&lt;/p&gt;
&lt;p&gt;另一个值得关注的项目是binder，在这个项目里你可以直接分享一个笔记本，在这个&lt;a href=&#34;https://mybinder.org/&#34;&gt;网站&lt;/a&gt;，只需要告诉binder你的 Github 库地址就可以，当然这个库里得有笔记本。这个项目相当于给了公共计算资源，你的GitHub笔记本会被生成一个 docker 镜像，然后借助dockerhub展示给用户，这个项目目前是免费的，也支持R，请不要滥用。&lt;/p&gt;
&lt;p&gt;有了jupyter项目与容器技术，科研数据分析的流程就可以实现在线化与可重复性，其实如前所述单纯R的生态也可以做到。这样研究成果发表时应该同时附带对应的数据分析流程报告且最好这份报告也支持在线验证，这样就很容易从技术上堵掉图片误用这类说不清道不明的漏洞。上传原始数据并同时上传数据处理脚本，所有处理过程都让电脑来完成，这样审稿只需要关注处理方法是否合理就可以了，因为这里面从数据到结果没有可以人为干涉的空间。我相信重要的发现一定是可重复的，那么起码要保证数据到结果之间100%的重现性。&lt;/p&gt;
&lt;p&gt;在验证学术问题上，实验数据比专家口水更管用，代码自动化重现要比手动点击靠谱。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>中介分析</title>
      <link>https://yufree.cn/cn/2020/12/25/mediation/</link>
      <pubDate>Fri, 25 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2020/12/25/mediation/</guid>
      <description>&lt;p&gt;最近看到有研究开始把中介分析引入到环境健康分析里来了，考虑到实验学科的数据分析能力长期落后统计分析，我这里也做个中介，把这种方法通过问题导向方式解释下。&lt;/p&gt;
&lt;p&gt;首先说下中介分析的背景，中介分析现在可以划分到因果分析中，但历史比因果分析要长，其解决的基本问题就是回归的解释问题。打个比方，一个人的身高影响因素可能来自于遗传，也可以来自后天营养水平，如果我仅仅研究遗传那部分，那么可以列一个公式：&lt;/p&gt;
&lt;p&gt;$$Y(身高) = X（遗传）+\epsilon$$&lt;/p&gt;
&lt;p&gt;遗传最简单的就是用父母平均身高，这个回归公式可以说是线性回归的起点。然而，父母身高虽然可以作为一个综合指标可以用来做预测，但我们并不知道机理，也就是究竟是什么样的遗传机制决定了身高。今天我们组学技术泛滥，可以在分子水平去定量身高的遗传影响机制，假设我们关注的是基因，也就有了下面这两个公式：&lt;/p&gt;
&lt;p&gt;$$Y(身高) = M_1（基因1）+ M_2(基因2)+&amp;hellip;+M_n(基因n)+\epsilon$$&lt;/p&gt;
&lt;p&gt;$$X(遗传) = M_1（基因1）+ M_2(基因2)+&amp;hellip;+M_m(基因m)+\epsilon$$&lt;/p&gt;
&lt;p&gt;这里基因就成了解释遗传影响身高的中介，要知道这个中介身份不是任意找的，是学科知识引导的，你只有认可基因决定遗传这个生物学遗传学的说法，才能构成这个身高-基因-遗传这个中介关系。也正是因为这一步依赖理论逻辑，所以可以纳入因果分析之中考量，用来回答因果机制问题。这里果就是身高，因的笼统指标就是父母身高，机制指标就是各种基因。这个结构范式很容易用到其他领域，用来解释两个综合指标间发生联系的具体机制，例如在暴露组学中，发现某种污染物与某种疾病间的联系是比较容易的，但解释污染物导致疾病的分子机制就需要毒理学研究。但一个很可能出现的情况就是污染物A影响了代谢物甲，代谢物甲会影响疾病1，但我们却不能直接观察到污染物A与疾病1的关系，这里的盲点在于代谢物甲的调节机制可能是非线性的，但如果不发现这个调节机制我们对污染的评价就是不全面的，这里就需要中介分析。&lt;/p&gt;
&lt;p&gt;在中介分析的语境下，我们听到更多的是直接效应与间接效应，此时有可能同时存在X对Y的直接影响与X通过M影响Y的间接效应。当然你也可以理解为X是一个综合因素，我们提出的M只能解释X的一部分，剩下解释不了的都成了X的直接效应。但无论如何，这个中介物一定要是比X解释性更强的因素而不能是更综合的，否则你没法分析机理，打比方身高你可以用牛奶饮用量作为X，用牛奶里某种维生素作为M，但要是你打算解释维生素如何影响身高，就不能把牛奶饮用量当中介物，统计模型上是没问题的，但因果逻辑上属于胡说八道，除非你打算提出一个新理论说维生素通过改变牛奶成分影响身高，不出意外会被审稿人花式玄学调侃。&lt;/p&gt;
&lt;p&gt;那么中介分析怎么做，其实你问题想明白了解决思路也就比较清晰了，首先做直接的回归：&lt;/p&gt;
&lt;p&gt;$$Y = X+\epsilon$$&lt;/p&gt;
&lt;p&gt;然后，用你的M去解释下X：&lt;/p&gt;
&lt;p&gt;$X = M+\epsilon$&lt;/p&gt;
&lt;p&gt;之后，把M跟X结合起来去解释下Y：&lt;/p&gt;
&lt;p&gt;$Y=X+M+\epsilon$&lt;/p&gt;
&lt;p&gt;从第一个公式，你可以知道X是否影响Y；从第二个公式，你可以知道中介M有没有找对；第三个公式，你可以知道控制了M后X是否还有直接影响。这里所谓的是否有影响就是看回归系数是否与0有显著差异。在最后一个回归上，如果加入M后，X没直接影响了，那么这个中介就能完全解释Y了，也就不用考虑X了。然而，如果存在直接影响，那么就可以进一步评价直接与间接影响大小了。这三个回归做完，基本就清楚中介效应是否存在了。当然，这个框架没法处理我前面说的非线性问题或交互作用，不过基本就能厘清传统中介分析怎么做了。你可以得到中介效应（ME）与直接效应（DE），总效应自然就是第一个回归给出的结果，其实也就是前面两个效应的加和。不过真实实验数据，M与X可能同时被其他因素影响，X与Y也可能被其他因素影响，这些因素可能已知，可能未知。数据的收集也很难符合X跟M都随机的要求。&lt;/p&gt;
&lt;p&gt;此时就要引入因果分析了，其实就是对传统模型泛化，把直接效应与间接效应再各分为控制的（controlled）与自然的（natural）来将交互作用纳入到效应考察之中。这里需要额外做四个假设：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对控制效应与自然效应，要保证X与Y之间没有混杂因素，也就是共果&lt;/li&gt;
&lt;li&gt;对控制效应与自然效应，要保证M与Y之间没有混杂因素&lt;/li&gt;
&lt;li&gt;对自然效应，要保证X与M之间没有混杂因素&lt;/li&gt;
&lt;li&gt;对自然效应，要保证M与Y间没有由X引发的混杂因素&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个数据要求就比较高了，特别是最后两个，但如果假设没问题，可以用 Pearl’s mediation formulas 去估计总直接效应与纯间接效应或总间接效应与纯直接效应，此时X与M的交互作用会被总效应吸收。不过这些计算交给软件去做就可以了，你只需告诉模型是否要考虑交互作用，或者可以都试试。另外，中介效应需要做敏感性分析，一般用bootstrap来做。&lt;/p&gt;
&lt;p&gt;如果你真打算用中介分析，一定要搞清楚你的模型假设是啥，要对数据结构有充分理解，这也是所有因果分析都需要注意的。对数据假设越多，模型系统偏差可能就越多或越少，不要为了用中介分析去用中介分析，有时候很简单的模型或许粗糙，但结论却更可靠，只要你清楚你在干什么。&lt;/p&gt;
&lt;p&gt;参考资料&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.publichealth.columbia.edu/research/population-health-methods/causal-mediation&#34;&gt;因果中介分析的介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/mediation/index.html&#34;&gt;R的中介分析包：mediation 包&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/doing-and-reporting-your-first-mediation-analysis-in-r-2fe423b92171&#34;&gt;基于鸢尾花数据做的中介分析案例&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>降维可视化</title>
      <link>https://yufree.cn/cn/2020/10/28/dimension-reduction/</link>
      <pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2020/10/28/dimension-reduction/</guid>
      <description>&lt;p&gt;高维数据的降维可视化是其探索性分析的起点，无法直观显示的模式或规律很难令人信服，当然能直观显示并不代表对应模式或规律的真实存在。这里我们关注的是高维度样本间的模式或规律而不是维度间的关系，因此可视化也是基于样本间关系的，特别是其异质性或相似性。虽然是三维空间生物，我们的视觉捕捉的是二维图像然后大脑利用透视阴影等信息处理为立体信息，但在可视化问题上搞透视跟阴影经常会产生误导，所以这里只讨论降维到2D平面的方法。样本间的关系在二维平面上无所谓就是用距离来描述，离得近表示样本相似，离得远表示样本差异大，这是降维可视化理论上的出发点，但一定注意很多常用降维方法并不保证这个出发点，需要理解算法搞清楚适用环境与目的，不要为了使用降维而降维。这里讨论下降维可视化常见方法及其在科研中的应用。&lt;/p&gt;
&lt;h3 id=&#34;主成分分析pca&#34;&gt;主成分分析（PCA）&lt;/h3&gt;
&lt;p&gt;主成分分析是科研中最常用的降维可视化手段，其基本原理是方差最大化来选择新的投影方向，新的投影是原有多维数据维度的线性组合。主成分分析的具体实现可以是特征值分解，也可以是奇异值分解（SVD），其中特征值分解是早期主成分分析的主要实现手段而奇异值分解则可以有更广的应用范围，例如不用去算协方差矩阵就可以给出奇异值及容易并行化等。&lt;/p&gt;
&lt;p&gt;主成分分析的降维可视化就是利用方差累积解释度较高（例如超过80%）的维度来展示样品，因为主成分分析可以降序给出各主成分的方差解释度，当降序后前几维方差累积解释度较高时就可以用较少的前几个维度或主成分来可视化较高维度的数据了。至于说各主成分上原始维度的投影及其物理意义就需要数据背后的知识来支持了，新的维度可被看成潜在变量，因此主成分分析可被认为因子分析的一种，用来揭示高维数据背后潜在正交变量。同时主成分分析对数据的要求是存在线性共相关，要是这条不满足，主成分分析无法实现降维。而正交变量有没有物理或生物意义主成分分析是不管的，解释不通可能说明数据不适合主成分分析的线性假设。&lt;/p&gt;
&lt;p&gt;另外需要注意为了各维度线性组合权重类似，各维度要标准化到同一尺度。不满足正态分布的可能还要进行对数处理，不过是否进行对数处理需要考虑维度间是加性还是乘性，因为你对数转化后主成分分析重组的维度描述的是维度间累乘的结果，你的每一步数据处理都影响对可视化的解释。举个例子，如果你给了数据点的长宽高三个维度，那么对数转化后映射出的主成分就是样品的体积差异，说到底还是要理解你数据的物理意义来降维。为了追求统计方法的使用范围而进行的对数化处理是削足适履，统计方法应针对复杂实际问题提出解决方案而不是反过来荒唐地要求事实符合统计假设。&lt;/p&gt;
&lt;p&gt;在分析化学中，测定的物质信号一般与其浓度或含量成正比，此时要不要取对数要明白物质浓度的生物学意义。生物学效应一般符合剂量效应曲线（更本质驱动的是化学反应动力学过程在酶促反应中的表现），很多时候是S型或线性的，此时剂量是要取对数。如果想看到表型上的多维度效应的线性组合，在分子层面需要对浓度信号做对数转换的，这就给了降维时对数处理合理的物理意义。同样的，如果两个分子存在于同一个路径或代谢通路上，其效应是累乘的，此时想在降维中保存样品中这类信息的作用就要做两次对数转化。但如果你不是考察生命过程或考察过程的剂量效应曲线是其他形态的，那就要仔细思考取对数的合理性，你思考越多后面的问题越少。&lt;/p&gt;
&lt;h3 id=&#34;独立成分分析ica&#34;&gt;独立成分分析（ICA）&lt;/h3&gt;
&lt;p&gt;矩阵分解并不只是有主成分分析这一种，如果我们放开对成分间正交的限制，也可以用来提取独立成分。独立成分分析里独立的定义就比较艺术了，不同算法可能使用了不同统计量，但不论哪一种都是用来保证从高维数据中提取相似度高的维度然后压缩为一个新维度。事实上独立成分分析更符合实验科学中对样品描述的假设，但无奈主成分分析还是当前主流。&lt;/p&gt;
&lt;h3 id=&#34;非负矩阵分解&#34;&gt;非负矩阵分解&lt;/h3&gt;
&lt;p&gt;独立成分分析与主成分分析都会引入负数，负数在数学上无可厚非但科学上经常讲不通，因此也有专门针对非负矩阵的分解方式，此时拆出来的新维度之可能是原有维度的加性组合。如果你确定只想可视化数据中的加性组合，可以考虑这种降维方法。&lt;/p&gt;
&lt;h3 id=&#34;多维标度分析mds&#34;&gt;多维标度分析（MDS）&lt;/h3&gt;
&lt;p&gt;多维标度分析的降维思路是定义一个stress函数，最小化低维度坐标距离与高维度样本距离，然后这组坐标就可以捕捉到高维度样本间的关系了。当高维度样本距离是欧式距离时，最小化这个函数等同于求解主成分分析，所以你可以把主成分分析看成多维标度分析的一个特例。但如果距离不是欧式距离或者你对数据进行了非参转化，那么多维标度分析就可以看成一种低维展示样品的技术了。这里可以定义做几维的映射，而且需要的是距离矩阵而非原始数据就可以进行降维，这都是多维标度分析的应用场景更广。当然，跟主成分分析类似，降维都是要丢信息的，可以通过一些统计量来评价降维后是否反映了原始数据的信息。&lt;/p&gt;
&lt;h3 id=&#34;t-sne&#34;&gt;t-SNE&lt;/h3&gt;
&lt;p&gt;数据样本间的关系可以是线性的，也可以是非线性的，在非线性条件下用主成分分析降维效果不会太好，此时可以尝试 t-SNE。t-SNE 是一种基于流形分析的最优化方法，不过这里最优化的是目标是尽可能保持数据间原有的局部关系，MDS或PCA则侧重了全局映射。在 t-SNE中，样本点间的欧式距离用来计算样本间相似度并认为其符合正态分布，优化目标是让新的低维坐标点间距离分布概率与样本相似度距离分布概率接近，而t-SNE的最终输出其实依赖了图论里输出的算法。t-SNE是有些参数要调的同时也依赖随机点，所以每次生成的低维映射会有不同。如果你选这种方法，那就默认你更关注样本间的局部非线性关系。&lt;/p&gt;
&lt;h3 id=&#34;umap&#34;&gt;UMAP&lt;/h3&gt;
&lt;p&gt;UMAP主要是为了克服t-SNE算起来比较慢的问题，其核心算法对不同样本点临近考虑一个距离半径，如果半径跟其他样本点重合就合并为一组，在UMAP中这个半径并非固定的，而是根据样本本身周围样本点的稀疏情况来决定。这样当高维度样本点间关系确定后通过低维输出就可以了，这部分跟 t-SNE 是差不多。t-SNE 与 UMAP 都是依赖流形分析来解决非线性问题，都用到了拓扑结构的思想。不过还是那句话，不能为了用这种方法而用，你写论文方法部分总不能写这玩意高大上我才用吧，虽然不否认有些编辑确实好这口。&lt;/p&gt;
&lt;h3 id=&#34;自组织映射网络som&#34;&gt;自组织映射网络（SOM）&lt;/h3&gt;
&lt;p&gt;自组织映射网络也出现在一些科研实验数据中，这个降维方法的源头在人工神经网络。传统人工神经网络是反馈学习的，自组织映射网络则是依赖竞争学习的。SOM的输出是自定义的，一般都定义成一个二维平面网格，样本来了通过神经元函数去计算其跟哪个网格输出最接近，相似的样品自然会输出到相似的网格上，可视化出来后就可以在自定义的布局上看到自组织出的样品分布。如果你抽取出神经元来看不同维度的加权情况也可以探索下原始维度在布局上的影响。到SOM降维可视化就已经从捕捉相似性到映射相似性了，相应的对读者的数据分析能力要求也越来越高或越来越黑箱，进入魔术状态就不好了。&lt;/p&gt;
&lt;p&gt;其实除了上面说到的，还有很多非线性降维可视化手段，例如Locally Linear Embedding、Neighbourhood Component Analysis、Autoencoders、Kernel principal component analysis等，但如果你搞明白前面这些，后面这些一来不难理解，二来也会发现可能根本就用不上。科研用降维手段不是炫技，就是解决最开始那个探索样品间关系的问题，而探索也只是形成假设的第一步。诚然如果选错了方法可能错过一些信息，但如果样本间异质性如果足够，上述方法都或多或少能展示出来。如果异质性需要很精细调参数才能展示，更大的可能有两个：样本量不够或现象本身可重复性差，不应在这种数据上浪费资源，重新采样可能是更好的选择。&lt;/p&gt;
&lt;h3 id=&#34;参考资料&#34;&gt;参考资料&lt;/h3&gt;
&lt;p&gt;1 &lt;a href=&#34;https://compgenomr.github.io/book/dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html&#34;&gt;Computational Genomics with R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2 &lt;a href=&#34;https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction&#34;&gt;Nonlinear dimensionality reduction-wikipedia&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>线性模型</title>
      <link>https://yufree.cn/cn/2020/10/12/linear-model/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2020/10/12/linear-model/</guid>
      <description>&lt;p&gt;线性模型的基本形式就是因变量是由自变量作用加和而成，在这个语境下，其实把自变量改为变量，放宽独立性限制，也能将一些非线性部分，例如高幂次的自变量及变量间的乘积或交互作用考虑进去，这样，线性模型几乎可以覆盖绝大多数科研中常用的假设检验与模型。在实际问题的抽象上，只要可以把目标数值的变动用其他数值的拆解或组合表示出来，那么可以粗略认为标准化后其他数值的回归系数可用来比较不同数值间的贡献，而对于该系数的显著性检验则可以说明该系数的影响是否显著。&lt;/p&gt;
&lt;p&gt;打个比方，流行病学里常说的某种疾病发病率或风险比在考虑了人群性别、年龄、BMI、吸烟史等的影响后发现某污染物起了显著影响，这就是说在一个目标变量为病发病率或风险比的线性模型中，性别、年龄、BMI、吸烟史作为协变量而污染物作为自变量，模型拟合结束后发现污染物的系数经假设检验为显著差异于零，也就是没影响。这里，协变量与自变量在回归上是平等的，可以把协变量理解为控制变量，如果你考察吸烟的影响，那么吸烟与否就是自变量，包含污染物在内其他项就成了协变量。不过所有考察变量选择的原则在于其理论上或经验上被认为与目标变量有关系且无法通过随机采样、配对等手段消除影响，这种情况对于观测数据比较常见。&lt;/p&gt;
&lt;p&gt;当线性模型的自变量只有一项时，其实考察的就是自变量与响应变量间的相关性。当自变量为多项时，也就是多元线性回归，考察的是你自己定义的“自变量”与“协变量”还有响应变量的关系。如果自变量间不能互相独立，那么最好将独立的部分提取出来作为新的变量，这种发现潜在变量的过程归属于因子分析，可以用来降维。自变量本身存在随机性，特别是个体差异，这种随机性可能影响线性模型自变量的系数或斜率，也可能影响线性模型的截距，甚至可能同时影响，此时考虑了自变量的随机性的模型就是线性混合模型。线性混合模型其实已经是层级模型了，自变量的随机性来源于共同的分布。如果自变量间存在层级，例如有些变量会直接影响其他变量，那么此时线性模型就成了决策/回归树模型的特例了。如果层级关系错综复杂，那不依赖结构方程模型是没办法搞清楚各参数影响的。然而模型越复杂，对数据的假设就越多，对样本量的要求也就越高。同时，自变量或因变量有些时候也要事先进行连续性转换，这就给出了logistics回归、生存分析等特殊的回归模型。科研模型如果是依赖控制实验的，那么会在设计阶段随机化绝大部分变量，数据处理方面到线性混合模型就已经很少见了。但对于观测数据，线性混合模型只是起点，对于侧重观察数据的社会科学研究，样本量与效应大小是结论可靠性的关键，精细的模型无法消除太多的个体差异。&lt;/p&gt;
&lt;p&gt;高维数据是线性模型的一大挑战，当维度升高后，变量间要么可能因为变异来源相似而共相关，要么干脆就是随机共相关。在某些场景下，高维数据可能都没有目标变量，需要先通过探索性数据分析找出样本或变量间的组织结构。这种场景下应通过变量选择过程来保留独立且与目标变量有潜在关系的变量。也就是说，变量选择的出发点是对数据的理解，优先考虑相关变量而非简单套用统计分析流程。当然，统计方法上也有变量选择的套路，评判标准可能是信息熵或模型稳健度的一些统计量，可以借助这些过程来简化模型或者说降维。对于线性模型而言，就是均方误、Mallow’s $C_p$、AIC、BIC还有调节R方等，可借助回归模型软件来完成。&lt;/p&gt;
&lt;p&gt;回归或模型拟合都存在过拟合的风险，所谓过拟合，就是模型对于用来构建模型的数据表现良好，但在新数据的预测性上却不足的情况。与过拟合对应的是欠拟合，此时拟合出的模型连在构建模型的数据验证上表现都不好。这里的表现可以用模型评价的一些指标，其实跟上面进行变量选择的指标是一样的，好的模型应该能捕捉到数据背后真实的关系，也因此在训练数据与新数据上表现一致。&lt;/p&gt;
&lt;p&gt;在统计学习领域里，工程实践上最简单的验证过拟合与欠拟合的方法就是对数据进行切分，分为用来构建模型的训练集与验证模型预测性能的检测集，更细的分法则将检测集分为可在模型调参过程中使用多次的检测集与最后最终评价模型的一次性验证集，三者比例大概6:3:1，也可根据实际情况来定。也就是说，模型的构建不是一次性完成的，而是一个反复调整模型参数的过程来保证最终的模型具备良好的预测性与稳健度。&lt;/p&gt;
&lt;p&gt;在技术层面上，调参过程有两种基本应对方法，第一种是重采样技术，第二种是正则化，两种方法可以组合使用。重采样技术指的是通过对训练集反复采样多次建模来调参的过程。常见的重采样技术有留一法，交叉检验与bootstrap。留一法在每次建模留一个数据点作为验证集，重复n次，得到一个CV值作为对错误率的估计。交叉检验将训练集分为多份，每次建模用一份检验，用其他份建模。bootstrap更可看作一种思想，在训练集里有放回的重采样等长的数据形成新的数据集并计算相关参数，重复多次得到对参数的估计，计算标准误。在这些重采样技术中，因为进行的多次建模，也有多次评价，最佳的模型就是多次评价中在验证集上表现最好的那一组。&lt;/p&gt;
&lt;p&gt;正则化则是在模型构建过程中在模型上对参数的效应进行人为减弱，用来降低过拟合风险。具体到线性模型上，就是在模型训练的目标上由单纯最小化均方误改为最小化均方误加上一个对包含模型参数线性组合的惩罚项，这样拟合后的模型参数对自变量的影响就会减弱，更容易影响不显著，如果自变量过拟合的话就会被这个正则化过程削弱。当惩罚项为模型参数的二次组合时，这种回归就是岭回归；当惩罚项为模型参数的一次绝对值组合时，这种回归就是lasso；当惩罚项为一次与二次的组合时，这种回归就是弹性网络回归。实践上正则化过程对于降低过拟合经常有神奇效果，同时正则化也可作为变量选择的手段，虽然岭回归无法将系数惩罚为0，但lasso可以，这样在参数收缩过程中也就同时实现了变量选择。&lt;/p&gt;
&lt;p&gt;为了说明实际问题，有时候单一形式的模型是不能完全捕捉数据中的变动细节的，我们可以在工程角度通过模型组合来达到单一模型无法达到的预测性能。模型组合的基本思想就是对同一组数据生成不同模型的预测结果，然后对这些结果进行二次建模，考虑在不同情况下对不同模型预测结果给予不同的权重。这种技术手段可以突破原理限制，而最出名的例子就是人工神经网络里不同神经元采用不同核函数的做法了。&lt;/p&gt;
&lt;p&gt;对于科研数据的线性回归，还有两个常见问题，一个是截断问题，另一个是缺失值处理。截断问题一般是采样精度或技术手段决定的，在数值的高位或低位无法采集高质量数据，此时可以借助截断回归等统计学方法来弥补。另一种思路则是在断点前后构建不同的模型，这样分别应对不同质量的数据。对于数据缺失值的问题，统计学上也提供了很多用来删除或填充缺失值的方法，填充数据不应影响统计推断，越是接近的样本，就越是可以用来填充缺失值，当然这个思路反着用就是个性化推荐系统模型的构建了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R语言中的网络可视化</title>
      <link>https://yufree.cn/cn/2020/06/24/r-network-analysis/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2020/06/24/r-network-analysis/</guid>
      <description>


&lt;p&gt;这个问题反复掉坑，在这总结下，省的记吃不记打。&lt;/p&gt;
&lt;p&gt;所谓网络可视化，一定是有节点与节点间连线组成，节点一般指代一个样品或特性，连线则代表了样品间或特性间的关系。也就是说，网络的最小单元就是一个两点连线，也就是起点与终点，虽然描述一个网络很直观，但具体到数据结构上就存在一些问题。常规样本数据一般是每一行代表一个样品，每一列代表一个描述样品的维度，样品或维度间的关系并不能展示在原始数据结构里，所以我们需要将样品-维度的数据框转成描述网络的起点-终点数据结构才好可视化。但事实上，你更应该需要一种描述网络的数据类型，然后根据类型定义可视化方法，也就是将原始数据转为网络数据类型，这种类型定义也方便了除可视化外其他针对网络的分析方法开发与使用。&lt;/p&gt;
&lt;p&gt;在 R 中，有两个包提供了描述网络的基础类型定义，一个是 &lt;code&gt;network&lt;/code&gt;包，另一个是&lt;code&gt;igraph&lt;/code&gt;包。这两个包允许用户生成一个专门描述网络的对象，也定义了该对象类型的绘图方法，也就是说，如果你可以直接 &lt;code&gt;plot&lt;/code&gt; 一个网络对象，实现快速可视化。很多网络可视化的工具，例如 &lt;code&gt;ggnetwork&lt;/code&gt;包或&lt;code&gt;ggraph&lt;/code&gt;包或&lt;code&gt;GGally&lt;/code&gt;包的&lt;code&gt;ggnet2&lt;/code&gt;函数，都支持输入的对象为 &lt;code&gt;network&lt;/code&gt;包或&lt;code&gt;igraph&lt;/code&gt;包里定义的网络类型。不过，这里面&lt;code&gt;ggraph&lt;/code&gt;包还可以基于&lt;code&gt;tidygraph&lt;/code&gt;包使用&lt;code&gt;tbl_graph&lt;/code&gt;对象来描述网络关系，几乎完全覆盖了&lt;code&gt;igraph&lt;/code&gt;包的内容，当然，装这个就得装 &lt;code&gt;tidyverse&lt;/code&gt;全家桶。&lt;/p&gt;
&lt;p&gt;不论哪一种对象类型，网络对象一定可以抽象出两张表，一张表保存节点的属性，另一张表保存节点间连线的属性。而对于可视化而言，节点属性表其实就是原始数据框，而连线属性表则要保存我们计算出的节点间关系，例如节点间相关性、距离等。然后很自然每一种对象类型都设计了独立的针对节点与边的赋值方法，有了这些方法就可以自定义一些节点或边的属性方便可视化。不过，这里很多人手头只有数据框，也就是只有节点那张表，关系的表还是要自己生成的，在图论里这叫做邻接矩阵（adjacency network）。最简单就是一个相关矩阵或距离矩阵，不过也不一定就是方阵或三角阵，这里面坑多我就不展开说了，多数可视化包都支持你导入一个邻接矩阵或者更原始的起点终点数据框来生成网络对象，然后你可以对节点与边进行属性定义，在可视化时指定需要可视化的属性就可以了。很多人（其实就是我）卡在第一步数据导入就放弃了，但过了第一步能生成网络对象后后面就特别容易进行后续分析。当然，至于说网络可视化的具体布局，其实是存在一些预先设定好的美观布局的，你可以根据需求进行调整，但不要误导读者，做好图例。&lt;/p&gt;
&lt;p&gt;这里特别提一下&lt;code&gt;qgraph&lt;/code&gt;包，这个包几乎依赖了上面所有提过的包，但这在应用学科中并不少见，例如这个包主要是为心理测量学设计的。但很有意思的是，如果你去搜索 R 语言的网络可视化教程，基本都会找到心理测量学或社会科学背景的人写的东西，而且质量很高，例如&lt;a href=&#34;https://kateto.net&#34;&gt;Katya Ognyanova&lt;/a&gt;的博客，&lt;a href=&#34;http://sachaepskamp.com/&#34;&gt;Sacha Epskamp&lt;/a&gt; 的博客，&lt;a href=&#34;https://cvborkulo.com&#34;&gt;Claudia van Borkulo&lt;/a&gt; 的博客还有&lt;a href=&#34;https://psych-networks.com/&#34;&gt;这里&lt;/a&gt;，特别最后一个总结并追踪相当多近些年网络科学的主题。打个比方，通常我们说节点间的关系，一般就是想到相关性，但两个节点间也可以用是否独立来构建联系而相关只是独立与否的一种，偏相关行不行？或者如果计算二元而非连续特征值（社会科学里定量研究常用）间的独立性就需要用到 Ising 模型。同时构建出的网络是不是稳定也需要正则化例如 lasso 或重采样来对变量间关系进行调整，去掉不稳定的联系。另外，如何检测一个网络中的社群？有哪些算法？其实背后也是潜在变量分析的影子，这些主题在心理学领域被挖得很深。&lt;/p&gt;
&lt;p&gt;如果跳到生物信息学领域，有一个 &lt;code&gt;WGCNA&lt;/code&gt;包用的特别多，但据我观察很多写教程的人都没搞清楚原理与模型假设。&lt;code&gt;WGCNA&lt;/code&gt;包是基于巴拉巴西的无尺度网络构建的，基本原理就是先对所有基因构建两两相关性矩阵，然后从相关性矩阵中探索出共表达的基因模块，相当于把几千维的基因降维到不到十个且最好能联系上生物学意义例如某个通路啥的，具体计算则是每个模块进行主成分分析（其实是SVD分解），然后用第一个主成分作为这个模块的代表对你的研究分组进行差异分析，找出哪个模块有影响然后解释。这里面核心步骤里有两个坑，第一个在相关性矩阵到基因模块这里，第二个在主成分分析那边。第一个坑是因为其探索模块用了无尺度网络的假设，首先得去选一个幂级数来计算邻接矩阵，这个幂级数是拟合无尺度网络的度分布搞出来的，很多数据本身不符合无尺度网络的度分布，所以硬套这个假设是不合适的。第二个坑跟第一个有关系，只有模块内部第一个主成分可解释方差很高才能这么用，但由于第一个坑很多人用了默认值，第二个坑也就只用了第一个主成分，很多时候方差解释连三分之一都不到，虽然能讲故事，但明显是有偏的。当然这个包里也是涵盖了很多对于用户而言天书级的概念，很多人不求甚解套默认值也把文章给发了，完全就当神奇降维盒子在用了。其实说白了网络分析是另一层意义上的因子分析，起一个降维作用，只是降维方式不是简单的线性组合而是引入了图论的一些统计量罢了，但我看到很多人用起来套代码，解释上完全就是胡说八道，特别是代谢组学里会出现套基因组学的分析方法而不验证假设盲目追求自动化。不过我也看到了很多基于图论的生物信息学文章，很多想法非常超前但引用非常少且真正生产数据的人基本看不懂或不看，这就还不如心理测量学那边研究人员的学科内科普做得好。&lt;/p&gt;
&lt;p&gt;说个题外话，其实我能知道很多包的问题不是跟开发者打过交道，而是我查过很多包的源码，非统计与计算机科学背景开发者写的包其实是沉默的大多数，他们一般只会用基础R包函数来实现自己想要的功能，如果没有就会去依赖其他包，很少用 &lt;code&gt;Rcpp&lt;/code&gt;，不开并行计算，基本不关心速度，用S3 对象而不是S4，这倒是科研编程的日常状态。有时候读他们的源码有种见字如面的感觉：有的人明显是其他语言转过来的，有次读一个包的代码怎么看怎么别扭，后来发现这个开发者的母语是 java，很多定义方式都是那边传过来的。有的人注释掉的代码其实有另一重意思，源码里保留了很多进化遗迹，类似化石。有的人严谨，每个函数都写测试，文档明显打磨过语言。有的人飘逸，通篇找不到注释，很多编码风格都不一致，感觉是爆栈网复制过来的。有的人很明显是&lt;code&gt;tidyverse&lt;/code&gt; 风格出现后才开始学的 R ，对基础函数用法非常不熟。非统计与计算机科学开发者的代码通常存在很多不严谨的地方，没有经过软件工程的训练，更多是为了解决特定目的而快速实现的，不过很多代码展示的想象力非常丰富多彩。&lt;/p&gt;
&lt;p&gt;好了，说这么多还是要给点最直观的例子。下面我就手工生成几个网络并做下基础可视化，这里我不会用最常见的那种起点终点数据结构，因为这个东西是需要从原始数据生成的，很少有原始数据本身就是这种关系结构，而且此处我也不涉及 &lt;code&gt;ggplot2&lt;/code&gt; 风格的绘图包，用基础绘图系统来做，函数统一为&lt;code&gt;plot&lt;/code&gt;，当然不同的对象类型会有不同的绘图参数。&lt;/p&gt;
&lt;div id=&#34;network-版&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;network&lt;/code&gt; 版&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(110)
library(network)
# 生成一个3节点网络
net &amp;lt;- network.initialize(3)
# 画出来
plot(net,vertex.cex=10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 添加一条边
add.edge(net,2,3) 
# 画出来
plot(net,vertex.cex=10, displaylabels=T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 添加两个点
add.vertices(net,2)
# 画出来
plot(net,vertex.cex=10, displaylabels=T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 模拟一个5*12的数据框
df &amp;lt;- matrix(rnorm(60),5)
# 用邻接矩阵直接生成网络
dfcor &amp;lt;- cor(df)
# 去掉低相关性边
dfcor[dfcor&amp;lt;0.5] &amp;lt;- 0
netcor &amp;lt;- as.network(dfcor,matrix.type = &amp;#39;adjacency&amp;#39;)
plot(netcor)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 增加节点/边属性
set.vertex.attribute(netcor, &amp;quot;class&amp;quot;, length(netcor$val):1)
set.edge.attribute(netcor,&amp;quot;color&amp;quot;,length(netcor$mel):1)
# 可视化属性
plot(netcor,vertex.cex=5,vertex.col=get.vertex.attribute(netcor,&amp;quot;class&amp;quot;),edge.col=get.edge.attribute(netcor,&amp;#39;color&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-1-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;igraph-版&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;igraph&lt;/code&gt; 版&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(110)
library(igraph)
# 生成一个3节点网络
net &amp;lt;- graph.empty(n=3, directed=TRUE)
# 画出来
plot(net)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 添加两条边
new_edges &amp;lt;- c(1,3, 2,3)
net &amp;lt;- add.edges(net, new_edges)
# 画出来
plot(net)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 添加两个点
net &amp;lt;- add.vertices(net, 2)
# 画出来
plot(net)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 模拟一个5*12的数据框
df &amp;lt;- matrix(rnorm(60),5)
# 用邻接矩阵直接生成网络
dfcor &amp;lt;- cor(df)
# 去掉低相关性边
dfcor[dfcor&amp;lt;0.5] &amp;lt;- 0
net &amp;lt;- graph.adjacency(dfcor,weighted=TRUE,diag=FALSE)
plot(net)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-2-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 增加节点/边属性
V(net)$name &amp;lt;- letters[1:vcount(net)]
E(net)$color &amp;lt;- &amp;quot;red&amp;quot;
E(net)[ weight &amp;lt; 0.7 ]$width &amp;lt;- 2
E(net)[ weight &amp;lt; 0.7 ]$color &amp;lt;- &amp;quot;green&amp;quot;
# 可视化属性
plot(net)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-2-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;网络可视化只是网络分析的基础，很多基于网络稳定性分析还有网络群组分析都是可以基于更基础的概率图模型来进行，这些分析都有明确的背景问题来源，但涉及的知识点非常多，从统计物理到图论到随机过程，不过如果你带着自己的问题去探索，总会有新的发现。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>个性化模型</title>
      <link>https://yufree.cn/cn/2020/05/20/personalized-model/</link>
      <pubDate>Wed, 20 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2020/05/20/personalized-model/</guid>
      <description>&lt;p&gt;统计模型中，统计量通常是构建在群体属性上的，一群人平均体重60kg是一个群体描述而不是个体描述，平均人数据不是现实意义上的人而更多是我们认识世界的一个视角。当我们使用群体描述时默认群体的某个描述是均质的或不确定性是可以估计的，这只能说是一种简化的世界观。真实世界里均质群体不太好找，你得用随机采样来获取，但受限于真实采样手段，完全无偏的采样非常难获取。究其原因，人类作为整体的某些特性可能是趋同的，但具体到个人不确定性就极高了，在叫做小数定律，也就是极端的情况更容易出现在小样本里。&lt;/p&gt;
&lt;p&gt;对于个体而言，整体统计量的描述经常很苍白，是根据语境决定效用的，很多现象个体上你无法描述，好比不能用温度描述一个运动中的分子一样。例如，群体里某种疾病65岁以上老年人死亡率1%，具体到某一个老年人就没法等同于说他的死亡率1%。这里有两方面原因，一来65岁以上老年人死亡率1%是一个观察结论且很粗，打比方这1%的人几乎都是吸烟的但你不知道因为没收集这个数据，当吸烟对这种疾病起了决定性作用时，你用1%这个数相当于拿无关人的数据做分母稀释了群体内部分均质性很强的小团体，统计量对于异质数据的描述不存在微观价值而宏观描述也不准，最常见的例子就是收入长尾分布里平均值与实际感受差异很大或被平均，有些时候这个过程也会是主动生成的，例如不同的社会经济地位的消费行为模式很大程度上是被不同的社会经济地位内在的亚文化或圈子通过互相认可来确定的，这就涉及个体间互相反馈同化或割裂的过程，一个群体看似铁板一块，其实内部结构与沟通效率差异很大；另一方面，这个某一个老年人也是有其自身属性的，例如吸烟史或长期吃素营养不良。同时，很多指标也没有个体独立性，我们在使用统计量时，经常是先抽象后具象，这一缩一放里细节默认互相抵消，然而如果我们的目的就是要知道具体某个人的情况，所有细节都应该考察。现在群体中聚类找到跟某一种现象相关的指标或机理，然后具体去看某个人这个指标如何，这就是所谓表型-机理-预测的建模过程。&lt;/p&gt;
&lt;p&gt;现有医学体系与流行病学本质上是构建在平均人模型上的进行循证的，这是一种符合现代化的公平与标准化模型。基于平均人模型，我们可以提出致病机理，针对机理可以开发通用药物。但临床上病历的作用可能更大，正常人出现咳血可能就得住院，但病历上标注结核病的可能就给点多喝水规律作息的建议，医生要是严格遵循循证医学要求会造成医学只能集中在常见病上发展，因为那些人数太少的罕见病可能不会被识别或聚类诊断。最简单的例子就是测体温，很多人自己的平均体温跟人群的平均体温是两个数，要是一刀切，可能他的数值正常对他自己而言已经相当不正常了，而这在诊断上就会出现漏诊。&lt;/p&gt;
&lt;p&gt;然而，这个过程本质上还是先整体聚类然后认为他们是相对均质的，真实世界，打比方有一种疾病，你把所有患病的人凑一起但找不到这个群体共性的特异性指标，那么只能说我们当前技术找不到新的视角。不过，在个体水平上却可能会有非常显而易见的问题，例如这个病非常邪门，触发条件是某年某月某日你看了我的博客，那么显然常规分析根本就想不到收集这个数据。然而，如果个人对自己行为进行记录，可能会发现这个行为模式但却毫无统计意义。也就是说，如果我们想发现这种不知道自己不知道的东西，能做的就是尽可能多的收集数据然后进行穷举式尝试。另一种可能就是个体基线数据的异常分析，这里没有平行世界让你采样，就得用不同时间点的你作为基线。&lt;/p&gt;
&lt;p&gt;这就是所谓精准医疗技术研发的两个核心思想，第一个就是尽可能多测量指标，例如各种组学技术与传感器；第二个就是对个体的各类指标进行历史追踪。实际应用中，这两个核心思想都要用，但技术复杂度极高，你能测准一个数构建个时间序列都不容易，测成千上万个互相相关且存在交互作用与级联反应的指标再去考察时间序列变化，原理上很清晰但实测全是坑，数据可比性，可重复性在个体层面没法依赖大样本而只能依赖历史数据作为参考，而数据解读在原理不清晰的状况下更是困难重重，虽然预测会变得简单。没错，如果依赖个人历史数据，预测几乎等同于异常值分析，前提是你的历史数据有可识别的模式，要是没有，那么意义可能也不大。&lt;/p&gt;
&lt;p&gt;本质上，这是在寻找个性化的机理解释，而其复杂性是疾病本身引起的，一种表面上看起来很显然的疾病其分子机理可能极为复杂，在不同生理结构和尺度上互相矛盾，例如基因层有变化，蛋白层没有，但表型又有了。或者基因蛋白水平都很明显指示了疾病，但就是没症状，其原因可能是另外的基因或蛋白被激活抑制了我们所谓指示疾病的信号但我们分析手段漏了这个指标。这种情况下我们收集历史数据是看不到基线异常的，因为数据不全。这时候组学技术可以起作用，也就是不分青红皂白能测全测，即使这个指标没有被现有知识标注过也要收集到信号来构建历史基线，但问题是我们看到了异常也很懵圈，因为不知道啥意思，分子水平上一个人一天的起伏与异常值可能多的离谱，但其最终结果可能过很久才出现，久到我们根本就意识不到这两件事有关系。也就是说，按现有思路与知识去构建个性化模型，我们不一定输在模型上，而是输在对构建模型的机理想象力上。其实个性跟模型其实本来就对立，个性强调独一无二而模型强调共性抽象，个性化模型只有在个体历史数据可被模式识别的条件下才成立。&lt;/p&gt;
&lt;p&gt;有没有框架或思路可能先于我们的知识来构建个性化模型？这几乎一定要涉及一个可以自己演绎，回溯与学习的模型框架。我不想用人工智能这个词，因为人工智能到顶也就是人的智能水平，人的智能水平要是够用也不至于现在一线科研人员都在挣扎着从噪声里提取信号了。这个系统要有极强的异常侦查能力且可以集成现有生理知识对历史数据进行长距离逻辑分析，但比较麻烦的是他可能无法从错误中学习，因为其监测的主体如果死了或得病了，其预测能力会增强但这个人可能用不上了。而能学习的方式就是有个数据后台对某指标类似的人进行实时聚类，如果同类人有一定数量出了问题就对另一类还没出问题但有趋势的进行预警，但这个预警可能也没啥用，没有机理支持最多就是让这个人出事前知道自己会出事而已，当然这个个性化模型也可以用来研究机理，用现在人的数据预测未来人的风险，也许这代人用不上成果但下一代可能受益。在药物开发上，个体分子水平的变化可以用在研制针对个人的药物或用药指南，但也是构建在机理或数据的支持之上，如果只有个体数据，机理演绎功能就必须也得个性化，类似现在的个性化推荐，不过熟悉推荐系统设计的同学可能马上意识到其背后也是聚类与语义分析，真正的未知个性化都得是构建在机理上，但机理自身也可能是个性化的，只有底层生物物理化学规律是跑不了的，所以个性化模型一定要有这部分知识的实时更新知识库。同时，这种数据一旦产生就会有隐私问题，能预测疾病的数据也可能预测其他事，但不同于通过数据杂交来获取统计意义的规律，个性化模型的基线差异规律正是构建在个体多指标的交互作用上，你把部分指标换成别人的就没了规律。能解决的思路可能就是捆绑销售，如果你想让个性化模型有更强大的预警功能，就得出让你数据本身整体的研究价值而禁止掉其他用途。&lt;/p&gt;
&lt;p&gt;最后的讨论是：个性化模型是否真的必要？究竟有多少病我们还没发现？有多少指标异常有生理学意义？现在越来越多可穿戴设备在炒精准医疗的概念但收集一堆步行心率数据除了收几个赞外可能还不如一个跌倒报警来的实在，而跌倒报警也是基于异常分析。现在流行量化人生，但在我看来只是消费主义的另一个变体罢了，你量化了但没有科学支撑就是行为艺术，也不用跟我扯公司层面宣传出的科学支撑，我过手经过实验设计与质量控制的人体样本也有四位数了，都不用说规律，单是消除分析阶段引入的误差就各种麻烦，让我信商业公司收集数据结论的信度与效度，我得查查他们的论文，我可能不质疑技术但会质疑里面的科学。其实虽然存在大量未知，但人身上这些零件保质期七八十年就差不多了，个性化模型本来就是对边缘人士设计的，对于普通人身上贴一堆传感器意义有多大其实很玄学，也许就是谈资或装腔，临床大夫诊断肯定优先看病历而不是电子产品给的监测，后者可能警察会更感兴趣。我所在的私立医院确实存在很多富有的家族投资某种家族病研究的事，顺带可能也救了类似家族病的人，但直面完全未知疾病的预防研究在很多人看来就是没事找事。不过，这也是预防的悖论，你不会因为阻止了某些事收到表扬，但一定会因为没有阻止某些事被骂成狗，这种费力不讨好的研究在现代化体系里其实可有可无。&lt;/p&gt;
&lt;p&gt;其实个性化模型在非医疗的商业推广中已经广泛应用了，作为科研主题非常有意思，可以挑战一些极端环境，但其背后的精准医疗目前概念意义更多些，真要是搞清楚了应用场景，潮水一退多少人穿着皇帝新衣，一目了然，当然他们一定会把概念术语搞得无比复杂让你只会乖乖掏钱买单，现代社会恰饭才是硬道理。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>疾病模型</title>
      <link>https://yufree.cn/cn/2020/02/28/d-model/</link>
      <pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2020/02/28/d-model/</guid>
      <description>&lt;p&gt;我经常跟别人吹用&lt;code&gt;caret&lt;/code&gt;包可以做到几百个模型的验证，做疾病模型很轻松，但实际没实操过。最近实际做了一批样品发现想跟做还真不是一回事，这里记录一下。&lt;/p&gt;
&lt;p&gt;这里我们遇到的问题是测了血样里的几千个代谢物峰，当然我也不知道具体是哪些物质。因为把无关变量加到模型里会提高模型整体方差，所以第一步我做了个自下而上的筛选。也就是说，不同于将疾病作为响应，把代谢物作为预测变量，我首先做的是把代谢物作为响应，把疾病状态作为预测响应的一个变量，同时对那些可能造成代谢物响应的协变量进行控制。也就是如下模型：&lt;/p&gt;
&lt;p&gt;代谢物响应 = f(疾病状态, BMI, 年龄, 家族病史)&lt;/p&gt;
&lt;p&gt;这一步是找出跟疾病状态有关系的代谢物，就是说对几千个物质逐一构建模型并进行错误发现率控制，结果发现一共也就十几个代谢物跟疾病相关。需要注意的是这里我默认预测或解释疾病只用这些代谢物就够了，但真实情况却可能是有些代谢物之间会有相互作用而在单一代谢物水平的建模是忽略了这种相互作用的。不过我这么操作主要是为了把不相关的代谢物去掉，由于我样本量有限，而过多代谢物几乎一定会在模型训练阶段过拟合，这一步操作是为了保证后面疾病模型的统计功效。&lt;/p&gt;
&lt;p&gt;得到十几个代谢物就去解释疾病对于一个暴露组学研究所来说属于严重失误，所以我从合作方那边拿到了被试的问卷调查数据，寻找并筛查了不超过十个与疾病相关的暴露变量。也就是说，目前我手里的变量都是在各自水平上与疾病有关系的，那么很大可能这些变量间也会有相关，例如有些营养指标会与代谢物相关，此时就可以用非监督学习方法对这些变量进行聚类来构建代谢物与暴露指标的关系。不过，我的数据里似乎代谢物跟暴露指标没啥关系，距离都挺远。那我们就可以走下一步来讨论疾病的影响因素了，这个自上而下的模型如下：&lt;/p&gt;
&lt;p&gt;疾病状态 = f(代谢物响应,暴露指标)&lt;/p&gt;
&lt;p&gt;这里我想需要知道代谢物与暴露指标那个对疾病状态影响更大，这就是个比较经典的机器学习问题了。由于前面自下而上的单变量筛选，虽然我的样品不到100个，但预测变量不到20个，勉强跳过了组学的高维诅咒。不过选啥模型就是另一个问题了，随机森林是生物新手的首选，支持向量机似乎有点过时，深度学习有点类似大炮打蚊子，要不要正则化…其实这里我的问题就是要有个衡量变量重要程度的模型，而且这个模型的预测效能要好。&lt;/p&gt;
&lt;p&gt;其实虽然我大概了解每个模型的假设与需要优化的参数，但我不认为存在一个完美模型，每一种模型都是从一个角度来审视数据中的信息。如果这个假设靠谱，那么对我而言最简单的探索方法就是尽可能多选择原理不同的模型进行训练，然后把训练好的单一模型组装为一个宏模型再次进行训练，得到不同模型的预测权重，然后在验证集上检验这个宏模型的组合预测。&lt;/p&gt;
&lt;p&gt;这里用了 &lt;code&gt;caretEnsemble&lt;/code&gt;包，我同时尝试了六种模型，从预测结果上看大概都在70-80%的预测准确率，然而当我组装出宏模型后，预测准确率就会稳定在80%左右，验证集上准确率甚至更高。这其实是一种非技术驱动的性能提升方法，我并没有去依赖某一个模型的精细打磨，而是暴力组合后训练出不同模型权重来进行非技术性性能提升，这实际就是一个神经元为不同统计模型的人工神经网络。虽然从预测结果上并不完美但组合后性能确实提升了，但当我检查这非常六加一的变量重要性时，我发现有一种代谢物的重要性总是排第一。很遗憾，这个代谢物数据库里没有，属于未知物，但无论如何，整个流程走通了。&lt;/p&gt;
&lt;p&gt;总结一下，要对疾病进行解释，可以先在单一分子水平上去除掉无关变量，然后耦合不同组学及流行病学的相关变量并用无监督学习来探索这些疾病相关变量间的关系，最后用多个原理不同的模型组合出一个预测效能好的模型，然后检查变量重要性来区别不同组学或流行病学变量的相对重要程度。这个流程最大的优势在于不依赖单一模型优化而是利用宏模型来整合结果，这对于疾病研究应该已经足够了，因为疾病就想知道两个问题的答案：那些因素可以导致疾病？这些因素间的关系是怎样的？&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>因果革命</title>
      <link>https://yufree.cn/cn/2019/09/29/why/</link>
      <pubDate>Sun, 29 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2019/09/29/why/</guid>
      <description>&lt;p&gt;最近读了图灵奖得主 Juder Pearl 的科普书《为什么》，这应该是第一本关于因果分析的科普著作，在介绍内容前，我先八卦下这个目前比较小众但未来可能引发因果革命的领域。本书作者，来自 UCLA 的 Juder Pearl 与哈佛的 Donald Rubin 算得上因果分析的两位泰斗，前者提出了因果图（PCM）且在机器学习领域建树颇深而后者则比较反对这个方法，提出了RCM来做因果分析，后来证明PCM与RCM是完全等价的。后者有个学生 Andrew Gelman，如果你做贝叶斯分析就不可能不知道 Gelman 大人与 Stan。因果分析的另一个大牛是哈佛的 James Robins，提出了G方法。看这个阵容就要知道因果分析跟贝叶斯分析与机器学习有点天然联系，我完全迷茫的结构方程模型也属于这个阵营的一个分支，也会沾边人工智能的发展。&lt;/p&gt;
&lt;p&gt;八卦完了先整体评价下这本书：《为什么》属于比较烧脑的书，我因为学过因果分析的公开课，到了西奈山又参加过因果分析的讲座，所以虽然也是外行，但感觉用因果图来理解这本书会极大降低阅读难度。不过也得说明白因果图是很好的思维工具不假但天然在定量与表示交互作用上有劣势。此外统计之都丁鹏老师的一系列因果分析文章也是很好的辅助理解的文章，如果你打算搞明白一件事，同时服用这些会比只看一个人的书与观点要好。我不是研究因果分析的，但作为普通科研工作者也感到很有必要了解下因果分析，其中很多讨论的主题可以联系到当今的p值滥用、可重复性危机与模型可解释性的问题。&lt;/p&gt;
&lt;p&gt;任何学科都应起源于悖论。高尔顿板常用来解释正态分布，但当年高尔顿设计这个是为了展示一个遗传悖论：如果第一代的后代会产生一个分布，那么多代之后这个分布会越来越宽。打个比方，父母身高170，孩子180，孙子185，那么如果存在长高遗传的话，总有个N代孙子身高一层楼，毕竟每一代都会产生一个均值为这一代身高的分布，你肯定找的到更高的下一代，如此反复就应该出现巨人子孙。现实则是不论哪一代，身高分布基本稳定，人群中很高的人后代也高但不会那么高，很矮的人后代也矮但不会那么矮，这就是向群体均值回归与两代人身高相关的现象。高尔顿为此迷惑不解，主要是遗传意义上讲不通，高了后还是高，但没那么高，后者暗示遗传上的因果并不完全成立。但他学生皮尔逊则很简单认为这种回归也好或相关也好就是概率的，因果关系是相关的一个特例，统计学不能也不应该讨论因果性，进而奠定了后面研究中相关不代表因果的基调，用严谨性与概率锁死了观察研究，这个表述到今天还活跃在科学研究的各个领域。&lt;/p&gt;
&lt;p&gt;因果问题一直是统计禁区，从学统计第一天我们就知道相关不代表因果，那么问题来了：如果相关性是个逻辑死胡同，知识如何增长？打个比方，我看到人群中某个基因的高表达与某种疾病的发病率存在相关性，怎么解释？是发病导致基因突变还是基因突变导致发病？一般到这一步就需要控制实验了，也就是说我们把控制实验当成因果关系的确认而对观测数据持谨慎态度。但这实验怎么做？找一组人，用 RNAi 沉默掉他们的基因看是不是得病？现代伦理不允许。用动物模型？物种差异咋办？。仿真？我们搞清楚分子层面的构建机制了吗？目前相对公认的结果来自分析流行病学，金标是队列研究、临床实验与病例对照实验。严格意义上只有临床实验最靠谱，但那就真成了拿人做实验了，治疗目的问题不大，但研究目的就存在伦理危机。自然科学里控制实验容易些，很多时候因果性不言而喻（也因为这个缺乏了统计思维）。社会科学与医学就必须解决从观察结果中发现因果性的问题，不然永远只能讨论现象，因果分析就来源于此。&lt;/p&gt;
&lt;p&gt;其实皮尔逊虽然从概率角度澄清了单一因素在解释现象的局限性，但他自己也发现了很多伪相关或随机相关，更麻烦的是他的学生赖特在豚鼠研究中构建了多因素路径模型来解释遗传问题。赖特在豚鼠遗传问题上假设了遗传、发育、环境与随机四种来源并用路径代数分析估计了各自影响的比重，虽然赖特自己并不认为他在用相关推导因果，但 Juder Pearl 认为这是第一次用图论来进行因果关系的探索。赖特定量算出了豚鼠子代体重增长中遗传的贡献，然而这个方法在之后的50年里被统计学界打压了。统计学家认为赖特的方法只能就事论事构建逻辑关系来进行考察，但当时的统计学主流是用“固定程序”解决问题，而大量的精力耗费在了“固定程序”的理论与细节构建。统计学家在跟别人合作时通常会让对方把数据转化为他们能处理的分布进而应用标准方法来分析，但这严重忽视了现实世界的复杂性并造成了大量统计方法的误用。现在我们知道赖特实际做的是把专业知识形成的假设因果关系进行了定量分析，而脱离专业知识的统计学正是其目前被应用导向的计算机科学超越的一个原因。在我看来，因果分析是联系统计学、专业知识与计算机科学的一个关键方法与思维工具，科研里探索的就是因果与机制，定量考察因素影响的因果分析方法是一个重要步骤。&lt;/p&gt;
&lt;p&gt;到此我们回顾下相关，如果不能说明因果，又能说明什么？两个因素之间因果也就是一个到另一个，但相关之所以说明不了因果，其实是因为存在第三种因素或更多。但简化下三个因素之间的关系其实大概就下面这四种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A导致B，B导致C A-&amp;gt;B-&amp;gt;C&lt;/li&gt;
&lt;li&gt;A导致B跟C A-&amp;gt;B, A-&amp;gt;C&lt;/li&gt;
&lt;li&gt;A跟B共同导致C，控制C后A跟B相关  A-&amp;gt;C B-&amp;gt;C, ( C ): A -&amp;gt; B&lt;/li&gt;
&lt;li&gt;随机相关&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里我先不画因果图，只用箭头表示因果，用括号表示控制。第一种很明显就是直接因果关系，这里因果关系是传递的，如果你控制了中间那一个，因果关系就阻断了，这是所谓的中介效应；第二种我们会观察到B与C相关，然而这种相关在控制A后会消失，A就是我们经常提到的公因或混杂因素，例如巧克力消费量与诺奖得主数相关，背后是因为他们都跟国家经济发展水平相关，当国家经济发展水平一致时，巧克力与诺奖间的相关性就会消失。这两种接受过统计学教育的都比较容易理解，第三种就不那么直观了。糖尿病会导致血糖高，吃糖多也会导致血糖高，正常我们是看不到糖尿病与吃糖多相关的，因为这是两回事，健康人吃糖多也会血糖高。然而，当我们控制一个高血糖浓度区间去看糖尿病发病率与吃糖行为时，他们就相关了，也就是高血糖的人既可能是病人也可能临时糖吃多了，如果我们只看一个时间点，很容易发现吃糖与糖尿病的相关。也许你会说吃糖就是导致糖尿病，但如果我们看的是I型糖尿病（遗传）呢？你也会发现相关。这种相关只在控制了血糖后出现，不控制两个没有相关性，这种相关是共果或对撞关系在控制后导致的。很多人做研究喜欢在模型里加上尽可能多的协变量，这个方法可以处理第二种情况，但协变量如果是第三种情况控制后会把不相关的两个因素搞成相关的，这个情况很多研究人员意识不到会导致错误结论的出现。第四种情况也存在，特别是高维数据，这就是因果分析与p值问题的连接点，多重检验里如果排除随机相关或显著性已经是当前学术界意识到的问题了。&lt;/p&gt;
&lt;p&gt;也就是说，当我们看到两个变量相关，除了因果与公因，还存在共果与随机相关，共果是不恰当控制变量导致的而随机相关则对高维数据是一个灾难。因果分析所要做的就是区分这些并正确估计或探索变量间的关系，理清楚了这些基本关系，也就可以通过演绎法探索相关背后的因果了。&lt;/p&gt;
&lt;p&gt;这里有个概念要说清楚，那就是控制。什么是控制？控制就是让某个因素恒定，在线性模型里就是加上一个变量，此时拟合后其他变量的拟合系数就是控制了这个因素后的效果。当然，如果存在非线性关系例如交互作用，控制就会复杂些。控制的神奇之处在于我们不需要事先处理而仅仅是分析时加入考虑即可。在《为什么》里，Juder Pearl 引入了 do 演算的概念来补充概率论里条件概率对控制干预的先天不足。P(A|B)与P(A|do(B))实际计算可能是一回事，但在因果图里就很不一样了，do 表示一种单向干预而概率论里的条件概率没有方向性。举个例子：&lt;/p&gt;
&lt;p&gt;A -&amp;gt; B -&amp;gt; C, D-&amp;gt;A, D-&amp;gt;C&lt;/p&gt;
&lt;p&gt;现在我们想知道A与C的关系，目前走 ABC 路径是通的，AC路径会因为公因也是通的，那此时你就无法认定A与C之间是否是因果关系，这个时候要做的就是对D进行控制，阻断AC的公因途径；对B进行控制，阻断ABC的中介途径。这样之后看A与C间的关系就知道剩下的因果关系了。其实更严谨的方法是RCM，也就是 Donald Robin 提出的因果分析框架，不过用因果图更直观些。这个过程叫做 do 分离，用 do 的控制行为隔离掉非因果影响。当我们根据推测的变量间关系画出因果图后，do 分离就可以很直观的看出来。例如：A -&amp;gt; B -&amp;gt; C &amp;lt;- D -&amp;gt; E 这五个变量的关系中A与E间的通路被C阻断了，所以我们什么都不用做可以直接相关判断因果。然而，如果我们对C进行控制，B与D因为公果控制相关，此时路径就通了，此时我们无法直接考察A与E的因果关系，但如果我们接着控制了B或D，路径再次被阻断就又可以讨论了。也就是说，如果我们打算考察两个变量因果关系，首先要根据实际情况画出因果图，然后 do 分离掉所有两个变量间的路径，此时两者的相关就是因果。这个方法很直观，当变量间关系很复杂时，我们依然可以通过考察通路来找出控制方法。由于神奇的贝叶斯定律可以让你调转条件概率的方向，所以代数上你可以在一定条件下将控制过程转为条件概率，这也就实现了从观测数据里提取因果关系的目的，这套演算过程就是 do 演算，配合因果图使用非常方便。从图里手工推导 do 演算的公式对科研人员要求过高，不过现在软件已经可以代劳，你只要画出因果图，软件就可以告诉你两个变量间因果关系推导的条件概率公式，代入你的数据就可以看到结果了。&lt;/p&gt;
&lt;p&gt;有了这个工具，书里面提到的历史公案就简单明了了许多。先看代际遗传，高尔顿的因果图就是 遗传 -&amp;gt; 身高并且认为相关系数是1，但如果身高同时被遗传与环境决定，那么相对稳定的环境贡献了回归现象而遗传则解释了相关性。科研实验中的控制变量法或随机本质上就是对所有可能的公因变量或中介变量进行控制来阻断考察变量间的非因果关系，然而此时我们也明白了如果控制的是共果变量，那么实际上我们有可能打通原来不相关变量间的通路。在这里 do 的含义一定是基于因果的而不仅仅是无条件控制，这个洞见非常有价值。&lt;/p&gt;
&lt;p&gt;我们再来看吸烟与肺癌的案例，虽然很多人都知道我们最终是通过Cornfield不等式与希尔标准来接纳的因果关系，但用因果图来理解的话其实就是 Fisher 认为存在遗传因子即导致吸烟，也导致肺癌。Cornfield 不等式的实质就是认为如果存在这个遗传因子，那么其与吸烟的相关性应该与肺癌的相关性同步增加，然而现实数据中肺癌风险在吸烟者那边高了9倍，也就是说这个基因在吸烟者那边也得高9倍。在那个年代其实没有找到这个基因，有意思的是后来真的找到了这个促进吸烟的基因，只不过对风险贡献没那么大。一个更典型的案例出现在观察数据中，研究人员发现吸烟母亲的低出生体重婴儿存活率比不吸烟母亲的要高，好像吸烟提高了低体重婴儿的出生率。这里因果图是 吸烟 -&amp;gt; 出生体重 -&amp;gt; 婴儿死亡率，当我们控制了出生体重后，吸烟降低了婴儿死亡率，这明显反常识了。这个问题其实是五年前才解决的，研究人员提出了一个先天缺陷作为出生体重与死亡率的公因，此时吸烟与先天缺陷都会导致出生体重低，如果我们控制出生体重，相当于打通了 吸烟 -&amp;gt; (出生体重) &amp;lt;- 先天缺陷的路径，又因为先天缺陷跟婴儿死亡率相关，所以吸烟与婴儿死亡率间存在了一条控制后的非直接因果通路。具体来说，如果先天缺陷比吸烟对低体重影响更大，造成高死亡率，那么我们只看低体重婴儿的话就容易把原因归结到吸烟而不是先天缺陷之上，到此观察数据的矛盾就解决了。本质上因果图可以帮我们探索新知识，而传统统计分析则更多就事论事而导致矛盾。不过直观归直观，其实 RCM 在推导上更严谨，不过我估计外学科理解因果分析还是会收敛到因果图上，直观可视化在技术推广上总是占优。不过这个阶段工具很关键，哪个软件简单易用，其背后的模型就可能最终胜出。&lt;/p&gt;
&lt;p&gt;接下来就是辛普森悖论了。当我们整体看两组数据时会有差异，然而当我们将数据分层后这个差异就会消失或逆转。具体到辛普森悖论就是一组新药对人群整体有益，然而当我们按性别区分时，这个药对男性也好女性也好都没了益处。现在比较流行的解释是基于一个数学事实：部分的比例之和与整体的比例之间没有必然联系。在辛普森悖论中，对照组发病率女性为1:19，男性为12:28，服药组女性为3:37，男性为8:12，这样我们看到 1:19&amp;lt; 3:27，12:28&amp;lt;8:12，然而我们不看性别时，整体是13/47&amp;gt;11/49。这里形成悖论的关键在于人们会理所当然认为部分比例的差异结果会传递到整体，其实 A/B&amp;lt;C/D, E/F&amp;lt;G/H，但(A+E)/(B+F)跟(C+G)/(D+H)间是不能传递这种关系的，数学上就不成立。不过这只是解释，不解决最初的问题，到底这药有用没用。这里就又牵扯到因果图了，我们得知道性别对服药与结果的影响，也就是说得知道性别的角色。首先，性别不是服药与发病结果的结果，毕竟不是变性药；其次，性别是否决定服药？经过调查发现，女性确实比男性更容易记得服药；最后，性别是不是会导致结果，调查也发现男性容易患病。OK，这样分析后性别就是服药与患病的公因，因此服药与患病间不形成 do 分离，我们需要控制性别。怎么控制呢？很简单，就是分开性别讨论然后按性别在人群中比例加权，最后的结果就是辛普森悖论中的新药对整体其实是有害无益的，悖论解决。我们回顾下这个过程会发现，悖论的解决实际上是依赖了因果图构建过程中外部信息的整合，单纯看数据不看背景知识的纯统计探索在这种场景下会陷入悖论。所以说因果图或因果分析其实就是当前数据科学三个背景中那个专业知识与统计学的重叠区工具，只是当前懂专业知识的与懂统计学的还没进行很好的学科融合。&lt;/p&gt;
&lt;p&gt;不过性别不总是共同原因，也可能是中介。这个也比想象的常见，简单说就是下面的结构：&lt;/p&gt;
&lt;p&gt;A -&amp;gt; X -&amp;gt; B&lt;/p&gt;
&lt;p&gt;当我们打算估计 A 到 B 的净效应时，X也作为中介在起作用。比较经典的案例就是伯克利录取问题，整体上呈现对女性的性别歧视，然而分院系看却大都是女性占优势。这个案例经常跟辛普森悖论放到一起讨论，但因果图上是不同的。辛普森悖论里性别是共因，伯克利录取里，A是性别，B是录取结果，而院系则是作为中介存在的。性别不同的学生会选择不同的院系，而不同院系录取率本来也不同。通常关于这个现象的解释是女性会选择难录取的院系，这件事造成了整体与结果的不同。不过解法上倒是类似，因为控制了 X 之后就中断了这个路径而可以直接估计 A 与 B 的关系。但控制 X 相当于得到的是这个 X 下的录取率，要想知道全局影响，还是要加权平均。传统分析里直接效应间接效应的讨论无法区别中介与共因，因果图则可以很好区别出来进行解释。不过中介问题的复杂性不在这里，如果存在一个对中介与结果的公因，对中介进行控制就是错的。举例来说，如果居住地会影响院系与结果，例如某个州大学的甲院系就是只录取本州男性与外州女性而乙院系则只录取本州女性与外州男性，那么我们对院系的控制实际上打开了性别与居住地的后门路径而造成了错误估计：&lt;/p&gt;
&lt;p&gt;A -&amp;gt; X -&amp;gt; B, C -&amp;gt; X, C -&amp;gt; B&lt;/p&gt;
&lt;p&gt;这里C是居住地，此时我们控制 X 会导致 A -&amp;gt; (X) &amp;lt;- C -&amp;gt; B 这条路径打通，结果就是A对B的估计中还是存在混杂。这里我们也能看到悖论的魅力，如果条件发生改变，解决悖论的方法也要随之改变而不是简单控制就能搞定的。有时候完全不控制可能就是最好解决方案而过多的控制会让结论无法被理解，这点对机器学习里 y = f(x) 的简单逻辑框架来说是灾难性的，更多的特征值不见得拟合效果更好，反而让你说不清楚结果与变量的关系。当然，现在主流似乎也不打算说清楚。&lt;/p&gt;
&lt;p&gt;既然说到辛普森悖论，我就顺道聊下这个悖论的变体。最古老的变体是田忌赛马，为了保证整体上取胜，我们可以在部分作战采取放弃策略，这个在军事学发展中阵地战战术中比较常用，说白了就是在战力不占优的条件下通过兵力在防线上的掉配来取得一定效果，这个太古典了，现代战争的变体是海陆空高科技兵种配合作战，早就脱离了电视剧里对着砍的策略（但对着砍有戏剧性与视觉冲击力），当前作战形式基本都是地面小队配合空中扫荡与坦克掩护，你要是敢把几千人排到一个开阔阵地上等对砍搞决斗，就等着无人机包饺子吧。我猜测这是互相博弈的一个结果，因为双方将领如果都明白辛普森悖论原理，那么肯定会最大化自己的优势去打对方的弱势，而现代战争是科技战，人数啥也说明不了而更多是劣势，所以不超过10人的小队突击在对战中灵活性最高且容易发挥科技领先优势。现代公司发展中，很多大公司采用传统上的多级体制其实就是在搞阵地战，技术上如果信息流通顺畅，小组制应该是整体取胜策略，我们已经看到很多互联网爆款产品最初其实都是来自小组而不是靠人数，当然后期巩固成果是完全另一类可持续策略了。扯远了，我们再看下政治学里的辛普森悖论：杰利蝾螈。这个概念指的是通过划分选区来操纵选举结果的一种策略，通过把铁票仓分到对方的边缘选区可以在代议制选区民主里赢得多数票，民主的一人一票不代表整体不能被操纵，这对大杂居小聚居的西方移民国家尤其重要，如果争夺中间派不切实际，那就把极端分子渗透到中间派选区搞事情。&lt;/p&gt;
&lt;p&gt;前面说的都是辛普森悖论的分类变量比例版，很自然，这个悖论也存在线性模型版。也就是说，一组数据做回归，变量系数是正的，但是我们把数据切成两部分，其同一变量的系数就会变成负的。这个线性版的成因除了跟前面比例版一样外还存在另一个成因，那就是噪音。当参与回归的变量只能解释数据变异中一小部分时，其回归估计参数的不确定性会被未知变异所主导，如果分割数据的方法与未知变异混杂，那么变量的方向就会不稳定。这里还是要强调下参数估计一定要包含不确定性，如果不确定性高不代表结果不靠谱而更可能是数据结构还没搞清楚或数据量不够。其实这种信噪比不够的问题更多还来连接 Gelman 大人关心的那两类错误的讨论（方向S与数量级M估计错误），也许我们花大力气估计清楚了一个微弱变量的效益，这是否值得？学术上当然值但现实意义就不好说了。辛普森悖论并不是个案，除了我上面提到的，经济生活中也有，例如股票指数走高里不代表每个板块都高，这就联系到风险控制与对冲了。近期美国被动基金规模超过了主动基金，有人就怀疑这会是下一次衰退的动因，过大的被动指数规模收缩了个体风险但也收缩了整体金融行业的多样性，如果个体衰退信号最终被被动基金放大，那就是整体的崩盘。被动基金好比把鸡蛋放到不同篮子里，篮子被不同公司的被动基金产品放在自家的货车里，但其实所有货车都是铁索连环在一起保持稳定。而与之对应的主动基金则是小汽车运鸡蛋，风险高容易翻车而货车阵列比较稳，但所有的车都跑在路上，如果路被台风破坏，小汽车的鸡蛋自然报废但规模不大，连环大货车要是翻了就火烧赤壁大家的鸡蛋都得烤熟了。现在的问题是1）是否存在这样的巨大风险与2）连环阵列分摊风险也传递了风险，这种组织结构是不是靠谱。前一个问题答案其实是存在的，例如气候变化带来的巨灾频率增加、水资源短缺与全球政府的债务问题等，后者就需要具体量化研究了。但有个很简单的判断，如果过去总是少数人拿到经济发展的大头利益而被动基金是打算分这一杯羹，那最终是一定会有一场政治经济博弈发生的，完全的自由与完全的管制都等达到可持续平衡，但部分自由与部分管制会提供更多的机会。辛普森悖论告诉我们存在整体收益而部分损失的可能与部分都受益但整体依旧损失，整体与部分数学上其实没关系，数据上就要看背景了，最好用因果分析来探索下究竟是什么导致的差异及加权后整体究竟怎样，当然，如何加权，用什么加权，逻辑通不通其实有时是艺术而不是科学：你把因果箭头反着来能说通也会产生一个估计，自圆其说不解决科学求真的问题。&lt;/p&gt;
&lt;p&gt;前面我一直在说加权，感觉好像搞清楚因果关系后剩下的就是加权平均的计算，其实实际也是这样。因果分析还有种做法就是通过计算个体因果效应后计算整体平均因果效应（ACE）来做的，这个平均过程也可看出某种加权，就是做了跟不做后的净效应。不过这仅仅是计算层面，不包含因果假设。&lt;/p&gt;
&lt;p&gt;$$ACE = E[Y_{i,1}] −E[Y_{i,0}]$$&lt;/p&gt;
&lt;p&gt;因果图另一个直观应用例子是工具变量。当然工具变量在计量经济与公共卫生里有不同的表述，但本质上就是如果我观察 A -&amp;gt; B 时存在 C 同时影响 A 与 B，那我们就找一个不受 C 的变量 X，这个变量直接作用于 A，此时我们计算 A 与 X 的相关性与 B 与 X 的相关性，根据下面的因果图：&lt;/p&gt;
&lt;p&gt;X -&amp;gt; A -&amp;gt; B, C -&amp;gt; A, C -&amp;gt; B&lt;/p&gt;
&lt;p&gt;X 与 B 之间没有直接因果，在效应计算中，X 与 B 的相关只会来自于 A 的中介，也就是 A 与 X 的相关性与 A 与 B 的相关性乘积，如果我们打算估计 A 与 B 因果效应，就是用 B 与 X 的回归系数去除 A 与 X 的回归系数。这个过程中因为 C 跟 X 间没有因果通路，所以相当于跳过了混杂因素的讨论。这个路径影响等于回归系数乘积的代数技巧使得计算上非常清晰，但还是牢记工具变量的选择不是探索的而是因果的，需要背景知识。有了工具变量，研究人员就从无穷的相关与混杂中解放出来而可以通过因果图构建来求解目标因果关系。当因果图很复杂时，你需要去控制一些变量或引入工具变量实现你关心变量的 do 分离，进而估计净效应。&lt;/p&gt;
&lt;p&gt;因果图天然与贝叶斯分析有联系，且不论条件概率的转化，因果图跟贝叶斯网络就存在内生联系。构建因果图需要的背景知识可以类比成贝叶斯预设的网络层级，因此在求解上类似。复杂因果图实际也常常跟结构方程模型放在一起讨论，因为这两个解决的问题基本就是一回事，不过结构方程模型里并不存在控制而更多侧重参数求解，因果图除了可以玩控制变量 do 分离外，也可以接纳非线性关系。在有明确定义的结构因果模型（也就是复杂因果图）里，可解释性被放在了第一位，也就是说数据驱动的假设被模型驱动所取代，这放在机器学习领域里看起来像是开倒车了。不过因果图很难直观表示交互作用而更多用在线性模型里，所以看起来结构方程模型似乎是目前因果图用的最多的地方。&lt;/p&gt;
&lt;p&gt;聊完因果图，这本书的术部分就算完成了，但其实 Juder Pearl 从一开始就是打算从哲学层面也就是道的角度来推动因果分析的。在他看来，当前的机器学习也好，人工智能也好，都卡在了因果分析的第一层，也就是关联这一层，到处都是相关且多数人似乎放弃了模型的解释性而一味追求预测与关联。他进而提出了因果关系的第二层（干预层）与第三层（反事实层）。其实这有点类似科学哲学的发展，第一层类似归纳分析与观察的实证主义，第二层则可以对应实验主义，第三层则是则类似证伪的思路，进而构想出一个不同于现实世界的客观知识的世界（波普的世界三）。不同于主流对模型可解释性的放弃，因果分析从一开始就构建在因果关系之中，解释起来相对容易（自然要加持因果图），不同数据间存在自己的数据逻辑结构，高层级的算法要能通过干预与反事实假设来发现结构，只有这样的算法才能实现强人工智能，因为人的思考就是因果的。 Juder Pearl 的哲学观里因果不是相关的特例而是反过来，相关里浸润着不同的因果，因果分析就是对这些东西的关系加以区分。&lt;/p&gt;
&lt;p&gt;这个观点确实像是开历史倒车，但也可能是一次对当前科研的矫正。太多研究人员在大数据浪潮下放弃了可解释性而追求预测精度，且因为算力的加强，原来很多理论求解几乎全变成了数值求解。同时深度学习等神经网络的应用也让解释模型越来越变成一个笑话，不过如果我们真的不研究认识问题，那人类是否还能发现新理论与新机制？另外这真的靠谱吗？以极端天气为例，现在这个领域有两个派别，一个是走物理模型预测，另一派则是统计模型，前者被偏微分方程推导搞得痛苦不堪，后者则在忽略地形信息时经常搞出在两条河流隔大山的情况下预测出两条河在台风过境时贯通的结果。从来都没有单纯的数据驱动的研究，所有的数据驱动背后都有对问题的建模，当模型假设错了以后，数据再多也是缘木求鱼。不过是不是物理或化学模型就没问题了？也不是，理化生的知识大都是构建在还原论上的，理论会被设定在理想环境之下，这也对现实脱了节。自下而上与自上而下不存在哪个是真理的问题而仅仅就是个历史问题，因果分析可能是相互连接的一个工具。不过目前因果分析更多是应用在偏软科学与数据优先的学科里，理论优先的学科从一开始就是在讨论因果，但因为使用的是统计工具，所以内在的相关不能定因果的观念事实上是被学科自己的理论给压制了，但这也造成了对统计工具的滥用与误用。因果革命可能在两方面实现突破：一是统计学家对实际问题背景知识的接纳与考量而不是研究纯理论细节；另一方面是科学家对统计工具的重新理解，从因果图角度探索自己科学问题的解决方案。但无论如何，因果革命都是很有前景的，因为这个工具是为思考服务的，现代人缺的就是这一块。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>细分领域自媒体受众估计</title>
      <link>https://yufree.cn/cn/2018/10/16/sub-area-estimation/</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2018/10/16/sub-area-estimation/</guid>
      <description>&lt;p&gt;这个估计方法不限于细分领域公众号，也可以用来估计有效粉丝与影响力。想写这个是因为经常看到一些大忽悠，声称可以帮自媒体或个人吸粉，牛皮都是上万起步，这种事不是没有，但公众的注意力是有限资源，支撑不了那么多的行业大号。也就是说，很多行业做自媒体受众上限就锁死了专业性商业变现的可能，只有通过内容泛化或娱乐化，而这时只有头部几个（通常是一个甚至没有）可以做到专业性与受众面兼顾。简单说就是很多自媒体在选择一个小众领域后就注定只能靠兴趣跟泛化的广告支撑了。&lt;/p&gt;
&lt;p&gt;这里我用环境黑板报这个号做个案例。根据《国民经济行业分类》，全国的行业大概有20个门类，95个大类，432个中类，1094个小类。根据第三次经济普查，法人单位里的从业人员总共3.56亿，个体户9千万，各大类人数都过百万。其中，环境从业人员约22万，科研从业人员约850万，国家机构2709万。同时，有2.6亿的在读学生，教师1514万，高校学生3559万，硕士153.5万，博士31.3万，理学硕士在读14万，博士在读6万，正高20万，副高50万，中级64万。&lt;/p&gt;
&lt;p&gt;从上面可以看出，如果我们目标是环境从业人员、理科学生与老师，覆盖面最大就是100万左右，如果考虑家长，至少也有百万级别，如果影响政府，那就是千万级别。不过按照我们的专业能力，最多就是按百万受众，关注量1万是天花板，阅读量最大也就是1万，更正常的数据应该是2000人。也就是说，我们受众被行业规模锁死，但也是体现专业性的地方。&lt;/p&gt;
&lt;p&gt;不过100万应该是一个自媒体可影响规模，我预计信息技术可以让1%的受众收到信息。如果从业人员在10万这个水平，那么信息流通就非常困难了，因为规模大概是1000人，此时行业协会或国家部门而不是自媒体的话语权会更重。如果从业人员在1万这个水平，那么slack群组、qq群、微信群就比较适合了。如果在1000这个水平，最有效的方式应该就是定时开会。如果100人水平，那最好依附更大群体例如杂志或期刊进行交流，否则信息沟通一定不畅。但如果本来就是受众，那么100人左右规模应该是内部交流最顺畅的。但如果是1000万受众，公共宣传、电视广播与门户网站可能更合适。如果人数过亿，不好意思，信息应该传不动，因为这个规模的均质人群应该是不存在的，除非是国家，但就算国家也只能通过想象存在。&lt;/p&gt;
&lt;p&gt;也就是说，百万到千万受众才是媒体最喜欢的作用范围，往下的细分领域即便信息技术存在，信息交流效率也高不了，主要因为可交流主题有限。同理，如果可交流主题具有普遍性，那么在150这个邓巴数下应该最高效，例如八卦信息。据此可以用真实数据来验证下，例如群规模与谣言传播，还有就是通过控制内容难度来筛选读者，看是不是曲高和寡，或者看看笑话与搞笑图片的传播路径，小众人群的信息传递方式等。&lt;/p&gt;
&lt;p&gt;应该可以定义一个类似信息熵的传播熵，只有专业水平达到一定程度才能成为受众，我预计这个熵低的文章传播效果好不了，但如果足够通俗，受众面广，这个文章就可能流行。依赖这个设计一个自然语言处理打分来预测阅读量或进行文章润色应该很有应用前景的。&lt;/p&gt;
&lt;p&gt;这个限制因素是很容易忽略的，有兴趣大家可自行搜索查看一些专业论坛细分领域的活跃人数，目测没有超过200人的。因此很多论坛都会开茶馆或水区让新手有机会融入，对于自媒体而言就是去接一些八杆子打不着的广告或转发受众更广的心灵老鸭汤、搞笑视频或标题党。在娱乐这件事上，受众跟认可国家的是一样的，因为每个人都看得懂也乐得起来。&lt;/p&gt;
&lt;p&gt;因此，我不相信一个纯专业的自媒体可以收获百万千万的用户，要么数据掺了水，要么就是专业性掺了水。道理非常简单，你让爱因斯坦到鸟巢讲广义相对论的推导过程，哪怕票卖满了听懂的人数也不会太多。当然，这决挡不住几乎所有现场的人都会发个朋友圈，这大概就是专业跟娱乐的区别吧。&lt;/p&gt;
&lt;p&gt;说点题外的，中国的人口基数决定了很多在国外只能当兴趣的东西在国内流行起来后就成了产业，淘宝很多买卖放到人口/语言基数小的地方就是赔钱赚吆喝；加拿大人口不到美国1/10，很多市场直接无法形成，形成后也存在垄断与超额溢价；当几十年后全球人口不再增长，我们将面临很多小众兴趣的消失与更严重的泛娱乐化。举个极端的例子，当世界只有9个人时，篮球这项运动就没有存在的意义了，当然正确的说法是：选手为智人而不是机器改造人或机器人的篮球比赛将不会存在了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>两种结论错误与研商</title>
      <link>https://yufree.cn/cn/2018/05/17/type-sm/</link>
      <pubDate>Thu, 17 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2018/05/17/type-sm/</guid>
      <description>&lt;p&gt;读论文结论时其实我们都在跟着作者的事实推理逻辑进行决策，而决策就有对有错，这与事实或规律本身无关，只代表当下的认知水平。正是因为承认这一点，科研才不会纠结于错误，或者说科研就是在错误中前行的。同样的数据是有可能得到完全不同结论的，这是个时间的函数，逼近而不是揭示真相。所以，在这个有决策的过程中错误是可以用概率来描述的，p值的流行很大程度上是因为它给了一个通用版的决策方法与阈值，随之而来的就是两种错误，一种是假阳性，一种是假阴性。&lt;/p&gt;
&lt;p&gt;所谓真假，必有对照，多数假设检验的空假设就是个对照基础，这个基础一般是一个分布或就是随机条件。多数对这种判断诟病的根源也在这里，因为真实实验或观察中基线往往不服从分布或随机，为此统计学家提供了大量手段来平衡掉不随机的部分让随机成为基线，在此基础上进行的差异比对就是一个令人信服的相对正确结论。在结论的修饰语中，相对正确是理想化的，令人信服才是被发表出来的原因，多数人没搞懂这一点去解读文献其实是一种科黑。&lt;/p&gt;
&lt;p&gt;显然，平衡掉不随机的部分需要你事先知道这部分是什么，很遗憾，目前科研特别是基于观察的研究并不能事先知道，有时候就是想发现这些不知道自己不知道的东西。这种情况下基于p值或空假设的假设检验其实是不应该用的，打个比方，你发现观测数据中A基因与甲疾病相关，但究竟是不是A基因引发甲疾病还是需要用控制变量来验证的，很有可能A基因与甲疾病同样被B基因调控，但你根本就没测B基因，所以研究本身就是不完整的。那么通过组学技术知道的不知道的我一起去测不就完整了吗？也不是，当你测量数量增加时，假设检验的个数也增加了，此时你的p值阈值如果是0.05，那么10000个测量变量中会有500个即使随机测定都会出现差异的基因。去年有人建议把p值阈值设到0.005，但这根本不解决问题，只是把需要核实的数量减少了，虽然这也有一定意义。举个例子，10000个基因中有一个是真实的，你测定后按照0.05发现了501个，按照0.005发现了51个，也就是说需要验证的数量减少了。但真实研究中，你会遇到0.05发现了501个但0.005只发现了50个的情况，真实差异由于效应量或造成的差异量不够大而被你的决策方法给漏掉了。甚至也会出现0.05发现了480个而0.005只发现了48个的情况。也就是说，当你观察的问题效应不大时，p值有可能不管怎么调整都无法发现。这个锅不在p值，在于你要研究的效应效应太低而你用了不恰当的研究方法与假设来检验这个现象。这类效应大小问题就是 type M 型错误，只要你假设检验很多，这个问题就很难规避。&lt;/p&gt;
&lt;p&gt;读博期间跟室友卧谈时我曾说过，现在只能相信强结论，也就是说无论你用哪种统计方法去进行检验，这个现象都是客观存在的，不会因为决策方法的变化而出现结论差异。不过这个提法现在看还是太理想了，因为强结论真的很强或显而易见，属于科研里低垂的果实，前人都摘的差不多了。如果一个现象足够强，p值一定会发现，贝叶斯方法也一定会发现，此时不存在效应大小问题。但更多的事实或规律是埋藏在当前认为的随机或噪音之中的，我们的分析水平也就刚刚好能把疑似信号与噪音进行区分，而这个区分是否靠谱则完全成了迷，统计学在这里帮不上忙，技术进步倒成了关键。我看到一些研究寄希望于数据挖掘技术解决学科内现象发现问题，这里我只能说对于显而易见但被忽视的现象是有帮助的，但对于高噪音数据，降低测量噪音对结论的帮助要远大于遴选能发现差异统计方法的努力。数据迷信会让你看到伪规律，而测量技术进步才会真的发现价值规律。我曾经也想把生活完全量化，但后来发现测量与传感方面的误差会让量化数据变成垃圾，大数据很美但也可能很虚。&lt;/p&gt;
&lt;p&gt;另一个则是方向问题，p值经常是双边概率取中间那一部分，所以当你看到一个很小的p值时，你并不知道这个效应的方向是更大还是更小，此时你还是需要去看效应值。在这个情况下，如果报导p值不报道效应，那么就好比我告诉你明天要变天但又不告诉你变成什么一样毫无意义。在多数实验设计中，变化几乎是一定存在的，例如我敲掉了某个基因去验证功能，基因的变化与功能肯定有区别，大都来源于观察实验，更有意义的是影响大小，这个大小更多需要专业判断而不是简单的p值。如果理科学生学了半天最后就知道用p值来判断结论，那么这个学位不给也罢。这类搞不清楚效应方向的问题是 type S 型错误，验证性实验特别需要注意。&lt;/p&gt;
&lt;p&gt;今天特意讲这个是因为我去年年底看了一篇论文，上面测量了很多种污染物的浓度，然后就对着很多健康指标进行了相关分析。这是一种多对多的结果遴选，在组学研究中也很常见，需要承认的是这是很多环境健康研究的惯用套路，然后只报道那些差异显著的结果。我将这篇论文转给了哥伦比亚大学的 Gelman 教授，询问他从数据分析角度有没有什么建议，他告诉我会在半年后在博客上公开回复这个问题（他档期真的很满）。然后这个月我看到了&lt;a href=&#34;http://andrewgelman.com/2018/05/15/reduce-type-m-errors-exploratory-research/&#34;&gt;回复&lt;/a&gt;，总结下就是 Gelman 教授认为1）显著性检验是不靠谱的，2）通过多层模型来减小M型错误影响（这是一种我认为很符合中庸之道的模型）并且3）尽可能多的平衡掉已知效应。更重要的是， Gelman 教授指出这属于探索性分析而非验证性分析，对于结论不应该太过信赖。这个回复是很中肯的，但一线研究人员能否理解并应用就不好说了。如果把对当今科研中的问题理解程度量化为“研商”，我想国内对于研商的培养是缺失非常严重的，从学生到老师职业功利性都远大于
对研究本身的理解，或者说我们缺少一个氛围。如果你去看 Gelman 教授的回复，你会发现博客下面的评论中引发了更多对科研成果报导、开放获取期刊等问题的讨论。而国内的科研博客评论里普遍理性讨论少，简单评价多，这个氛围的形成需要包括你我在内的一代甚至几代人的努力。&lt;/p&gt;
&lt;h2 id=&#34;小结&#34;&gt;小结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;除了假阳性与假阴性错误，科研结论中还存在效应大小错误与方向的掩盖&lt;/li&gt;
&lt;li&gt;p值对于后面两种错误的解决帮助不大，贝叶斯分层模型有助于问题部分解决&lt;/li&gt;
&lt;li&gt;强结论很美好，但同时依赖数据分析与测量技术，后者容易被忽略但更为关键&lt;/li&gt;
&lt;li&gt;研商是区别科学家与科研从业人员的重要指标，国内对此培养欠缺&lt;/li&gt;
&lt;li&gt;在线公开讨论问题对于问题的理解与解决是有帮助的，这是互联网时代的研究红利&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>作为世界观的统计学</title>
      <link>https://yufree.cn/cn/2018/01/12/statistic-view/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2018/01/12/statistic-view/</guid>
      <description>


&lt;p&gt;去年啃了《The Seven Pillars of Statistical Wisdom》，我一直很喜欢这种能够告诉你来龙去脉的书。很多事很多人在做也知道怎么做能做好，这是工程师思维。但只有知道为什么去做才能更深层次的改造现有的方法或手段，这就是科学家思维了。现在的大学教育特别是理工科教育都过于强调职业精神，各专业之间都有很深的隔阂来凸显自己的价值，但追根溯源，所有知识都有个起源，或者是实际需求，或者仅仅就是好奇心，理解了来源很多东西再去想就会十分清晰，而不是淹没在一堆术语中。这本书就着力于讲清楚统计学的几个出发点，作者是芝加哥大学统计系的 Stephen M. Stigler 教授。虽然我统计知识都是半路出家上公开课的“夜校”学的，但读起来也并不吃力，推荐科研人员特别是做数据分析的科研人员都读一下。如果你懒得读，我就二手解读下，不保真。&lt;/p&gt;
&lt;div id=&#34;支柱一聚合&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;支柱一：聚合&lt;/h2&gt;
&lt;p&gt;统计学毫无疑问是一门独立的学科，经常很多人搞混统计跟数学，其实数学在各个学科里都更多是以工具角色出现，统计学也并不例外。最原始的统计需求就是对客观世界的抽象，跟农业最相关的天文观察要求所有测量要准确，但问题每次测出来都会有差异，那么就需要一个方法来描述相似但不一样的测量值，这就是统计聚合思想的来源。科幻小说中有照相机记忆的人是无法分析事物的，他们只能记住所有细节，而这个负担是非常重的，此时抽象的意义就很大了。现在比较火的大数据就好比这个人，细节丰富但需要抽象，不然就是一堆数字的堆砌。这里最常见的统计学术语就是众数、中位数还有均值，都是聚合抽象描述的体现。&lt;/p&gt;
&lt;p&gt;其实这个思想提出时也是被批判的，因为显然聚合出来的东西例如平均人不是客观存在的，也就没法指导具体事物的描述。但本来聚合描述的就不是具体事物，它用总结替代完整描述，通过选择性舍弃信息来获得更多信息，这个可以说是统计学的一个根基。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;支柱二信息测量&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;支柱二：信息测量&lt;/h2&gt;
&lt;p&gt;在这个根基之上我们提到了信息，那么另一个根基就是对信息的测量。当我们形成一个统计量，其实是丢掉了一些信息的，但更有意思的是对同一个事物的描述，即便测量的准确性上没有差别，后来的观察贡献的信息并不如早期多，信息量与观测数的开方正比而不是观测数（我严重怀疑这个根基借鉴了薛定谔的《生命是什么》）。举例来说，早期造币按批次称重，误差r，10个一起称的误差就并不是10r，100个一起称也不是100r，你称10个得到的误差与称100个得到的误差精度最多高一倍，也就是后面90个硬币提供的信息大概等同于前10个提供的信息，这个现象也是统计学里很常见的，基于此我们可以去搞采样及基于分布的理论而不至于担心丢失太多信息。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;支柱三似然度&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;支柱三：似然度&lt;/h2&gt;
&lt;p&gt;另一个基础思想则是似然度，前面两个都是面向测量的，有了测量就可以进行比较，最通常的比较就是跟随机事件比，有了随机事件就可以谈概率了。此时特定分布下概率就是似然度，看看某件事在大背景下出现的可能。p值理论的根基就是似然度概率且最初的p值概念里就是仅仅去看空假设下的发生概率。1920年Fisher提出，如果A代表科学目标，X代表数据，那么定义似然度函数L(A|X)为出现X的A的概率密度函数，X已知，找这个函数最大时的A，一阶导数为0找到参数，二阶导数描述准确性，但这里面最大的问题在于对于方差估计是有偏的，特别是数量少时，而维度高了这个问题就很严重了。&lt;/p&gt;
&lt;p&gt;抛开这个，基于概率的推理本身就是统计学很特殊的世界观，简单说就是只要概率不为零，一切皆可能。休谟认为奇迹是违反自然法则不能发生的，但 Price 用贝叶斯理论推导认为即使发生概率很小，多次实验后也会发生奇迹，在这里经验法则跟统计规律就出现了对立。传统世界观是决定论的、逻辑的，但统计世界观是概率的，不可知的或可更新的，值得注意的是，这种不可调和的差异也存在与量子力学与经典力学的世界观之间。很难说那种是世界本来面目，只能说这是两种认知角度，可以矛盾地存在于同一个人身上。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;支柱四内部比较&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;支柱四：内部比较&lt;/h2&gt;
&lt;p&gt;有了面向背景目标的似然度，统计学可以解决外部比对问题，也就是跟预设分布去比较。然而，现实问题更多是数据内部的异质性所要求的内部比较，很多耳熟能详的统计方法例如t检验，方差分析，Bootstrap等都是用来解决内部比较问题的。1908年， Gosset 用 Cushny-Peebles 数据展示单样本t检验，他考虑了样本方差在样本数较少且总体方差未知时如何估计，引入了自由度与样本方差，得到一个近似正态分布的t分布，这篇论文印错了数、分类也错了、引用年份也错了，但最后结果还可以有历史意义的。但这篇论文出版后很长时间无人问津，Fisher在1912年毕业后写信给 Gosset 后来转给 Pearson 但都没看懂，后来 Fisher 提出双样本t检验并结合相关系数与方差分析写在了1925年教科书 《Statistical Methods for Research Workers》 中，到这里这个相对通用的内部比较方法才开始真正流行。再往后Tukey 提出了jackknife，Efron 提出了Bootstrap，都是从样本内部进行比较来估计差异变化。值得注意的是数据量越大，内部比较出现的随机相关就越多，特别是时间序列，这是很容易遇到的研究错误。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;支柱五回归&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;支柱五：回归&lt;/h2&gt;
&lt;p&gt;回归思想应该是统计学作为世界观最直接的体现，一般人看世界是发展的或静止的或规律决定的，但统计学家看世界是自带回归视角的，也就是说，凡事都会回归到本来的样子，规律性是松弛有度的。&lt;/p&gt;
&lt;p&gt;用进化论来说，最初其理论体系是不完整的，里面假设了同一个亲代会产生不同的子代，如果不断产生，这个变异累计会无穷大，出现怪物，实际代际间差异并不大。这里的矛盾是3法则（a/b = c/d）例如身高体重比如果稳定可以知三得一，这样子代的高身高一定意味着高体重，但现实数据并非符合这个强规则。&lt;/p&gt;
&lt;p&gt;这个问题最早被高尔顿钉板所发现：如果关注极端小部分 会发现其主要来源是不极端的部分；相反不极端的部分也会有来自极端部分的回归。然后研究身高时，高尔顿发现孩子身高会有向父辈身高均值回归的现象：每个人的身高都有固定部分跟变动部分，固定部分是都一样的，这样代际变化可以用亲代子代的不完全相关来解释，达尔文的自然选择就可以构建在遗传上了，至此人口平衡与代际变异就可以有统计模型来和谐相处了。否则不论是强相关还是不相关都不能解释现实数据，回归思想可以说是统计学的中庸之道。&lt;/p&gt;
&lt;p&gt;这个将效应区分为固定跟临时两部分的思想也构成了经济学里消费函数的根基，人们消费固定部分是收入而不是短期刺激，因而政府短期加大开支并不能刺激消费，这个指导思想帮助弗里德曼拿了诺奖。&lt;/p&gt;
&lt;p&gt;多元问题在多元统计方法之前都是用几何学跟数据分析来解，最多两元，Galton提出相关系数后，Pearson等人发扬光大为多元分析。而贝叶斯统计先假设参数分布与这个参数下出现数据的似然度 求出现这个数据的参数，这种推断比较依赖假设，初始值变了就都变了。而统计学的另一个分支因果分析就是基于强假设进行推断。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;支柱六实验设计&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;支柱六：实验设计&lt;/h2&gt;
&lt;p&gt;前面统计学是收敛的，观察的，但当发展到实验科学年代，统计学就要去解决刻意观察获得规律的方法。这里面随机化是一个核心观念，用来确保除了你关心的变量，其余的都能随机或符合某个分布。1874在《科学原则》这本书里首次提到了控制变量法，一次测一个。但在统计学大放异彩的20世纪，Fisher 认为一次回答一个问题是错的，因为自然问题从来都是复杂的不能只回答一个，提出了加性模型。这里统计学要为复杂现象提供合理的设计工具，时至今日，在数据概念满天飞的时代数据收集似乎不是问题，很多人就会说更重要的是提出问题。这倒没错，但如果没有统计学思维加持，很多问题是无法对应实际数据的，我想A/B测试就是一个很好的例证，如果设计不当或有偏，拿到的现象就会产生误导。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;支柱七残差&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;支柱七：残差&lt;/h2&gt;
&lt;p&gt;这是我很欣赏的一个统计要素，本质上科学就是通过解释剩余现象进步，而当今其实理论体系里留给重大发现的空间是有限的，所有人都在精进1%，不过都是在80%-90%的基础上的，也就是大家伙都在噪音里探索信号的模式。具体到统计模型就是对模型解释不了的部分与模型诊断的思想，有了这个部分统计学就有了不断发展的动力与自我审视的原则。&lt;/p&gt;
&lt;p&gt;逻辑上看这本书其实有点内容上的前后重叠，但思想上却是很有启发的，如果一个人熟悉统计学的世界观，那么他可能会更好的与这个世界相处：既不会被教条的规则所折磨，也不会被充沛的情感所奴役。科研人员其实就经常盘旋在理性与感性之间，统计学可以很好的把感性观察或假设转化为理性规律，为科学进步保驾护航。统计学世界观其实是有点人文关怀在里面的，不论是把个体包裹在整体之中、为奇迹赋值、为发展提供理论空间还是回归的中庸之道，当然这几点也可能有完全不同的解读方法。&lt;/p&gt;
&lt;p&gt;我个人的体会就是这本书把我之前很多思考串起来了，虽然还是不成体系，但起码在看到一个统计方法时我更清楚地知道知道它到底想做什么了。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>假设检验的乌云</title>
      <link>https://yufree.cn/cn/2017/10/28/nhst/</link>
      <pubDate>Sat, 28 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2017/10/28/nhst/</guid>
      <description>


&lt;p&gt;19世纪的最后一天，开尔文男爵在展望20世纪物理学前景时提出了两朵乌云，后来这两朵乌云分别催生了相对论与量子力学。时至今日，物理学家还在为了统一相对论与量子力学而不懈努力，而由此衍生的圈量子场论跟弦论已经不是几句话说得清楚的了。然而，21世纪的科研天空可就不仅仅是两朵乌云了，简直可以说乌云密布，这其中最大的一块大概就是空假设显著性检验（NHST），以此为基引发了无数次的关于科研成果可重复性、发表歧视、多重检验与p值、因果推断、科学决策等的争论，这些争论有些出现在学术期刊，有些则在科研社交网络蔓延。作为一名科研人员，身处这样一个窘境而不自知是可怕的，这意味着很多在做的工作根基上就有问题，盲从与职业化的科研正在蚕食科研成果的威信。我并无能力提出完美解决方案，但有必要把问题先提出来，疑惑对科研总是有益的。&lt;/p&gt;
&lt;div id=&#34;空假设显著性检验nhst&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;空假设显著性检验（NHST）&lt;/h2&gt;
&lt;p&gt;NHST更常见的形式是p值，也就是在空假设成立的条件下某事件发生的概率。p值有多流行呢？根据 Jeff Leek 的&lt;a href=&#34;https://docs.google.com/presentation/d/1hzdSDaPPSE9xUYZHhOVfQIRPPdwe0A9SdE7QDsK3bOA/edit#slide=id.g255a5ace66_3_796&#34;&gt;估计&lt;/a&gt;，如果把p值当成一篇文献，那么其被引次数已经超过300万次了，当之无愧的史上被引次数之王，甩&lt;a href=&#34;http://www.nature.com/news/the-top-100-papers-1.16224&#34;&gt;第二名&lt;/a&gt;一个数量级。原因其实很简单，p值已经渗透到几乎所有学科的研究中了，特别是实验学科。可想而知，如果产生p值的 NHST 出了问题其影响力有多大。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;院士身份悖论&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;院士身份悖论&lt;/h2&gt;
&lt;p&gt;要了解NHST的问题，我们首先要看下一个基于NHST的悖论。张三研究员是一名中科院非外籍院士，我们对其有两个假设检验，第一个假设检验的空假设是张三是中国人，备择假设就是张三不是中国人，因为我国不承认双重国籍，所以张三身份不存在薛定谔的猫态，要么是，要么不是。第二个假设是张三是中科院非外籍院士，备择假设就是不是，也是互斥的。那么两院院士不到两千人，中国人口14亿，概率大概百万分之二，备择假设的概率是0，这个情况比较特殊，也就是备择假设永远不成立。现在我们不知道张三的国籍，但知道他是中科院非外籍院士，但根据NHST，张三不太可能是中国人因为绝大多数中国人都不是院士，那么拒绝第一个检验的空假设后我们就会发现，张三成了不是中国人的中科院非外籍院士，额，那张三究竟是哪国人？&lt;/p&gt;
&lt;p&gt;也就是说，如果一个假设对另一个假设来说很稀少，NHST会在很低的条件概率下拒绝掉，然后那些稀少的事情在NHST里就成了无法被检验的事情。这个例子最早是 Cohen &lt;a href=&#34;http://ist-socrates.berkeley.edu/~maccoun/PP279_Cohen1.pdf&#34;&gt;提出&lt;/a&gt;用来说明人们在使用NHST时的问题。本质上是多数人在使用p值时搞混了条件概率，拿上面院士身份来说，我们的假设 H0 在面对张三这个数据 D 时给出了拒绝 p(H0|D) = 0，这个决定是构建在假设 H0 成立时出现 D 的概率太低（即p(D|H0)）之上，也就是说NHST下，我们默认下面的概率是成立的：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(D|H_0) = p(H_0|D)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果你修过任何基础的统计学课程都会知道这两个概率之间差了一个贝叶斯大爷。通过使用贝叶斯定理，在新数据出现后原有概率是要被更新而不是直接拒绝掉的。通俗点说就是 NHST 属于革命派，不认可就打倒你；贝叶斯属于改良派，用新的证据更新原有理论。好了，这里我们回顾一下科学史，革命派跟改良派确实都出现过，但当学科基础相对稳定后，更多的科学知识是改良派搞出来的，除了物理学两朵乌云，多数科学研究都是N+1模式，你现在在科学领域搞从零到一基本等同于对好几代科研人员同时开群嘲，结果一般会被认为民科或伪科。这个悖论的本质就是把假设下的事实与事实下的假设搞混导致的，这是NHST的一个致命问题。然而致命问题可不止这一个。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;方法学悖论&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;方法学悖论&lt;/h2&gt;
&lt;p&gt;过去的100年，测量方法的精度是在不断提高的，而精度其实又会影响研究结果，很不幸，也是通过 NHST 来进行的。其实 NHST 在实验物理学里用的还是好好的，例如我去检测一个物理量，只有数据出现在其理论预测下数值四五个标准差以外才会对理论产生实质作用。此时，测量精度越高，由于测量误差导致的对原有理论的冲击就会越少，因为物理学的预测性要比化学生物等学科要好不少且此时 NHST 检测的原有理论是比较真实的。但在其他学科，特别是心理学跟医学的控制实验里，在实验开始前你几乎就可以确定空假设是不成立的，要不然你也没必要分组，此时你去搞 NHST ，几乎一定可以找到差异，此时测量精度如果不断上升，那么你会识别到一系列差异，但这些差异的效果是无法体现在p值里的，p值可能非常小，但效应却属于明显但很微弱，这样的结果也许可以发表，但对实际问题的解决几乎没有贡献。更极端的情况是如果你加大了样本量来提高统计功效，你总是能发现差异的，也就是你的空假设里原有学科理论为真也是会被方法学进步给推翻的。总结下就是 Meehl 在60年代就提出的&lt;a href=&#34;https://philpapers.org/rec/MEETIP&#34;&gt;悖论&lt;/a&gt;：方法学的进步与增大样本数对于相对硬（理论根基深厚）的学科证伪是正面的，但对相对软（理论比较模糊）的学科则是弱化。方法学悖论的根基其实是应用学科与基础学科的矛盾，基础学科用 NHST 检验观察事实中的理论，但应用学科用 NHST 来检验的是实验设计预测下的事实，此时实验设计的那个假设与 NHST 的空假设并不对应，而 NHST 先天弱化空假设的问题就凸显了。&lt;/p&gt;
&lt;p&gt;事实上，p值正在成为测量投资与努力而不是事实的标准，给定差异，我们总能找到足够的样本来发现这个差异（这也就是常说的功效分析）。也就是说，NHST 有时候功效不足测不到差异，有时候又一定会能测出差异，但科学事实并不会因为你使用了 NHST 而发生变化，特别是有意义的变化。而作为标准的p值其实在被样本数决定同时又综合了测定效果强度与不确定性，这样的一个标准其实有点多余，你完全可以用描述性统计与置信区间来分别表示效果强度与不确定性。p值也并不能增加新知识，考虑一个多元线性模型，我们只能在多元模型里得到参数，也就是有限检验，不能发现未知参数，但科学就是寻找未知；变量间的关系在数值改变后如何考察，正负关系如何预测，预测性也就无法实现。那么此时，还有必要使用 NHST 吗？&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;低垂的果实&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;低垂的果实&lt;/h2&gt;
&lt;p&gt;20世纪的技术有了意义深远的进步，但更现实的问题是，科研里低垂果实已经没有了，学科从分立走向交叉，开始不断服务社会，所有的科研都像是应用科研。服务社会职能的出现要求科学家回答的不再是科学问题而是现实问题，或者说，科学地回答现实问题。但现实问题非常复杂，科学家要想排除影响，大都采用控制实验来验证观察研究中的事实。注意，这里的事实不再是理论假设，而是一个现象，如果本来就观察到了差异，用 NHST 根本就不会让我们知道更多的事实，我们可以用无数独立手段证明这个事实的存在然后整合进学科知识体系，但并不能产生更多的思考，理论的预测效能在 NHST 里实际是体现不出来的。&lt;/p&gt;
&lt;p&gt;抛开这个问题，另一个更现实的问题是很多一线科研人员甚至还没搞懂 NHST ，说不好听一点，就是只会模仿别人论文，这样连错的都一块模仿了。这里面深层次的原因是科研人员的教育仅仅停留在了知识与逻辑层面，没有系统的科学思想训练与科学史背景。后面那两个对发文章不但没用，反而会让你怀疑科生，但没有后面这两个，你只会看到一个各干各的一团和气的研究氛围，没有评论与争执的岁月静好只会让整体科研水平永远停留在二流追随的状态。你去看各学科顶级期刊里的评论与回复，你会体会到哪里在发生的事，举个不恰当的例子，你到 Github 上去看，那些常用的软件都会在不断的更新与协作，而学术论文的更新与协作却少得多，一个重要的因素就是很多所谓科研成果永远都不会有人重复与验证，最大的作用就是放在简历里谋求职业生存，很多人自以为掌握了高端的果实但其实那些果子对学科发展并无意义，如同 NHST 一样，用之无味，弃之生存空间都没了。如果你把科研当职业，起码也要有点 Github 的分享与协作的职业素养。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;路在何方&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;路在何方&lt;/h2&gt;
&lt;p&gt;关于 NHST 其实还有很多问题例如多重比较跟p值发表歧视，但系统去看，p值也有着自己的生命力，我想更多人关心的是如果我不用 NHST ，拿什么证明我的结果可靠？如果没得选，这剂毒药还是得吃啊。答案其实上面都大概提到了，你如果坚持使用p值，那么就也请同时报告参数估计与置信区间，虽然这个方法也被人喷过。如果你打算完全开一条新路，那就去学贝叶斯统计，贝叶斯统计有自己成套的处理体系，简单说就是先假设参数分布，然后用数据更新分布，后验分布计算出来就同时有点估计跟方差估计，同时多重比较问题也不存在，但随机错误无法避免，此时参数估计方差大也能体现，后续研究可以使用这次的后验数据作为下次先验数据，这样你可以实现完全的 N + 1 模式科研，其验证与预测性也很大程度依赖采样与模拟技术，之前贝叶斯方法不能流行很重要的一个原因就在于计算比较贵，现在就便宜很多了。但我想说的是，这类知识因为提出时间不长除了几个数的过来的名校开设了课程外几乎完全需要自学，不过你要是真对科研感兴趣，这都不算什么。&lt;/p&gt;
&lt;p&gt;有人说我写过的几篇文章是在劝退，我还真没那个意思，看清现状后的理性选择对人对己都是负责的，如果你根本不适应这个现状还又觉得没了退路，那你肯定是被什么毒鸡汤绑架了，这世界上本来就没路。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>主成分分析那些事儿</title>
      <link>https://yufree.cn/cn/2017/09/14/pca/</link>
      <pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2017/09/14/pca/</guid>
      <description>


&lt;p&gt;目之所及，主成分分析应该是科研领域里最通用的一种数据分析手段。在相当长的一段时间里，我认为这种方法主要是用来进行探索分析的可视化手段与数据降维，但最近因为出现了一个绕不过去的数据问题就把主成分分析又拎出来看了一下，这才意识到这个方法其实四通八达，可以把很多数据分析的概念连接起来。&lt;/p&gt;
&lt;div id=&#34;从线到点&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;从线到点&lt;/h2&gt;
&lt;p&gt;首先还是回到一个最简单的场景，我有一堆数，想找出一个来做代表。从距离测量角度，就是找一个点让它离所有点最近，这就成了个优化问题，此时不同测量方法结论是不一样的。例如你考虑距离的绝对值最小，那你就会得到中位数；如果是差异的平方，求导后就是均值。回想下对一堆数找一个数，其实就是一种降维，从1维降低到0维。这里我们只考虑最小化差异的平方，那么求均值就是主成分分析把从1维降低到0维的应用场景。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;从面到线&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;从面到线&lt;/h2&gt;
&lt;p&gt;现在复杂一点，我们设想一个二维（无码）平面，如果我们对其降维，那么降一维就是线，降两维就是点。而且我们可以确定降两维的那个点肯定就在降一维的线上，不然你这个降维就丧失了代表性。至于如何保障代表性，一般来说要交给数学家。那么这条线会通过所有点的均值，此时你应该想起来二维线性回归也通过这个点，那条线可以通过最小二乘得到，会不会就是我们要找的那条线？这个答案是否定的，最小二乘里最小化的是因变量到回归线的值，但是这里主成分分析最小化的是所有点到一条线的垂直距离，模型上细微的差别导致结果也会有区别，事实上求解过程也不对等。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;主成分分析的求解思想&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;主成分分析的求解思想&lt;/h2&gt;
&lt;p&gt;虽然最小二乘回归线是高斯－马尔可夫定理下线性回归的最佳无偏估计，但主成分分析里二维降一维里那条线的求解思想并非回归到均值，常见有两种解释。第一种是寻找低维度空间，使投影点上到高维度点距离最近；另一种则是从统计角度寻找让点之间方差最大的方向映射，然后寻找跟这个方向正交的方向中方差最大的方向继续映射。从求解上，这两种解释都可转化成最优化问题，都是先归一化，然后求协方差矩阵，通过求导求特征向量跟特征值，那个方差最大的方向或距离最短子空间维度就是协方差矩阵里特征值最大的特征向量，剩下的方向或维度跟前面那个正交，再次找方差最大或距离最小即可。当然协方差矩阵不是一定要求的，如果你选用奇异值分解的套路就完全不用求。在这个求解策略下，解析解就是正交的，如果不是，那就不是主成分分析了。&lt;/p&gt;
&lt;p&gt;除此之外，理论上你也可以用隐藏变量模型迭代求解，不过有解析解不要用数值分析去逼近，而且有些矩阵运算可以进行分布式计算，这个在数据量大时是要特别考虑的。主成分分析求解上可以用矩阵是很大的优势，虽然理论上其概率解释并不完美。不同求解思想的多元分析方法其实背后都是有思想跟应用场景的，虽然理论上很多都是通用方法，但如果不适合你的数据就不要用。当前由于技术进步，之前很多很耗性能的方法目前都可以计算得到，如果搞科研我们要找那个最完美的，但工业应用可能更看重性价比。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;归一化&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;归一化&lt;/h2&gt;
&lt;p&gt;如果我们进一步考察三维空间，那么我们的降维就首先是一个平面，然后是平面上的线，然后是线上的点。此时如果你对所有数据点乘2，那么很自然点、线、面的坐标位置都要变化，这样你就可以理解一个事实，那就是主成分分析对尺度是敏感的，所以一般来说都要对不同尺度／维度的测量进行归一化，否则你的映射会被大尺度的维度带跑偏。到现在为止，我们可以大概对主成分分析有个直观感受：将高维度空间的点映射到低纬度空间的点且要保证这些点之间的差异关系最大程度地保留，至于怎么保留，不同求解思想实际求解结果一致，都可以用矩阵运算，内含了进行转换或映射时要沿着正交的维度走（使用了正定阵），所以求解完矩阵就可以得到符合要求的低维度空间，而且低维空间是正交的。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;投影点的方差&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;投影点的方差&lt;/h2&gt;
&lt;p&gt;主成分分析经常用来可视化，这里我们回到二维平面降维的场景仔细看看我们究竟可视化了什么。首先我们有一个二维点A，这个点投影到一维线上得到点B，这个点跟所有点的均值C连线就是到0维的投影。目前我们已知AC这个线，同时A到一维线的距离又要最小也就是垂直，这样A、B及C构成一个直角三角形。此时根据勾股定理BC这个距离最大，也就是一维到0维时所有投影点的距离之和最长，在这个方向中各点间方差最大程度保留，也就是找到了方差最大的方向。事实上，因为前面提到的直角三角形，每降低一次维度，点之间的距离比高维度都不可避免的减少，如果此时点聚到一起不一定相似度很高，但如果主成分占总方差比重比较大，那么这些点就很有可能十分相似。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;多维标度分析&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;多维标度分析&lt;/h2&gt;
&lt;p&gt;说到距离，其实主成分分析也是多维标度分析的一种。在经典多维标度分析中，测定点很困难，但可以测到点之间欧式距离并构建距离矩阵，因为其映射子空间里点之间方差最大时可以证明点之间的距离也是最大的，这个特性保证了当我们只有距离矩阵时进行主成分分析得到的低维映射也可以保证两个空间间距离最短，这样主成分分析事实上符合经典多维标度分析。也就是说，在你能够测到欧式距离而不是点时，是有可能重构出原始标度的，这点在结构生物学上有应用，但我完全不了解。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;概率化的主成分分析&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;概率化的主成分分析&lt;/h2&gt;
&lt;p&gt;主成分分析在求解上基本都走了矩阵运算的捷径，结果也是等价的。但这个过程不算是一个概率模型，因为可能产生不确定度的白噪音根本没出现在求解模型中。此时，我们应该意识到，这个子空间可能是某个概率模型的解，但如同我们只求了均值没求方差一样，似乎我们没有考虑模型的不确定度。这样我们需要从统计角度把主成分分析统一到基于统计学的数据分析中，这样也许会对将来构建相关假设检验模型有用，当然这也意味着我们可能不太方便再用矩阵运算来求解了。&lt;/p&gt;
&lt;p&gt;首先，我们对数据点进行假设，例如来自一个正态分布，那么主成分分析的问题就转化为求一个子空间，使得映射后的距离最小。让我们把这个映射关系描述成下面这样：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
t = Wx + \mu + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这里t是我们观察到的数据点，W是映射关系，维度不变可以理解成坐标轴旋转，x是映射后的点，&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;代表x的均值，&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;代表高斯随机变量。这样我们看到的点符合均值&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;，方差&lt;span class=&#34;math inline&#34;&gt;\(WW^t + \psi\)&lt;/span&gt;的正态分布，这里&lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;代表了随机误差，如果我们不考虑这一项，那么主成分分析是完全可以用特征值跟特征向量求解的，此时我们默认那个误差项0方差。但是，实际场景中我们都很清楚每一个高维样本点都至少存在测量误差，这个误差的方差不是0，那么此时我们应该在模型中包含误差项，但一个很尴尬的问题是我们对这个误差一无所知。此时我们假定所有点的误差项来自于某一个方差统一的正态分布，然后有了这个限制条件就可以求解了。加入了这一部分后，主成分分析就可以进行假设检验了，例如某个点是否属于异常值。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;em算法&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;EM算法&lt;/h2&gt;
&lt;p&gt;说到求解EM算法是绕不过去的，这个算法普适性比较强，存在隐藏变量的模型求解都可以用。主成分分析可以看作一种存在隐藏变量的模型，我们在低维空间看到的点背后是高维空间点但看不到，反之也成立。这样我们先随意假设一个新空间出来，这样我们就可以进行E阶段计算，也就是把看到的点投影到这个新空间上，然后计算距离。之后我们就可以进行M阶段，也就是最小化距离，这样就做出了一个比起始新空间距离更小的空间。然后再进行E阶段，M阶段，直到距离无法缩小。说白了就是模型不存在时先人工创造一个，然后不断按你的目标迭代让模型跟数据契合。在EM算法里，我们就可以很轻松把前面的方差项也扔进去一同优化，最后也能求解。这样概率化的主成分分析就有解了。不过这个算法具体实现存在很高的技巧性，我们吃现成就可以了。同时你会发现，其实EM算法思想可以用在很多不同模型参数求解上，马尔可夫过程、贝叶斯网络、条件随机场等有隐含变量的都可以用。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;因子分析&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;因子分析&lt;/h2&gt;
&lt;p&gt;其实在更多资料中引入概率化的主成分分析主要是为了引入因子分析，因子分析跟概率化主成分分析最大区别在于其不限制误差来自方差相同的正态分布。这当然增加了计算难度，但其实因子分析对于解释这种隐藏结构其实比主成分分析更靠谱。但是，因子分析求解上不如主成分分析容易理解，需要通过一些方法来决定因子数或干脆使用者自己决定。此外，因子分析是可以进行预测的，目标就是潜在因子。从概率角度讲主成分分析自然也可进行预测，不过你得想清楚应用场景。同时，因子分析得到的成分也是正交的，这点跟主成分分析一致。正交的优点在于映射之间不相关，但不一定独立，如果数据分布需要独立因素就需要独立成分分析。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;独立成分分析&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;独立成分分析&lt;/h2&gt;
&lt;p&gt;独立成分分析在独立成分符合正态分布时其实就是主成分分析，但当你独立成分并不来自正态分布时，独立成分分析就更有优势将其反推出来。因为独立跟相关是不同的，独立在统计学里比不相关约束条件更强，不相关不一定独立但独立一定不相关，独立因素间的互信息为0或者说高阶统计量上都不相关。最经典的应用就是鸡尾酒会问题，在一个嘈杂的场景里很多人都在说话，你在固定位置放了几个麦克风，这样麦克风收集到的就是多种声音的混合，现在你需要把混音中每个人的声音提取出来。此时你要用主成分分析，你会得到所有人声音的共性，但独立成分分析就可以分辨出每个个体，或者说潜在变量，所以你也猜到了，EM算法也可以求解独立成分分析。需要注意的是独立成分分析不管降维，基本你设定分多少个就有多少个。但不论主成分分析、因子分析还是独立成分分析，本质上都是线性模型的结构，也就是所谓的主成分、因子、独立成分都是原始数据的线性组合。&lt;/p&gt;
&lt;p&gt;在我看来生物组学数据更适合独立成分分析，你可以直接从数据中提取出一组独立模块进行注释，这样可以直接去关联生物学功能而不是反过来。当然你可能会说主成分分析或因子分析也可以做啊，但你如何保证其分布假设与正交假设？不过我们还是用主成分分析来说一下，因为很多人没意识到这个功能。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;聚类共性压缩降噪&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;聚类／共性／压缩／降噪&lt;/h2&gt;
&lt;p&gt;有些论文用主成分分析搞聚类画圈圈来说明样品间存在内在共性。这个在环境分析中比较常见，因为环境分析通常同时测定上百种化合物，前面提到低维映射里最大程度保留了样品点的差异，此时映射到一起就有可能说明那些样品污染特征接近，便于探索来源或环境过程。实际上此时不一定需要局限在主成分分析，可以直接用聚类分析等统计模型。&lt;/p&gt;
&lt;p&gt;很多人搞不清楚特征值、特征向量还有载荷等的细节，所以主成分分析就被用成了降维画图工具，但其实这个探索分析是针对背后隐藏变量的，具体到主成分分析就是共性。还是举个例子来说吧，我有100个样品，每个样品测了1000个指标，现在我就有了个&lt;span class=&#34;math inline&#34;&gt;\(100*1000\)&lt;/span&gt;的矩阵，通过主成分分析我得到了&lt;span class=&#34;math inline&#34;&gt;\(100*250\)&lt;/span&gt;的矩阵，这个矩阵包含了原数据95%的方差。好了，现在我问你，这250个新指标是什么？对，特征向量，特征向量就是新投影方向，投影可以看作隐含共性。特征值又是什么，共性的权重，越大代表越重要，毕竟可以代表更多的方差。那么载荷又是什么，大概可以理解成原来1000个指标对250个新指标的贡献。那么进行分析时我们在样本和指标之间多了一个共性层，一方面减少了数据维度，另一方面算是提取了指标间不相关的共性（但不一定独立，切记）。对于多出来的共性层，我们同时知道样品在这些共性上的分布，也知道每个指标对共性的分布，常见的biplot就可以同时绘制样品点跟指标在两个最重要共性上的分布，一目了然。此时我们的专业知识就要上场了，我们可能会通过指标相互作用发现共性背后对应隐含因素的物理含义，也可以发现某种分离样品的共性背后暗示的样品潜在来源。总之，多了一个共性层后，我们可以研究的机理就更明显了，例如自然语言处理里可以用其寻找文本主题，基因组学里可以用来寻找基因模块等。但需要提醒的是，这个“共性”并不代表客观规律，只是一种线性变换后的结果，如果跟实际想研究的因素不对应还不如直接上回归分析。&lt;/p&gt;
&lt;p&gt;主成分分析或者说实现主成分分析的奇异值分解的另一个应用就是可以用来压缩数据，上面的例子中100*1000的数据空间如果赶上稀疏矩阵十分浪费，此时就可以用奇异值分解压缩存储空间。从信号处理的角度去看，主成分分析跟傅立叶变换等变换过程本质上都是用一组新信号替代原来信号，由于一般认为信号方差高于噪音方差，通过变换时保留主成分或特定频谱，我们同时可以做到降噪。图形处理也可以用，而所有数据其实都可以用图形展示，那么作为图形降噪的主成分分析背后跟数据降噪是可以联系到一起的，特别环境痕量分析中的降噪。结合前面的结构重构、方差保留等性质，其实哪怕你就只会主成分分析，想明白了能做的事也很多。&lt;/p&gt;
&lt;p&gt;你只是需要一点想象力。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>差异、相似与模式</title>
      <link>https://yufree.cn/cn/2017/05/17/pattern/</link>
      <pubDate>Wed, 17 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2017/05/17/pattern/</guid>
      <description>


&lt;p&gt;最近审了一份主题是质谱数据分析的稿子，领域是分析化学，很明显的感受就是作者但是很明显是为了用新方法而用，没有从问题出发，所以借机讨论总结下科研数据分析的思考视角。
差异&lt;/p&gt;
&lt;p&gt;科研数据分析最基础的出发点就是寻找差异，你观察到了两组数据，这个分组是根据实验设计或人为划分的，你想了解两组数据差异。最朴素的思路就是分组聚合，例如选取出现最多的众数，排序中间位置的中位数以及平均值。但是这个思路只是简单的将一组数描述为一个数，并无法表示这组数的离散程度，也就是丢失了一部分可以进行对比的信息。如果你考虑上表示离散程度的方差，结果就成了对比两个数。这样的对比其实只是描述性的，如果你愿意且具备统计与数学功底，你可以构建出无数用来描述一组数据的单一或多个数值进行比较，这些数值在不同领域可能有不同称呼，可以理解为指标，或者称作统计量。统计量的构建至少满足两个条件：包含想要考察的信息与具备可比较的数学性质。前者比较好理解，后者就需要概率论做基础了，在必要时可根据实际问题修改统计量的数学描述方式。万万不可别人让你用什么就去用什么，这样永远是雾里看花。&lt;/p&gt;
&lt;p&gt;有了统计量，我们就可以进行比较了。不论你使用p值还是贝叶斯推断，其核心思路就是将统计量对应到一个分布里去，然后从概率视角看一下这个差异离不离谱（或者跟先验的概率分布比较），如果离谱就认为差异是显著的，跟0差异不大就认为没有明显的差异。但需要知道的是没有明显的差异是对应你假设检验方法而言的，不是说真实是没差异的，用不同的检验方式，结果可能不同。对于严谨的科学发现，主流的检验方式得到的结论应该是相似的，如果结果有差异，那么要么是检验方式无效或不适用，要么就是你的数据对差异判断并不支持。很多时候，如果技术发展不到位，数据的噪音掩盖信号，此时你不能验证结论，需要上更先进的仪器。从这个角度看，数据采集技术是否先进会制约科学发现，这也是很多学者寻求发展时经常要考虑平台是否先进的根本原因，没有技术平台，科学发现只能说模棱两可。好比你想用放大镜研究细胞结构，基本只能拿着鸡蛋看看了。&lt;/p&gt;
&lt;p&gt;差异多数时候是两两间的，但有时候我们的问题是在某个因素不同水平的影响是否显著，例如我想知道某种污染物的自由态浓度是否受高中低不同土壤含水量的影响，这里土壤含水量是因素，高中低是三个水平，此时用方差分析就可以得到土壤含水量是否影响污染物自由态浓度的判断。推而广之，如果水平是连续变量，那么此时的差异分析实质上是相关分析或者说是线性模型的一个特例。你总是要对模型系数进行假设检验来确定这个系数是否影响了你要考察的变量，如果你要进一步进行讨论，参考下面的模式那一部分。&lt;/p&gt;
&lt;p&gt;发现一个差异并从统计角度说明其出现概率比较低或跟先验知识包含不同的信息量是科研数据分析最常见的应用场景。基本思考流程可以归纳为首先构建代表性可比较的统计量，然后进行假设检验的比较，最后根据结果给出判断。这个判断过程有统计学与概率论做支撑，独立于观察过程，因此判断具备相对客观的属性。&lt;/p&gt;
&lt;p&gt;相似&lt;/p&gt;
&lt;p&gt;另一个科研中常见的数据分析场景是寻找数据的共性，但其实基本思路跟找差异比较接近但面对的数据结构会不太一样。在差异分析中，通常考察的是单一属性；相似性分析中，通常你会得到对同一客体的多个描述角度。举例而言，我得到两组河水样品，想知道两组水样是否接近，此时每组样品如果只测定一个指标，那么对比一下就完了；但如果测了很多组指标如何来衡量？两组河水pH值很接近但COD差距很大，可同时溶解氧几乎相同，如何判断？&lt;/p&gt;
&lt;p&gt;一个朴素的想法就是测量可比指标间的标准化差异，然后求绝对值和或平方和，越大表示相似度越小，或者用类似核函数的思想把高维数据映射到低维空间，还可以进行傅立叶变换来通过低维数值保留核心信息量进行比较。此外一个我非常欣赏的思路就是通过打乱分组构建随机统计量来对比实际发生的统计量出现概率，大概就是Fisher精确检验的套路，这个角度是纯统计思路且在计算不那么贵时很好用。&lt;/p&gt;
&lt;p&gt;相似性分析的科研应用场景会越来越多的，首先是数据库比对，目前组学技术发展很快，相关数据库累积也很快，你发现一个功能蛋白，反推出序列可以直接去搜库做进化树，这里面的相似性分析大都用到的动态编程与数据变换，不然速度跟不上。在质谱上就是谱库检索与比对，此时要考虑同位素分布、质量亏损、源内反应加合物等等，不然也很难比对相似性。另一个应用场景是非监督学习里的聚类分析，这里面相似性统计量是进行聚类的基础。其实虽然科学发现里寻找差异更符合探索逻辑，但实际上当前的数据驱动性研究更多是发现共性，当数据累积越来越多，对于共性的新研究方法或新统计量的提出可能更有价值。&lt;/p&gt;
&lt;p&gt;模式&lt;/p&gt;
&lt;p&gt;差异与共性分析都是最基础直观的科研数据分析思路，学科内规律性的东西有时候并不能直接从差异跟共性分析中得到，这时需要识别数据中的模式规律。所谓模式在科研结果中有两种，一种是探索未知，另一种是拟合已知。前者并不需要特别强的理论基础，用来发现模式；后者需要有比较强的理论支撑。&lt;/p&gt;
&lt;p&gt;拟合已知的最常见，例如线性拟合、多项式拟合等，很多时候由本学科的理论来提供基本形式，例如物理里的牛顿三定律、化学里的能斯特方程、生物里的米氏方程。这些方程的数学形式已经在学科内得到了认可，所以当你获取相关数据时，模式是固定的，你所需要求解的是某个参数。参数的稳定性也可以侧面反应理论的真实性，但拟合严格说更像是验证模式而不是发现模式。同时拟合背后的理论基础及来源也是要深入理解的，例如做吸附等温线有两种基本拟合方式：弗里德里希方程与朗缪尔方程，如果你搞不清机理随便用任何一种都会发现拟合效果都说得过去，但一个是纯理论另一个是经验公式，在使用时得考虑你研究目的。有一类研究比较看重预测效果，此时拟合就可以放宽，例如用多项式拟合考虑个自由度就可以了，甚至有时候可以考虑不同数值范围采用不同数学形式，在边界上用样条平滑下就可以了。但不论拟合理论公式还是经验公式，起码这类分析总还是有个数学公式来做骨架的，探索分析则没有这个限制。&lt;/p&gt;
&lt;p&gt;探索未知模式并不是说让你直接把所有科学问题都抽象成 y = f(x) ，然后你收集一大堆y跟相关x，直接扔到多层人工神经网络里去训练，然后搞个验证集看下效果就用，这是工程学思路，到头来也许解决问题，但你可能根本不理解问题。正常的探索过程基本还是有个模型指导的，你可以从最简单的线性模型开始尝试，然后不断提高模型的复杂度，例如引入交互作用或不同x用不同模型的广义加性模型。当然，你可以引入层级模型来探索数据内部结构。当模型复杂到一定程度，就有点人工神经网络的意思了。如果你是为了发现新模式，那么可视化手段是很好的探索起点。但如果是为了预测，其实样条平滑、小波分析等黑魔法就可以随意发挥了，大前提是你真的理解算法，知道自己在干什么。&lt;/p&gt;
&lt;p&gt;从寻找差异开始，我们很多朴素的想法背后其实都有基本模型在起作用，例如t检验就是方差分析的特例、方差分析就是线性模型的特例，线性模式又可以推广到广义线性模型。从最原始的单一统计量构建到多个独立统计量关系探索，再到考虑交互作用，再到层级结构，模型的复杂度可以不断提升。模型的生成过程也要伴随大量的验证与理论支持，并不是随意可以套用，除非你搞机器学习。其实你应该体会到数据分析与科学发展是紧密联系而不是割裂的，很多数据分析方法就是为解决特定科学问题提出来的，没有想象的那么黑箱，反倒是总把新数据分析方法当成黑箱的思路是很危险的。在绝大多数情况下，科学问题的数据分析方法都是针对性的，你也应该能从分析过程体会到背后的抽象与基本假设，否则并不真正理解，虽然这可能不妨碍你发文章。&lt;/p&gt;
&lt;p&gt;从问题视角出发去构建一个方法要比直接套用学科内常见方法更容易体会到《科学研究的艺术》。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>鸢尾花数据集背后的故事</title>
      <link>https://yufree.cn/cn/2017/01/15/iris-dataset/</link>
      <pubDate>Sun, 15 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2017/01/15/iris-dataset/</guid>
      <description>&lt;p&gt;如果说对机器学习或统计学习里最常见的示例数据集进行排序，那么鸢尾花数据集一定排的上号，而且不同于事后诸葛的泰坦尼克生还者数据，这个数据集理论上是可以拿来做预测的。设想某个清晨，你漫步花园并驻足于一朵鸢尾花前，然后你掏出尺子，测量了花萼长度、花萼宽度、花瓣长度跟花瓣宽度后静默片刻，淡淡的说到：“果然又是个维吉尼亚鸢尾。”留下一堆路人甲风中凌乱。&lt;/p&gt;
&lt;p&gt;但其实你是做不到的，新西兰统计学家Thomas Lumley最近发表了一篇&lt;a href=&#34;http://notstatschat.tumblr.com/post/155194690691/the-iris-data&#34;&gt;文章&lt;/a&gt;认为，这个数据集其实是Fisher或Anderson拿来想让读者做线性判别或无监督聚类的，而在真实的野外环境中，花从来都不是一个良好的种属判断条件而是探索一个假设的论据。。&lt;/p&gt;
&lt;p&gt;现在我们来看看当年究竟是为什么发布这个数据集。Anderson于1936年在《Annals of the Missouri Botanical Garden》上
发表了一篇题为《The Species Problem in Iris》的论文，不得不说我很少读到80年陈酿的论文，特别是这种用52页长篇大论讨论一个种属分类的，还没有摘要。&lt;/p&gt;
&lt;p&gt;文章第二章开头就给出了野外判断鸢尾花种属的判据：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/iris1.png&#34; alt=&#34;plot of chunk null&#34;&gt;&lt;/p&gt;
&lt;p&gt;从里面我们可以看到，三种鸢尾花的基本判据其实是种子，至于花瓣也可以用。但作者也明确提出，由于非常容易枯萎，对花的测量数据用在分类上并不可靠，甚至良好的保存手段都没有。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/iris2.png&#34; alt=&#34;plot of chunk null&#34;&gt;&lt;/p&gt;
&lt;p&gt;然而，作者通过5年的观察研究认为Iris versicolor 跟 Iris virginica 各自种类内部其实变化很大，但本质上还是不一样，作者就用两个英格兰小村庄作为对比，一个在砂石地上，另一个在石灰岩上，其建筑风格也许差不多，但建材不一样，所以无论如何都不一样。但随后作者提出，导致这一现象的原因很有可能是因为其中有一种是二倍体，所以形态上虽然像，但就不是一个种，“A simple hypothesis immediately sugguested itself”。为了说明这一点，Iris setosa登场了，因为这一类分布比较靠北，个头比较小，所以很有可能Iris versicolor是Iris setosa跟Iris virginica的杂合体。为了验证这个假设，作者依赖染色体个数的测量与花瓣花萼等数据，推测Iris versicolor与Iris virginica的亲缘关系要近于其与Iris setosa的距离，两者距离大概1:2。也就是说，在原始文献中，花的测量数据并不是用来分类而是用来计算三个物种间亲缘关系的。&lt;/p&gt;
&lt;p&gt;其实Fisher在&lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/full&#34;&gt;公布这个数据集&lt;/a&gt;时也说的很明确，这些测量数据就是来说明Iris versicolor是Iris virginica与Iris setosa的中间类型，拿来实际分类不靠谱。虽然Fisher自己就是拿这些数据搞了一个线性判别分析。而线性判别分析的实质是认为花的测量数据是来自于不同的分布，通过计算分布参数来进行区别。说的更像人话一点就是我对四个测量值进行一种线性变换，目的是让这种线性变化可以很好的区别三个分类。既然是线性变换最终还是会得到一个预测值，衡量三个分类这个预测值之间的距离就可以进行其关系的推测。结果自然是确认了1:2这个比例，而且后续的研究也在16sRNA上确认了这个发现。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/iris3.png&#34; alt=&#34;plot of chunk null&#34;&gt;&lt;/p&gt;
&lt;p&gt;其实从这个数据集的故事是我们可以清晰感觉到的不是一个统计学过程而是科研过程。从观察到提出假说，然后通过数据分析给出证据，最后通过后续的研究不断证明结论，从已知走向未知。而当今的很多研究，你很难找到假设检验的影子，更多偏重的是流程化科研，用更尖端的技术得到更准确的测量，然后甩给统计学家处理，缺少了最初的&amp;quot;insight&amp;quot;。或者说，相对专业的领域分工让科学家自己也变得工具化，缺少研究方法，特别是数据处理方法与实际问题的原理层互动从而将数据分析黑箱化与实用化，这不妨碍实际问题的解决，但会少很多发现的乐趣。&lt;/p&gt;
&lt;p&gt;当然，寻找insight可能是未来人工智能可以做的，但愿这一天晚点到来。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>高通量数据的多重检验问题</title>
      <link>https://yufree.cn/cn/2017/01/04/mutiple-test/</link>
      <pubDate>Wed, 04 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2017/01/04/mutiple-test/</guid>
      <description>


&lt;p&gt;各种组学分析技术的进展导致了我们在收集数据时更侧重数据信息的保存，然而我们收集的数据最终也会根据我们的想探索的问题来寻找答案，甚至有时候我们在实验设计分组时就打算考察某一个变量而为了获取更多的相关信息而采用了组学技术。这点是尤其要强调的，科研人员一定是面向科学问题解决科学问题，而不要为了应用新技术而应用新技术。当然，现实的情况是新技术特别是组学技术的发展为我们提供了大量的可同时测定的生物学指标（例如基因表达水平、蛋白表达水平、代谢产物表达水平）数据，大到我们事先也不知道会有什么模式会出现，这样就需要数据挖掘，特别是统计学知识来帮助我们发现新知。然而，组学技术产生的这类高通量数据是具有一些特质的，数据里确实会有我们关心分组的差异表达，但同时也有大量测量值对于我们设定的分组不敏感，然而当我们去对比组间差异时就会被这些数据干扰。&lt;/p&gt;
&lt;p&gt;举例而言，我对两组样品（暴露组跟对照组）中每一个样品测定了10000个指标，每组有10个样品，那么如果我想知道差异有多大就需要对比10000次，具体说就是10000次双样本t检验。那么如果我对t检验的置信水平设置在0.05，也就是5%假阳性，做完这10000次检验，我会期望看到500个假阳性，而这500个有显著差异的指标其实对分组不敏感也可以随机生成。假如真实测到了600个有显著差异的指标，那么如何区分其中哪些是对分组敏感？哪些又仅仅只是随机的呢？随机的会不会只有500个整呢？&lt;/p&gt;
&lt;p&gt;这就是多重检验问题，做经典科研实验时往往会忽略，深层次的原因是经典的科研实验往往是理论或经验主导需要进行检验的假说。例如，我测定血液中白血球的数目就可以知道你是不是处于炎症中，其背后是医学知识的支撑。然而，再组学或其他高通量实验中，研究实际是数据导向的，也就是不管有用没用反正我测了一堆指标，然后就去对比差异，然后就是上面的问题了，我们可能分不清楚哪些是真的相关，哪些又是随机出现的。&lt;/p&gt;
&lt;p&gt;当然这个问题出现也不是一天两天了，再&lt;a href=&#34;http://yufree.cn/blogcn/2013/12/16/rgabriel-package.html&#34;&gt;多重比较&lt;/a&gt;问题上就已经被提出过，只不过在多重比较里对比数因为排列组合比较多而在多重检验里纯粹就是因为同时进行的假设检验数目多。那么其实从统计角度解决的方法也基本来源于此。&lt;/p&gt;
&lt;div id=&#34;整体错误率family-wise-error-rate控制&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;整体错误率（Family-wise error rate）控制&lt;/h2&gt;
&lt;p&gt;对于单次比较，当我们看到显著差异的p值脑子里想的是空假设为真时发生的概率，当我们置信水平设定在0.95（I型错误率0.05）而p值低于对应的阈值，那么我们应该拒绝空假设。但对比次数多了从概率上就会出现已经被拒绝的假设实际是错误的而你不知道是哪一个。整体错误率控制的思路就是我不管单次比较了，我只对你这所有的对比次数的总错误率进行控制。还是上面的例子，对于10000次假设检验我只能接受1个错误，整体犯错概率为0.0001，那么对于单次比较，其I型错误率也得设定在这个水平上去进行假设检验，结果整体上错误率是控制住了，但对于单次比较就显得十分严格了。下面用一个仿真实验来说明：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 随机数的10000次比较
set.seed(42)
pvalue &amp;lt;- NULL
for (i in 1:10000){
  a &amp;lt;- rnorm(10)
  b &amp;lt;- rnorm(10)
  c &amp;lt;- t.test(a,b)
  pvalue[i] &amp;lt;- c$p.value
}
# 看下p值分布
hist(pvalue)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/null-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk null&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 小于0.05的个数
sum(pvalue&amp;lt;0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 477&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 小于0.0001的个数
sum(pvalue&amp;lt;0.0001)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这样我们会看到进行了整体的控制之后，确实是找不到有差异的了，但假如里面本来就有有差异的呢？&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
pvalue &amp;lt;- NULL
for (i in 1:10000){
  a &amp;lt;- rnorm(10,1)
  b &amp;lt;- a+1
  c &amp;lt;- t.test(a,b)
  pvalue[i] &amp;lt;- c$p.value
}
# 看下p值分布
hist(pvalue)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/diff-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk diff&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 小于0.05的个数
sum(pvalue&amp;lt;0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6559&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 小于0.0001的个数
sum(pvalue&amp;lt;0.0001)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 45&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上面我们模拟了10000次有真实差异的假设检验，结果按照单次检验0.05的阈值能发现约7000有差异，而使用0.0001却只能发现不到100次有显著差异。那么问题很明显，或许控制整体错误率可以让我们远离假阳性，但假阴性也就是II型错误率就大幅提高了，最后的结果可能是什么差异也看不到。&lt;/p&gt;
&lt;p&gt;下面我们尝试一个更实际的模拟，混合有差异跟无差异的检验：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
pvalue &amp;lt;- NULL
for (i in 1:5000){
  a &amp;lt;- rnorm(10,1)
  b &amp;lt;- a+1
  c &amp;lt;- t.test(a,b)
  pvalue[i] &amp;lt;- c$p.value
}
for (i in 1:5000){
  a &amp;lt;- rnorm(10,1)
  b &amp;lt;- rnorm(10,1)
  c &amp;lt;- t.test(a,b)
  pvalue[i+5000] &amp;lt;- c$p.value
}
# 看下p值分布
hist(pvalue)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/mix-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk mix&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 小于0.05的个数
sum(pvalue&amp;lt;0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3499&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 小于0.0001的个数
sum(pvalue&amp;lt;0.0001)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 21&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时结果就更有意思了，明明应该有5000次是有差异的，但阈值设定在0.05只能看到约3500次，而0.0001只能看到24次。&lt;/p&gt;
&lt;p&gt;上面的模拟告诉我们，降低假阳性会提高假阴性的比率，而且似乎本来0.05的阈值对于真阳性也是偏小的。同时，面对假设检验概率低于0.05的那些差异，我们也没有很好的方法区别哪些是真的，哪些是随机的。&lt;/p&gt;
&lt;p&gt;其实很多人都知道整体错误率控制是比较严格的，但也不是完全没人用，例如寻找生物标记物做重大疾病诊断时就不太能接受假阳性而可以接受一定的假阴性，此时如果标准放宽就会找到一大堆假信号，到时候标记不准就会对诊断产生负面影响。&lt;/p&gt;
&lt;p&gt;下面介绍下常见的整体错误率控制方法：&lt;/p&gt;
&lt;div id=&#34;bonferroni-方法&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bonferroni 方法&lt;/h3&gt;
&lt;p&gt;思路很简单，就是控制显著性，例如单次检验假阳性比率&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;控制在0.05，那么n次检验假阳性比率控制为&lt;span class=&#34;math inline&#34;&gt;\(\frac{\alpha}{n}\)&lt;/span&gt;。这样实际是对整体采用了个体控制的控制思路：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P(至少一个显著)=1-P(无显著差异) = 1-(1-\alpha/n)^n
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;我们来看下&lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt;随比较数增加的效果：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- c(1:10 %o% 10^(1:2))
p0 &amp;lt;- 1-(1-0.05)^n
p &amp;lt;- 1-(1-0.05/n)^n
# 不进行控制
plot(p0~n,ylim = c(0,1))
# Bonferroni方法控制
points(p~n,pch=19)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/Bonferroni-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk Bonferroni&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;其实，这样的控制得到的整体错误率是略低于0.05的，并且数目越大，整体错误率越低。这个方法十分保守，有可能什么差异你都看不到，因为都变成假阴性了。在实际应用中一般不调节p值的假阳性比率而直接调节p值，取原始p值跟整体检验数目的乘积与1的最小值作为调节p值，还可以用0.05或0.01进行判断，不过这时候控制的整体而不是单一检验了。&lt;/p&gt;
&lt;p&gt;当然这只是最原始的Bonferroni方法，后来Holm改进了这种一步法为逐步法，此时我们需要首先对原始p值进行排序，然后每个原始p值乘上其排序作为调节p值。例如三次多重检验的p值分别是0.01、0.03与0.06，其调节后的p值为0.03，0.06，0.06。如果我们控制整体假阳性比率低于0.05，那么调解后只有第一个检验可以拒绝空假设。值得注意的是Holm的改进是全面优于原始方法的，也就是说当你一定要去用Bonferroni方法控制整体错误率，优先选Holm的改进版。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sidak-方法&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sidak 方法&lt;/h3&gt;
&lt;p&gt;上面那种方法其实有点非参的意思，其实数学上我们是可以精确的把假阳性比率控制在某个数值的：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P(至少一个显著)=1-P(无显著差异) = 1-(1-\alpha&amp;#39;)^n = 0.05
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;求解可得到&lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;#39; = 1-0.95^{\frac{1}{n}}\)&lt;/span&gt;，此时我们就可以比较精确的控制整体错误率了，但是，这个方法有个前提就是各个检验必须是独立的，这在生物学实验里几乎不可能，所以这个方法的应用远没有Bonferroni方法广。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;错误发现率false-discovery-rate控制&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;错误发现率（False Discovery Rate）控制&lt;/h2&gt;
&lt;p&gt;刚才的模拟中我们可以看到，控制整体错误率比较严格，假阴性比率高，那么有没有办法找到假阴性比率低的呢？要知道我们其实只关心有差异的那部分中那些是真的，哪些是假的，无差异的可以完全不用考虑。那么我们可以尝试控制错误发现率，也就是在有差异的那一部分指标中控制错误率低于某一水平。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 所有有差异的
R &amp;lt;- sum(pvalue&amp;lt;0.05)
# 假阳性
V &amp;lt;- sum(pvalue[5001:10000]&amp;lt;0.05)
# 错误发现率
Q &amp;lt;- V/R
R&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3499&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 225&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.06430409&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上面的计算显示虽然我们漏掉了很多阳性结果，但错误发现率并不高。事实上如果我们控制错误率到0.01，错误发现率会更低：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 所有有差异的
R &amp;lt;- sum(pvalue&amp;lt;0.01)
# 假阳性
V &amp;lt;- sum(pvalue[5001:10000]&amp;lt;0.01)
# 错误发现率
Q &amp;lt;- V/R
R&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 999&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 34&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.03403403&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其实出现这个问题不难理解，空假设检验里p值是均匀分布的而有差异检验的p值是有偏分布且偏向于较小的数值，所以假阳性控制的越小，有偏分布占比例就越高，但同时会造成假阴性提高的问题。&lt;/p&gt;
&lt;p&gt;那么错误发现率会不会比整体错误率的控制更好呢？这里通过两种常见的控制方法进行说明。&lt;/p&gt;
&lt;div id=&#34;benjamini-hochberg方法&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Benjamini-Hochberg方法&lt;/h3&gt;
&lt;p&gt;这个方法跟Holm方法很像，也是先排序，但之后p值并不是简单的乘排序，而是乘检验总数后除排序：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p_i \leq \frac{i}{m} \alpha
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;举例来说就是假设三次多重检验的p值分别是0.01、0.03与0.06，其调节后的p值为0.03，0.045，0.06。那么为什么说这种方法控制的是错误发现率呢？我们来看下&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;是如何得到的：p值乘总数m得到的是在该p值下理论发现数，而除以其排序实际是该p值下实际发现数，理论发现数基于在这里的分布是均匀分布，也就是空假设的分布，这两个的比值自然就是错误发现率。下面我用仿真实验来说明一下：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pbh &amp;lt;- p.adjust(pvalue,method = &amp;#39;BH&amp;#39;)
ph &amp;lt;- p.adjust(pvalue,method = &amp;#39;holm&amp;#39;)
plot(pbh~pvalue)
points(ph~pvalue,col=&amp;#39;red&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/BH-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk BH&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;从上面图我们可以看出，如果控制整体错误率（红色），那么p值很容易就到1了，过于严格。而如果用BH方法控制错误发现率，那么原始p值越大，调节后的错误发现率也逐渐递增，这就符合了区分真实差异与随机差异就要假设真实差异更可能出现更小的p值这个现象。当然至于这个方法的推演细节，可以去读原始论文。值得注意的是这个错误发现率求的是有差异存在的情况，不然零发现就出现除数为零了。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;storey方法q值&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Storey方法（q值）&lt;/h3&gt;
&lt;p&gt;如果说BH方法还算是调节了p值，那么Storey提出的方法则直接去估计了错误发现率本身。刚才介绍BH算法时我提到总数m与p值的乘积是基于这里的分布是均匀分布，但实际上按照错误发现率的定义，这里应该出现的是空假设总数。直接使用所有检验数会造成一个问题，那就是对错误发现率的高估，为了保证功效，这里应该去估计空假设的总体比例。这里我们去观察混合分布会发现在p值较大的时候基本可以认为这里分布的都是空假设的p值，那么我们可以用：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat\pi_0 = \frac{\#\{p_i&amp;gt;\lambda\}}{(1-\lambda)m}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;估计这个比例&lt;span class=&#34;math inline&#34;&gt;\(\hat\pi_0\)&lt;/span&gt;，其中参数&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;的跟&lt;span class=&#34;math inline&#34;&gt;\(\hat\pi_0\)&lt;/span&gt;的关系可以用一个三阶方程拟合，然后计算出整体假阳性比例。有了这个比例，我们再去按照BH方法计算p值，然后两个相乘就会得到q值，而q值的理论含义就是在某一概率上低于这个概率所有数里假阳性的比重。打个比方，我测到某个指标的q值是0.05，这意味着q值低于这个数所有检验中我有0.05的可能性得到的是假阳性。。但我们会发现当空假设比重较高时BH结果跟q值很接近，而比重很低的话q值会变得更小，功效会提高，基本也符合我们对错误发现率的预期。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(qvalue)
q &amp;lt;- qvalue(pvalue)
# Q值
plot(q$qvalues~pvalue,col=&amp;#39;blue&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/qvalues-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk qvalues&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;如上图所示，q值增大后会最终逼近到0.5，而我们的模拟中空假设的比例就设定就是50%。我们重新模拟一个空假设比例5%的实验：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
pvalue &amp;lt;- NULL
for (i in 1:500){
  a &amp;lt;- rnorm(10,1)
  b &amp;lt;- a+1
  c &amp;lt;- t.test(a,b)
  pvalue[i] &amp;lt;- c$p.value
}
for (i in 1:9500){
  a &amp;lt;- rnorm(10,1)
  b &amp;lt;- rnorm(10,1)
  c &amp;lt;- t.test(a,b)
  pvalue[i+500] &amp;lt;- c$p.value
}
pbh &amp;lt;- p.adjust(pvalue,method = &amp;#39;BH&amp;#39;)
ph &amp;lt;- p.adjust(pvalue,method = &amp;#39;holm&amp;#39;)
q &amp;lt;- qvalue(pvalue)
plot(pbh~pvalue)
# Holm 方法
points(ph~pvalue,col=&amp;#39;red&amp;#39;)
# Q值
points(q$qvalues~pvalue,col=&amp;#39;blue&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/qbh-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk qbh&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;此时我们可以看到两者结果较为接近，q值理论上更完备，功效也更强，但算法上对&lt;span class=&#34;math inline&#34;&gt;\(\hat\pi_0\)&lt;/span&gt;的估计并不稳定，特别是比例靠近1的时候，所以BH方法可能还是更容易让人接受的保守错误发现率控制。详细的估计方法还得去啃Storey的&lt;a href=&#34;http://www.pnas.org/content/100/16/9440.full&#34;&gt;论文&lt;/a&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;小结&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;小结&lt;/h2&gt;
&lt;p&gt;多重检验问题是高通量数据里逃不掉的问题，要想找出真正的差异数据就要面对假阳性跟假阴性问题，这是一个不可兼得的过程，看重假阳性就用整体错误率，看重功效就用错误发现率控制。并不是说哪种方法会好一些，更本质的问题在于你对实际问题的了解程度及统计方法的适用范围。例如你选基因芯片时实际也进行了一次选择，改变了整体检验的p值分布，而不同的p值分布对应的处理方法也不太一样，有兴趣可以读下&lt;a href=&#34;http://varianceexplained.org/statistics/interpreting-pvalue-histogram/&#34;&gt;这篇&lt;/a&gt;。有时候你的实验设计本身就会影响数据的统计行为，而这个恰恰是最容易被忽视的。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>贝叶斯棒球</title>
      <link>https://yufree.cn/cn/2016/06/12/bayes-baseball/</link>
      <pubDate>Sun, 12 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2016/06/12/bayes-baseball/</guid>
      <description>


&lt;p&gt;最近看到一系列以棒球为主题的关于贝叶斯分析的&lt;a href=&#34;http://varianceexplained.org/posts/&#34;&gt;文章&lt;/a&gt;，赶忙总结了一下，省的忘了。我非常喜欢这类通过实际案例来进行分析的讲解方法，很容易举一反三。&lt;/p&gt;
&lt;div id=&#34;什么是贝塔分布&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;什么是贝塔分布？&lt;/h2&gt;
&lt;p&gt;贝塔分布的本质是概率分布的分布。我们来看一个棒球击球率的估计问题，一共打了300个球，81个击中，219个击空。你可以计算出一个击中的概率：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\alpha}{\alpha + \beta} = \frac{81}{81+219} = 0.27\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这个概率应该来自于一个分布，而这个分布可能是参数为 &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; 与 &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; 的贝塔分布。我们看下概率密度曲线：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
x &amp;lt;- seq(0,1,length=100)
db &amp;lt;- dbeta(x, 81, 219)
ggplot() + geom_line(aes(x,db)) + ylab(&amp;quot;Density of beta&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/beta-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk beta&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;观察这个概率密度分布图可以看出一个大约在0.2-0.35的概率区间，表示击球率可能的取值空间。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;为什么击球的概率分布符合贝塔分布&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;为什么击球的概率分布符合贝塔分布？&lt;/h2&gt;
&lt;p&gt;设想球员A打了一个球打中了，那么在没有先验知识的情况下我会认为他击中概率为1；这个球员又打中了一个球，那么还是1；但第三个没打中，我们会认为他击中概率是0吗？一般而言，这类连续击球问题可以用二项分布来描述，例如10个球打中8个的概率，我们假设这个击球概率为q，那么这个概率应该是个q的函数：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(q) \propto q^a(1-q)^b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;q对于一个实际问题（例如个人击球率）是常数，所以出现这个场景的概率实际上是a与b的函数。为了保障这个概率函数累积为1，需要除一个跟a与b有关的数。这个数可以用贝塔函数&lt;span class=&#34;math inline&#34;&gt;\(B(a,b)\)&lt;/span&gt;来表示，数学证明&lt;a href=&#34;https://en.wikipedia.org/wiki/Conjugate_prior#Example&#34;&gt;略&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;那么我们继续关注这个球员，如果接着打了一个中了，那么如何更新这个概率？根据贝叶斯公式，最后推导出的结果如下：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Beta(\alpha+1,\beta+0)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;根据公式可以看出我们对这个击球率的估计会高一点，这是贝塔分布的神奇之处，形式非常简单，理解也很直观。虽然贝塔分布不是为贝叶斯分析而设计的，但其数学性质非常便于进行贝叶斯分析。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;先验与后验&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;先验与后验&lt;/h2&gt;
&lt;p&gt;如果我们后续观察的击球少，那么不太容易影响到对概率的先验估计：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(0,1,length=100)
db &amp;lt;- dbeta(x, 81+1, 219)
ggplot() + geom_line(aes(x,db)) + ylab(&amp;quot;Density of beta&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/beta1-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk beta1&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;如果后续观察了大量的击球都中了，那么概率会偏向后面数据所提供的击球率：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(0,1,length=100)
db &amp;lt;- dbeta(x, 81+1000, 219)
ggplot() + geom_line(aes(x,db)) + ylab(&amp;quot;Density of beta&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/beta2-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk beta2&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;这是贝叶斯分析的核心思想，通过证据更新经验。经验是主观的或先验的，当证据足够多，结果就偏向事实。因此，最后得到的均值（后验0.83）一定是介于经验值（先验0.27）与证据值（全击中就是1）之间。&lt;/p&gt;
&lt;p&gt;另一种不那么严谨的理解方法是如果一个概率是稳定的，那么多次实验的结果差别不会太大，则有：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{a}{b} = \frac{c}{d} = \frac{a+b}{c+d}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果每次实验的概率持平，那么不存在不确定度；但如果前面实验的次数少而后面实验的次数多，那么概率会偏重于后面，这就是贝塔分布想说明的事。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;经验贝叶斯&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;经验贝叶斯&lt;/h2&gt;
&lt;p&gt;对于两个球员，一个打了10个球中了4个，另一个打了1000个球中了300个，一般击中概率0.2，你会选哪一个去培养？我们对于小样本量的统计推断会有天然的不信任，如何通过统计量来描述？下面用MLB的数据说明，首先提取出球员的击球数据：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
library(Lahman)
# 拿到击球数据
career &amp;lt;- Batting %&amp;gt;%
  filter(AB &amp;gt; 0) %&amp;gt;%
  anti_join(Pitching, by = &amp;quot;playerID&amp;quot;) %&amp;gt;%
  group_by(playerID) %&amp;gt;%
  summarize(H = sum(H), AB = sum(AB)) %&amp;gt;%
  mutate(average = H / AB)

# 把ID换成球员名字
career &amp;lt;- Master %&amp;gt;%
  tbl_df() %&amp;gt;%
  select(playerID, nameFirst, nameLast) %&amp;gt;%
  unite(name, nameFirst, nameLast, sep = &amp;quot; &amp;quot;) %&amp;gt;%
  inner_join(career, by = &amp;quot;playerID&amp;quot;)
# 展示数据
career&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Source: local data frame [9,342 x 5]
## 
##     playerID              name     H    AB average
##        (chr)             (chr) (int) (int)   (dbl)
## 1  aaronha01        Hank Aaron  3771 12364  0.3050
## 2  aaronto01      Tommie Aaron   216   944  0.2288
## 3   abadan01         Andy Abad     2    21  0.0952
## 4  abadijo01       John Abadie    11    49  0.2245
## 5  abbated01    Ed Abbaticchio   772  3044  0.2536
## 6  abbotfr01       Fred Abbott   107   513  0.2086
## 7  abbotje01       Jeff Abbott   157   596  0.2634
## 8  abbotku01       Kurt Abbott   523  2044  0.2559
## 9  abbotod01        Ody Abbott    13    70  0.1857
## 10 abercda01 Frank Abercrombie     0     4  0.0000
## ..       ...               ...   ...   ...     ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 击球前5
career %&amp;gt;%
  arrange(desc(average)) %&amp;gt;%
  head(5) %&amp;gt;%
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;playerID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;H&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AB&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;average&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;banisje01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Jeff Banister&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bassdo01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Doc Bass&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;birasst01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Steve Biras&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;burnscb01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C. B. Burns&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;gallaja01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Jackie Gallagher&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 击球后5
career %&amp;gt;%
  arrange(average) %&amp;gt;%
  head(5) %&amp;gt;%
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;playerID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;H&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AB&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;average&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;abercda01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Frank Abercrombie&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;adamsla01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Lane Adams&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;allenho01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Horace Allen&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;allenpe01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pete Allen&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;alstowa01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Walter Alston&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;如果仅考虑击球率会把很多板凳球员与运气球员包括进来，一个先验概率分布很有必要。那么考虑下如何得到，经验贝叶斯方法认为如果估计一个个体的参数，那么这个个体所在的整体的概率分布可作为先验概率分布。这个先验概率分布可以直接从数据的整体中得到，然后我们要用极大似然或矩估计的方法拿到贝塔分布的两个参数：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;career_filtered &amp;lt;- career %&amp;gt;%
    filter(AB &amp;gt;= 500)

m &amp;lt;- MASS::fitdistr(career_filtered$average, dbeta,
                    start = list(shape1 = 1, shape2 = 10))

alpha0 &amp;lt;- m$estimate[1]
beta0 &amp;lt;- m$estimate[2]

# 看下拟合效果

ggplot(career_filtered) +
  geom_histogram(aes(average, y = ..density..), binwidth = .005) +
  stat_function(fun = function(x) dbeta(x, alpha0, beta0), color = &amp;quot;red&amp;quot;,
                size = 1) +
  xlab(&amp;quot;Batting average&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/ebayes-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk ebayes&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;当我们估计个人的击球率时，整体可以作为先验函数，个人的数据可以通过贝塔分布更新到个体。那么如果一个人数据少，我们倾向于认为他是平均水平；数据多则认为符合个人表现。这事实上是一个分层结构，贝叶斯推断里隐含了这么一个从整体到个人的过程&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;career_eb &amp;lt;- career %&amp;gt;%
    mutate(eb_estimate = (H + alpha0) / (AB + alpha0 + beta0))
# 击球率高
career_eb %&amp;gt;%
  arrange(desc(eb_estimate)) %&amp;gt;%
  head(5) %&amp;gt;%
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;playerID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;H&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AB&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;average&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;eb_estimate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;hornsro01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Rogers Hornsby&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2930&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8173&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.358&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.355&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;jacksjo01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Shoeless Joe Jackson&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1772&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4981&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.356&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.350&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;delahed01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ed Delahanty&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2596&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7505&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.346&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.343&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;hamilbi01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Billy Hamilton&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2158&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6268&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.344&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.340&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;heilmha01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Harry Heilmann&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2660&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7787&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.342&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.338&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 击球率低
career_eb %&amp;gt;%
  arrange(eb_estimate) %&amp;gt;%
  head(5) %&amp;gt;%
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;playerID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;H&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AB&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;average&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;eb_estimate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bergebi01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Bill Bergen&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;516&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3028&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.170&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.179&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;oylerra01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Ray Oyler&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;221&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1265&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.175&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.191&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;vukovjo01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;John Vukovich&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;90&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;559&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.161&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.196&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;humphjo01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;John Humphries&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;52&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;364&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.143&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.196&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bakerge01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;George Baker&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;74&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;474&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.156&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.196&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 整体估计
ggplot(career_eb, aes(average, eb_estimate, color = AB)) +
  geom_hline(yintercept = alpha0 / (alpha0 + beta0), color = &amp;quot;red&amp;quot;, lty = 2) +
  geom_point() +
  geom_abline(color = &amp;quot;red&amp;quot;) +
  scale_colour_gradient(trans = &amp;quot;log&amp;quot;, breaks = 10 ^ (1:5)) +
  xlab(&amp;quot;Batting average&amp;quot;) +
  ylab(&amp;quot;Empirical Bayes batting average&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/ebayes2-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk ebayes2&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;数据点多会收缩到&lt;span class=&#34;math inline&#34;&gt;\(x=y\)&lt;/span&gt;，也就是个人的击球率；数据点少则回归到整体击球率。这就是经验贝叶斯方法的全貌：先估计整体的参数，然后把整体参数作为先验概率估计个人参数。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;可信区间与置信区间&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;可信区间与置信区间&lt;/h2&gt;
&lt;p&gt;经验贝叶斯可以给出点估计，但现实中我们可能更关心区间估计，也就是击球率的范围。一般这类区间估计可以用二项式比例估计来进行，不过没有先验经验的限制置信区间会大到没意义。经验贝叶斯会给出一个后验分布，这个分布可以用来求可信区间。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 给出后验分布
career_eb &amp;lt;- career %&amp;gt;%
    mutate(eb_estimate = (H + alpha0) / (AB + alpha0 + beta0))
career_eb &amp;lt;- career_eb %&amp;gt;%
    mutate(alpha1 = H + alpha0,
           beta1 = AB - H + beta0)
# 提取洋基队的数据
yankee_1998 &amp;lt;- c(&amp;quot;brosisc01&amp;quot;, &amp;quot;jeterde01&amp;quot;, &amp;quot;knoblch01&amp;quot;, &amp;quot;martiti02&amp;quot;, &amp;quot;posadjo01&amp;quot;, &amp;quot;strawda01&amp;quot;, &amp;quot;willibe02&amp;quot;)

yankee_1998_career &amp;lt;- career_eb %&amp;gt;%
    filter(playerID %in% yankee_1998)

# 展示球员的后验分布
library(broom)
yankee_beta &amp;lt;- yankee_1998_career %&amp;gt;%
    inflate(x = seq(.18, .33, .0002)) %&amp;gt;%
    ungroup() %&amp;gt;%
    mutate(density = dbeta(x, alpha1, beta1))

ggplot(yankee_beta, aes(x, density, color = name)) +
    geom_line() +
    stat_function(fun = function(x) dbeta(x, alpha0, beta0),
                  lty = 2, color = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/ci-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk ci&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 提取可信区间
yankee_1998_career &amp;lt;- yankee_1998_career %&amp;gt;%
    mutate(low  = qbeta(.025, alpha1, beta1),
           high = qbeta(.975, alpha1, beta1))
yankee_1998_career %&amp;gt;%
    select(-alpha1, -beta1, -eb_estimate) %&amp;gt;%
    knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;playerID&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;H&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AB&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;average&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;low&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;high&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;brosisc01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Scott Brosius&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1001&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3889&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.257&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.244&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.271&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;jeterde01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Derek Jeter&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3465&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11195&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.310&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.300&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.317&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;knoblch01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Chuck Knoblauch&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1839&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6366&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.289&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.277&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.298&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;martiti02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Tino Martinez&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1925&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7111&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.271&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.260&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.280&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;posadjo01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Jorge Posada&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1664&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6092&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.273&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.262&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.283&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;strawda01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Darryl Strawberry&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1401&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5418&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.259&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.247&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.270&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;willibe02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Bernie Williams&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2336&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7869&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.297&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.286&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.305&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 绘制可信区间
yankee_1998_career %&amp;gt;%
    mutate(name = reorder(name, average)) %&amp;gt;%
    ggplot(aes(average, name)) +
    geom_point() +
    geom_errorbarh(aes(xmin = low, xmax = high)) +
    geom_vline(xintercept = alpha0 / (alpha0 + beta0), color = &amp;quot;red&amp;quot;, lty = 2) +
    xlab(&amp;quot;Estimated batting average (w/ 95% interval)&amp;quot;) +
    ylab(&amp;quot;Player&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/ci-2.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk ci&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 对比置信区间与可信区间
career_eb &amp;lt;- career_eb %&amp;gt;%
    mutate(low = qbeta(.025, alpha1, beta1),
           high = qbeta(.975, alpha1, beta1))

set.seed(2016)

some &amp;lt;- career_eb %&amp;gt;%
    sample_n(20) %&amp;gt;%
    mutate(name = paste0(name, &amp;quot; (&amp;quot;, H, &amp;quot;/&amp;quot;, AB, &amp;quot;)&amp;quot;))

frequentist &amp;lt;- some %&amp;gt;%
    group_by(playerID, name, AB) %&amp;gt;%
    do(tidy(binom.test(.$H, .$AB))) %&amp;gt;%
    select(playerID, name, estimate, low = conf.low, high = conf.high) %&amp;gt;%
    mutate(method = &amp;quot;Confidence&amp;quot;)

bayesian &amp;lt;- some %&amp;gt;%
    select(playerID, name, AB, estimate = eb_estimate,
           low = low, high = high) %&amp;gt;%
    mutate(method = &amp;quot;Credible&amp;quot;)

combined &amp;lt;- bind_rows(frequentist, bayesian)

combined %&amp;gt;%
    mutate(name = reorder(name, -AB)) %&amp;gt;%
    ggplot(aes(estimate, name, color = method, group = method)) +
    geom_point() +
    geom_errorbarh(aes(xmin = low, xmax = high)) +
    geom_vline(xintercept = alpha0 / (alpha0 + beta0), color = &amp;quot;red&amp;quot;, lty = 2) +
    xlab(&amp;quot;Estimated batting average&amp;quot;) +
    ylab(&amp;quot;Player&amp;quot;) +
    labs(color = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/ci-3.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk ci&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;可信区间与置信区间（二项式比例估计）很大的区别在于前者考虑了先验概率进而实现了区间的收缩，后者则可看作无先验贝塔分布给出的区间估计，频率学派目前没有很好的收缩区间估计的方法。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;后验错误率&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;后验错误率&lt;/h2&gt;
&lt;p&gt;现实问题经常不局限于估计，而是侧重决策，例如如果一个球员的击球率高于某个值，他就可以进入名人堂（击球率大于0.3），这个决策常常伴随区间估计而不是简单的点估计：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 以 Hank Aaron 为例
career_eb %&amp;gt;%
    filter(name == &amp;quot;Hank Aaron&amp;quot;) %&amp;gt;%
    do(data_frame(x = seq(.27, .33, .0002),
                  density = dbeta(x, .$alpha1, .$beta1))) %&amp;gt;%
    ggplot(aes(x, density)) +
    geom_line() +
    geom_ribbon(aes(ymin = 0, ymax = density * (x &amp;lt; .3)),
                alpha = .1, fill = &amp;quot;red&amp;quot;) +
    geom_vline(color = &amp;quot;red&amp;quot;, lty = 2, xintercept = .3)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/lp-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk lp&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 提取该球员数据
career_eb %&amp;gt;% filter(name == &amp;quot;Hank Aaron&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Source: local data frame [1 x 10]
## 
##    playerID       name     H    AB average eb_estimate alpha1 beta1   low
##       (chr)      (chr) (int) (int)   (dbl)       (dbl)  (dbl) (dbl) (dbl)
## 1 aaronha01 Hank Aaron  3771 12364   0.305       0.304   3850  8819 0.296
## Variables not shown: high (dbl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 计算其不进入名人堂的概率
pbeta(.3, 3850, 8818)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.169&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里我们引入后验错误率与后验包括率两个概念。后验错误率（Posterior Error Probability）可类比经典假设检验中的显著性水平&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;；后验包括率（Posterior Inclusion Probability）可类比经典假设检验中的置信水平&lt;span class=&#34;math inline&#34;&gt;\(1-\alpha\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 所有球员的后验错误率分布，大部分不超过0.3
career_eb &amp;lt;- career_eb %&amp;gt;%
    mutate(PEP = pbeta(.3, alpha1, beta1))
ggplot(career_eb, aes(PEP)) +
    geom_histogram(binwidth = .02) +
    xlab(&amp;quot;Posterior Error Probability (PEP)&amp;quot;) +
    xlim(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/ap-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk ap&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 后验错误率与击球率的关系
career_eb %&amp;gt;%
    ggplot(aes(eb_estimate, PEP, color = AB)) +
    geom_point(size = 1) +
    xlab(&amp;quot;(Shrunken) batting average estimate&amp;quot;) +
    ylab(&amp;quot;Posterior Error Probability (PEP)&amp;quot;) +
    geom_vline(color = &amp;quot;red&amp;quot;, lty = 2, xintercept = .3) +
    scale_colour_gradient(trans = &amp;quot;log&amp;quot;, breaks = 10 ^ (1:5))&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/ap-2.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk ap&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;后验错误率高于0.3的多数是击球率与击球数都高的人，因为经验贝叶斯方法惩罚了击球数低的人。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;错误发现率fdr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;错误发现率（FDR）&lt;/h2&gt;
&lt;p&gt;错误发现率可用来控制一个整体决策，保证整体犯错的概率低于某个数值，错误发现率越高，越可能把假阳性包括进来。假如我们把进入名人堂的决策作为一个整体，则可允许一定的整体错误率，因为每个人的后验错误率可以计算且期望值线性可加和，我们可以得到一个整体的错误率：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 取前100个球员
top_players &amp;lt;- career_eb %&amp;gt;%
    arrange(PEP) %&amp;gt;%
    head(100)
# 总错率率
sum(top_players$PEP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.69&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 平均错误率
mean(top_players$PEP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0469&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 错误率随所取球员的变化
sorted_PEP &amp;lt;- career_eb %&amp;gt;%
    arrange(PEP)

mean(head(sorted_PEP$PEP, 50))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.00113&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(head(sorted_PEP$PEP, 200))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.241&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;错误率在排序后前面低后面高，但这个错误率不特指某个球员，而是包含到某个球员的整体犯错的概率。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;q值&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;q值&lt;/h2&gt;
&lt;p&gt;q值定义为排序后累积到某个样本的整体平均错误率，类似多重比较中对整体错误率控制的p值。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 生成每个球员的q值
career_eb &amp;lt;- career_eb %&amp;gt;%
    arrange(PEP) %&amp;gt;%
    mutate(qvalue = cummean(PEP))
# 观察不同q值对名人堂球员数的影响
career_eb %&amp;gt;%
    ggplot(aes(qvalue, rank(PEP))) +
    geom_line() +
    xlab(&amp;quot;q-value cutoff&amp;quot;) +
    ylab(&amp;quot;Number of players included&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/qvalue-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk qvalue&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 观察小q值部分
career_eb %&amp;gt;%
    filter(qvalue &amp;lt; .25) %&amp;gt;%
    ggplot(aes(qvalue, rank(PEP))) +
    geom_line() +
    xlab(&amp;quot;q-value cutoff&amp;quot;) +
    ylab(&amp;quot;Number of players included&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/qvalue-2.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk qvalue&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;200个人进入名人堂可能有1/4的球员不合适，如果是50个人进入名人堂那么基本不会犯错。&lt;/p&gt;
&lt;p&gt;q值是一个整体而非个体的平均错误率，具有累积性，不代表q值大的那一个就是错的。q值在频率学派的多重比较里也有定义，虽然没有空假设（有先验概率），但实质等同。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;贝叶斯视角下的假设检验&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;贝叶斯视角下的假设检验&lt;/h2&gt;
&lt;p&gt;前面描述的是击球率如何求，如何进行区间估计与多个体的错误率控制，面向的个体或整体，那么如何解决比较问题。设想多个球员，我们考虑如何去比较他们击球率：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 选三个球员
career_eb %&amp;gt;%
  filter(name %in% c(&amp;quot;Hank Aaron&amp;quot;, &amp;quot;Mike Piazza&amp;quot;, &amp;quot;Hideki Matsui&amp;quot;)) %&amp;gt;%
  inflate(x = seq(.26, .33, .00025)) %&amp;gt;%
  mutate(density = dbeta(x, alpha1, beta1)) %&amp;gt;%
  ggplot(aes(x, density, color = name)) +
  geom_line() +
  labs(x = &amp;quot;Batting average&amp;quot;, color = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/ht-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk ht&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;如果两个球员击球率的概率密度曲线比较接近，那么即便均值有不同我们也无法进行区分；如果重叠比较少，那么我们有理由认为他们之间的差异显著。那么贝叶斯视角下如何定量描述这个差异是否显著？&lt;/p&gt;
&lt;div id=&#34;模拟&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;模拟&lt;/h3&gt;
&lt;p&gt;单纯取样比大小然后计算比例：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 提取两人数据
aaron &amp;lt;- career_eb %&amp;gt;% filter(name == &amp;quot;Hank Aaron&amp;quot;)
piazza &amp;lt;- career_eb %&amp;gt;% filter(name == &amp;quot;Mike Piazza&amp;quot;)
# 模拟取样10万次
piazza_simulation &amp;lt;- rbeta(1e6, piazza$alpha1, piazza$beta1)
aaron_simulation &amp;lt;- rbeta(1e6, aaron$alpha1, aaron$beta1)
# 计算一个人超过另一个人的概率
sim &amp;lt;- mean(piazza_simulation &amp;gt; aaron_simulation)
sim&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.606&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;数值积分&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;数值积分&lt;/h3&gt;
&lt;p&gt;两个概率的联合概率分布，然后积分一个球员大于另一个的概率：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- .00002
limits &amp;lt;- seq(.29, .33, d)
sum(outer(limits, limits, function(x, y) {
  (x &amp;gt; y) *
    dbeta(x, piazza$alpha1, piazza$beta1) *
    dbeta(y, aaron$alpha1, aaron$beta1) *
    d ^ 2
}))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.604&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;解析解&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;解析解&lt;/h3&gt;
&lt;p&gt;两个贝塔分布一个比另一个高是有含有贝塔函数的解析解的：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_A \sim \mbox{Beta}(\alpha_A, \beta_A)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_B \sim \mbox{Beta}(\alpha_B, \beta_B)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\rm Pr}(p_B &amp;gt; p_A) = \sum_{i=0}^{\alpha_B-1}\frac{B(\alpha_A+i,\beta_A+\beta_B)}{(\beta_B+i) B(1+i, \beta_B) B(\alpha_A, \beta_A) }\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h &amp;lt;- function(alpha_a, beta_a,
              alpha_b, beta_b) {
  j &amp;lt;- seq.int(0, round(alpha_b) - 1)
  log_vals &amp;lt;- (lbeta(alpha_a + j, beta_a + beta_b) - log(beta_b + j) -
               lbeta(1 + j, beta_b) - lbeta(alpha_a, beta_a))
  1 - sum(exp(log_vals))
}

h(piazza$alpha1, piazza$beta1,
  aaron$alpha1, aaron$beta1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.605&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;正态近似求解&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;正态近似求解&lt;/h3&gt;
&lt;p&gt;贝塔分布在&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;与&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;比较大时接近正态分布，可以直接用正态分布的解析解求，速度快很多：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h_approx &amp;lt;- function(alpha_a, beta_a,
                     alpha_b, beta_b) {
  u1 &amp;lt;- alpha_a / (alpha_a + beta_a)
  u2 &amp;lt;- alpha_b / (alpha_b + beta_b)
  var1 &amp;lt;- alpha_a * beta_a / ((alpha_a + beta_a) ^ 2 * (alpha_a + beta_a + 1))
  var2 &amp;lt;- alpha_b * beta_b / ((alpha_b + beta_b) ^ 2 * (alpha_b + beta_b + 1))
  pnorm(0, u2 - u1, sqrt(var1 + var2))
}

h_approx(piazza$alpha1, piazza$beta1, aaron$alpha1, aaron$beta1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.606&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;比例检验&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;比例检验&lt;/h2&gt;
&lt;p&gt;这是个列联表问题，频率学派对比两个比例：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;two_players &amp;lt;- bind_rows(aaron, piazza)

two_players %&amp;gt;%
  transmute(Player = name, Hits = H, Misses = AB - H) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Player&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Hits&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Misses&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hank Aaron&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3771&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8593&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mike Piazza&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2127&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4784&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prop.test(two_players$H, two_players$AB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	2-sample test for equality of proportions with continuity
## 	correction
## 
## data:  two_players$H out of two_players$AB
## X-squared = 0.1, df = 1, p-value = 0.7
## alternative hypothesis: two.sided
## 95 percent confidence interval:
##  -0.0165  0.0109
## sample estimates:
## prop 1 prop 2 
##  0.305  0.308&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;贝叶斯学派对比两个比例：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;credible_interval_approx &amp;lt;- function(a, b, c, d) {
  u1 &amp;lt;- a / (a + b)
  u2 &amp;lt;- c / (c + d)
  var1 &amp;lt;- a * b / ((a + b) ^ 2 * (a + b + 1))
  var2 &amp;lt;- c * d / ((c + d) ^ 2 * (c + d + 1))

  mu_diff &amp;lt;- u2 - u1
  sd_diff &amp;lt;- sqrt(var1 + var2)

  data_frame(posterior = pnorm(0, mu_diff, sd_diff),
             estimate = mu_diff,
             conf.low = qnorm(.025, mu_diff, sd_diff),
             conf.high = qnorm(.975, mu_diff, sd_diff))
}

credible_interval_approx(piazza$alpha1, piazza$beta1, aaron$alpha1, aaron$beta1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Source: local data frame [1 x 4]
## 
##   posterior estimate conf.low conf.high
##       (dbl)    (dbl)    (dbl)     (dbl)
## 1     0.606 -0.00182  -0.0151    0.0115&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;多个球员对比一个：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2016)

intervals &amp;lt;- career_eb %&amp;gt;%
  filter(AB &amp;gt; 10) %&amp;gt;%
  sample_n(20) %&amp;gt;%
  group_by(name, H, AB) %&amp;gt;%
  do(credible_interval_approx(piazza$alpha1, piazza$beta1, .$alpha1, .$beta1)) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(name = reorder(paste0(name, &amp;quot; (&amp;quot;, H, &amp;quot; / &amp;quot;, AB, &amp;quot;)&amp;quot;), -estimate))
f &amp;lt;- function(H, AB) broom::tidy(prop.test(c(H, piazza$H), c(AB, piazza$AB)))
prop_tests &amp;lt;- purrr::map2_df(intervals$H, intervals$AB, f) %&amp;gt;%
  mutate(estimate = estimate1 - estimate2,
         name = intervals$name)

all_intervals &amp;lt;- bind_rows(
  mutate(intervals, type = &amp;quot;Credible&amp;quot;),
  mutate(prop_tests, type = &amp;quot;Confidence&amp;quot;)
)

ggplot(all_intervals, aes(x = estimate, y = name, color = type)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  xlab(&amp;quot;Piazza average - player average&amp;quot;) +
  ylab(&amp;quot;Player&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/mp-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk mp&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;由此，置信区间与可信区间的主要差异来自于经验贝叶斯的区间收敛，也就是对整体先验概率的考虑。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;错误率控制&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;错误率控制&lt;/h2&gt;
&lt;p&gt;如果我打算交易一个球员，那么如何筛选候选人？肯定是先选那些击球率更好的球员：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 对比打算交易的球员与其他球员
career_eb_vs_piazza &amp;lt;- bind_cols(
  career_eb,
  credible_interval_approx(piazza$alpha1, piazza$beta1,
                           career_eb$alpha1, career_eb$beta1)) %&amp;gt;%
  select(name, posterior, conf.low, conf.high)

career_eb_vs_piazza&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Source: local data frame [9,342 x 4]
## 
##                    name posterior conf.low conf.high
##                   (chr)     (dbl)    (dbl)     (dbl)
## 1        Rogers Hornsby  2.84e-11   0.0345    0.0639
## 2          Ed Delahanty  7.10e-07   0.0218    0.0518
## 3  Shoeless Joe Jackson  8.77e-08   0.0278    0.0611
## 4         Willie Keeler  4.62e-06   0.0183    0.0472
## 5            Nap Lajoie  1.62e-05   0.0158    0.0441
## 6            Tony Gwynn  1.83e-05   0.0157    0.0442
## 7        Harry Heilmann  7.19e-06   0.0180    0.0476
## 8            Lou Gehrig  1.43e-05   0.0167    0.0461
## 9        Billy Hamilton  7.03e-06   0.0190    0.0502
## 10        Eddie Collins  2.00e-04   0.0113    0.0393
## ..                  ...       ...      ...       ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 计算q值
career_eb_vs_piazza &amp;lt;- career_eb_vs_piazza %&amp;gt;%
  arrange(posterior) %&amp;gt;%
  mutate(qvalue = cummean(posterior))

# 筛选那些q值小于0.05的
better &amp;lt;- career_eb_vs_piazza %&amp;gt;%
  filter(qvalue &amp;lt; .05)

better&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Source: local data frame [50 x 5]
## 
##                    name posterior conf.low conf.high   qvalue
##                   (chr)     (dbl)    (dbl)     (dbl)    (dbl)
## 1        Rogers Hornsby  2.84e-11   0.0345    0.0639 2.84e-11
## 2  Shoeless Joe Jackson  8.77e-08   0.0278    0.0611 4.39e-08
## 3          Ed Delahanty  7.10e-07   0.0218    0.0518 2.66e-07
## 4         Willie Keeler  4.62e-06   0.0183    0.0472 1.36e-06
## 5        Billy Hamilton  7.03e-06   0.0190    0.0502 2.49e-06
## 6        Harry Heilmann  7.19e-06   0.0180    0.0476 3.27e-06
## 7            Lou Gehrig  1.43e-05   0.0167    0.0461 4.85e-06
## 8            Nap Lajoie  1.62e-05   0.0158    0.0441 6.28e-06
## 9            Tony Gwynn  1.83e-05   0.0157    0.0442 7.62e-06
## 10           Bill Terry  3.03e-05   0.0162    0.0472 9.89e-06
## ..                  ...       ...      ...       ...      ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这样我们筛到一个可交易的群体，总和错误率不超过5%。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;影响因子&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;影响因子&lt;/h2&gt;
&lt;p&gt;击球率高还有可能是因为得到的机会多或者光环效应，一开始凭运气打得好，后面给机会多，通过经验累积提高了击球率：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;career %&amp;gt;%
  filter(AB &amp;gt;= 20) %&amp;gt;%
  ggplot(aes(AB, average)) +
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE) +
  scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/if-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk if&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;击球数低方差会大，这比较正常，很多人挂在起跑线上了。直接使用经验贝叶斯方法会导致整体向均值收敛，这高估了新手的数据：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prior_mu &amp;lt;- alpha0 / (alpha0 + beta0)
career_eb %&amp;gt;%
  filter(AB &amp;gt;= 20) %&amp;gt;%
  gather(type, value, average, eb_estimate) %&amp;gt;%
  mutate(type = plyr::revalue(type, c(average = &amp;quot;Raw&amp;quot;,
                                      eb_estimate = &amp;quot;With EB Shrinkage&amp;quot;))) %&amp;gt;%
  ggplot(aes(AB, value)) +
  geom_point() +
  scale_x_log10() +
  geom_hline(color = &amp;quot;red&amp;quot;, lty = 2, size = 1.5, yintercept = prior_mu) +
  facet_wrap(~type) +
  ylab(&amp;quot;average&amp;quot;) +
    geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/ife-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk ife&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;为了如实反应这种情况，我们应该认为击球率符合贝塔分布，但同时贝塔分布的两个参数受击球数的影响，击球数越多，越可能击中。这个模型可以用贝塔－二项式回归来描述：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu_i = \mu_0 + \mu_{\mbox{AB}} \cdot \log(\mbox{AB})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\alpha_{0,i} = \mu_i / \sigma_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\beta_{0,i} = (1 - \mu_i) / \sigma_0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_i \sim \mbox{Beta}(\alpha_{0,i}, \beta_{0,i})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[H_i \sim \mbox{Binom}(\mbox{AB}_i, p_i)\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;拟合模型&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;拟合模型&lt;/h3&gt;
&lt;p&gt;寻找拟合后的模型参数，构建新的先验概率：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gamlss)
# 拟合模型
fit &amp;lt;- gamlss(cbind(H, AB - H) ~ log(AB),
              data = career_eb,
              family = BB(mu.link = &amp;quot;identity&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## GAMLSS-RS iteration 1: Global Deviance = 91083 
## GAMLSS-RS iteration 2: Global Deviance = 72051 
## GAMLSS-RS iteration 3: Global Deviance = 67972 
## GAMLSS-RS iteration 4: Global Deviance = 67966 
## GAMLSS-RS iteration 5: Global Deviance = 67966&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom)
# 展示拟合参数
td &amp;lt;- tidy(fit)
td&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   parameter        term estimate std.error statistic p.value
## 1        mu (Intercept)   0.1441  0.001616      89.1       0
## 2        mu     log(AB)   0.0151  0.000221      68.5       0
## 3     sigma (Intercept)  -6.3372  0.024910    -254.4       0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 构建新的先验概率
mu_0 &amp;lt;- td$estimate[1]
mu_AB &amp;lt;- td$estimate[2]
sigma &amp;lt;- exp(td$estimate[3])

# 看看AB对先验概率的影响
crossing(x = seq(0.08, .35, .001), AB = c(1, 10, 100, 1000, 10000)) %&amp;gt;%
  mutate(density = dbeta(x, (mu_0 + mu_AB * log(AB)) / sigma,
                         (1 - (mu_0 + mu_AB * log(AB))) / sigma)) %&amp;gt;%
  mutate(AB = factor(AB)) %&amp;gt;%
  ggplot(aes(x, density, color = AB, group = AB)) +
  geom_line() +
  xlab(&amp;quot;Batting average&amp;quot;) +
  ylab(&amp;quot;Prior density&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/fitbb-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk fitbb&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;求后验概率&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;求后验概率&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 计算所有拟合值
mu &amp;lt;- fitted(fit, parameter = &amp;quot;mu&amp;quot;)
sigma &amp;lt;- fitted(fit, parameter = &amp;quot;sigma&amp;quot;)
# 计算所有后验概率
career_eb_wAB &amp;lt;- career_eb %&amp;gt;%
  dplyr::select(name, H, AB, original_eb = eb_estimate) %&amp;gt;%
  mutate(mu = mu,
         alpha0 = mu / sigma,
         beta0 = (1 - mu) / sigma,
         alpha1 = alpha0 + H,
         beta1 = beta0 + AB - H,
         new_eb = alpha1 / (alpha1 + beta1))
# 展示拟合后的击球率
ggplot(career_eb_wAB, aes(original_eb, new_eb, color = AB)) +
  geom_point() +
  geom_abline(color = &amp;quot;red&amp;quot;) +
  xlab(&amp;quot;Original EB Estimate&amp;quot;) +
  ylab(&amp;quot;EB Estimate w/ AB term&amp;quot;) +
  scale_color_continuous(trans = &amp;quot;log&amp;quot;, breaks = 10 ^ (0:4))&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/fitpo-1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk fitpo&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 对比
library(tidyr)

lev &amp;lt;- c(raw = &amp;quot;Raw H / AB&amp;quot;, original_eb = &amp;quot;EB Estimate&amp;quot;, new_eb = &amp;quot;EB w/ Regression&amp;quot;)

career_eb_wAB %&amp;gt;%
  filter(AB &amp;gt;= 10) %&amp;gt;%
  mutate(raw = H / AB) %&amp;gt;%
  gather(type, value, raw, original_eb, new_eb) %&amp;gt;%
  mutate(mu = ifelse(type == &amp;quot;original_eb&amp;quot;, prior_mu,
                     ifelse(type == &amp;quot;new_eb&amp;quot;, mu, NA))) %&amp;gt;%
  mutate(type = factor(plyr::revalue(type, lev), lev)) %&amp;gt;%
  ggplot(aes(AB, value)) +
  geom_point() +
  geom_line(aes(y = mu), color = &amp;quot;red&amp;quot;) +
  scale_x_log10() +
  facet_wrap(~type) +
  xlab(&amp;quot;At-Bats (AB)&amp;quot;) +
  ylab(&amp;quot;Estimate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/fitpo-2.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot of chunk fitpo&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;矫正后我们的数据更复合现实了，其实这是贝叶斯分层模型的一个简单版本，通过考虑更多因素，我们可以构建更复杂的模型来挖掘出我们所需要的信息。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>木</title>
      <link>https://yufree.cn/cn/2015/10/11/tree/</link>
      <pubDate>Sun, 11 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2015/10/11/tree/</guid>
      <description>&lt;p&gt;木是树的旧称，甲骨文里会写成类似一个星号，上半部分代表枝叶，下半部分代表根。在下半部分加入一横就是本，根本同源；在上半部分加入一横就是末，代表枝叶末梢。所谓本末倒置大致来源于此，而这一横并非象形而是指代，代表就是这里。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https//yufree.github.io/blogcn/figure/mu.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;进化上也喜欢用树木状结构，但跟真实的树木结构不同的在于进化上只用了树的一半，在这种表示中根是一个原点而非地下的分支。首先使用这种表示方法的是公元1801年植物学家Augustin Augier为表示植物种类关系所做的图，不久（1809年）拉马克在《动物哲学》中就为动物也做了一副从蠕虫到哺乳动物的进化树，然而拉马克并不认可起源唯一，所以他画了一系列平行的从简单到复杂的分叉树。后来比较出名的是Edward Hitchcock为植物与动物所做的进化树，这张图表现的物种关系虽然很真实，但Edward Hitchcock认为产生差异的原因是神而不是进化。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https//yufree.github.io/blogcn/figure/Edward.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;然后就到了达尔文画的那个进化树。从原始的生命形态到今天复杂多样的物种，达尔文认为这是一条进化之路，很多分支的出现是无方向的，但由于其它分支竞争关系消亡了，最后呈现出的就是树形结构。海克尔强化了从低级到高级的种属间关系并给出了人类起源——爪洼直立猿人。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https//yufree.github.io/blogcn/figure/Darwins.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;当然，海克尔最出名的还是胚胎演化上的那幅图，图上表示出胚胎越早越相似，这幅图出现在我们的中学课本之中。1997年Science上有文章指出这幅图纯属臆断，跟实验证据不符，但画的实在太好，过于流行，文章还用了个耸人的题目：Haeckel&amp;rsquo;s Embryos: Fraud Rediscovered。其实如果结合最新的发育生物学研究，在发育的中间阶段恰恰存在一个古老基因的集中表达期，也就是海克尔自认为观察到的发育重演现象，算是歪打正着。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https//yufree.github.io/blogcn/figure/Haeckel.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https//yufree.github.io/blogcn/figure/haeckel2.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;如今演化生物学上并不认为原核生物是符合进化树所描述的关系，因为原核生物间存在基因交互的现象。但真核生物似乎是因为高等了些，基因交互不那么容易，所以还是被普遍认为符合进化树的描述，分支越接近，进化上亲缘关系越近而出现分支的节点则代表了共同的祖先。&lt;/p&gt;
&lt;p&gt;其实不止生物学喜欢用树，在决策过程里也可以用到树。其实说是树，看中的是更抽象的分支结构。下面是一个简单的例子，我有一个仪器有三种开发策略，不同的开发策略对应不同的成本与收益，通过分支结构与分支结构上的概率就可以很清晰的比较不同决策下产生的期望收益。这个决策场景下不同选择是互斥的，选了一个分支就没机会到第二个分支去了，这样就可以通过概率来计算不同分支下的收益进行决策。这是一种常见的理性思考模型，适用于各种工作生活场景。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https//yufree.github.io/blogcn/figure/dt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;但其实决策树也是一种层级结构的机器学习算法，每个节点代表一个分类标准或特征值的判断标准，树的枝叶代表最终给出的响应。在机器学习的预测模型y=f(x)中，y代表最终响应，x代表跟响应相关的特征或属性，f则代表一种模型算法。f的构建过程可以看作寻找一种让响应相对一致的特征分类或回归方法。决策树就比较符合这种直观的想法，每个枝叶的形成过程所经历的特征判断就是“寻找一种让响应相对一致”的过程。&lt;/p&gt;
&lt;p&gt;这个过程实现的具体过程要用到自上而下的贪心算法。具体而言就是首先找遍历所有不同的x，在每个特征x下找出最小化响应y的均值与实际y差的平方的一个特征x0，这样就实现了响应的第一层二分，也就构建了树的第一个主分支，这个过程在不同分支上递归进行就可以训练得到一颗回归树。分类树的构建与此类似，不同的是要引入分类错误率的概念或者说y分类的内在均一度作为训练目标。训练过程可以引入类似lasso或岭回归的惩罚项，最后当我们得到一颗训练好的决策树后就可以进行预测了。&lt;/p&gt;
&lt;p&gt;预测过程更加直观，我们用手头的x去对应节点上的判断标准进入不同的分支，递归进行，最后到枝叶上就是预测结果。值得说明的是这种层级结构与线性回归还是有很大区别的，有些变量会在不同层次节点上反复被使用到，有些则基本用不到，因此决策树也可用来进行变量的筛选并对其重要性进行评价。当然，也可以用来玩游戏。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https//yufree.github.io/blogcn/figure/treegame.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;另一个跟树接近的东西是无监督的聚类数据分析，生成的冰柱图也可看作树，不过更像是根。具体算法就是单纯的计算样本间多个变量间的距离，达到一定距离则产生分支。这个距离一般是欧式距离，但个人比较喜欢曼哈顿距离。中国北方城市格局总是方方正正的豆腐块，所以要去一个地方时间基本上是可预测的，如果换成巴黎那种放射结构，距离计算上就要考虑更多空间分布了。&lt;/p&gt;
&lt;p&gt;说到豆腐块，棋盘可以看作一种豆腐块结构，最近去内蒙古博物院看到一种蒙古象棋，很像国际象棋，但估计玩法上会简单些。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https//yufree.github.io/blogcn/figure/mchese.JPG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;参考&lt;/p&gt;
&lt;p&gt;1《白鱼解字》
2 &lt;a href=&#34;https://en.wikipedia.org/wiki/Tree_of_life_(biology)&#34;&gt;https://en.wikipedia.org/wiki/Tree_of_life_(biology)&lt;/a&gt;
3 &lt;a href=&#34;http://songshuhui.net/archives/52003&#34;&gt;http://songshuhui.net/archives/52003&lt;/a&gt;
4 &lt;a href=&#34;http://vserver1.cscs.lsa.umich.edu/~spage/ONLINECOURSE/R4Decision.pdf&#34;&gt;http://vserver1.cscs.lsa.umich.edu/~spage/ONLINECOURSE/R4Decision.pdf&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>理解基因组数据分析之结果注释与通路分析篇</title>
      <link>https://yufree.cn/cn/2015/08/29/genomewide-data-4/</link>
      <pubDate>Sat, 29 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2015/08/29/genomewide-data-4/</guid>
      <description>&lt;p&gt;上面三篇费了半天口舌的最终目的就是找出靠谱的差异基因，但到这里只算是研究的开始，下一步我们要为这些差异基因寻找意义。&lt;/p&gt;
&lt;p&gt;分子生物学的研究方法一般就是围着中心法则转圈并找出同一层次的上下游变化。举例而言，如果我们实际发现一个突变性状想找到调控基因，那么你最好能纯化出这个蛋白或者根据相似性原理找一组同源蛋白，拿到序列后想办法搞出抗体来，然后活体或质粒去验证下有没有高表达，找出功能与目标蛋白的联系。当你确定找对了蛋白，剩下要做的就是反推序列合探针找基因，找到了基因也要做下功能验证。验证完了发现对不上可以考虑下RNA了，这种调控就比较麻烦了。等基因找到了，就该讨论基因上下游调控了，如果不用基因芯片，那就通过类似通路去推理，然后围着中心法则转一圈去寻找出现共同变化的基因。这里面实验上的奇技淫巧特别多，传统老派的研究人员也比较认可这类基于免疫分析的套路。&lt;/p&gt;
&lt;p&gt;组学技术基本就把上面的思路反过来了，我们直接通过基因芯片或基因组测序拿到差异基因或序列，然后沿着中心法则一路走下去做验证，最后自上而下给出调控机理。组学技术不仅仅体现在基因层，灵活使用基因芯片是可以找出转录因子，对于信号转导的研究也没什么问题。不同于传统方法通过假设去验证，组学带来的信息不是太少而是太多，没有假设就给了一堆验证，这时候就有必要通过数据库的检索与信息挖掘来给出我们想发现的假设或有意思的现象。&lt;/p&gt;
&lt;p&gt;因为前面大都基于基因芯片做的讨论，在讨论注释之前有必要介绍下测序研究的工作流程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;命令行下&lt;code&gt;wget&lt;/code&gt;得到公用服务器测序数据的FASTQ文件文件或从仪器端&lt;code&gt;scp&lt;/code&gt;过来&lt;/li&gt;
&lt;li&gt;用fastqc检查下测序质量&lt;/li&gt;
&lt;li&gt;将FASTQ文件对应到基因组里生成BAM文件，&lt;code&gt;bowtie&lt;/code&gt;常用来映射 DNA测序而&lt;code&gt;tophat&lt;/code&gt;来映射RNA测序，因为全局比对就不用&lt;code&gt;BLAST&lt;/code&gt;这种比较局部的算法了&lt;/li&gt;
&lt;li&gt;用&lt;code&gt;Rsamtools&lt;/code&gt;包在R中创建BamFile对象&lt;/li&gt;
&lt;li&gt;用&lt;code&gt;GenomicAlignments&lt;/code&gt;包进行序列比对计数&lt;/li&gt;
&lt;li&gt;用&lt;code&gt;DeSeq2&lt;/code&gt;包对数据进行与基因芯片类似的降噪，用于有实验设计的基因组研究&lt;/li&gt;
&lt;li&gt;可视化&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们可以看到，在测序研究中同样也是为了发现不同，而这个不同多半来自与公布出的基因组比对。那么首先考虑的是原始数据如何存储？在Bioconductor中，存储与序列相关的对象为&lt;code&gt;IRanges&lt;/code&gt;与&lt;code&gt;Grange&lt;/code&gt;。&lt;code&gt;IRanges&lt;/code&gt;结构比较简单，就是起点长度终点，常见操作如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source(&amp;quot;http://bioconductor.org/biocLite.R&amp;quot;)
biocLite()
library(IRanges)
# 单个范围，有起点终点也有宽度
(ir &amp;lt;- IRanges(5,10))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## IRanges of length 1
##     start end width
## [1]     5  10     6
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 可对范围进行平移
shift(ir, -2)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## IRanges of length 1
##     start end width
## [1]     3   8     6
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 也可缩小范围
narrow(ir, start=2)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## IRanges of length 1
##     start end width
## [1]     6  10     5
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 还可以展示范围一侧的序列
flank(ir, width=3, start=TRUE, both=FALSE)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## IRanges of length 1
##     start end width
## [1]     2   4     3
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 从开头重新定义长度
resize(ir, 1)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## IRanges of length 1
##     start end width
## [1]     5   5     1
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 下面包括多个范围
(irs &amp;lt;- IRanges(start=c(3,5,17), end=c(10,8,20)))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## IRanges of length 3
##     start end width
## [1]     3  10     8
## [2]     5   8     4
## [3]    17  20     4
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 总覆盖范围
range(irs)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## IRanges of length 1
##     start end width
## [1]     3  20    18
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 实际覆盖范围
reduce(irs)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## IRanges of length 2
##     start end width
## [1]     3  10     8
## [2]    17  20     4
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 总范围内没覆盖到的部分
gaps(irs)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## IRanges of length 1
##     start end width
## [1]    11  16     6
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 所有范围片段化
disjoin(irs)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## IRanges of length 4
##     start end width
## [1]     3   4     2
## [2]     5   8     4
## [3]     9  10     2
## [4]    17  20     4
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;Grange&lt;/code&gt;就是范围加上链、染色体、基因组的信息，见下面&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(GenomicRanges)
# 构件一个GRange对象
(gr &amp;lt;- GRanges(&amp;quot;chrZ&amp;quot;, IRanges(start=c(5,10),end=c(35,45)),
              strand=&amp;quot;+&amp;quot;, seqlengths=c(chrZ=100L)))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## GRanges object with 2 ranges and 0 metadata columns:
##       seqnames    ranges strand
##          &amp;lt;Rle&amp;gt; &amp;lt;IRanges&amp;gt;  &amp;lt;Rle&amp;gt;
##   [1]     chrZ  [ 5, 35]      +
##   [2]     chrZ  [10, 45]      +
##   -------
##   seqinfo: 1 sequence from an unspecified genome
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 添加基因组信息
genome(gr) &amp;lt;- &amp;quot;hg19&amp;quot;
# 也可进行类似IRange的操作
shift(gr, 10)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## GRanges object with 2 ranges and 0 metadata columns:
##       seqnames    ranges strand
##          &amp;lt;Rle&amp;gt; &amp;lt;IRanges&amp;gt;  &amp;lt;Rle&amp;gt;
##   [1]     chrZ  [15, 45]      +
##   [2]     chrZ  [20, 55]      +
##   -------
##   seqinfo: 1 sequence from hg19 genome
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 可以增加序列信息列，例如分组等
mcols(gr)$value &amp;lt;- c(-1,4)
# 可以保存多个序列
(gr2 &amp;lt;- GRanges(&amp;quot;chrZ&amp;quot;,IRanges(c(19,33),c(38,35)),strand=&amp;quot;*&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## GRanges object with 2 ranges and 0 metadata columns:
##       seqnames    ranges strand
##          &amp;lt;Rle&amp;gt; &amp;lt;IRanges&amp;gt;  &amp;lt;Rle&amp;gt;
##   [1]     chrZ  [19, 38]      *
##   [2]     chrZ  [33, 35]      *
##   -------
##   seqinfo: 1 sequence from an unspecified genome; no seqlengths
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 可以对多个序列比对找出重复的，第一个范围对应第二个范围的位置
fo &amp;lt;- findOverlaps(gr, gr2)
queryHits(fo)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## [1] 1 1 2 2
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;subjectHits(fo)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## [1] 1 2 1 2
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 可以直接截取范围
gr %over% gr2
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## [1] TRUE TRUE
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;gr[gr %over% gr2]
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## GRanges object with 2 ranges and 1 metadata column:
##       seqnames    ranges strand |     value
##          &amp;lt;Rle&amp;gt; &amp;lt;IRanges&amp;gt;  &amp;lt;Rle&amp;gt; | &amp;lt;numeric&amp;gt;
##   [1]     chrZ  [ 5, 35]      + |        -1
##   [2]     chrZ  [10, 45]      + |         4
##   -------
##   seqinfo: 1 sequence from hg19 genome
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外有一种&lt;code&gt;Rle&lt;/code&gt;数据类型也可使用，更节省空间。&lt;/p&gt;
&lt;p&gt;从上面我们可以看到R中构建一个序列对象与进行序列比对的底层操作，这也是很多基于R的比对可视化基础。当我们得到BamFile对象时其比对的基础就是一组范围。基因组级别的比对属于多对多，用&lt;code&gt;GenomicAlignmets&lt;/code&gt;包提供的比对函数比较方便。如果你对比的基因组有新版本，这时候可用&lt;code&gt;rtracklayer&lt;/code&gt;包中liftOver功能来重新比对。此外，也可以包含对应序列用来后续的突变研究。&lt;/p&gt;
&lt;p&gt;终于到了注释这一段了，首先要确定比对的是序列还是基因基因组，前面对应测序研究，后面对应基因芯片。通过序列中给出的参考基因组信息与芯片对应的基因组信息，我们可以从Bioconductor中很方便的找到比对基因组。序列层基因组测序就可以直接可视化去探索下基因功能了，基因芯片则可通过系统生物学给出的一些包对接网络进行通路分析例如GO跟KEGG。值得注意的是不同数据库对同样的序列或基因有着不同的提法，这时候调用对应物种的注释地图包就可以利用键值任意转换，进而实现不同目的的分析。&lt;/p&gt;
&lt;p&gt;下面还是用基因芯片数据来进行一个演示，从数据导入到通路分析。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 从GEO上下载某个实验编号为GSE34313的实验数据，该网站也托管了很多实验数据方便他人重复
library(GEOquery)
# 该实验考察了一种激素对平滑肌基因表达的影响
g &amp;lt;- getGEO(&amp;quot;GSE34313&amp;quot;)
# 表达数据
e &amp;lt;- g[[1]]
# 提取分组数据
e$condition &amp;lt;- e$characteristics_ch1.2
levels(e$condition) &amp;lt;- c(&amp;quot;dex24&amp;quot;,&amp;quot;dex4&amp;quot;,&amp;quot;control&amp;quot;)
# 观察数据是否需要正则化
boxplot(exprs(e), range=0)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/genome7.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 选取两个分组进行对比
lvls &amp;lt;- c(&amp;quot;control&amp;quot;, &amp;quot;dex4&amp;quot;)
es &amp;lt;- e[,e$condition %in% lvls]
es$condition &amp;lt;- factor(es$condition, levels=lvls)
# 寻找差异基因
library(limma)
design &amp;lt;- model.matrix(~ es$condition)
fit &amp;lt;- lmFit(es, design=design)
fit &amp;lt;- eBayes(fit)
tt &amp;lt;- topTable(fit, coef=2, genelist=fData(es)$GENE_SYMBOL)
# 用GO号选取某个基因组进行差异比较
idx &amp;lt;- grep(&amp;quot;GO:0006955&amp;quot;, fData(es)$GO_ID)
(r1 &amp;lt;- roast(es, idx, design))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;##          Active.Prop    P.Value
## Down      0.16269841 0.01350675
## Up        0.09325397 0.98699350
## UpOrDown  0.16269841 0.02700000
## Mixed     0.25595238 0.00800000
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 测试多个基因组
# 读取人类基因组数据库
library(org.Hs.eg.db)
# 提取基因组定义
go2eg &amp;lt;- as.list(org.Hs.egGO2EG)
golengths &amp;lt;- sapply(go2eg, length)
govector &amp;lt;- unlist(go2eg)
# 基因匹配
idxvector &amp;lt;- match(govector, fData(es)$GENE)
table(is.na(idxvector))
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## 
##  FALSE   TRUE 
## 239596   8306
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 获得改变基因
idx &amp;lt;- split(idxvector, rep(names(go2eg), golengths))
# 筛选基因数大于10的基因组
idxclean &amp;lt;- lapply(idx, function(x) x[!is.na(x)])
idxlengths &amp;lt;- sapply(idxclean, length)
idxsub &amp;lt;- idxclean[idxlengths &amp;gt; 10]
# 计算基因组表达差异谱
r2 &amp;lt;- mroast(es, idxsub, design)
head(r2)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;##            NGenes  PropDown     PropUp Direction PValue        FDR
## GO:0005125    174 0.2758621 0.04597701      Down  0.001 0.01043919
## GO:0008083    167 0.2874251 0.07784431      Down  0.001 0.01043919
## GO:0070098     65 0.2461538 0.06153846      Down  0.001 0.01043919
## GO:0043433     63 0.2698413 0.12698413      Down  0.001 0.01043919
## GO:0042102     54 0.1666667 0.09259259      Down  0.001 0.01043919
## GO:0006959     53 0.2452830 0.09433962      Down  0.001 0.01043919
##            PValue.Mixed   FDR.Mixed
## GO:0005125        0.001 0.002596639
## GO:0008083        0.001 0.002596639
## GO:0070098        0.001 0.002596639
## GO:0043433        0.001 0.002596639
## GO:0042102        0.001 0.002596639
## GO:0006959        0.001 0.002596639
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 提取受影响大的基因组
r2 &amp;lt;- mroast(es, idxsub, design)
# 注释上GO数据库里的功能
library(GO.db)
# 提取受影响大的基因组功能
r2tab &amp;lt;- select(GO.db, keys=rownames(r2)[1:10],
                columns=c(&amp;quot;GOID&amp;quot;,&amp;quot;TERM&amp;quot;,&amp;quot;DEFINITION&amp;quot;), 
                keytype=&amp;quot;GOID&amp;quot;)
# 展示数据
r2tab[,1:2]
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;##          GOID
## 1  GO:0005125
## 2  GO:0008083
## 3  GO:0007623
## 4  GO:0071222
## 5  GO:0070098
## 6  GO:0043433
## 7  GO:0042102
## 8  GO:0006959
## 9  GO:0048661
## 10 GO:0030593
##                                                                                  TERM
## 1                                                                   cytokine activity
## 2                                                              growth factor activity
## 3                                                                    circadian rhythm
## 4                                             cellular response to lipopolysaccharide
## 5                                                chemokine-mediated signaling pathway
## 6  negative regulation of sequence-specific DNA binding transcription factor activity
## 7                                         positive regulation of T cell proliferation
## 8                                                             humoral immune response
## 9                             positive regulation of smooth muscle cell proliferation
## 10                                                              neutrophil chemotaxis
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;OK，这一系列四篇文章只算是一个基因组数据分析的入门，先形成类比思想工作就好展开一些。下面给出一些链接，可深入学习。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://bioconductor.org/help/workflows/&#34;&gt;Bioconductor上各种数据分析的流程图&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://genomicsclass.github.io/book/&#34;&gt;Raff教授公开课的开源教材&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://blog.qiuworld.com:8080/&#34;&gt;糗世界 中文教程比较多 实战经验丰富&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>理解基因组数据分析之建模与可视化篇</title>
      <link>https://yufree.cn/cn/2015/08/28/genomewide-data-3/</link>
      <pubDate>Fri, 28 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2015/08/28/genomewide-data-3/</guid>
      <description>&lt;p&gt;原始数据导入时我们所遇到的背景问题说到底是测定的系统误差，期望可以看作0。但有些影响是我们不希望看到但依旧存在的，例如你测定了100多份基于地区分布的人体样本中的感兴趣基因组，但由于采样原因不可能同时采集，而时间的差异会直接导致诸如温度等影响了样本的均质性，这种情况下进行统计推断就需要平衡掉这些因素的影响。那么，如何屏蔽呢？&lt;/p&gt;
&lt;p&gt;首先我们要推广下t检验到线性回归。两组数据的差异比较可以构建如下模型：&lt;/p&gt;
&lt;p&gt;$$Y = \alpha * X + \beta$$&lt;/p&gt;
&lt;p&gt;Y是响应，假设有两组，我们给组A赋值1，组B赋值0，那么系数$\beta$就是组B的均值而系数$\alpha$则是组A与组B的差值。这个差值实际上就是t检验里的差异，对这个系数的估计出的t值就是t检验的t值。&lt;/p&gt;
&lt;p&gt;下面运行下代码看看&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 模拟生成两组数据
set.seed(42)
group1 &amp;lt;- rnorm(100,100,10)
group2 &amp;lt;- rnorm(100,130,13)
# 进行t检验
t.test(group1,group2)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## 
## 	Welch Two Sample t-test
## 
## data:  group1 and group2
## t = -18.173, df = 195.17, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -31.63463 -25.44050
## sample estimates:
## mean of x mean of y 
##  100.3251  128.8627
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 构建分组变量
fac &amp;lt;- c(rep(0,100),rep(1,100))
# 构建数据变量
dat &amp;lt;- c(group1,group2)
# 回归分析
fit &amp;lt;- lm(dat~fac)
summary(fit)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = dat ~ fac)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -30.256  -6.523   0.566   6.532  36.262 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   100.33       1.11   90.35   &amp;lt;2e-16 ***
## fac            28.54       1.57   18.17   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11.1 on 198 degrees of freedom
## Multiple R-squared:  0.6252,	Adjusted R-squared:  0.6233 
## F-statistic: 330.2 on 1 and 198 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上面我们可以看到fac的参数估计得到的t值与t检验是一样的，截距也就是组B的均值，而斜率则是组A与组B的差。由此我们可以看到t检验可以看作线性回归针对两组变量的一个特例。而进入线性回归领域我们可以做的东西就多了，例如我们平时所说的单因素方差分析与多重比较本质上就是线性回归针对多组变量的特例。而线性回归更多是用在处理连续变量的，那么本质上你的分组也可以是连续的。扯远了，在基因组相关分析上我们引入回归分析多半不是针对样本分组的，而是用来平衡影响因素的。&lt;/p&gt;
&lt;p&gt;回到我们简单的两组差异比较上问题可以描述为两组样本的采集并不随机。在测定身高问题上，当我们采集样本时间超过1年，那么不同年份测定的身高会受身高自然生长（青少年）的影响。那么我们在采集完数据后不能因为采集时间过长而重新采集，而这在很多实验条件下也是不可能完成的任务。那么我们可以先探索性的分年份观察下数据，如果存在年份差异，那么我们可以构建下面的模型：&lt;/p&gt;
&lt;p&gt;$$Y = \alpha * X + \gamma * Year + \beta$$&lt;/p&gt;
&lt;p&gt;这样就相当于把采样时间纳入模型之中。数据处理时对年份数据我们进行归一化处理，这样均值也就为0。当我们解释$\beta$时，可以完全忽视年份的影响，而这样得到的$\beta$实际意义就是在一个平均年上的均值，由此年份产生的影响就消除了。本质上，这是一个通过将影响因素随机化的手段，都随机化掉也就更接近我们想知道问题的答案。同样的方法也常用于流行病学研究，例如考察影响心血管疾病发病率因素的模型都会把是否吸烟添加到生存分析的模型中，所以在文献报道中常出现的“考虑了吸烟，肥胖等因素后”实际上在数据层就是通过在回归分析里加入影响因素来去除影响的。&lt;/p&gt;
&lt;p&gt;但是上面那是知道影响是什么，如果不知道呢？也就是不知道不知道的因素如何调整？完全不知道的话也就不调整了，但如果你知道存在但无法描述呢？最常见的就是背景干扰，我反正也不去描述，只要这个背景稳定我们直接扣除就完了，好比实验随机化了。眼不见当然心为净了，但我们其实可以分分钟给个热图出来，这时候我们就会发现，其实不能描述的影响还是很多的。&lt;/p&gt;
&lt;p&gt;下面用人种分性别数据来观察下这个问题，我们已知的混杂变量是测定时间，分组变量是性别。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 安装bioconductor
source(&amp;quot;http://bioconductor.org/biocLite.R&amp;quot;)
biocLite()
# 安装绘图用的rafalib包跟RcolorBrewer包还有示例的GSE5859Subset数据集
# install.packages(&#39;rafalib&#39;)
# install.packages(&#39;RColorBrewer&#39;)
# devtools::install_github(&amp;quot;genomicsclass/GSE5859Subset&amp;quot;)
# genefilter包用来同时进行高维数据操作
# biocLite(&#39;genefilter&#39;)
# 加载软件包与数据集
library(GSE5859Subset)
library(rafalib)
library(genefilter)
library(RColorBrewer)
# GSE5859Subset数据集收集了不同人种的基因表达数据的一部分
data(GSE5859Subset)
# 按照性别分组
sex &amp;lt;- sampleInfo$group
# 提取混杂变量时间
batch &amp;lt;- factor(format(sampleInfo$date,&amp;quot;%m&amp;quot;))
# 提取染色体编号
chr &amp;lt;- geneAnnotation$CHR
# 对不同时间进行t检验观察混杂是否存在
tt&amp;lt;-rowttests(geneExpression,batch)
# 提取Y染色体
ind1 &amp;lt;- which(chr==&amp;quot;chrY&amp;quot;) ##real differences
# 提取时间尺度上不在Y染色体上的变化最大的50个基因中
ind2 &amp;lt;- setdiff(c(order(tt$dm)[1:25],order(-tt$dm)[1:25]),ind1)
# 从上面两组基因之外再随机选50个基因
set.seed(1)
ind0 &amp;lt;- setdiff(sample(seq(along=tt$dm),50),c(ind2,ind1))
# 这三组基因提取的各代表一定有变化的（Y染色体女性没有），采样时间导致的变化以及其他随机找的50个基因
geneindex&amp;lt;-c(ind2,ind0,ind1)
# 截取相应数据
mat&amp;lt;-geneExpression[geneindex,]
# 归一化
mat &amp;lt;- mat -rowMeans(mat)
# 绘图
mypar(1,2)
# 所有截取的数据
icolors &amp;lt;- colorRampPalette(rev(brewer.pal(11,&amp;quot;RdYlBu&amp;quot;)))(100)
image(t(mat),xaxt=&amp;quot;n&amp;quot;,yaxt=&amp;quot;n&amp;quot;,col=icolors)
# 所有数据归一化
y &amp;lt;- geneExpression - rowMeans(geneExpression)
# 绘制样本间分性别相关趋势图
image(1:ncol(y),1:ncol(y),cor(y),col=icolors,zlim=c(-1,1),
       xaxt=&amp;quot;n&amp;quot;,xlab=&amp;quot;&amp;quot;,yaxt=&amp;quot;n&amp;quot;,ylab=&amp;quot;&amp;quot;)
axis(2,1:ncol(y),sex,las=2)
axis(1,1:ncol(y),sex,las=2)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/genome.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;这张图上我们可以看出原始数据上面至少有两种模式可用来校准偏差，单纯考虑年份影响并不能解决所有的混杂模式，另外样品按性别去看相关也很难看出单一的趋势。那么如何把这些看得到但不好用分类或连续变量剔除的混杂因素给找出来呢？&lt;/p&gt;
&lt;p&gt;这种情况要考虑使用主成分分析降维，当维度比较低的时候，单个维度都说明一种模式，数学上更有利的是主成分之间是正交的。但这样做有一个Bug，那就是你处理前是不知道哪个主成分会包含真实效果的，而扣除掉占主要方差的主成分后你会欣喜的发现剩下的全是噪音了。那么问题来了：一方面要保证主要效果不能丢失（但可能已经掩盖在噪音里了），另一方面还要把其余的混杂尽可能的删掉，有没有这样的分析方法呢？&lt;/p&gt;
&lt;p&gt;有，替代变量分析。当我们不知道混杂变量是什么先用主成分分析对除了真实效果的那部分矩阵提取主成分，然后提取主成分在每一个基因上的投影作为权重，把权重迭代到原来矩阵上，然后在进行回归，直到转化的因子方差不怎么变化为止。这样实际上就是不断通过加权强化混杂变量的影响，最后在不清楚混杂变量模式的情况下把它们通过迭代找出来作为一个整体替代变量排除掉。类似的分析方法也有不少，不过我就理解了这一种，殊途同归，自行理解。&lt;/p&gt;
&lt;p&gt;下面继续上面的案例，看看效果。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 安装相关软件包sva与limma
# biocLite(c(&#39;sva&#39;,&#39;limma&#39;))
# 读取进行替代变量分析与回归的sva跟limma包
library(limma)
library(sva)
# 构建基础模型
mod &amp;lt;- model.matrix(~sex)
# 寻找替代变量
svafit &amp;lt;- sva(geneExpression,mod)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;## Number of significant surrogate variables is:  5 
## Iteration (out of 5 ):1  2  3  4  5
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# 构建包含替代变量的模型
svaX&amp;lt;-model.matrix(~sex+svafit$sv)
# 拟合模型
lmfit &amp;lt;- lmFit(geneExpression,svaX)
# 提取替代变量矩阵
Batch&amp;lt;- lmfit$coef[geneindex,3:7]%*%t(svaX[,3:7])
# 提取效应矩阵
Signal&amp;lt;-lmfit$coef[geneindex,1:2]%*%t(svaX[,1:2])
# 提取误差矩阵
error &amp;lt;- geneExpression[geneindex,]-Signal-Batch
# 归一化作图
Signal &amp;lt;-Signal-rowMeans(Signal)
mat &amp;lt;- geneExpression[geneindex,]-rowMeans(geneExpression[geneindex,])
mypar(1,4,mar = c(2.75, 4.5, 2.6, 1.1))
image(t(mat),col=icolors,zlim=c(-5,5),xaxt=&amp;quot;n&amp;quot;,yaxt=&amp;quot;n&amp;quot;)
image(t(Signal),col=icolors,zlim=c(-5,5),xaxt=&amp;quot;n&amp;quot;,yaxt=&amp;quot;n&amp;quot;)
image(t(Batch),col=icolors,zlim=c(-5,5),xaxt=&amp;quot;n&amp;quot;,yaxt=&amp;quot;n&amp;quot;)
image(t(error),col=icolors,zlim=c(-5,5),xaxt=&amp;quot;n&amp;quot;,yaxt=&amp;quot;n&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/genome1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;可以看出，通过替代变量分析我们最大程度上保留了原有效应并剔除了替代变量。在这个基础上配合前文提到的经验贝叶斯方法进行基因筛选，得到的基因为我们比较感兴趣的部分了，下一篇将在此基础上讨论如何对结果进行注释或者基因组序列比对。&lt;/p&gt;
&lt;p&gt;在进行下一部分之前，我们有必要讨论下一些基因组数据分析的可视化方法，这些方法不见得新，但有利于我们发现数据本身的问题而不是蒙着头只凭过往经验做分析。&lt;/p&gt;
&lt;h2 id=&#34;可视化&#34;&gt;可视化&lt;/h2&gt;
&lt;p&gt;基因组数据的结构决定其可视化必然要适应高维属性并面向实际问题。下面分问题阐述且前面提到的火山图与热图就不单独介绍了：&lt;/p&gt;
&lt;h3 id=&#34;重复性问题&#34;&gt;重复性问题&lt;/h3&gt;
&lt;p&gt;你进行了一组技术重复，然后发现两组数据相关性不错，那么是不是数据就可信了？不一定，第一次你测到是1，3，5，7但第二次测到的是10，30，50，70。这种情况相关性非常好，但都差了一个数量级。我们用数据做个演示。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# biocLite(&amp;quot;SpikeInSubset&amp;quot;)
library(SpikeInSubset)
# 载入原始数据
data(mas95)
# 提取表达谱信息并用散点图绘制
mypar(1,2)
r &amp;lt;- exprs(mas95)[,1] ##original measures were not logged
g &amp;lt;- exprs(mas95)[,2]
plot(r,g,lwd=2,cex=0.2,pch=16,
     xlab=expression(paste(E[1])),
     ylab=expression(paste(E[2])), 
     main=paste0(&amp;quot;corr=&amp;quot;,signif(cor(r,g),3)))
abline(0,1,col=2,lwd=2)
# 寻找95%的数据集中的部分
f &amp;lt;- function(a,x,y,p=0.95) mean(x&amp;lt;=a &amp;amp; y&amp;lt;=a)-p
a95 &amp;lt;- uniroot(f,lower=2000,upper=20000,x=r,y=g)$root
abline(a95,-1,lwd=2,col=1)
text(8500,0,&amp;quot;95% of data below this line&amp;quot;,col=1,cex=1.2,adj=c(0,0))
# 在对数范围上重新绘图
r &amp;lt;- log2(r)
g &amp;lt;- log2(g)
plot(r,g,lwd=2,cex=0.2,pch=16,
     xlab=expression(paste(log[2], &amp;quot; &amp;quot;, E[1])),
     ylab=expression(paste(log[2], &amp;quot; &amp;quot;, E[2])),
     main=paste0(&amp;quot;corr=&amp;quot;,signif(cor(r,g),3)))
abline(0,1,col=2,lwd=2)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/genome2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;从上面的图中我们可以看出基因组技术重复数据大部分位于低响应区，且数值越小，方差越大。这种情况得到的相关性可能被少数数据所影响，因而我们采用绝对差值与均值做图给出比较。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mypar(1,1)
plot((r+g)/2,(r-g),lwd=2,cex=0.2,pch=16,
     xlab=expression(paste(&amp;quot;Ave{ &amp;quot;,log[2], &amp;quot; &amp;quot;, E[1],&amp;quot;, &amp;quot;,log[2], &amp;quot; &amp;quot;, E[2],&amp;quot; }&amp;quot;)),
     ylab=expression(paste(log[2],&amp;quot; { &amp;quot;,E[1],&amp;quot; / &amp;quot;,E[2],&amp;quot; }&amp;quot;)),
     main=paste0(&amp;quot;SD=&amp;quot;,signif(sqrt(mean((r-g)^2)),3)))
abline(h=0,col=2,lwd=2)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/genome3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;这样的图叫做MA-plot或者说Bland-Altman plot，在基因组数据分析文章中算是比较常见。从图上我们知道，两者对数差的平均方差为1，也就是说差距约为2倍。这个2倍所说明的重复性问题需要按实际情况来看。&lt;/p&gt;
&lt;h3 id=&#34;异常值&#34;&gt;异常值&lt;/h3&gt;
&lt;p&gt;拿到数据后可以先来个最直观的boxplot异常值检测，出现特别离谱的，多半是有大问题。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ge &amp;lt;- geneExpression
# 伪造一组有问题的数据
ge[,12] &amp;lt;- ge[,12]/log2(exp(1))
mypar(1,1)
# 用boxplot检查
boxplot(ge,range=0,names=1:ncol(ge),col=ifelse(1:ncol(ge)==42,1,2))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/genome4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 用matplot绘制分位数曲线
qs &amp;lt;- t(apply(ge,2,quantile,prob=c(0.05,0.25,0.5,0.75,0.95)))
matplot(qs,type=&amp;quot;l&amp;quot;,lty=1)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/genome5.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 绘制平滑直方图
shist(ge,unit=0.5)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/genome6.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;高维数据展示&#34;&gt;高维数据展示&lt;/h3&gt;
&lt;p&gt;除了热图以外，其实也有其他展示高维数据的方法，基本都是基于主成分分析。为了展示方便，一般都是截取前两个主成分，然后用颜色标记出样本类别，在R中可以用&lt;code&gt;cmdscale&lt;/code&gt;方便的给出两个主成分。这是一种综合展示方法，纯描述探索性的，用来发现问题。&lt;/p&gt;
&lt;p&gt;另外一种展示是基于聚类分析的，也就是当我不知道分类时，我可以用聚类先聚出几类来，然后画出来看看样子。当然也可以用来寻找变化相近的基因与某种器官或功能的联系。总之一图胜千言，看到了问题才好解决。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>理解基因组数据分析之差异比较篇</title>
      <link>https://yufree.cn/cn/2015/08/27/genomewide-data-2/</link>
      <pubDate>Thu, 27 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2015/08/27/genomewide-data-2/</guid>
      <description>&lt;p&gt;先从最简单的例子来，假设我们比较两组人的平均身高该如何做？我们要对两组人采样，采样数最好一样，然后测量每个样本的身高，记录后分组计算均值与方差。如果目的是比较均值，那么首先要考虑使用的方法，如果是t检验，那么先对均值的方差进行F检验来确定是否需要等方差t检验，然后就是双样本t检验，结果显著（p&amp;lt;0.05）我们就说这两组人的身高有显著性差异（无差异就是0），给出两组身高的均值与差异的置信区间，我们的数据分析就完了。上述过程槽点略多，不吐槽了，如果你在实验室一线至少要掌握上面那个实验设计与数据分析的思路来寻找差异，多数科学发现的本质就是寻找未知的差异。&lt;/p&gt;
&lt;p&gt;下面是一组R代码，模拟了两组数据均值的对比，读懂输出的结果，另外注意R默认的t检验是不等方差的。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 模拟生成两组数据
group1 &amp;lt;- rnorm(100,100,10)
group2 &amp;lt;- rnorm(100,130,13)
# 进行t检验
t.test(group1,group2)
# 结果
#	Welch Two Sample t-test
#
#data:  group1 and group2
#t = -16.857, df = 186.154,
#p-value &amp;lt; 2.2e-16
#alternative hypothesis: true difference in means is not equal to 0
#95 percent confidence interval:
# -30.50828 -24.11558
#sample estimates:
#mean of x mean of y 
# 100.9668  128.2787 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果是基因组数据分析，基本思路就是设定实验组与对照组，对比找出差异，对有差异的数据在相应物种基因组里定位然后寻找功能注释，后面就是讲故事阶段了。但不同于我们对比身高，基因组数据分析对比的不单单是一组变量例如身高，而是一堆变量例如基因A，B，C，D，……，Z。当然不止26个，这时我们遇到的问题就是一个样本，几百上千维的描述如何保证样本代表性。这个问题的解决一般要靠实验设计，对比差异的话至少也要有两组，每组多少生物学重复通过试验来检测下统计功效，在满足一定功效的前提下反推需要的样本数。&lt;/p&gt;
&lt;p&gt;在基因芯片表达谱这个矩阵上，我们希望得到的信息是高度依赖所研究的科学问题的。大概有如下这两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;我的样本不分组，都是得了一种恶性病，我想找出这种病在基因层次的描述。这种情况你需要采集样本的基因组数据，对样本基因表达谱进行聚类，找出变化相对一致的基因。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;我在实验设计就有两组数据，这种情况可以直接对这两组进行t检验，然后对差异基因进行数据库检索或功能定位，进而依据组别判断数据的生物学意义。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;照这样看似乎并不难，本质上就是跟对照或数据库的数据找差异，可以看作是t检验的推广。但问题实际比想象的要复杂，举例而言，两组样本每个样本测定1000个基因，在0.05置信区间上至少出现50个假阳性，更何况一般芯片测的数比这要多，而常识上多数基因是不受影响的，也就是即便假阳性比率不高，但只要有就会影响我们回答科学问题。下面我用模拟数据来说明下这个问题。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 安装bioconductor
source(&amp;quot;http://bioconductor.org/biocLite.R&amp;quot;)
biocLite()
# genefilter包用来同时进行高维t检验，得到所有基因的分组差异
# Biobase包用来提供处理表达数据集的方法
# biocLite(&#39;genefilter&#39;)
library(genefilter)
library(Biobase)
# GSE5859数据集收集了不同人种的基因表达数据
# devtools::install_github(&amp;quot;genomicsclass/GSE5859&amp;quot;)
library(GSE5859)
# 读入数据
data(GSE5859)
# 提取分组信息，这里先选取两个人种
info &amp;lt;- pData(e)
g &amp;lt;- info$ethnicity
gASNCEU &amp;lt;- which(g == &#39;ASN&#39; | g == &#39;CEU&#39;)
group &amp;lt;- factor(g[gASNCEU])
# 提取表达谱原始数据
exp &amp;lt;- exprs(e)[,gASNCEU]
# 进行分组t检验
results &amp;lt;- rowttests(exp,group)
# 提取结果p值
pvals &amp;lt;- results$p.value
# 模拟随机表达谱并提取p值
m &amp;lt;- nrow(exp)
n &amp;lt;- ncol(exp)
randomData &amp;lt;- matrix(rnorm(n*m),m,n)
nullresults &amp;lt;- rowttests(randomData,group)
nullpvals &amp;lt;- nullresults$p.value
# 随机分组比较并提取p值
permg &amp;lt;- sample(group)
permresults &amp;lt;- rowttests(exp,permg)
permpvals &amp;lt;- permresults$p.value
# 观察p值分布与火山图
par(mfrow = c(2,3))
hist(pvals,ylim=c(0,9000))
hist(permpvals,ylim=c(0,9000))
hist(nullpvals,ylim=c(0,9000))

plot(results$dm,-log10(pvals),
     xlab=&amp;quot;Effect size&amp;quot;,ylab=&amp;quot;- log (base 10) p-values&amp;quot;)
plot(permresults$dm,-log10(permpvals),
     xlab=&amp;quot;Effect size&amp;quot;,ylab=&amp;quot;- log (base 10) p-values&amp;quot;)
plot(nullresults$dm,-log10(nullpvals),
     xlab=&amp;quot;Effect size&amp;quot;,ylab=&amp;quot;- log (base 10) p-values&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/volcano.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;从上面的分析我们可以看出p值的分布在实际样品与随机数据是有差异的。随机数据中p值的分布是均匀分布，但样品中明显是偏态分布，而且火山图可以看出均值差较小时出现了很多有显著性差异的数值。毋庸置疑，这会干扰我们后续的通路分析。&lt;/p&gt;
&lt;p&gt;数据也就长这样了，问题就是多次比较后假阳性比较多。我们可以用比较保守的Bonferroni矫正来用比较小的p值控制假阳性，当然为了保证统计功效，也可以使用控制错误发现率的方法来减少假阳性。一般控制错误发现率都是针对全局的，但q值法提供了一种对每个对比进行检验的方法，当q值比较小，我们认为这个结果还是比较靠谱的。这样当一个基因同时满足q值与p值小于0.05，那么我们大概可以说这组数的差异是显著的，我们也筛到了想要的基因。&lt;/p&gt;
&lt;p&gt;另一个更常用的解决方法是使用经验贝叶斯方法。我们注意看一下随机数据生成的火山图，这里面与实际数据相比差异与p值大致趋势一致。当我们假设多数基因应该是不变化时，我们就应该同时收敛差异与p值的离散状况。这里我们假设各组基因的变化服从一个超分布，然后计算t值时对方差进行贝叶斯加权。当基因组内差异小其方差权重小而整体权重大，当差异大时整体权重小，这样就实现了t值的一个收敛，最后给出的火山图也跟着收敛了。经验贝叶斯方法算是基因组数据分析里比较常用的方法，单看效果应该不错，可以配合q值去筛选差异基因。&lt;/p&gt;
&lt;p&gt;下一篇我们从t检验推广到线性模型，并想办法矫正掉一些批次效应。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>帕斯卡的钱包</title>
      <link>https://yufree.cn/cn/2015/08/18/effective-altruists/</link>
      <pubDate>Tue, 18 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2015/08/18/effective-altruists/</guid>
      <description>&lt;p&gt;如何做慈善？是跑到穷苦山沟里建学校？初一十五舍粥？还是说——做科研？这周Vox的一篇&lt;a href=&#34;http://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai&#34;&gt;文章&lt;/a&gt;给我安利了一个概念：有效利他。&lt;/p&gt;
&lt;p&gt;对一个有效利他主义者而言，做慈善眼光要足够长远，头脑要足够理性，行动要足够高效。换言之，如果你打算让世界变得好一点，就要有足够的智慧去经营。举例而言，你是个金融博士生，可能打算用假期去支教，但很遗憾，你这笔账算的可能太贵，从经济学上说就是要考虑机会成本，作为金融专业人士，你并不一定擅长教学但你却可能擅长运作资本，同样的假期你可以让一笔钱翻一番，然后捐出你的利润改善乡村教师待遇就是你最佳也是最有效的利他策略。也就是说，有效利他带有现代经济社会的基因，那就是追逐效果最大化。&lt;/p&gt;
&lt;p&gt;所谓最大化，我们就必须要做对比，A问题比B问题严重，那经费就毫不犹豫给A，不用在乎B问题可能存在眼下奄奄一息的受害者。毕竟长远来看，有效利他主义者的每一次行动都可能降低了更大问题的风险。那么长远怎么看？尽量公平的去看，人类活到现在累积个体有1080亿，如果人类可以生存5000万年，那么总累积人口将会有30000000亿，这些人都应该是平等的，起码在有效利他的伦理中是平等的，那么我们现在要做的就是着眼当下的严重问题以维持人类生存5000万年的概率最大化。在这个视角下，当前的70多亿人中的几亿贫穷人口简直可以看作计算误差，所以改善他们的生活不是有效利他主义的首选，他们的首选有且只有一个目的：让人类这个物种生存下去，其他都可看作扯淡。&lt;/p&gt;
&lt;p&gt;好了，下面我们来谈谈这个逻辑的问题，在问题之前先来个名为帕斯卡钱包的思想实验。有一天帕斯卡遇到了一个强盗，但这个强盗没带枪。于是强盗就说小帕你看咱们都不容易，我今天真是走的匆忙，要不你先把钱包给我，明天再来这里我给你看看我的枪？帕斯卡当然黑线了，直接拒绝。这时强盗说要不这样，你钱包里有多少钱？帕斯卡说10块。强盗说你先给我，我明天到这里给你100。帕斯卡这时快疯了，鬼才信你！强盗说要不看你也是搞概率的，你告诉我你有几成概率相信我？帕斯卡想了下说千分之一。强盗说那好，你钱包有10块给我净损失10块，我明天到这里的概率千分之一，那么我许诺明天给你两万块，这样你的期望收益就是10块了对吧？那我许诺给你两万，你把钱包给我。帕斯卡想了下也觉得这笔帐比较合算，就把钱包给这个没带枪的强盗了。&lt;/p&gt;
&lt;p&gt;这个思路很滑稽吧，但这恰恰是有效利他主义的逻辑。如果发生小行星撞地球的概率只有千万分之一但一旦发生就会造成人类生存概率的极大下滑，理性的人会把损失乘以概率得到期望，当这个期望对当前而言可以用一定量资源就可以避免，那么我们就该为了大多数人去投资这类项目。其实就是一个投资的思路，当某项目成功概率很低但成功后受益巨大，那么经济上就该投资。不过这个逻辑的问题就在帕斯卡的钱包思想实验中体现出来了：未来那么低的概率是否值得托付？抑或是我们当前目光太短浅？&lt;/p&gt;
&lt;p&gt;所以对于有效利他主义者而言救助小猫小狗什么的都是浪费，钱要用到刃上，为此他们给出了一个清单，在这清单上的就是真正重要的，其他的都是感性泛滥的资源浪费。其中主要的十二件可能对人类生存构成威胁的是：气候剧变，核战争，全球流行病，生态崩溃，全球经济崩溃，小行星，超级火山爆发，合成生命，纳米技术，人工智能，全球暴政以及未知的未知。怎么样，听起来是不是很科幻？但确实有那么一批人给出了这些事件可能造成人类生存危机的概率，虽然低的可怜，但考虑上30000000亿的人类共同体，任何降低这些事风险的行为都是有效利他主义者所追求的。&lt;/p&gt;
&lt;p&gt;说到这里你可能觉得这些人大概脑子不正常，也确实不那么正常，这些人构成的组织一般都设在硅谷，而他们多为高科技从事人员，不乏Peter Thiel与Elon Musk这种明星大碗，所以他们不是想想就算了，而是真的在做。而他们认为所有这些当中人工智能可以有效提高生存概率（虽然存在AI灭绝人类的可能），所以这些有效利他主义者的钱没有走向非洲而是进了硅谷，投给了AI开发人员。&lt;/p&gt;
&lt;p&gt;从传统意义的捐款到AI研发，慈善这个概念正变的目的越来越明确，行事越来越功利，但你不可否认，他们确乎是在做一些对他人有利的事，只不过这个受惠人中我们在世的这70多亿可能也只是个统计误差。确实，在有效利他者看来，关注眼前的伦理还是太过自私；而对于一个普通人而言，他们所做的就像是帕斯卡钱包实验中的那个强盗，拿了我们现在的资源，阻止了未来虚无缥缈的危机。&lt;/p&gt;
&lt;p&gt;有效利他主义是慈善事业的终结者，他们可能是英雄，也可能是患者，得治。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spark中关键概念的理解</title>
      <link>https://yufree.cn/cn/2015/06/18/spark-concept/</link>
      <pubDate>Thu, 18 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2015/06/18/spark-concept/</guid>
      <description>&lt;p&gt;其实我自己对spark的应用场景是没什么需求的，但几个月前不知道怎么想的在edx上选了一门伯克利的&lt;a href=&#34;https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/info&#34;&gt;spark课&lt;/a&gt;，所以就入了坑。一共五周，现在开到第三周，因为对python不熟加上记性也不好，先把其中比较干货的东西捡出来，此外剩下两周的课可能由于外出开会耽误。我没有计算机科学背景，所以仅按照自己理解与讲义来写，疏漏之处见谅。&lt;/p&gt;
&lt;h2 id=&#34;spark简介&#34;&gt;Spark简介&lt;/h2&gt;
&lt;p&gt;Spark是用来解决大量数据处理问题的一个工具。由于现在数据产生非常快，单机在收集、储存与处理数据上是性能不足的。如果我们用集群的话收集与存储是没问题了，但如何快速处理数据让数据变成知识也是需要工具的。此外，集群出于成本考虑多采用分布式的结构，所以这个工具要做的就是从这些分布式集群中快速准确的提取信息，而这也是spark的设计初衷。&lt;/p&gt;
&lt;p&gt;我们来理解一个分布计算场景：这里有一大段文本，我们把它们分成N份去储存，现在我打算计算词频，该如何做？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;方案一：每个储存单元作为一个处理单元，进行分词后各自计算自己分到文本的词频，然后汇总发送到另一个独立处理单元单独作汇总。这个方案是分层的，高层汇总单元（比较贵）挂了也就挂了，而同时响应并发数据很容易把这个单元搞宕机。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方案二：既然两层会挂掉，那我就在中间继续添加独立汇总层，例如在进行最终汇总前面加两个处理单元，每个处理单元只处理下层有限个储存单元的数据然后汇总。这样由于存在缓冲层甚至多个缓冲层，我们的处理单元成本可以相对一致，整体处理的稳定性会好一些。但这仍然是分层逻辑，高层挂了还是全挂。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方案三：还是分层，但是这次是逻辑分层，在词频问题上就是我们的处理层不是一台中枢而是多个处理单元。每个处理单元只收集处理逻辑上的一部分，例如处理中心A只响应词频高于100的，B只响应50～100的…这样出现宕机只会损失一部分运算。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在这里前面负责分词那一部分可理解为数据的map，也就是映射成可独自处理的小部分，而后面根据词频分别汇总可理解为reduce，也就是处理为想要的部分。整个流程就是先map到想要处理的东西，然后reduce为处理后的东西，不断循环。类似R里面从原始数据中提取想要的数据后进行处理，只是应用场景换成比较高大上的集群，而规模一大就需要有工具来把底层分发处理工作高效化，这样看spark实际提供了一个处理对象，底层的黑活（例如某个处理单元挂了重启）通过spark完成，我们只管用熟悉的数据处理方式来处理spark对象就可以了。&lt;/p&gt;
&lt;p&gt;其实这个问题不是spark首先发现解决的，Hadoop也是来处理这个问题的，但由于Hadoop都是硬盘读写操作，大量的I/O会降低处理速度，spark的一大高明之处在于把硬盘读写省了，都在内存里玩，如果中间处理品需要，可以另外cache。&lt;/p&gt;
&lt;h2 id=&#34;resilient-distributed-datasetsrdd&#34;&gt;Resilient Distributed Datasets(RDD)&lt;/h2&gt;
&lt;p&gt;RDD是spark的核心概念，所有要处理的数据都以RDD形式存储或使用。RDD可以直接生成或通过其他格式转化，这个处理对象从硬盘数据生成后运行在内存里，然后你就可以用熟悉的编程语言来处理这个对象了，目前支持Scala，Java，R与Python，同时Spark也提供了不少自带的函数来进行数据分析，前提是你得学下Scala。&lt;/p&gt;
&lt;h2 id=&#34;driver-and-workers&#34;&gt;Driver and Workers&lt;/h2&gt;
&lt;p&gt;Spark里一个程序是由两部分组成：Driver与Workers。Workers工作在集群节点或线程中，而上面说的RDD是分布在这些Workers中。Driver就是你的应用需求了，把需求提给Workers就完成编程了。&lt;/p&gt;
&lt;h2 id=&#34;rdd的创建与操作&#34;&gt;RDD的创建与操作&lt;/h2&gt;
&lt;p&gt;以下讨论我使用的是PySpark中的术语，也就是使用Spark的Python接口包。&lt;/p&gt;
&lt;p&gt;首先是RDD的创建，RDD可以来自python的list对象，也可以来自RDD的转化与直接从硬盘读取。在RDD创建时，你可以指定RDD的分区，例如指定为5就是说会分发到5个集群节点去处理。这里是可以精细化配置的，当然你得对集群有概念，反正我没概念。&lt;/p&gt;
&lt;p&gt;然后是RDD的操作，有两种类型，一种是transformations，另一种是actions。当你指定transformations时操作不会立即执行，属于lazy loading，当指派了actions后，操作才会执行。此外如前所述，RDD可以缓存到内存或硬盘上，对于使用者而言只要缓存了就ok，底层工作让spark来做就够了。&lt;/p&gt;
&lt;p&gt;filter及与Hadoop类似的map功能是属于transformations的，也就是说我可以先写一大段transformations，但只要没有actions的功能例如计数（count）或收集（collect），这些语句是不被执行的。举个例子，我打算从集群的数据里map某个条目，然后filter其中符合某些特征的条目，最后计数。只有最后这个是actions，如果没有这个命令，前面那一套都不执行。&lt;/p&gt;
&lt;h2 id=&#34;spark程序生命周期&#34;&gt;Spark程序生命周期&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;从外部数据中建立RDD&lt;/li&gt;
&lt;li&gt;通过transform变成新的RDD&lt;/li&gt;
&lt;li&gt;cache()一些关键RDD为了复用&lt;/li&gt;
&lt;li&gt;执行actions来进行计算并输出结果&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;broadcast-variables&#34;&gt;Broadcast Variables&lt;/h2&gt;
&lt;p&gt;当某些变量需要只读的分发给所有workers，spark可以通过广播这些变量到所有workers。举例而言，当你需要反复利用同一个数据表做查询，如果每个workers都计算一遍就不如把这个表先生成广播到所有workers里来的高效。&lt;/p&gt;
&lt;h2 id=&#34;accumulators&#34;&gt;Accumulators&lt;/h2&gt;
&lt;p&gt;聚合所有workers结果回driver而workers之间不需要传送时，spark提供accumulators来汇总，提高性能。举例而言，当我做求和时需要汇总各个workers的数值到driver，我并不需要workers去读取driver上的数值，这时accumulators就可以在全局上进行汇总。&lt;/p&gt;
&lt;h2 id=&#34;pyspark实例&#34;&gt;PySpark实例&lt;/h2&gt;
&lt;h3 id=&#34;rdd的建立&#34;&gt;RDD的建立&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 从python list里构建&lt;/span&gt;
data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# 这个构建行为不是actions，只是指定构建方式，包括分发数&lt;/span&gt;
RDD &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize(data,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# 从Hadoop输入格式建立&lt;/span&gt;
distFile &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;textFile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;README.md&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;rdd的transformation与lambda函数&#34;&gt;RDD的transformation与lambda函数&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 构建RDD&lt;/span&gt;
rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;])
&lt;span style=&#34;color:#75715e&#34;&gt;# 用python的lambda函数来构建映射&lt;/span&gt;
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# RDD:	[1,2,2,4] → [2,4,4,8]	&lt;/span&gt;
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filter(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: x &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# RDD: [1,2,2,4] → [2,2,4]&lt;/span&gt;
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;distinct()	
&lt;span style=&#34;color:#75715e&#34;&gt;# RDD: [1,2,2,4] → [1,2,4]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从这里我们可以看出这个操作非常类似R中对数据框的操作，但因为是lazy的，没有action命令它们不会实际被执行。&lt;/p&gt;
&lt;h3 id=&#34;rdd的action与lambda函数&#34;&gt;RDD的action与lambda函数&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;])
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reduce(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; a,b: a&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;b)	
&lt;span style=&#34;color:#75715e&#34;&gt;# Value: 6&lt;/span&gt;
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# Value: [1,2]	&lt;/span&gt;
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect()	
&lt;span style=&#34;color:#75715e&#34;&gt;# Value: [1,2,3]	&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Spark的一大优点在于比Hadoop提供了更多的操作，这一点需要详查文档体会。&lt;/p&gt;
&lt;h3 id=&#34;broadcast-variables-1&#34;&gt;Broadcast Variables&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# driver端	&lt;/span&gt;
broadcastVar &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;broadcast([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;])	
&lt;span style=&#34;color:#75715e&#34;&gt;# worker端&lt;/span&gt;
broadcastVar&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;value
&lt;span style=&#34;color:#75715e&#34;&gt;# [1,2,3]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;accumulators-1&#34;&gt;Accumulators&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;accum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;accumulator(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)	
rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;])	
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(x):	
 &lt;span style=&#34;color:#66d9ef&#34;&gt;global&lt;/span&gt; accum
 accum &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; x			
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;foreach(f)	
accum&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;value
&lt;span style=&#34;color:#75715e&#34;&gt;# Value: 10	&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>另一个安斯库姆四重奏？</title>
      <link>https://yufree.cn/cn/2015/04/24/anscombe-quartet/</link>
      <pubDate>Fri, 24 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2015/04/24/anscombe-quartet/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://zh.wikipedia.org/wiki/%E5%AE%89%E6%96%AF%E5%BA%93%E5%A7%86%E5%9B%9B%E9%87%8D%E5%A5%8F&#34;&gt;安斯库姆四重奏&lt;/a&gt;算得上数据可视化里最经典的案例了，四组不同的数据做线性相关，得到相关系数一样（0.816）但实际作图可以看出很大差异。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/anscombe.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;第一次看到这图的时候有人就说这辈子能搞出个类似的数据集就名留青史了，而我当时觉得作为一个外行这辈子能再看到个类似的可视化案例也算不虚此生了，前天还真看到了。&lt;/p&gt;
&lt;p&gt;Plos Biology 上刚发表了一篇PERSPECTIVE来吐槽条形图，作者收集了2014年一季度前25%的生理学期刊中发表的703篇文章并对其中的图像进行了统计。结果如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;条形图是这些文章作者最喜欢的图形展示方法，85.6%的文章包含至少一张条形图&lt;/li&gt;
&lt;li&gt;77.6%的条形图使用的是均值配合标准误，15.3%使用均值配合标准差&lt;/li&gt;
&lt;li&gt;61.3%的文章使用线图，散点图配合误差线，误差多数使用标准误&lt;/li&gt;
&lt;li&gt;使用直方图，散点图与箱式图等展示数据分布的不到五分之一&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然后作者开始吐槽了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;你们懂不懂啥时候用SD啥时候用SE啊&lt;/li&gt;
&lt;li&gt;不懂没关系啊，反正都不该用啊&lt;/li&gt;
&lt;li&gt;那些都是描述性统计啊，你样本那么少直接画出来多好，算什么均值啊&lt;/li&gt;
&lt;li&gt;展示数据跟假设检验两个概念，别混了啊&lt;/li&gt;
&lt;li&gt;做对比用t检验你查过数据分布了吗？有异常值检验啊&lt;/li&gt;
&lt;li&gt;不会没关系啊，反正我建议用非参方法啊，当然我知道损失功效啊&lt;/li&gt;
&lt;li&gt;用非参方法要展示中值啊，中值的差跟差的中值不是一回事啊&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面这些缺少直观理解，请看下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/bar3.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;散点图完全不同的数据画成条形图是一样的，误差线一样长，看下面的检验结果，参数方法与非参方法完全不同。这副图与安斯库姆四重奏有异曲同工之处，本质上都是在表示没有图形展示的假设检验可能会遗漏重要信息。其实对于一个实验设计良好的工作是不该出现上面的问题的，但现在很多人在论文撰写或数据展示及检验时喜欢套用别人的展示方法，完全不明就里。更尴尬的是你就算告诉他们数据有问题，他们也不知道怎么改，非参方法要么没听说过，要么仅仅是听说过没用过。至于数据展示，这种所谓 Univariate scatterplots 的图根本不知道怎么画，不过作者算得上宅心仁厚，亲自做了一个&lt;a href=&#34;https://www.ctspedia.org/do/view/CTSpedia/TemplateTesting&#34;&gt;excel模版&lt;/a&gt;供其他人使用。&lt;/p&gt;
&lt;p&gt;其实类似的图形也有如下的展示分布的方法（附注R代码），但数据太少时就别考虑了，垃圾进，垃圾出。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;小提琴图&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;par&lt;/span&gt;(mfrow&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
  mu&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
  si&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.6&lt;/span&gt;
  bimodal&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;rnorm&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;mu,si),&lt;span style=&#34;color:#a6e22e&#34;&gt;rnorm&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;,mu,si)) 
  uniform&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;runif&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;-4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
  normal&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;rnorm&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
  &lt;span style=&#34;color:#a6e22e&#34;&gt;vioplot&lt;/span&gt;(bimodal,uniform,normal)
  &lt;span style=&#34;color:#a6e22e&#34;&gt;boxplot&lt;/span&gt;(bimodal,uniform,normal)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/vioplot.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;差异散点图&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 这个太简单了，就是当要展示的两组数据为配对数据时，直接对其差异作普通散点图并附上参考线&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/drr2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;抖动散点图&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;number1 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rhyper&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;400&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
number2 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rhyper&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;400&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;par&lt;/span&gt;(mfrow&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
&lt;span style=&#34;color:#a6e22e&#34;&gt;plot&lt;/span&gt;(number1,number2)
&lt;span style=&#34;color:#a6e22e&#34;&gt;plot&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;jitter&lt;/span&gt;(number1),&lt;span style=&#34;color:#a6e22e&#34;&gt;jitter&lt;/span&gt;(number2))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/jitter.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;平滑散点图&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;par&lt;/span&gt;(mfrow&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
number1 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rhyper&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
number2 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rhyper&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;smoothScatter&lt;/span&gt;(number1,number2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/smplot.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;其实数据展示在能表意清晰的条件下越原始越好，这样能更好的展示原始数据的意义，如果加入过多的总结性描述，总有种隐藏信息的感觉。&lt;/p&gt;
&lt;h2 id=&#34;参考文献&#34;&gt;参考文献&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002128&#34;&gt;Beyond Bar and Line Graphs: Time for a New Data Presentation Paradigm（开放获取）&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>微博是否符合齐普夫定律？</title>
      <link>https://yufree.cn/cn/2015/04/15/weibo-data-zipf-law/</link>
      <pubDate>Wed, 15 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2015/04/15/weibo-data-zipf-law/</guid>
      <description>&lt;p&gt;最近看到一篇&lt;a href=&#34;http://www.jmir.org/2015/1/e22/&#34;&gt;论文&lt;/a&gt;，作者利用微博关键词出现的频率来预测空气污染的状况并认为来自社交媒体的数据能为环境监测提供更多的细节。这个想法很不错，但其实抛开文章的视角，微博的文本分析技术上实现并不困难，下面以齐普夫定律的验证做一个展示。&lt;/p&gt;
&lt;p&gt;首先找一个开源的微博语料库，我找到的是&lt;a href=&#34;http://www.nlpir.org/?action-viewnews-itemid-231&#34;&gt;NLPIR微博内容语料库&lt;/a&gt;，里面有23万条微博内容。然后从里面提取词汇与词频，目的是用来验证下文献计量学中的&lt;a href=&#34;http://zh.wikipedia.org/zh/%E9%BD%8A%E5%A4%AB%E5%AE%9A%E5%BE%8B&#34;&gt;齐普夫定律&lt;/a&gt;，也就是发现字词的使用次数（f）与字词的使次数排名（r）之乘积，会等于常数C。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/Zipf.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;上图来自维基百科&lt;/p&gt;
&lt;h2 id=&#34;分析代码&#34;&gt;分析代码&lt;/h2&gt;
&lt;p&gt;以下代码可在下载数据并设定数据路径后重复。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 读入xml包&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(XML)
&lt;span style=&#34;color:#75715e&#34;&gt;# 读取数据并提取文本信息&lt;/span&gt;
doc &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;xmlTreeParse&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;NLPIR微博内容语料库.xml&amp;#39;&lt;/span&gt;,useInternal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;TRUE&lt;/span&gt;)
rootNode &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;xmlRoot&lt;/span&gt;(doc)
doc1 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;xpathSApply&lt;/span&gt;(rootNode,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;//article&amp;#34;&lt;/span&gt;,xmlValue)
&lt;span style=&#34;color:#75715e&#34;&gt;# 去除无关标点与数字&lt;/span&gt;
doc2 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;gsub&lt;/span&gt;(pattern&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http:[a-zA-Z\\/\\.0-9]+&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,doc1)
&lt;span style=&#34;color:#75715e&#34;&gt;# 中文分词&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(Rwordseg)
doc3 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;segmentCN&lt;/span&gt;(doc2)
&lt;span style=&#34;color:#75715e&#34;&gt;# 构建语料库 去掉标点与数字与高频词&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(tm)
doc4 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Corpus&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;VectorSource&lt;/span&gt;(doc3))
doc5 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tm_map&lt;/span&gt;(doc4, removePunctuation)
doc6 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tm_map&lt;/span&gt;(doc5, removeNumbers)
&lt;span style=&#34;color:#75715e&#34;&gt;# 高频无意义词在这里可以搞到 https://github.com/yufree/democode/tree/master/data&lt;/span&gt;
x &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;scan&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stopwords.txt&amp;#34;&lt;/span&gt;, what&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
doc7 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tm_map&lt;/span&gt;(doc6, removeWords, x)
doc8 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tm_map&lt;/span&gt;(doc7, stripWhitespace)
&lt;span style=&#34;color:#75715e&#34;&gt;# 构建全范围的词频矩阵&lt;/span&gt;
control&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(minDocFreq&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,wordLengths &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;Inf&lt;/span&gt;),bounds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(global &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#66d9ef&#34;&gt;Inf&lt;/span&gt;)),weighting &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; weightTf,encoding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UTF-8&amp;#39;&lt;/span&gt;)
doc.tdm&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;TermDocumentMatrix&lt;/span&gt;(doc8,control)
&lt;span style=&#34;color:#75715e&#34;&gt;# 这里截取词频高于5长度为2的词&lt;/span&gt;
control2&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(minDocFreq&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,wordLengths &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;),bounds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(global &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#66d9ef&#34;&gt;Inf&lt;/span&gt;)),weighting &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; weightTf,encoding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UTF-8&amp;#39;&lt;/span&gt;)
doc.tdm2&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;TermDocumentMatrix&lt;/span&gt;(doc8,control2)
&lt;span style=&#34;color:#75715e&#34;&gt;# 这里截取词频高于5长度为3的词&lt;/span&gt;
control3&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(minDocFreq&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,wordLengths &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;),bounds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(global &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#66d9ef&#34;&gt;Inf&lt;/span&gt;)),weighting &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; weightTf,encoding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UTF-8&amp;#39;&lt;/span&gt;)
doc.tdm3&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;TermDocumentMatrix&lt;/span&gt;(doc8,control3)
&lt;span style=&#34;color:#75715e&#34;&gt;# 这里截取词频高于5长度为4的词&lt;/span&gt;
control4&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(minDocFreq&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,wordLengths &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;),bounds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(global &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#66d9ef&#34;&gt;Inf&lt;/span&gt;)),weighting &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; weightTf,encoding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UTF-8&amp;#39;&lt;/span&gt;)
doc.tdm4&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;TermDocumentMatrix&lt;/span&gt;(doc8,control4)
&lt;span style=&#34;color:#75715e&#34;&gt;# 这里截取词频高于5长度为5的词&lt;/span&gt;
control5&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(minDocFreq&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,wordLengths &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;),bounds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(global &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#66d9ef&#34;&gt;Inf&lt;/span&gt;)),weighting &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; weightTf,encoding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UTF-8&amp;#39;&lt;/span&gt;)
doc.tdm5&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;TermDocumentMatrix&lt;/span&gt;(doc8,control5)
&lt;span style=&#34;color:#75715e&#34;&gt;# 得到词频列表&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(slam)
freq &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rowapply_simple_triplet_matrix&lt;/span&gt;(doc.tdm,sum)
freq2 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rowapply_simple_triplet_matrix&lt;/span&gt;(doc.tdm2,sum)
freq3 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rowapply_simple_triplet_matrix&lt;/span&gt;(doc.tdm3,sum)
freq4 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rowapply_simple_triplet_matrix&lt;/span&gt;(doc.tdm4,sum)
freq5 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rowapply_simple_triplet_matrix&lt;/span&gt;(doc.tdm5,sum)
&lt;span style=&#34;color:#75715e&#34;&gt;# save(freq,freq2,doc8,file =&amp;#39;constellation.RData&amp;#39;)&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 验证齐普夫定律&lt;/span&gt;
order &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;order&lt;/span&gt;(freq&lt;span style=&#34;color:#a6e22e&#34;&gt;[order&lt;/span&gt;(freq,decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)],decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)
freq0 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; freq&lt;span style=&#34;color:#a6e22e&#34;&gt;[order&lt;/span&gt;(freq,decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)]
order2 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;order&lt;/span&gt;(freq2&lt;span style=&#34;color:#a6e22e&#34;&gt;[order&lt;/span&gt;(freq2,decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)],decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)
freq20 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; freq2&lt;span style=&#34;color:#a6e22e&#34;&gt;[order&lt;/span&gt;(freq2,decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)]
order3 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;order&lt;/span&gt;(freq3&lt;span style=&#34;color:#a6e22e&#34;&gt;[order&lt;/span&gt;(freq3,decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)],decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)
freq30 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; freq3&lt;span style=&#34;color:#a6e22e&#34;&gt;[order&lt;/span&gt;(freq3,decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)]
order4 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;order&lt;/span&gt;(freq4&lt;span style=&#34;color:#a6e22e&#34;&gt;[order&lt;/span&gt;(freq4,decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)],decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)
freq40 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; freq4&lt;span style=&#34;color:#a6e22e&#34;&gt;[order&lt;/span&gt;(freq4,decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)]
order5 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;order&lt;/span&gt;(freq5&lt;span style=&#34;color:#a6e22e&#34;&gt;[order&lt;/span&gt;(freq5,decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)],decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)
freq50 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; freq5&lt;span style=&#34;color:#a6e22e&#34;&gt;[order&lt;/span&gt;(freq5,decreasing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; T)]
&lt;span style=&#34;color:#75715e&#34;&gt;# 结果可视化&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# plot(log(order)~log(freq0))&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;png&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;logzipfplot.png&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;par&lt;/span&gt;(mfrow&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
&lt;span style=&#34;color:#a6e22e&#34;&gt;plot&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;(order2)&lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;(freq20),main&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;word length: 2&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;plot&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;(order3)&lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;(freq30),main&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;word length: 3&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;plot&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;(order4)&lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;(freq40),main&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;word length: 4&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;plot&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;(order5)&lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;log&lt;/span&gt;(freq50),main&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;word length: 5&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;dev.off&lt;/span&gt;()
&lt;span style=&#34;color:#a6e22e&#34;&gt;png&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;czipfplot.png&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;par&lt;/span&gt;(mfrow&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
&lt;span style=&#34;color:#a6e22e&#34;&gt;plot&lt;/span&gt;(order2&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;freq20,main&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;word length: 2&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;plot&lt;/span&gt;(order3&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;freq30,main&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;word length: 3&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;plot&lt;/span&gt;(order4&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;freq40,main&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;word length: 4&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;plot&lt;/span&gt;(order5&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;freq50,main&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;word length: 5&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;dev.off&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/logzipfplot.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/czipfplot.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;现象描述与讨论&#34;&gt;现象描述与讨论&lt;/h2&gt;
&lt;p&gt;结果很意外，我在很多帖子中看到人们肆无忌惮的使用该定律作为论据，但事实上且不论这本来就是一个经验定律，从我对微博数据分析的结果上看，齐普夫定律似乎并不符合微博语言习惯。&lt;/p&gt;
&lt;p&gt;理论上第一张图会都是直线，但只有我们把词按5个一组进行区分时才能勉强看到一条直线。如果看下前50个词我们会发现这种分法可能捕捉到的更多是英文单词，所以可能是微博中大量出现的英文反映了齐普夫定律的语言使用环境。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;zynga 发展中国家      happy      phone 
       504        189        148        131 
     webos      china 南京大屠杀      party 
       130        125        119        108 
     gmail      world      store      style 
       106        104         98         98 
个人所得税      ilook      never      there 
        93         90         89         88 
     gucci 高尔夫球场      touch      adobe 
        81         79         59         57 
     weico      weibo      hello      rovio 
        57         56         53         53 
     heart 印度尼西亚      icann      green 
        50         49         49         47 
     belle      kitty      leave 人民检察院 
        46         45         45         44 
原教旨主义      first      light 毛泽东思想 
        44         44         44         43 
中国科学院      black      david      kevin 
        42         42         42         42 
     brian      yahoo 中央气象台      nexon 
        41         41         40         40 
     nexus      apple      muddy      still 
        40         39         39         39 
     would      ralph 
        39         38 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;但其实当分词在两三个时，出现的更多是中文词汇，这时候反而偏离了齐普夫定律，更像个抛物线规律。&lt;/p&gt;
&lt;p&gt;两个字一个词&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;中国  腐败  城管  一个  北京  微博  问题  政府 
38951 37655 28787 24242 15834 14984 12972 12651 
 社会  今天  美国  国家  工作  公司  经济  已经 
11418 11310 11264 11261 10326  9722  8402  8227 
 现在  时间  表示  事件  香港  发现  世界  发生 
 8222  7941  7913  7755  7710  7466  7258  7196 
 进行  知道  人员  安全  生活  目前  新闻  调查 
 7189  7184  7077  7032  6985  6916  6901  6378 
 记者  今年  孩子  市场  官员  昨天  事故  企业 
 6043  6037  6034  5944  5919  5809  5748  5706 
 看到  图片  朋友  部门  视频  成为  全国  认为 
 5614  5587  5572  5496  5440  5386  5370  5339 
 大学  媒体 
 5317  5219 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;三个字一个词&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;北京市 越来越 房地产 嫌疑人 公安局 老百姓 互联网 
  3266   2122   2074   2033   1928   1913   1804 
公务员 候选人 联合国 公安部 人民币 电视台 消费者 
  1789   1747   1728   1715   1657   1620   1577 
亿美元 负责人 委员会 国务院 铁道部 办公室 哈哈哈 
  1515   1514   1513   1457   1417   1243   1243 
进一步 中纪委 第一次 派出所 开发商    ceo    the 
  1241   1213   1125   1099   1096   1090   1068 
俄罗斯 幼儿园 临时工 董事长 发言人 意味着 机动车 
  1054   1006    982    952    916    902    897 
利比亚 大学生    you 万美元 全世界 新华社 朝阳区 
   821    798    796    781    766    764    762 
领导人 身份证 志愿者 一个月 总经理 自行车 发布会 
   750    741    717    705    688    661    651 
奢侈品 
   642
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;那么有人会说：你去掉了高频词，这个操作自然会导致齐普夫定律的不成立。没错，如果原本的数据集遵循齐普夫定律，那么我们去掉的高频词会导致常数C达不到。显然（我不是故意用这个词的），当排序被提前理论上会导致前面数值偏小，但不会导致后面也偏小，而应该是逼近常数C。&lt;/p&gt;
&lt;p&gt;我们来看第二张图，如果微博数据符合齐普夫定律，那么我们应该看到一条水平线，结果出现了一个山峰，还是前坡陡后坡缓。&lt;/p&gt;
&lt;p&gt;于是我接着探索，看下出现在顶峰的词是什么鬼：&lt;/p&gt;
&lt;p&gt;两个字一个词&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;人心 阻止 一套 原文 随便 展开 男性 下次 无能 征集 
 421  442  422  443  420  643  421  442  422  441 
中方 轻松 手中 名人 走红 审判 借贷 滋生 水果 早安 
 423  443  420  421  425  442  422  426  643  423 
团购 森林   lt 债务 买房 观众 咖啡 环卫 遭到 警告 
 642  441  641  640  420  639  443  440  638  421 
嘉宾 城区 火锅 争取 选项 转型 原本 姐姐 震惊 角色 
 425  442  422  426  423  448  444  441  424  420 
军事 欧元 好多 好处 富人 商务 会长 有所 沉默 真心 
 443  643  440  421  425  641  422  640  442  445 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;三个字一个词&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;执行官 侦查员 著作权 原材料 总公司    see 男朋友 
    91     91     91     92     84     63     91 
徐家汇 一口气 知情人 婴幼儿 交易日 信息化 小金库 
    90     92     84     63     91     90     92 
针对性    lte 写字楼 中文版    lee 银行家 直辖市 
    84     81     63     89     80     88     83 
专卖店 孙悟空 多一些 太阳能    tot 中南海 微生物 
    62     63     91     90     61     81     92 
抑郁症 手续费    fbi 自己人 小学校 新街口 一周年 
    84     89     80     64     62     83     88 
尼古丁    gif 从业者 国家队 一两个 演播室 芝加哥 
    63     61     91     90     81     84     71 
星期六 玉泉营 想象力 生产力 小博士 十多年 科技界 
    80     64     87     92     62     89     63 
债权人 
    60 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我对着这些词想半天也没搞出个所以然，但这里谈点直觉：我认为这批词词频与排序的乘积可以用来表征其在语料库中的影响力。在符合齐普夫定律的语料库中，影响力大的词无疑就是那些高频词，但这些词其实没什么卵用：因为太常见，常规自然语言处理都会有排除高频词的步骤。那么在不符合齐普夫定律且排除掉高频词的语料库中如何寻找有代表性的词？这些词不能太常见，但又不能不常见，否则形不成规律性。我感觉上面用词频与排序乘积大的词可能是符合这一要求的词汇，它们规模不大，但极有可能在不同分组中展示出巨大差别，而这种区别很有可能稀释到广泛意义上的微博日常讨论中。&lt;/p&gt;
&lt;h2 id=&#34;半成品结论&#34;&gt;半成品结论&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;即使进行了高频词的去除，微博环境特别是中文语境也并不符合齐普夫定律&lt;/li&gt;
&lt;li&gt;中文语料库中展示出的词频与排序山峰式规律可能用来筛选分组中高影响力关键词&lt;/li&gt;
&lt;li&gt;微博语料库中的中英文混杂现象值得注意&lt;/li&gt;
&lt;li&gt;欢迎自行探索&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>理解基因组数据分析之数据读取与数据结构篇</title>
      <link>https://yufree.cn/cn/2015/03/16/genomewide-data/</link>
      <pubDate>Mon, 16 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2015/03/16/genomewide-data/</guid>
      <description>&lt;p&gt;这一系列的文章源于edx上PH525这一系列&lt;a href=&#34;https://www.edx.org/course/statistics-r-life-sciences-harvardx-ph525-1x&#34;&gt;课程&lt;/a&gt;，从实用角度出发分为四部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原始数据读取与数据结构&lt;/li&gt;
&lt;li&gt;高维数据的分组差异比较&lt;/li&gt;
&lt;li&gt;基因组数据建模与可视化&lt;/li&gt;
&lt;li&gt;结果注释与通路分析&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其实就是按工作流程走的，首先设计实验，跑芯片，然后拿到机器读数。根据厂家不同，原始数据的处理方式也不同，我们采用基于R的bioconductor来进行分析。为此我们需要知道基因组数据的读取方法及在bioconductor中的数据结构。之后就是按照我们的科学问题进行差异比较，这里面会遇到假阳性等问题。之后就是构建模型排除一些影响，筛出我们真正关心的基因或序列片段。最后就是为这些基因片段进行注释与分析，回答科学问题。&lt;/p&gt;
&lt;h2 id=&#34;原始数据读取&#34;&gt;原始数据读取&lt;/h2&gt;
&lt;p&gt;先说数据源，按技术分两种讲吧。&lt;/p&gt;
&lt;p&gt;先看面向已知的microarrays，也就是微阵列基因芯片，可以用来测定基因表达。原理上你的微阵列上的探针序列是知道的，所以不太合适用来做未知基因的筛查或测序，当然这个看思路。微阵列基因芯片的应用场景还包括通过甲基化水平研究表观遗传表达、通过ChIP-on-chip研究转录因子或结合位点或着进行流行病学与基因组学的交叉去解决些科学问题。&lt;/p&gt;
&lt;p&gt;另一种就是面向未知的，例如RNA测序或NGS，也就是二代测序，可以探索未知。就NGS而言，原则上一代二代三代测序都可以用来探索未知，例如测定物种基因组。另外可解决的科学问题包括研究同一物种不同人群的表达，同一个体不同器官的表达，同一器官不同生长阶段的表达，同一生长阶段不同环境因素下基因表达。这里面有些问题基因芯片也能做或配合一些技巧去做，但RNA测序或NGS得到的信息量更多一些。&lt;/p&gt;
&lt;p&gt;其实在实际场景下有些人是不管技术的，但会区别厂商品牌。比较常见的就是affymetrix、illumina跟agilent这三家。原理不多说，基本都是依靠光信号。如果算上二代测序品牌可能还要多些，但商业化的大都能在bioconductor上搜索到相关数据的读取软件包。由于技术进步，高通量测序所得到的数据越来越多也越来越不直观，因此对这类数据进行数据分析就显得很重要了。&lt;/p&gt;
&lt;p&gt;下面用Affy包为例，演示下读取数据。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 安装bioconductor与affy包
# source(&amp;quot;http://bioconductor.org/biocLite.R&amp;quot;)
# biocLite()
# biocLite(&#39;affy&#39;)
library(affy)
# 读取样本采集信息
tab &amp;lt;- read.delim(&amp;quot;sampleinfo.txt&amp;quot;, check.names = FALSE, as.is = TRUE)
# 读取样本数据，探针层次
ab &amp;lt;- ReadAffy(phenoData = tab)
# 直接读取为基因层数据，适用于特定科学问题
ejust &amp;lt;- justRMA(filenames = tab[, 1], phenoData = tab) 
# 对样本进行背景校正与正则化，从探针层转化为基因层数据
e &amp;lt;- rma(ab)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;原始的数据就是探针水平的光信号，也就是CEL文件，这个读取后要做背景矫正，矫正了背景要正则化，如果你有spike-in的探针也可以进行一把基于探针的背景矫正，都做完了就可以连同矫正后的误差等信息一并保存到一个ExpressionSet类型对象（Bioconductor里的基因芯片基础数据类型）中等待进一步处理。&lt;/p&gt;
&lt;p&gt;在解释ExpressionSet类型对象的数据结构前，分析者有必要清楚这些矫正与正则化的原因。根据affymetrix的测序原理，每个基因大概有十几个探针，在芯片上我们可以通过加入已知浓度梯度的阳性或阴性探针来作为背景矫正的基础。这样我们以浓度作为x轴，响应作为y轴，得到的应该是十几条平行的直线。但真实的数据往往是在低浓度时探针变化不大而高浓度呈递增状态，也就是先平后升。如果我们用一个模型来描述就是&lt;/p&gt;
&lt;p&gt;$$Y = a + bx + e$$&lt;/p&gt;
&lt;p&gt;a表示低浓度截距，x表示浓度，b是浓度系数。这样低浓度时a比较大，浓度起作用有限，当浓度高时误差就不起作用了。这种情况下我们要描述单个探针的变化就比较麻烦了，得想办法从背景中提取出信号。厂商提供的解决方案是加入一些伪探针来模拟同样浓度下的噪声，用差减方式得到信号，或者对两者相关性建模模拟出信号，但这样同样会导致低浓度下信号方差。另一个方法就是对噪音建模，模拟出方差与信号的关系，这样造成了偏差，但实际效果不错。扣掉背景可以让我们知道一组探针光信号的真实变化。&lt;/p&gt;
&lt;p&gt;除了背景矫正，另一个要做的就是正则化，出发点是我们通常认为多数基因在单一处理下应该是不变的，但实际数据确可能出现整体的偏移。这种情况我们可以用局部回归或分位数回归等正则化方法让数据回到零为中心的状态，配合加标的方法可以做到最大程度上减弱偏差与噪音的影响。&lt;/p&gt;
&lt;p&gt;上面啰嗦半天其实你也看到了，代码里就用了&lt;code&gt;e &amp;lt;- rma(ab)&lt;/code&gt;一句就搞定了，因为这条命令包含了背景扣除与正则化还有将探针数据转为基因层数据等一系列命令，算是个懒人包，可以用来处理微阵列数据。如果你打算处理RNA测序数据，可能面临的问题不太一样，需要的模型也要重新设计，听我一句劝，用别人现成的吧，除非你是学生物统计的。到这一步你已经有一组基因层的数据了。&lt;/p&gt;
&lt;h2 id=&#34;数据结构&#34;&gt;数据结构&lt;/h2&gt;
&lt;p&gt;ExpressionSet是Bioconductor为了衔接后续的基因芯片表达数据分析包提供了一个通用的数据类型，也是eSet对象的一个类。不管哪一家的芯片都要转成这个类型才好进行下一步分析，这样做的原因往往是方便后续分析的开发人员，也便于提供统一接口。用R语言说就是可以直接为ExpressionSet对象写方法，而不用对每一家公司都提供数据格式支持，当然可能损失点效率，只是可能。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 读取Biobase包与示例数据，该包提供一些对ExpressionSet类型对象的基础操作
library(Biobase)
data(sample.ExpressionSet)
sample.ExpressionSet
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个类型有如下要求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;至少要包括assayData（也就是第一部分转换到的）&lt;/li&gt;
&lt;li&gt;assayData的数据保存在m＊n的矩阵中，m个基因，n个样本&lt;/li&gt;
&lt;li&gt;phenoData会用来保存n＊p的矩阵，n个样本，p个样本特征例如实验分组，测定时间之类的&lt;/li&gt;
&lt;li&gt;featureData用来保存每个基因的信息，m＊t，m个基因，t个基因描述。这里如果你读取数据直接读了探针层的，这个部分可以描述基因；如果是基因层的，这里用来保存基因的注释&lt;/li&gt;
&lt;li&gt;experimentData用来保存该组数据的实验信息与出处等&lt;/li&gt;
&lt;li&gt;annotation用来保存一个供其他Bioconductor包调用的注释，可理解为特征码&lt;/li&gt;
&lt;li&gt;protocolData用来保存基因芯片上真对每一个测定数据所保存的方案或信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;搞清楚这个结构的目的其实在于数据共享与数据操作。当你进行了一组实验并发表文章后，你的数据可以直接用这个对象类型封装为一个Bioconductor包上传供其他研究人员使用。至于数据操作则是很多方法都是针对这个类型写的，例如提取assayData直接用&lt;code&gt;exprs(e)&lt;/code&gt;就可以把阵列矩阵搞出来了，详细信息都可以在帮助页面查到。此外也有SnpSet的数据类型来对应相关的研究内容。&lt;/p&gt;
&lt;h2 id=&#34;小结&#34;&gt;小结&lt;/h2&gt;
&lt;p&gt;读到这里希望理解下面几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基因芯片的数据结构&lt;/li&gt;
&lt;li&gt;读取基因芯片时对原始数据的预处理原因&lt;/li&gt;
&lt;li&gt;读取或构建基因芯片数据集&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>《穹顶之下》影响力测算</title>
      <link>https://yufree.cn/cn/2015/03/07/chai-influence/</link>
      <pubDate>Sat, 07 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2015/03/07/chai-influence/</guid>
      <description>&lt;p&gt;同多数环境相关专业同学一样，这一周我的微博微信被《穹顶之下》相关讨论轮番轰炸，不论褒贬大家都有一个共识：那就是这部纪录片确实产生了很大的影响力，所以才有必要去争论。我也很好奇，究竟一部纪录片而不是制作它的人或团队会产生什么样的影响？到底大不大？存不存在偏见？因此我做了一点探索分析，这部分分析完全基于网络开放工具，也因此每个接触互联网的人都可以重现我的结果，至于结论，各花入各眼。&lt;/p&gt;
&lt;h2 id=&#34;思路&#34;&gt;思路&lt;/h2&gt;
&lt;p&gt;全文中提取高频词汇作为关键词，然后对关键词的搜索趋势进行分析作为《穹》的影响力测算依据，分析尺度有三：这7天来趋势的变化；搜索地域差异；搜索人群特征。也就是说从时间，空间与参与者三个角度评价《穹》的影响，起码符合记叙文三要素。我本打算用谷歌趋势，结果搜索量太低形不成趋势，因此转而使用&lt;a href=&#34;http://index.baidu.com/&#34;&gt;百度指数&lt;/a&gt;，虽然结果可重复，但存在一些缺点我们文末再议。&lt;/p&gt;
&lt;h2 id=&#34;报告文本&#34;&gt;报告文本&lt;/h2&gt;
&lt;p&gt;其实第二天我们就可以从网络上搜索得到《穹》的&lt;a href=&#34;http://vdisk.weibo.com/s/tEXDd1OHvRYy&#34;&gt;全文&lt;/a&gt;，利用在线&lt;a href=&#34;http://timdream.org/wordcloud/&#34;&gt;词云制作工具&lt;/a&gt;，在去除掉一些无意义的高频词后得到如下词云：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/wordcloud.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;很佩服，全文高频词里没有特别专业的词汇，但又确实在讨论一个专业问题。本来我想通过高频词搜索的一些性质来进行下一步检索，现在看不行。&lt;/p&gt;
&lt;h2 id=&#34;关键词选择&#34;&gt;关键词选择&lt;/h2&gt;
&lt;p&gt;因为我也完整看了一遍视频，所以列选了一些关键词，首先看穹顶与12369这两个词：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/keywords1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;《穹》播出后12369的搜索热度要强于纪录片本身，这说明人们确实对内容而不仅仅是片子本身产生了兴趣，而且后续的媒体指数也跟进了，因此我选了更多视频中出现的关键词而不是视频本身或制作人本身作为下一步探索影响力的基础。&lt;/p&gt;
&lt;h2 id=&#34;时间趋势&#34;&gt;时间趋势&lt;/h2&gt;
&lt;p&gt;见下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/keywords2.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/keywords3.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/keywords4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;我选择的关键词为褐煤、空气净化器、pm10、环保部与12369。你要问我为什么不多选点，那是百度太抠门，最多选5个（谷歌也有限制）。现象如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;7天过去，搜索强度没有回归到基线，影响力仍在&lt;/li&gt;
&lt;li&gt;12369，褐煤的搜索高峰要早于空气净化器与环保部&lt;/li&gt;
&lt;li&gt;移动端上述现象不明显&lt;/li&gt;
&lt;li&gt;移动端的影响力上升下降的峰比PC端更陡峭&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;针对第一条，我很想知道究竟一个新闻事件造成的影响能用多久回归到基线，据此对比最近另一个热词duang的搜索趋势：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/duang.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;在duang面前，12369已经恢复到了基线…为此我又尝试了另外几组词，最后在尝试马航时，我发现两者强度相对接近：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/mh.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;去年年底的马航惨剧互联网用了11天左右恢复到基线，类比12369，估计这次关于纪录片的讨论可能持续半个月左右。很强了，现在的信息更迭速度可以用秒来记，能抓住人们半个月的注意力很不容易。有人批中国喜欢搞运动，但有比没有强啊，三分钟热度也是热度，有热度就效果。&lt;/p&gt;
&lt;p&gt;后面几条的现象会在人群特征中重新提及。&lt;/p&gt;
&lt;h2 id=&#34;空间趋势&#34;&gt;空间趋势&lt;/h2&gt;
&lt;p&gt;刚才是全国的搜索趋势，具体到城市会不会有差异？去年虽然全年空气质量达标的城市不多，但总还是有，污染较重的也有，为此我选取济南，唐山，拉萨，三亚作为检测城市，结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/sp.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/sppc.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/spmo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;整体上，污染城市比非污染城市对视频关键词的搜索指数高（当然我也不清楚百度究竟用了什么算法，很可能是人口基数造成的），甚至非污染城市已经回到了基线，但有意思的是在搜索上移动端的行为地域差异性小于PC端，另外一个有意思的地方是污染城市在视频播出前对12369就产生了搜索波动，我估计是跟当天污染状况有关。&lt;/p&gt;
&lt;p&gt;据此，地域上影响力是有差异的，视频对污染城市的人影响似乎更大些。&lt;/p&gt;
&lt;h2 id=&#34;人群特征&#34;&gt;人群特征&lt;/h2&gt;
&lt;p&gt;这里只使用了3月份数据来隔离之前搜索的影响（百度不提供精确到天的人群特征搜索查询），还是前面5个关键词：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/age.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;看起来似乎中年人比青年人更关注环保部而中老年人更关注空气净化器，而空气净化器这个词的性别比中女性要高于男性。但上面这个现象可能是假象，原因在于我们不知道基线在哪里？合不合理。你去看这个年龄分布会发现存在歧视：年龄很小与很大的网民可能不会去使用互联网，同时性别比也太离谱，基于此我增加了两个基线关键词，一个是新闻，我认为这个词各个年龄段的人都会关注，另一个是exo，虽然我不清楚这跟XO还有EX有啥区别，但风闻搜索这个词的女性要远高于男性，结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/control.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;额，就当我假设的基线正确吧，我们可以看到，中年人确实要比基线搜索更多的环保部，而中老年人则更喜欢搜索空气净化器，年轻人似乎对12369抱有更大的期望。至于性别，其实刚才的说法是站不住脚的，因为基线里女性比例要高于所有视频相关关键词，也就是说，关注视频的男性要比基线多，只是在空气净化器这个词上，男性关注相对小。另外根据exo这个词反映的性别比，我感觉百度给出的参数似乎有点偏误。&lt;/p&gt;
&lt;h2 id=&#34;小结&#34;&gt;小结&lt;/h2&gt;
&lt;h3 id=&#34;时间尺度&#34;&gt;时间尺度&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;大致持续10～15天回归基线&lt;/li&gt;
&lt;li&gt;PC端比移动端滞后，变化相对缓慢&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;空间尺度&#34;&gt;空间尺度&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;污染重的城市比轻的城市关注度高，基线回归慢&lt;/li&gt;
&lt;li&gt;PC端趋势比移动端差异明显&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;人群&#34;&gt;人群&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;网络人群活跃度本身存在基线差异，做对比要设定基线与假设&lt;/li&gt;
&lt;li&gt;青年人更响应视频号召去打电话&lt;/li&gt;
&lt;li&gt;中年人比青年人更关注环保部，重行动&lt;/li&gt;
&lt;li&gt;中老年人更关注空气净化器，重健康&lt;/li&gt;
&lt;li&gt;女性比男性关注空气净化器，但低于新闻基线&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;吐槽&#34;&gt;吐槽&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;上面的结论一定有你觉得不合理的，数据就在那里，请自行探索事实&lt;/li&gt;
&lt;li&gt;百度的趋势搜索功能限制太多，希望开放数据API，这样可以用google的&lt;a href=&#34;https://github.com/google/CausalImpact&#34;&gt;因果推断包&lt;/a&gt;来进行更有统计意义的时序推断，当然我知道这不太可能&lt;/li&gt;
&lt;li&gt;关于影响力，上面所做的其实是基于互联网搜索数据的，存在偏误，所以不要得出太广泛的结论，讲段子除外&lt;/li&gt;
&lt;li&gt;现在在线工具可以回答你很多问题，所以不要总是盯着自己生活圈内那一点点资讯，你的社交网络圈在同化你的同时也异化了你，跳出来看一下试试&lt;/li&gt;
&lt;li&gt;互联网是一个娱乐致死的最佳样本&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>如何用图来折磨读者</title>
      <link>https://yufree.cn/cn/2015/02/14/bad-graph/</link>
      <pubDate>Sat, 14 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2015/02/14/bad-graph/</guid>
      <description>&lt;p&gt;威斯康星大学的&lt;a href=&#34;http://kbroman.org/&#34;&gt;Karl Broman&lt;/a&gt;教授除了喜欢晒&lt;a href=&#34;https://twitter.com/kwbroman/status/523221976001679360/photo/1&#34;&gt;照片&lt;/a&gt;让别人用各种模型计算自己洗衣机里的袜子到底有&lt;a href=&#34;http://www.senresearch.org/exact-and-approximate-probabilities-for-laundry-socks-problem.html&#34;&gt;多少双&lt;/a&gt;以外还有个小爱好：告诉大家如何画一张可以让读者如坠云雾的&lt;a href=&#34;https://www.biostat.wisc.edu/~kbroman/topten_worstgraphs/&#34;&gt;图&lt;/a&gt;。我在实验室是坚(zhuang)决(bi)不用excel，sigmaplot，origin等交互式软件出图的，但一直没有说原因，下面结合Broman教授的网页聊下在科研制图中折磨读者的一些方法：&lt;/p&gt;
&lt;h2 id=&#34;饼图&#34;&gt;饼图&lt;/h2&gt;
&lt;p&gt;如果你使用R，用&lt;code&gt;?pie&lt;/code&gt;可以看到在Note部分有如下语句：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Pie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;空口无凭，我们来看看下面两个图的效果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/browser.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;人眼对角度与面积等二维数量的感知并不如一维的长短敏感，因此你看左边的图会觉得搞不清楚具体数值，而条形图直接看坐标一目了然。更愚蠢的表现就是把数字标到饼图上，拜托，用个表多清楚。什么？饼图里加误差线？你想折磨死读者啊。什么要搞成3D的？加上透视效果的饼图在我眼里除了想掩饰问题没有任何出现的必要。当然除非你想讲个冷笑话，例如下面这张（图源来自&lt;a href=&#34;http://yihui.name&#34;&gt;谢益辉&lt;/a&gt;）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/pyramid.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;那么什么时候用饼图呢？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NEVER&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;能用饼图就可以用条形图替代，且更清楚直观。&lt;/p&gt;
&lt;h2 id=&#34;条形图&#34;&gt;条形图&lt;/h2&gt;
&lt;p&gt;条形图本来就不是让你加误差线的，多数需要使用条形图的场景都可以使用抖动散点箱式图或者小提琴图来展示原始数据。误差线本来是用来表明数据离散程度的，如果直接标注上原始数据离散程度一目了然。那为什么原来不用抖动散点图呢？我猜测是因为条形图在过去方便手画，也相对简洁，但现在用软件作图可以不用考虑作图难度，以表意优先。其实条形图太容易掩盖原始数据异常点的问题了。&lt;/p&gt;
&lt;p&gt;我们来看一下用条形图和抖动散点箱式图来展示同一组数据会有什么情况。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/bar1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;再看看加入在Treatment里加入异常值的情况，当然这里比较极端。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/bar2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;也许有人会说用误差线去判断差异是否显著，这其实是两个问题，一个是展示数据，另一个是进行统计推断，搞不清楚可以看&lt;a href=&#34;http://yufree.github.io/blogcn/2013/08/18/error-bar.html&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;另外在毒理学方向的文献中经常有人喜欢用配对条形图表示剂量－效应关系，这样很不好，就几个数配到一起是看不清楚的，其实带有参考线的散点图就很好，试对比下面几个图那个表述前后差异更清晰。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/drr1.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/drr2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;当你关注点是前后差异时，最好就直接把前后差异表示出来，人眼做减法不那么灵。&lt;/p&gt;
&lt;h2 id=&#34;3d图&#34;&gt;3D图&lt;/h2&gt;
&lt;p&gt;每当有人问我如何做3D图时我都习惯性反问：有没有一定要做3D图的必要？人眼连2D的面积对比都不敏感，更不用说3D了。如果纯粹是为了让图片漂亮些，花些功夫在配色上可能更有效，原则很简单：你看不明白的，别人同样看不明白。例如，对比下面这两幅图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/3D1.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/3D2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;有些场景必须要用3D图时一定考虑透视对数据展示的影响，不然会让人产生错误的理解。&lt;/p&gt;
&lt;h2 id=&#34;小结&#34;&gt;小结&lt;/h2&gt;
&lt;p&gt;在作图时，以表意优先，不要使用任何可能出现掩盖数据的图表。此外，饼图与3D图可以直接上黑名单，在条形图的使用场景里要叠加原始数据，让读者可以直接读出答案而不是通过模型来猜。&lt;/p&gt;
&lt;h2 id=&#34;参考文献&#34;&gt;参考文献&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://genomicsclass.github.io/book/pages/plots_to_avoid.html&#34;&gt;PH525课程讲义&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://clearscience.info/wp/?p=546&#34;&gt;Nature Method 专栏&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;文中Broman教授网站链接&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>大隐隐于数</title>
      <link>https://yufree.cn/cn/2014/12/18/hide-in-date/</link>
      <pubDate>Thu, 18 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2014/12/18/hide-in-date/</guid>
      <description>


&lt;p&gt;最小二乘法有个“基本假设”：残差要符合正态分布。结果最近读《Advanced Data Analysis from an Elementary Point of View》时，作者来了句：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;None of these assumptions was needed in deriving the optimal linear predictor.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;哭了，这些年我没少拿QQ图忽悠人，结果还忽悠错了。后来想想，如果残差不符合正态分布，那是不是说可以随意分布，或者说，画个图。&lt;/p&gt;
&lt;p&gt;放狗一搜，还真有人做了，这年代有个原创太不易了。&lt;a href=&#34;http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/stat_res_plots.html&#34;&gt;Leonard A. Stefanski&lt;/a&gt;在2007年在《The American Statistician》上发表了一篇搞笑式的&lt;a href=&#34;http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/Residual_Surrealism_TAS_2007.pdf&#34;&gt;论文&lt;/a&gt;介绍如何在回归分析的残差中藏图。大意是生成一个数据集，如果你对这个数据集做回归分析然后对残差作图就可以把隐藏在残差中的信息解出来。相比之下，感觉什么其他解密方法都成浮云了，抗解密指数极高。&lt;/p&gt;
&lt;p&gt;我仔细读了下算法，没读懂；再读，还没懂；我是不会读第三遍的（据说三遍不懂就等于承认自己笨蛋，绝不给机会）。不过仔细看了下，发现作者不懂R，但提供了别人写的R脚本来&lt;a href=&#34;http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/000_R_Programs/John_Staudenmayer/residplots.R&#34;&gt;实现&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;算法细节不提了（因为没看懂），我们可以把这个脚本看作一个加密算法。这个算法需要你输入一个点阵图，也就是你要加密的信息，横轴看作残差&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;，纵轴看作拟合的值&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;，这两个值实际都要中心化（也就是和为0），这样你会得到一个&lt;span class=&#34;math inline&#34;&gt;\(R^T Y = 0\)&lt;/span&gt;的等式。同时根据残差的定义，有&lt;span class=&#34;math inline&#34;&gt;\(P_x Y_0 = Y\)&lt;/span&gt;与&lt;span class=&#34;math inline&#34;&gt;\((I - P_x)Y_0 = R\)&lt;/span&gt;，这样我们的问题实际转化为了在已知&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;与&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;的条件下求解矩阵&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;与真实值&lt;span class=&#34;math inline&#34;&gt;\(Y_0\)&lt;/span&gt;的问题。不过仅仅求解一个X是无法隐藏残差中的信息的，我们需要引入另一个矩阵&lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;来扰乱X的计算并给出初始随机值，这样通过数值迭代求解我们可得到一个多变量数据框。乍看就是一组数据，如果对其按照线性回归拟合并对残差作图就可以得到原始信息了。这种情况我会把私钥设为响应值的列号，例如42，这样生成数据后毁掉算法，除了那个列号的持有者谁也无法从一堆随机数中找出关系。当然数据集数目不大可以穷举，但这个算法足够耗时，所以可以用来传递私人信息。&lt;/p&gt;
&lt;p&gt;上面一坨其实可以不用看，因为不懂也可以用。如果你仅仅拿到脚本会发现一个尴尬的问题：从哪去找个这样的点阵图片？必须imagemagick啊，安装后准备一个包含你想传达信息的图片，然后用下面的命令生成ASCII形式的图片：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;convert &amp;#39;fig.jpg&amp;#39;  -resize 100x100 -extent 100x100 -monochrome -compress none &amp;#39;fig.pbm&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后记事本打开脚本，把其中&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;temp &amp;lt;- scan(&amp;quot;~/logo.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;改成&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;temp &amp;lt;- scan(&amp;quot;fig.pbm&amp;quot;, skip = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;就可以了。这样你在脚本最下面加入下面这句：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;save(data,file = &amp;#39;data.RData&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;就可以在工作目录下得到一个数据文件，将文件传出去，解密者使用下面命令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;load(&amp;#39;data.RData&amp;#39;)
reg &amp;lt;- lm(Y~X,data=data)
plot(reg$fitted,reg$resid,pch=16,main=&amp;quot;Residual plot from data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;就可以看到隐藏的信息了。我修改了这个脚本，放在&lt;a href=&#34;https://github.com/yufree/democode/tree/master/Residual%20Sur&#34;&gt;这里&lt;/a&gt;，一个生成数据，另一个解密，不过你要把你自己的文件名输入进去，另外就是懂一点R。稍加改动就可以写成将Y隐藏于X中的形式，这算得上一个熟人间的暗号系统了，不过对我而言又是个没啥大用的课余小制作。&lt;/p&gt;
&lt;p&gt;附一个logo成品：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/fish.png&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kaggle 入门：泰坦尼克号幸存者项目</title>
      <link>https://yufree.cn/cn/2014/12/07/kaggle-titanic/</link>
      <pubDate>Sun, 07 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2014/12/07/kaggle-titanic/</guid>
      <description>


&lt;div id=&#34;背景&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;背景&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com&#34;&gt;Kaggle&lt;/a&gt;是一个线上数据科学竞赛类网站。简单说，上面会提供数据与需求，目的是构建一个&lt;span class=&#34;math inline&#34;&gt;\(f(x) = y\)&lt;/span&gt;的模型。参赛者会得到一个有&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;与&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;的训练集与只有&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;的测试集，你需要在训练集上构建模型，输入测试集的&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;得到你预测的&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;。之后你需要提交你的&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;到Kaggle，它会计算一个正确率作为竞赛的评判标准，分数高的会有奖金或仅仅是个荣誉。对于入门者，Kaggle提供了一个泰坦尼克号幸存者项目作为入手项目，项目提供的数据就是泰坦尼克号上乘客的信息与其幸存状况数据，以上为背景。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;数据准备&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;数据准备&lt;/h2&gt;
&lt;p&gt;名为train.csv的训练集数据与test.csv的测试集数据可从网站上下载，请将其移动到工作目录并读取。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 读取数据
train &amp;lt;- read.csv(&amp;#39;train.csv&amp;#39;)
test &amp;lt;- read.csv(&amp;#39;test.csv&amp;#39;)
# 观察数据结构
str(train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:	891 obs. of  12 variables:
##  $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
##  $ Name       : Factor w/ 891 levels &amp;quot;Abbing, Mr. Anthony&amp;quot;,..: 109 191 358 277 16 559 520 629 417 581 ...
##  $ Sex        : Factor w/ 2 levels &amp;quot;female&amp;quot;,&amp;quot;male&amp;quot;: 2 1 1 1 2 2 2 2 1 1 ...
##  $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
##  $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
##  $ Ticket     : Factor w/ 681 levels &amp;quot;110152&amp;quot;,&amp;quot;110413&amp;quot;,..: 524 597 670 50 473 276 86 396 345 133 ...
##  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Cabin      : Factor w/ 148 levels &amp;quot;&amp;quot;,&amp;quot;A10&amp;quot;,&amp;quot;A14&amp;quot;,..: 1 83 1 57 1 1 131 1 1 1 ...
##  $ Embarked   : Factor w/ 4 levels &amp;quot;&amp;quot;,&amp;quot;C&amp;quot;,&amp;quot;Q&amp;quot;,&amp;quot;S&amp;quot;: 4 2 4 4 4 3 4 4 4 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;str(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:	418 obs. of  11 variables:
##  $ PassengerId: int  892 893 894 895 896 897 898 899 900 901 ...
##  $ Pclass     : int  3 3 2 3 3 3 3 2 3 3 ...
##  $ Name       : Factor w/ 418 levels &amp;quot;Abbott, Master. Eugene Joseph&amp;quot;,..: 210 409 273 414 182 370 85 58 5 104 ...
##  $ Sex        : Factor w/ 2 levels &amp;quot;female&amp;quot;,&amp;quot;male&amp;quot;: 2 1 2 2 1 2 1 2 1 2 ...
##  $ Age        : num  34.5 47 62 27 22 14 30 26 18 21 ...
##  $ SibSp      : int  0 1 0 0 1 0 0 1 0 2 ...
##  $ Parch      : int  0 0 0 0 1 0 0 1 0 0 ...
##  $ Ticket     : Factor w/ 363 levels &amp;quot;110469&amp;quot;,&amp;quot;110489&amp;quot;,..: 153 222 74 148 139 262 159 85 101 270 ...
##  $ Fare       : num  7.83 7 9.69 8.66 12.29 ...
##  $ Cabin      : Factor w/ 77 levels &amp;quot;&amp;quot;,&amp;quot;A11&amp;quot;,&amp;quot;A18&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Embarked   : Factor w/ 3 levels &amp;quot;C&amp;quot;,&amp;quot;Q&amp;quot;,&amp;quot;S&amp;quot;: 2 3 2 3 3 3 2 3 1 3 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# 观察输出
table(train$Survived)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   0   1 
## 549 342&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;根据输出的预测&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;根据输出的预测&lt;/h2&gt;
&lt;p&gt;初步探索可知，我们的自变量有11个，因变量为二元输出。因为活下来的要多过幸存的，我们最保守的模型是基于输出的，也就是预测测试集上的乘客都没幸存。提交，正确率63%，目前估计要排到2000+。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;test$Survived &amp;lt;- rep(0, 418)
# 按照Kaggle要求构建数据框
submit &amp;lt;- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
# 写为csv文件
write.csv(submit, file = &amp;quot;alldie.csv&amp;quot;, row.names = F)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;考虑单一分类变量&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;考虑单一分类变量&lt;/h2&gt;
&lt;p&gt;OK，现在我们开始考虑使用自变量来进行预测，先考虑分类变量性别。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 构建分类列连表
prop.table(table(train$Sex, train$Survived))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         
##                   0          1
##   female 0.09090909 0.26150393
##   male   0.52525253 0.12233446&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;第一个模型预测了全部死亡，现在我们看到如果性别为女性，幸存概率要高很多，那么考虑这个变量后我们就在第一个模型基础上预测如果性别为女就能活下来。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 第一个模型
test$Survived &amp;lt;- 0
# 考虑性别变量
test$Survived[test$Sex == &amp;#39;female&amp;#39;] &amp;lt;- 1
# 按照Kaggle要求构建数据框
submit &amp;lt;- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
# 写为csv文件
write.csv(submit, file = &amp;quot;alldiebutfemale.csv&amp;quot;, row.names = F)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK，目前正确率77%，大概排1800+。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;考虑连续变量&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;考虑连续变量&lt;/h2&gt;
&lt;p&gt;我们现在考虑下连续变量年龄，看看分布。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 绘制直方图
hist(train$Age)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/titanichist.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;按照当年看过的电影，妇女，老人，小孩应该是优先上救生船的，因此我们认为小于20岁跟大于60岁的乘客更容易幸存，探索下。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;train$oldchild &amp;lt;- 0
# 提取老弱
train$oldchild[train$Age &amp;lt; 20 | train$Age &amp;gt; 60] &amp;lt;- 1
# 看看女性部分
aggregate(Survived ~ oldchild + Sex, data=train, FUN=sum)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   oldchild    Sex Survived
## 1        0 female      177
## 2        1 female       56
## 3        0   male       81
## 4        1   male       28&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;额，似乎不太对，好像老幼女性死的更多，看来电影与事实有出入，那么我们反着预测试试。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 第一个模型
test$Survived &amp;lt;- 0
# 考虑性别变量
test$Survived[test$Sex == &amp;#39;female&amp;#39;] &amp;lt;- 1
# 老弱可能要挂
test$Survived[test$Sex == &amp;#39;female&amp;#39; &amp;amp; (test$Age &amp;lt; 20 | test$Age &amp;gt; 60)] &amp;lt;- 0
# 按照Kaggle要求构建数据框
submit &amp;lt;- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
# 写为csv文件
write.csv(submit, file = &amp;quot;alldiebutfemalemiddleage.csv&amp;quot;, row.names = F)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;额，正确率73%，比刚才低了。回想一下，我刚才做的不过就是不断的分组，甚至是连续变量分组，这样到一定程度就会数据稀疏到欠拟合。同时因为分组中另一组简单认为幸存或不幸存也造成了较大偏差，所以整个模型都不好了。如果我们深入回想，现在需要的是一点过拟合，通过不断分类来确定最终分类其实就是种了一颗决策树，那么我们应该试试决策树。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;决策树&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;决策树&lt;/h2&gt;
&lt;p&gt;决策树的原理很简单，就是不断寻找能将数据分成区别最大的两块的阈值，然后再到下一层去迭代寻找，直到你的子分类中全是一样的输出。具体到这个例子，我们需要考虑用那些变量：PassengerId没啥意义；Name也看不出跟输出有什么显性联系；Ticket上的编码长得像密码，不要；Cabin数据残缺严重，不要；Embarked是上船位置，似乎跟沉船也没啥关系，不要。上面是一个主观变量的筛选，其实如果依赖专业知识会更快，当然也有基于统计学的变量筛选，暂时不提。决策树你就看作&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;就可以了，算法别人都写好了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# rpart包里的函数要比tree好些
library(rpart)
fit &amp;lt;- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare, data=train, method=&amp;quot;class&amp;quot;)
plot(fit)
text(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/titanictree.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;plot&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;从结果上看，第一变量是性别：对男性而言，年龄是第二分割点；对女性而言，仓位则成了第二分割点，所以决策树的分割相对还是比较精细的。我们用预测函数得到结果看看：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Prediction &amp;lt;- predict(fit, test, type = &amp;quot;class&amp;quot;)
submit &amp;lt;- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit, file = &amp;quot;tree.csv&amp;quot;, row.names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这次正确率达到了79%，光荣挺进前1000。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;交叉检验&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;交叉检验&lt;/h2&gt;
&lt;p&gt;前面我已经隐约提到了过拟合与欠拟合的问题，当你模型过分依赖训练集数据时，数据会过拟合；反之模型太不依赖训练集，例如最初我们用输出变量预测的状况，数据就会欠拟合。过拟合会导致模型捕捉了原始数据中的细节噪声，模型整体变异度也就是方差大，输出不稳；欠拟合则会导致模型没有识别到数据中的结构，不管你怎么变换自变量输出都不变，也就是偏差大。说到这里你也就清楚了，这不就是bias－variance tradeoff吗！这样说来，我们就可以使用一些统计学习的方法来构建更稳健的模型，例如交叉检验。你可以将交叉检验理解为降低过拟合风险的方法，实际上有些机器学习算法例如人工神经网络几乎可以完全拟合训练集，这时我们需要加点正则项或者叫惩罚函数来模拟过拟合的状态，进而生成预测性能良好的模型或者就直接用交叉检验。&lt;/p&gt;
&lt;p&gt;交叉检验一般把训练集分个5到10份，每次训练留一份作为检测集，根据检测集结果调整模型参数，迭代直到参数不变化。这样我们模型虽然没有惩罚函数，但不断的调试可以保证参数不会过拟合到一类数据上。不过也不用演示了，在&lt;code&gt;rpart&lt;/code&gt;包里默认就有交叉检验了，刚才你用默认的算法去做就已经包含交叉检验了。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;其他算法&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;其他算法&lt;/h2&gt;
&lt;p&gt;基于决策树的算法中有种叫做随机森林的算法效能不错，其逻辑在于随机抓一把变量来生成决策树，重复多次，然后对结果取均值或投票。这样就相当于加了一个惩罚函数，要知道决策树是没有惩罚而只能通过交叉检验来提高效能。这里面age的缺失比较重，为了去除缺失值，我们可以考虑用其他变量建个决策树补一下空白。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# tree 补下age的缺失值
Agefit &amp;lt;- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare,
                data=train[!is.na(train$Age),], method=&amp;quot;anova&amp;quot;)
train$Age[is.na(train$Age)] &amp;lt;- predict(Agefit, train[is.na(train$Age),])
library(randomForest)
fit &amp;lt;- randomForest(as.factor(Survived) ~ Pclass + Age + Sex + SibSp + Parch + Fare, data=train, ntree = 2000)
Prediction &amp;lt;- as.numeric(predict(fit, test))-1
submit0 &amp;lt;- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
# 预测出的缺失值用决策树结果充数 这可看作模型嵌套
submit0[is.na(submit0$Survived),2] &amp;lt;- submit[is.na(submit0$Survived),2]
write.csv(submit, file = &amp;quot;randomforest.csv&amp;quot;, row.names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结果跟决策树差不多，不过这种01输出的数据结构其实也可以用广义线性模型中的logistic回归来做。或者你也可以尝试人工神经网络，支持向量机，岭回归，lasso什么的，等你都尝试一遍后基本该遇到的情况就都遇到了，那个时候就该尝试进军其他挑战了。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>列联表分析那点事儿</title>
      <link>https://yufree.cn/cn/2014/02/18/2by2-table/</link>
      <pubDate>Tue, 18 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2014/02/18/2by2-table/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Contingency_table&#34;&gt;列联表&lt;/a&gt;是一类常见表格，其行与列代表了两种分类变量，行列交织给出分别隶属两个分类的频数或者概率，其基本形式如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(n_{11}\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;$n_{12} $&lt;/th&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(n_{1+}\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(n_{21}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;$n_{22} $&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(n_{2+}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(n_{+1}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(n_{+2}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(n_{ij}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;简单说就是4个数据间的故事，当将其中组成部分的频数除以总数，我们可以得到概率列联表，所有表格概率综合为1。另外，固定某行(或列)求和，可以得到该行(或列)对应属性的总频数，除以总数可得某属性在样本总体中的概率。&lt;/p&gt;
&lt;div id=&#34;控制实验&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;控制实验&lt;/h1&gt;
&lt;p&gt;在控制实验中，如果列表示处理是否有效而行表示是否处理，其列联表常用来判定某一处理是否有效或某一因子是否起作用。处理间的对比用相对风险(relative risk, RR)表示，直观上理解为两个处理是否产生变化，也就是用&lt;span class=&#34;math inline&#34;&gt;\(\frac{\frac{n_{11}}{n_{1+}}}{\frac{n_{21}}{n_{2+}}}\)&lt;/span&gt;来进行估计。RR的分布为独立变量的除法不易计算，所以取对数来计算标准误（取对数后符合正态假设），这个标准误的对数估计为&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{n_{12}}{n_{11}n_{1+}} + \frac{n_{22}}{n_{21}n_{2+}}}\)&lt;/span&gt;，证明用到&lt;strong&gt;Delta Method&lt;/strong&gt;，另一个前提为抽样比例的标准误估计为&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{p \cdot (1-p)}{n}}\)&lt;/span&gt;，据此可计算RR的置信区间。&lt;/p&gt;
&lt;p&gt;另一个常用的对比描述为胜率比(odds risk, OR)，有效比例较低时近似于RR，其公式为&lt;span class=&#34;math inline&#34;&gt;\(\frac{\frac{n_{11}}{n_{12}}}{\frac{n_{21}}{n_{22}}}\)&lt;/span&gt;，同样，其标准误估计为&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}}\)&lt;/span&gt;，可据此计算OR的置信区间。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;小样本&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;小样本&lt;/h1&gt;
&lt;p&gt;在样本数较大时，卡方检验会得到较好的结果；但样本数较小时，由于正态近似不能达到，并不能保证错误率，可以使用&lt;strong&gt;Fisher exact test&lt;/strong&gt;进行保证&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;关于&lt;strong&gt;Fisher exact test&lt;/strong&gt;，有个很经典的故事叫&lt;a href=&#34;https://en.wikipedia.org/wiki/Lady_tasting_tea&#34;&gt;女士品茶&lt;/a&gt;，后人以此为书名写了一本统计学史的&lt;a href=&#34;http://book.douban.com/subject/1626392/&#34;&gt;科普书&lt;/a&gt;，十分精彩。精确检验的核心在于H0假设的模拟，也就是处理前后没区别，这时两独立变量也就是处理前后的概率服从&lt;strong&gt;超几何分布&lt;/strong&gt;，这样可以精确计算出所有可能，进而得到精确的可能性，也就是p值，也可进行两尾检验。同样的，我们也可以用&lt;strong&gt;Monte Carlo&lt;/strong&gt;模拟方法来对这个值进行估计，本质一样。以下为R中测试代码：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- matrix(c(4, 1, 2, 3), 2)
fisher.test(dat, alternative = &amp;quot;greater&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;大样本&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;大样本&lt;/h1&gt;
&lt;p&gt;样本数较大时，精确检验同样有效，但这时使用卡方检验在推断上功效可能更好。卡方统计量的定义为&lt;span class=&#34;math inline&#34;&gt;\(\sum \frac{(Observed − Expected)^2}{Expected}\)&lt;/span&gt;，卡方统计量的本质为多个iid的平方在一定自由度下的分布，从公式上看可以理解为Expected在计数条件下既是期望也是方差，实际的证明可以参考&lt;a href=&#34;http://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/lecture11.pdf&#34;&gt;这里&lt;/a&gt;。在2*2列联表里，出现对比的就两个期望，所以卡方统计量的自由度始终为1。在R中，进行卡方检验的代码为：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- matrix(c(44, 77, 56, 43), 2)
chisq.test(dat)
chisq.test(dat, correct = FALSE)
chisq.test(x, simulate.p.value = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中，correct为连续性校正，默认开启。simulate.p.value表示用&lt;strong&gt;permutation test&lt;/strong&gt;进行p值的精确计算。卡方检验的适用度很广，非常适合计数变量列联表的处理，特别是样本的独立性检验或符合度检验，但样本数小于10还是推荐精确检验来提高功效。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simpson-悖论&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simpson 悖论&lt;/h1&gt;
&lt;p&gt;如果用列联表进行统计推断，那么很难绕过Simpson悖论：当存在第三方因子变量的干扰时，分层概率与边际概率经常会得到相反的结论。其实从数学角度并不算悖论，满足下面关系的数字可以轻松构建一个Simpson悖论：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{a}{b} &amp;lt; \frac{c}{d}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{e}{f} &amp;lt; \frac{g}{h}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{a + e}{b + f} &amp;gt; \frac{c + g}{d + h}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在统计推断上确实这个悖论不好解释，因为这涉及到实际的因果推断。统计上有时是会考虑分层抽样所造成的混杂变量影响，一种方法就是将分层造成的方差均一化加权，如果考虑计算的是胜率比，可以使用&lt;strong&gt;CMH test&lt;/strong&gt;，其H0假设为所有分层间的胜率是一致为1的。在R中可用以下代码实现：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- array(c(11, 10, 25, 27, 
16, 22, 4, 10,
14, 7, 5, 12,
2, 1, 14, 16,
6, 0, 11, 12,
1, 0, 10, 10,
1, 1, 4, 8,
4, 6, 2, 1),
c(2, 2, 8))
mantelhaen.test(dat, correct = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;同样，correct为连续性校正，也可使用exact = TRUE来进行精确计算。这样，我们可以得到某种分层策略会不会影响到胜率变化或者简单理解为胜率的异质性检验。最后，在推断上可以这样考虑：边际胜率比适合决策，条件胜率比适合分析影响。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;重看控制实验&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;重看控制实验&lt;/h1&gt;
&lt;p&gt;控制实验不同于观察实验，其中的一些设计方法可以对应更有针对性的列联表分析方法或者满足特殊假设下的分析要求。在研究疾病问题上，大面积采样并不容易，所以我们可以采用前瞻式实验设计或回顾式实验设计，前者固定因素数，后者固定疾病数。于此对应可以计算胜率比，其实数学上这个数没什么变化。但在计算标准误上可以根据更精确的方法来计算置信区间，这时就可以用到&lt;strong&gt;permutation test&lt;/strong&gt;了，这样对于胜率比的置信区间估计相对功效高一些。&lt;/p&gt;
&lt;p&gt;此外，当控制实验是面向同一组人时，我们会去考虑是否存在配对，这样上述分析的独立性假设可能就不满足了。这时考虑因子影响的话其实是对&lt;span class=&#34;math inline&#34;&gt;\(n_{12}\)&lt;/span&gt;与&lt;span class=&#34;math inline&#34;&gt;\(n_{21}\)&lt;/span&gt;的差异进行卡方检验，这样的检验H0为配对前后无变化，称做&lt;strong&gt;McNemar’s test&lt;/strong&gt;。在R中，通过下面的方法实现：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mcnemar.test(matrix(c(794, 86, 150, 570), 2),
correct = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;同样的，correct为连续性校正。从理解上，配对在某种程度上考虑了内在方差一致性问题，因此该检验与&lt;strong&gt;CMH test&lt;/strong&gt;在某种程度上是相通的，同样可以进行精确检验。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;非参方法&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;非参方法&lt;/h1&gt;
&lt;p&gt;上面设计参数估计都会有假设的分布，非参方法虽不涉及参数分布的假设，但一般会考虑样本为iid。非参方法有的完全不考虑数值，有的则会有一定的加权，具体情况可根据实际需要进行。R中常用&lt;strong&gt;wilcox test&lt;/strong&gt;来进行非参检验:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff &amp;lt;- c(.07, .07, .00, -.04, ...)
wilcox.test(diff, exact = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在处理列联表问题上，非参方法更不受异常值影响，但损失功效，所以可采用精确检验的思想来进行，比较适合基因组数据的处理。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;小结&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;小结&lt;/h1&gt;
&lt;p&gt;列联表问题的处理上，小样本用精确检验，大样本用卡方检验，混杂因素考虑CMH检验，如果数据是配对的则采用McNemar检验，感觉有异常值就wilcox检验，在计算成本不大的情况下，给出精确的p值可能更有说服力。&lt;/p&gt;
&lt;div id=&#34;参考资料&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;参考资料&lt;/h2&gt;
&lt;p&gt;1 &lt;a href=&#34;https://class.coursera.org/biostats2-002&#34;&gt;coursera公开课&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2 &lt;a href=&#34;https://github.com/bcaffo/MathematicsBiostatisticsBootCamp2&#34;&gt;Caffo教授讲义的Github Repo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3 &lt;a href=&#34;http://www.amazon.com/Mathematical-Statistics-Analysis-Duxbury-Advanced/dp/0534399428&#34;&gt;Mathematical Statistics and Data Analysis&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>漫谈统计学习中的 Bias-Variance Trade-Off思想</title>
      <link>https://yufree.cn/cn/2014/02/17/bias-variance/</link>
      <pubDate>Mon, 17 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2014/02/17/bias-variance/</guid>
      <description>


&lt;p&gt;最近在听一门统计学习的公开课，由&lt;a href=&#34;http://statweb.stanford.edu/~tibs/ElemStatLearn/&#34;&gt;ESL&lt;/a&gt;的两个作者Trevor Hastie与Robert Tibshirani讲授，不过使用的教材是ESL的简化版——&lt;a href=&#34;http://www-bcf.usc.edu/~gareth/ISL/&#34;&gt;《An Introduction to Statistical Learning》&lt;/a&gt;。虽说是简化版但写作风格明显是面对不同群体的，ISL偏应用，ESL更侧重数学原理解析，更抽象些。但这两本书的核心都是围着统计学习转的，而统计学习的一个核心论题就在Bias-Variance的取舍上，简单说就是偏差与方差的取舍。&lt;/p&gt;
&lt;p&gt;初涉统计学习最直观的图形就是过拟合与欠拟合训练集的模型对测试集残差的倒U型曲线，解释上也会偏重说明过拟合问题，下面就从原理层解释下这个问题：&lt;/p&gt;
&lt;p&gt;首先，我们知道统计学习实际上关注的是模型与现实问题，将现实的数据转化提炼出抽象的模型用来预测或做关系推断。那么先把无监督学习跟关系推断放一边，我们面对的是&lt;span class=&#34;math inline&#34;&gt;\(Y = f(X) + \epsilon\)&lt;/span&gt; 的回归预测问题，这里&lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;是理想模型。那么从公式上看我们可以想像，即使是理想模型，也存在&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;，这是由Y背后的分布决定的，即预测值存在于一个范围而不是定值，通过其出现的概率密度函数，我们可以得到取值的变动范围。&lt;/p&gt;
&lt;p&gt;其次，要清楚实际的统计学习过程是借助&lt;span class=&#34;math inline&#34;&gt;\(\hat f(x)\)&lt;/span&gt;来进行的，而&lt;span class=&#34;math inline&#34;&gt;\(\hat y = \hat f(x)\)&lt;/span&gt;,也就是每一种建模方法实际对应了一种对理想&lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;的估计。那么理想&lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;是没有偏差的，但实际的建模过程是存在偏差的，这个偏差是可以有方向的，用bias表示。而在建模过程中模型对理想模型&lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;也存在变动范围，受训练集影响大的模型变动大，Variance就高，这就是过拟合的来源。&lt;/p&gt;
&lt;p&gt;再次，不要忘记我们建模实际关心的不是&lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt;而是&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;，那么在评价建模过程时我们引入一个统计量&lt;span class=&#34;math inline&#34;&gt;\(MSE\)&lt;/span&gt;来衡量预测值与实际值的差异，有&lt;span class=&#34;math inline&#34;&gt;\(MSE = E(Y - \hat Y)^2 = [f(x) - \hat f(x)]^2 + Var(\epsilon)\)&lt;/span&gt;，这个公式的证明很重要，但基本就是一个代数过程。从结果上看，差异的来源由两部分构成，一部份源于模型的偏差，另一部分源于&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;自身的偏差，前者想想办法可以降低，后者由你响应变量的自身属性决定，基本不可约了。&lt;/p&gt;
&lt;p&gt;最后，关注下&lt;span class=&#34;math inline&#34;&gt;\([f(x) - \hat f(x)]^2\)&lt;/span&gt;，这一部份实际由两部分构成，就是上面提到的variance与bias。这两部分都低的建模方法更接近实际，但统计学习的一些方法是面临一个取舍问题的，那就是 Bias-Variance Trade-Off。&lt;/p&gt;
&lt;p&gt;这里用交叉验证中的留一法与k叠交叉检验来说明下这个问题，留一法的核心在于留下一个作为验证集，用其余的数据建模，建n个模型求验证集的&lt;span class=&#34;math inline&#34;&gt;\(MSE\)&lt;/span&gt;来进行模型评价。k叠交叉检验则将数据分为k份，大部份拿来建模，留一部分拿来验证。从Bias角度，由于留一法实际上使用了近乎所有数据，而k叠交叉检验会有&lt;span class=&#34;math inline&#34;&gt;\(n/k\)&lt;/span&gt;部分不参与建模，所以相同训练集条件下留一法的Bias会很小。但从variance角度，使用的数据基本相同，过于依赖同一套数据，在测试集上会表现模型方差偏大，反观k叠交叉检验，Bias可能由于训练不够偏高，但构建的模型独立性较好，抵消掉了对同一数据集的依赖，得到的模型在variance上会相对小。这样我们会看到一个有意思的现象，伴随模型复杂度的提高，留一法与k叠交叉检验的&lt;span class=&#34;math inline&#34;&gt;\(MSE\)&lt;/span&gt;实际是差不多的，但其中Bias与variance的组成是不一样的，从预测效果上看，可能variance低的模型更具吸引力，因此k叠交叉检验会比留一法更受欢迎。&lt;/p&gt;
&lt;p&gt;另一个解释的角度是从残差相关度上理解的，如果预测值与实际值的差距在训练集上高度相关，这样如果假设建模过程的variance对某一特定模型是固定的，那么训练集上的高度相关就反过来导致测试集变动范围增大。此外，数据生成过程如果存在自变量的相关，其势必造成参数标准误估计的偏低，进而影响模型的稳健性，这一过程还是比较常见的：时间序列分析。&lt;/p&gt;
&lt;p&gt;总之，Bias-Variance Trade-Off 贯穿在近乎每一个建模过程之中，统计学习的一个全局观就体现在其目标针对的是全局最优的理想模型。从这个角度出发，技术上的改进本质上是发现并平衡掉各种实际情况中的不完美或不理想。在这个层面上用好数学工具是可以从根上将问题分析透彻的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>多重比较从原理到应用</title>
      <link>https://yufree.cn/cn/2013/12/16/rgabriel-package/</link>
      <pubDate>Mon, 16 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2013/12/16/rgabriel-package/</guid>
      <description>&lt;h1 id=&#34;多重比较概论&#34;&gt;多重比较概论&lt;/h1&gt;
&lt;p&gt;方差分析解决的是分类变量对响应变量的影响问题，通常是用分类变量所解释的变异比上分类变量以外的变异去进行F检验。换句话讲，如果分类变量可以解释大部分响应变量的变异，我们就说这种分类变量对响应变量的解释有意义。例如下面这组数据：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;总变异为10, 如果我们分组为按照相同的数放到一起，那么组内变异就是0，组间变异为10，这时我们就说这种分组有效的解释了响应变量，F值趋向正无穷。如果我们完全随机分组，组内与组间的变异差不多，那么这种分类方法并不解释响应变量，反映到F值上就是1。&lt;/p&gt;
&lt;p&gt;但是仅仅知道是否受影响是不够的，如同上面的例子，我们知道的仅仅是存在一种分类方法可以解释响应的全部变化，其内部也是均匀的，但不同分类水平间的差异我们并不知道，这就是多重比较的起源。实际生活中如果差异很明显往往统计学工具不用出场，所以你应该预想到多重比较或仅仅是均值比较适用的场景往往差异我们不能直观感受，需要统计学工具来帮忙。&lt;/p&gt;
&lt;p&gt;同时要注意，如果我们对两组数据做置信度0.05的t检验，我们遇到假阳性的概率为5%。但如果面对多组数据例如3组，进行两两比较的话就有$$choose(3,2)$$也就是3组对比，那么我们遇到假阳性的概率就为$$1-(1-0.05)^3$$，也就是14.3%，远高于0.05的置信度。组越多，两两对比就越多，整体上假阳性的概率就越来越大，到最后就是两组数据去对比，无论如何你都会检验出差异。&lt;/p&gt;
&lt;p&gt;那么多重比较如何应对这个问题呢？有两种思路，一种思路是我依旧采取两两对比，进行t检验，但p值的选取方法要修改，例如Bonferroni方法中就把p的阈值调整为进行多重比较的次数乘以计算得到的p值。如果我们关心的因素为2，那么计算得到的p值都要乘2来跟0.05或0.01的边界置信度进行比较；另一种思路则是修改两两比较所用的统计量，给出一个更保守的分布，那么得到p值就会更大。不论怎样，我们这样做都是为了降低假阳性，但同时功效不可避免的降低了。&lt;/p&gt;
&lt;h1 id=&#34;多重比较的可重复性&#34;&gt;多重比较的可重复性&lt;/h1&gt;
&lt;p&gt;我们设计一个实验考察一个因素对响应变量的影响，结论无过于有影响，没影响。多重比较的前提是有影响，给出的答案是对影响的估计：影响有多大。那我们重复这个实验所要考虑的问题就是能否重现影响，影响的方向与大小是否与文献报道一致。&lt;/p&gt;
&lt;p&gt;就方向而样，虽然我们都不承认0假设（要不然还做什么实验），但当我们默认设定为双尾检验时，假阳性就被默认发生在两个方向上了，这样的多重比较必然导致在其中一个方向上的错误率被夸大了。&lt;/p&gt;
&lt;p&gt;就影响大小而言，如果我们每次重复都选择效应最强的那一组，重复越多，预设的偏态就越重，换言之，我们的零假设因为重复实验的选择偏好而发生了改变。&lt;/p&gt;
&lt;h1 id=&#34;三种错误&#34;&gt;三种错误&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;type I 假阳性
&lt;ul&gt;
&lt;li&gt;per-comparison error rate (PCER) 进行多次对比得到的假阳性的概率&lt;/li&gt;
&lt;li&gt;familywise error rate (FWER) 将多组比较看作一个大组，这时造成的错误率&lt;/li&gt;
&lt;li&gt;false discovery rate (FDR) 控制假阳性与总拒绝率的比例&lt;/li&gt;
&lt;li&gt;一般而言 PCER ≤ FDR ≤ FWER FWER更容易不拒绝空假设，更保守&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;type II 假阴性&lt;/li&gt;
&lt;li&gt;type III 有差异 方向错误&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;类型&#34;&gt;类型&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Single-step procedures 单步法 只考虑对H0的影响，不考虑其他影响&lt;/li&gt;
&lt;li&gt;Stepwise procedures 逐步法 考虑其他假设检验对单一检验的影响
&lt;ul&gt;
&lt;li&gt;Step-down procedures 排序后先对比第一个，有差异对比下一个，当出现无差异时停止对比&lt;/li&gt;
&lt;li&gt;Step-up procedures 排序后对比，有差异时停止对比，之后均认为有差异&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;两两比较 不同组之间进行均值比较，最常见&lt;/li&gt;
&lt;li&gt;对比 除了考虑不同组间均值比较，还考虑均值间线性组合的新均值的差异性，F检验有时是因为对比而不是两两比较产生的显著性&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;单步法等方差&#34;&gt;单步法等方差&lt;/h2&gt;
&lt;h3 id=&#34;tukeys-hsd两两比较&#34;&gt;Tukey&amp;rsquo;s HSD(两两比较)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;基于t范围分布&lt;/li&gt;
&lt;li&gt;等方差同数目，如果数目不同则使用Tukey-Kranmer方法&lt;/li&gt;
&lt;li&gt;两两比较最佳，数目相同功效弱于下降法&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bonferroni两两比较&#34;&gt;Bonferroni(两两比较)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;切割α，如果进行了c次推断，整个错误率为cα&lt;/li&gt;
&lt;li&gt;通用方法，应用在任一个推断&lt;/li&gt;
&lt;li&gt;方法简单，但十分保守&lt;/li&gt;
&lt;li&gt;只对比部分的话可自定义c值&lt;/li&gt;
&lt;li&gt;适用于指定对比数情况，此时功效高于Tukey&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dst两两比较&#34;&gt;DST(两两比较)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;对Boneferroni方法的改进，功效更高&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gt2-test两两比较&#34;&gt;GT2 test(两两比较)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;功效高于Dunn-Sidak方法&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gabriel两两比较&#34;&gt;Gabriel(两两比较)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;分组数目相同等同于GT2，不同时功效高，但不保证α&lt;/li&gt;
&lt;li&gt;易于可视化&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scheffe-test两两比较&#34;&gt;Scheffe test(两两比较)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;两两比较中功效最弱&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tukeys-hsd对比&#34;&gt;Tukey&amp;rsquo;s HSD(对比)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;涉及2～3个均值时功效最高&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bonferroni对比&#34;&gt;Bonferroni(对比)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;指定对比数&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dst对比&#34;&gt;DST(对比)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;指定对比数，功效高于Bonferroni&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scheffe-test对比&#34;&gt;Scheffe test(对比)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;保证α，两两比较功效最高&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;单步法异方差对比与两两比较&#34;&gt;单步法异方差(对比与两两比较)&lt;/h2&gt;
&lt;h3 id=&#34;gh-procedure&#34;&gt;GH procedure&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;不保证整体错误率，有时会超过，保守但功效高&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;c&#34;&gt;C&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;保证整体错误率&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;t3&#34;&gt;T3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;保证整体错误率&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;brown-forsythe-scheffe&#34;&gt;Brown-Forsythe-Scheffe&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;功效最高&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;单步法空白对比&#34;&gt;单步法空白对比&lt;/h2&gt;
&lt;h3 id=&#34;dunnett&#34;&gt;Dunnett&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;用于对比controls的变化&lt;/li&gt;
&lt;li&gt;其他组对比不考虑&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hsus-mcb&#34;&gt;Hsu&amp;rsquo;s MCB&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;对比均值与其他最好（自定义，可最大，亦可最小）&lt;/li&gt;
&lt;li&gt;目的寻找比其他好的而不是不同的&lt;/li&gt;
&lt;li&gt;对比次数最少，功效强，但低于Dunnett&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;stepdown-procedures&#34;&gt;Stepdown procedures&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;基于Tukey法&lt;/li&gt;
&lt;li&gt;先比较最大最小，q值取分组数&lt;/li&gt;
&lt;li&gt;比较最大第二小，q取分组数-1&lt;/li&gt;
&lt;li&gt;继续直到出现无差异停止&lt;/li&gt;
&lt;li&gt;当不需要置信区间且样本数相同时使用&lt;/li&gt;
&lt;li&gt;不推荐SNK与Duncan，推荐REGWF或REGWQ方法&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;snk&#34;&gt;SNK&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;不保证α&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;duncan&#34;&gt;Duncan&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;不保证α&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ryan-einot-gabriel-welsch-fisherregwf&#34;&gt;Ryan-Einot-Gabriel-Welsch-Fisher(REGWF)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;F检验加强版，保证α&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ryan-einot-gabriel-welsch-qregwq&#34;&gt;Ryan-Einot-Gabriel-Welsch-Q(REGWQ)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;q值法加强版，保证α&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-up-procedures&#34;&gt;step-up procedures&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Welsch&lt;/li&gt;
&lt;li&gt;Hochberg&lt;/li&gt;
&lt;li&gt;Dunnett and Tamhane&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;简略选择指南&#34;&gt;简略选择指南&lt;/h1&gt;
&lt;h2 id=&#34;总体控制错误率&#34;&gt;总体控制错误率&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;两两比较用Tukey法&lt;/li&gt;
&lt;li&gt;对比用Scheffe test&lt;/li&gt;
&lt;li&gt;指定对比数考虑 Gabriel &amp;gt; GT2 &amp;gt; DST &amp;gt; Bonferroni&lt;/li&gt;
&lt;li&gt;跟control比较用Dunnett&lt;/li&gt;
&lt;li&gt;方差不相等用GH，C，T3等方法&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;错误率&#34;&gt;错误率&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;保证α用Tukey Scheffe Dunnett&lt;/li&gt;
&lt;li&gt;不保证用其他的&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;探索与确认&#34;&gt;探索与确认&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;事前分析确定对比数用 Gabriel GT2或Scheffe&lt;/li&gt;
&lt;li&gt;事后确定对比用Tukey或各种stepwise方法&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;其他&#34;&gt;其他&lt;/h1&gt;
&lt;p&gt;写这篇文章是为了理清多重比较的一些基本概念，同时我自己也写了一个R包叫做&lt;a href=&#34;http://cran.r-project.org/web/packages/rgabriel/index.html&#34;&gt;rgabriel&lt;/a&gt;，这个包可用在指定对比数的多重比较与可视化中。此外，特别感谢&lt;a href=&#34;yihui.name&#34;&gt;谢益辉&lt;/a&gt;对其中SMM分布的指导与帮助。如有时间精力，我会将同样基于该分布的GT2与T3方法写入包中。&lt;/p&gt;
&lt;h1 id=&#34;参考资料&#34;&gt;参考资料&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Rafter, J.A., Abell, M.L., Braselton, J.P., 2002. Multiple comparison methods for means. Siam Review 44, 259–278.&lt;/li&gt;
&lt;li&gt;Bretz, F., Hothorn, T., Westfall, P., 2010. Multiple Comparisons Using R. CRC Press.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cos.name/cn/topic/142002&#34;&gt;http://cos.name/cn/topic/142002&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>小议非二元响应变量的EC50</title>
      <link>https://yufree.cn/cn/2013/10/22/r-ec50/</link>
      <pubDate>Tue, 22 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2013/10/22/r-ec50/</guid>
      <description>&lt;p&gt;其实解决这个问题的R包还是很多的，MASS里就有一个&lt;code&gt;dose.p&lt;/code&gt;来计算LD50，我们来仔细看看其中的代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;dose.p &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(obj, cf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;) {
    eta &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;family&lt;/span&gt;(obj)&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;link&lt;/span&gt;(p)
    b &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;coef&lt;/span&gt;(obj)[cf]
    x.p &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; (eta &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; b[1])&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;b[2]
    &lt;span style=&#34;color:#a6e22e&#34;&gt;names&lt;/span&gt;(x.p) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;paste&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;p = &amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;format&lt;/span&gt;(p), &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;, sep &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
    pd &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;cbind&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, x.p)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;b[2]
    SE &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sqrt&lt;/span&gt;(((pd &lt;span style=&#34;color:#f92672&#34;&gt;%*%&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;vcov&lt;/span&gt;(obj)[cf, cf]) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; pd) &lt;span style=&#34;color:#f92672&#34;&gt;%*%&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
    &lt;span style=&#34;color:#a6e22e&#34;&gt;structure&lt;/span&gt;(x.p, SE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SE, p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; p, class &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;glm.dose&amp;#34;&lt;/span&gt;)
}
print.glm.dose &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;function&lt;/span&gt;(x, &lt;span style=&#34;color:#66d9ef&#34;&gt;...&lt;/span&gt;) {
    M &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;cbind&lt;/span&gt;(x, &lt;span style=&#34;color:#a6e22e&#34;&gt;attr&lt;/span&gt;(x, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;SE&amp;#34;&lt;/span&gt;))
    &lt;span style=&#34;color:#a6e22e&#34;&gt;dimnames&lt;/span&gt;(M) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;names&lt;/span&gt;(x), &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Dose&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;SE&amp;#34;&lt;/span&gt;))
    x &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; M
    &lt;span style=&#34;color:#a6e22e&#34;&gt;NextMethod&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;print&amp;#34;&lt;/span&gt;)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;前面一个eta求解的是广义线性模型在生存概率p下的响应值，b取的是模型参数，说白了就是给定响应求浓度。回想一下EC50的定义，其实就是先建模后预测一个数值。至于说SE，也是通过模型参数来计算的。所以看来看去整个问题的核心就在于广义线性模型的建立。一般而言LD50因为响应变量为二元的，所以要进行logistic变换，之后如果自变量浓度跨度较大，要进行log变换。&lt;/p&gt;
&lt;p&gt;但在EC50的计算过程中，响应变量通常不是二元的，如果检测器给出的是连续变量，那实际上这个问题就略复杂一点，也就是把响应变量缩小到[0,1]范围内,直接应用线性模型对剂量进行线性回归，而EC50就是线性模型截距跟剂量参数比值的负数，但要是求SE就还得是MASS里面的方法，也就是通过模型参数的SE去做估计。这里用的变换其实就是常见的四参数求解EC50的方法，实际情况中用这个的较多，但本质上就是一个广义线性模型中的logistic回归或probit回归。&lt;/p&gt;
&lt;p&gt;说到底，在ligistic回归中对响应变量是有明确要求为二元服从二项分布的，但实际应用中的响应值却是转化（多为极值范围限定到[0,1]空间）过去的，这就是很多疑惑的起源。其实对于S型曲线还有其他种类的子模型，有的更有针对性，如Hill equation。另外需要注意的是该模型的适用范围，最好先做图看看，如果混合了其他过程如生长，胁迫，模型就需要修改一下。此外，也可以尝试局部回归与非参的方法，这对于估计EC50这一推断值的标准误或方差非常有效，而这却是经验所不能达的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>从因子分析到垃圾邮件</title>
      <link>https://yufree.cn/cn/2013/08/29/fromctod/</link>
      <pubDate>Thu, 29 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2013/08/29/fromctod/</guid>
      <description>&lt;p&gt;想不到真的拖了1年才续写这个系列，因子分析的要义其实就是挖掘背后的信息。例如我给你些综合性经济指标，让你把其中出口的跟内需的部分挖出来你该如何去做？指标都是综合性的，不会说你想抽出一部分就可以。在主成分分析中，其实想得到的是综合性指标去降维而因子分析是想挖掘现有数据中的一部分。例如GDP中出口占多少？但GDP还有其他的组成部分，甚至还有自己独特的一部分，而其他指标也是这样，都有符合意图的一部分的同时也有自己独特的一组，所以算法上就会要求因子分析去分离每个变量中的这两种因素：公共因素与独立因素。而且前面也过于理想化了，你拿到的分离出的新因子可能不代表出口，换言之你可能找不到实际意义，这就悲剧了。但如果你新分出来的因子在解释问题上表现优异，那你可以根据实际问题给他起个名字。所以说因子分析可以用来建立假设，这也是探索分析的一种。但是，也有从假设出发的因子分析，它隶属于结构方程模型（SEM），可以拿来做很复杂的分析，所以社科的人比较喜欢这货。&lt;/p&gt;
&lt;p&gt;假设我能通过统计学的知识得到强有力的背后特征且也是我需要的，这个描述就一定合理吗？不一定，有时候好用的东西讲不出道理，不好用的直观模型反而更受欢迎。这里引一段&lt;a href=&#34;http://cos.name/2012/04/causality4-observational-study-ignorability-and-propensity-score/&#34;&gt;丁鹏&lt;/a&gt;对统计学的描述:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;……统计中所谓的“因果”是“某种”意义的“因果”，即统计学只讨论“原因的结果”，而不讨论“结果的原因”。前者是可以用数据证明或者证伪的；后者是属于科学研究所探索的。用科学哲学家卡尔·波普的话来说，科学知识的积累是“猜想与反驳”的过程：“猜想”结果的原因，再“证伪”原因的结果；如此循环即科学。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以看起来原因的结果似乎都是通过数据探索来完成的，而这个原因并不一定是真的。或者说将假设中结果的原因与数据反映的原因的结果去对应，很有可能是对不上号的，特别是观测而不是实验数据。不过如果从问题出发，简化的问题可能更容易求解，这一系列前面关于降维的讨论核心也在这里：没有物理意义的降维却可能解决实际的计算问题。那么这跟垃圾邮件有什么关系？&lt;/p&gt;
&lt;p&gt;我们描述一个世界跟描述一个问题是不同的，如果使用同一套交流体系，世界要有实体指代，而问题却可能只是信息层面上的。一段描述感受的文字有信息量，但不一定有对应实体，而垃圾邮件就是这一类描述的代表，对我们没用的信息。处理这个问题你不能用铁锨或垃圾桶，你需要对信息进行筛选，而这里就会用到很多统计学上的方法。首先，我们要能够识别出垃圾邮件，但说实话等你识别出来就已经因垃圾邮件耽误时间了，所以这时候就需要把这种活交给程序去做。但你是无法告诉程序那是对那是错的，因为垃圾邮件本就不是一个特定名词，定义不清的事就只能交给经验了。所谓机器学习就是人类偷懒的一个表现。那面对这个问题如何下手呢？&lt;/p&gt;
&lt;p&gt;既然是机器学习，那就告诉他特征吧，例如有促销或办证字样的就是垃圾邮件。这样我们建立一个关键词数据库就OK了，但发送邮件的也不傻，我对关键字加盐或发图片你就没招了吧？这样我们从内容上就不好判断了，那就靠来源，建个黑名单。但垃圾邮件发送者也知道你这招，每次换个邮箱就可以了，反正现在搭或租一个邮箱服务器也不难。那我用白名单，但这样有些重要信件就丢失了。而且每个人对垃圾邮件的定义也不一样，有些人对某些邮件还是感兴趣的，这时就需要在客户端部署个性化邮件筛选器，而这里就用到了&lt;a href=&#34;http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_two.html&#34;&gt;贝叶斯方法&lt;/a&gt;。然而，这场战斗不会这么容易结束，而当垃圾邮件被再抽象为是与否的二元响应变量后，自变量从哪里来，它们又真的跟垃圾与否相关吗？下篇将从垃圾邮件出发，经过一番忽悠，讲到回归诊断。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>赌徒分金问题</title>
      <link>https://yufree.cn/cn/2013/08/21/class-problem0/</link>
      <pubDate>Wed, 21 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2013/08/21/class-problem0/</guid>
      <description>


&lt;p&gt;1494年帕西里奥的著作首先提出，具体描述如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;两个赌徒A、B各下注a元，胜率各为50%，约定先赢S局者可得2a元，当a赢&lt;span class=&#34;math inline&#34;&gt;\(s_1\)&lt;/span&gt;局，b赢&lt;span class=&#34;math inline&#34;&gt;\(s_2\)&lt;/span&gt;局时，比赛因故中断，问题是如何公平分配所有赌注2a元。例如S为6，&lt;span class=&#34;math inline&#34;&gt;\(s_1\)&lt;/span&gt;为5，&lt;span class=&#34;math inline&#34;&gt;\(s_2\)&lt;/span&gt;为2。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;原作者给出的答案的是&lt;span class=&#34;math inline&#34;&gt;\(s_1:s_2\)&lt;/span&gt;分配，也就是说按照5:2去分配&lt;/li&gt;
&lt;li&gt;1556年，塔太格利亚提出若&lt;span class=&#34;math inline&#34;&gt;\(s_1&amp;gt;s_2\)&lt;/span&gt;，那么首先A可以取回自己的赌本a元，然后去走B的&lt;span class=&#34;math inline&#34;&gt;\(\frac{s_1-s_2}{s}*a\)&lt;/span&gt;元，换句话，就是按照&lt;span class=&#34;math inline&#34;&gt;\(\frac{S+s_1-s_2}{S-s_1+s_2}\)&lt;/span&gt; 的比例，也就是3：1去分配&lt;/li&gt;
&lt;li&gt;1603年，法雷斯太尼提出应按照&lt;span class=&#34;math inline&#34;&gt;\(\frac{2S-1+s_1-s_2}{2S-1-s_1+s_2}\)&lt;/span&gt; 的比例，也就是7：4去分配&lt;/li&gt;
&lt;li&gt;1539年，卡丹诺记&lt;span class=&#34;math inline&#34;&gt;\(r=S-s\)&lt;/span&gt;提出按&lt;span class=&#34;math inline&#34;&gt;\(\frac{r_2(r_2+1)}{r_1(r_1+1)}\)&lt;/span&gt; 的比例，也就是10：1去分配&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里要明晰的是期望与概率的关系，前面几种算法除了卡丹诺的都根据的是既成事实也就是经验来判断分配比例，那个最初的胜率50%基本不考虑。但公平的游戏每一局都是独立的而如果比赛继续，最多经过&lt;span class=&#34;math inline&#34;&gt;\(r=r_1+r_2-1\)&lt;/span&gt;局就会结束。如果A获胜，在r局中赢r1局即可，也就是个二项分布的累计概率p，B取胜的概率为(1-p)，注金按这个比例分配就可以，也就是15：1，这个结果是帕斯卡在1654年提出的。在这种算法下，前面的成绩只决定了后面比赛的长短，而获胜的可能性则完全交给了50%的胜率。如果从头考虑这个问题，发展到第七局这个赛况也是从最初的胜率来的，如果以此为起点，前面概率无论多么小都是1了，好比将杨辉三角的顶端砍掉，从一个分支开始考虑问题发展。&lt;/p&gt;
&lt;p&gt;此外，还有一种思路是从最坏可能去思考，也就是比赛最长时的概率是多少，最短又是多少，然后分别计算不同比赛长度的概率，前面的都是当前优势方的概率，累计可得，最后一种情况也就是最长比赛长度对应的是当前劣势方取胜的概率，其实只要算出这个数p，1-p就可以得到答案了。同时我们知道，最后一次必然是劣势方取胜，所以这个概率分布有点类似几何分布，其实这是负二项分布，几何分布只是它的一个特例。那么这种计算方法有什么应用呢？因为计算的是最长距离，可以用在时间估计上，当然二项分布也可以，就是需要绕一下。这个思路是费尔马在与帕斯卡通信时提出的。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;后来这个问题被收到惠更斯的著作《几率的规律》中，这本书更关注于公平赌博并首次提出了期望的概念，同时，书中也关注了在赌博中一系列分金的问题，包括上面问题的多人推广，书末给出了5个问题，解起来技巧性强。下面给出其中一个：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A、B两人按照 ABBAABBAAABB… 方式掷两颗色子，如果A掷出6点或B掷出7点，游戏结束，先掷出为胜，求A、B的胜率。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在一次独立试验中，两颗公平色子出现6点或7点的概率本就不同，A的6点为0.139，B的7点为0.167。如果这是一个公平游戏，那么只能在次序上做文章，也就是A的尝试要有优势才会公平。在这个游戏中，A先掷就是出于这样的考虑，那么这个次序真的公平吗？假定&lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt;为A在(i-1)轮游戏后取胜的概率，可以得到下面的公式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_1=0.139+p_2*(1-0.139)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_2=(1-0.167)*p_3\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_3=(1-0.167)*p_4\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_4=0.139+p_1*(1-0.139)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;联立方程组，可得&lt;span class=&#34;math inline&#34;&gt;\(p_1\)&lt;/span&gt;为0.458，也就是说这个游戏设计上对A就是不利的，这里计算的技巧在于p4也就是3场比赛结束后A获胜的概率的计算灵活的使用了A的两场结果：首场不胜以后胜的概率&lt;span class=&#34;math inline&#34;&gt;\(p_1*(1-0.139)\)&lt;/span&gt;，加上这场胜的概率0.139，而由于赛程序列是无限的，所以对每一场A参与的游戏，这场不赢以后赢的概率可认为是一致的，所以就有了这个解法。看起来解法巧妙不过仔细想想，似乎只用了开始4局就推断出了整体结论，后面的次序不起作用了，而且如果把这个解法用到B获胜概率的计算上就会出现下面的公式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_1=(1-0.139)*p_2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_2=0.167+p_3(1-0.167)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_3=0.167+p_4(1-0.167)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(p_4=(1-0.139)*p_1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样解出的&lt;span class=&#34;math inline&#34;&gt;\(p_1\)&lt;/span&gt;为0.247，对B也不利，其实这个解法巧妙归巧妙，但无限不规律的序列用近似去解应该是有问题的，最后产生悖论式的结果很可能源于对无限这个概念的模糊。&lt;/p&gt;
&lt;p&gt;本文内容大量参考陈希孺老师的《数理统计学简史》，可作为读书笔记看待。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>简析条形图(bar plot)上的误差线</title>
      <link>https://yufree.cn/cn/2013/08/18/error-bar/</link>
      <pubDate>Sun, 18 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2013/08/18/error-bar/</guid>
      <description>


&lt;p&gt;经常会遇到有人问条形图上误差线画什么的问题，有人说标准差（sd），有人说标准误（se），有的直接说置信区间（CI），其实这倒也不是什么大问题，你按什么画就在文章中注明就是了。后来看到JCB上有一篇科普&lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2064100/&#34;&gt;文章&lt;/a&gt;，分析的比较到位，就把里面的干货跳出来翻译一下并对其中的难点进行解读，既是总结也是提高，懒得看过程可直接看文末的规则。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;概念问题&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;22%&#34; /&gt;
&lt;col width=&#34;10%&#34; /&gt;
&lt;col width=&#34;47%&#34; /&gt;
&lt;col width=&#34;19%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;误差线&lt;/th&gt;
&lt;th&gt;种类&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;th&gt;公式&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;范围&lt;/td&gt;
&lt;td&gt;描述性&lt;/td&gt;
&lt;td&gt;极值间距离&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_{max}-x_{min}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;标准差&lt;/td&gt;
&lt;td&gt;描述性&lt;/td&gt;
&lt;td&gt;数据点与均值的平均差异&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(SD =\sqrt{\frac{\sum_{}^{}{(x-\bar{x})}^{2}}{n-1}}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;标准误&lt;/td&gt;
&lt;td&gt;推断性&lt;/td&gt;
&lt;td&gt;重复多次均值的变化&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(SE = \frac{SD}{\sqrt{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;置信区间(95%)&lt;/td&gt;
&lt;td&gt;推断性&lt;/td&gt;
&lt;td&gt;一个有95%信心出现均值的范围&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\pm {t}_{n-1} \times SE\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;标准差&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;标准差是描述性统计里用来表示数据本身均值范围的，两倍标准差范围以外就可能是异常值了，标准差的使用不牵扯均值对比推测，仅仅是描述性的。样本标准差会随着样本数增加接近总体标准差，可用来作为总体标准差的估计，不随样本数变化而变化。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;既然随着样本数增加样本标准差与总体标准差是一致的，怎么又说不随样本数变化？&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;你可以这样理解，总体方差是客观存在的，我们用样本去对总体方差进行估计，具体的算法就是上面那个公式，可用点估计方法自行推导，得到的就是一个接近总体方差的数，这个数当然不会随样本数发生变化了。至于说公式，要记住伴随样本数增大，分子也在增大，所以整体上这个数是不会随样本数发生变化，毕竟只是一个估值无偏性的问题。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;标准误&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;置信区间是针对均值自身而言的，是对均值真实值出现范围的估计，在这一范围内每个点都可能是真值，在置信区间的计算中也会用到标准误。因为涉及均值出现范围，一般就会涉及均值比较与估计的问题，谁比谁大或小，是否显著，这属于推断性统计。置信区间与样本是相关的，越大越不准，越小表示准确度高（样本数自然要大一些）。在使用这类误差线时要考虑自己是否有此意图。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;95%置信区间中样本平均值的地位&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;这个95%的置信度可以用仿真实验来掩饰，谢益辉写的R扩展包animation中conf.int()可以很清楚的演示这一过程：不断从总体中取样并计算95%置信区间，重复n次，最后统计区间包含总体均值的概率你会发现有95%的区间包含的真值。区间包含真值的概率是95%，而不是真值在这个区间里变动，计算出的置信区间可能不包含真值，毕竟置信度为95%。样本的均值是没有固定位置的会跟着取样走，但总体均值不会乱跑，因为不知道，所以用含有置信度的区间估计会更可靠一些。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;标准误与置信区间的区别&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;看公式就知道了，标准误跟着样本数走，样本数越大，标准误越小，很多文章会使用MSE，这代表了均值的标准误。应该说重复越多，这个数就越压缩均值出现的范围，一般而言都是样本数为3，不是因为多了不行，而是说3个样本可以说明问题，有条件当然样本多了好了，结果会更准。置信区间还涉及一个t值的问题，在样本数较少例如3的时候，t值比较大，约为4，样本数多于10，一般就是2左右了。置信区间在一定程度上对样本数不如标准误敏感，给出MSE与样本数是可以推测置信区间的，样本数为3就是4倍MSE，为10就是3倍MSE。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;如何利用置信区间来判断显著性&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;置信区间是统计估计问题，显著性是统计推断问题，这是首先需要分清楚的，然后看下面这个来自原文的图就很清楚了。通过间距判断就可以，这里需要纠正的问题就是一定要间距完全分开才有显著性差异，根据情况来。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/cisig.jpg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;alt text&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://yufree.github.io/blogcn/figure/cisig2.jpg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;alt text&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;样本数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用样本数要注意你是一个样本重复测定n次，还是n个样本测定1次。前者表示同一样本，n实际为1，后者表示独立样本，样本数为n。如果你展示的是一组代表性独立数据，那就不用给出重复测定误差线，这对总体推断没多大意义。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;实验设计中的可重复性究竟指的是什么？&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;一个实验设计三个平行，重复了4次，那么n应该是多少？n为4，因为这4次测定是与你要检验的假说有关的，那三个平行取均值就可以了，作为对数据真实性的保证。保证数据可用与重复性是两个概念，这一点是经常被混淆的。有人做实验重复了10次发现其中有1次结果是可用的就用这组数据去写文章，里面实际只有平行，没有重复。实际的科研是要考虑这10次结果的，当然前提是每次实验所有操作都是一致的，只用一组数据去写文章是碰运气，可以说完全没有重复性，这里每一次重复代表获得一次独立样本。当然这也分情况，根据你的题目自行考虑。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;如何表示重复测量数据？&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;做分析的会比较关注，组内重复测量数据对于组间比较是没有意义的。例如在暴露实验中，同一时间点的数据带有误差线的暴露组与对照组是可比的，但是不同时间点的数据置信区间就没什么意义了，或者你可以用配对t检验差值的方法来考虑同一组内不同时间点测定区别是否显著。一般遇到这个问题都是考虑影响因素的时候，最好每个因素单一考虑，当然你也可以设计正交实验。重复性与独立性是相对的，根据你的实验设计来决定。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;规则&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;使用误差线要注明种类&lt;/li&gt;
&lt;li&gt;要注明样本数n&lt;/li&gt;
&lt;li&gt;误差线与显著性只用在独立重复实验上，代表性的实验结果不应该包含误差线与P值，因为这相当于n=1&lt;/li&gt;
&lt;li&gt;推断性实验的误差线最好使用标准误或置信区间，对于n为3的实验，可直接列出3次的结果，不标注误差线&lt;/li&gt;
&lt;li&gt;95%置信区间表示有95%信心里面有总体的均值，n为3时，标准误的4倍为这个区间&lt;/li&gt;
&lt;li&gt;n为3，两倍标准误不重复覆盖，P &amp;lt; 0.05, 刚好覆盖，P接近0.05；n大于10，间距1倍标准误，P接近0.05，两倍就是0.01&lt;/li&gt;
&lt;li&gt;置信范围表示误差线时，n为3，重叠一臂，P为0.05；重叠半臂，P为0.01&lt;/li&gt;
&lt;li&gt;同一组内的重复实验，标准误与置信区间不能用来表示组内差异&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;原文：Cumming G, Fidler F, Vaux DL.Error bars in experimental biology.J Cell Biol. 2007 Apr 9;177(1):7-11.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>final8-最终版5-修正篇3-我要毕业2-准备延期1.doc</title>
      <link>https://yufree.cn/cn/2013/07/25/data-analysis/</link>
      <pubDate>Thu, 25 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2013/07/25/data-analysis/</guid>
      <description>&lt;h2 id=&#34;问题提出&#34;&gt;问题提出&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/finaldoc.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;谁都不想自己的论文版本乱的一塌糊涂，但现实就是喜欢跟你开玩笑，最终的版本一环套一环很没有条理。&lt;/p&gt;
&lt;h2 id=&#34;问题分析&#34;&gt;问题分析&lt;/h2&gt;
&lt;p&gt;计划不如变化快，这很正常。一篇论文中的图表可能要做多个版本而同一版本可能同时多个人给你修改，修回的论文自已可能还要尝试多种修改方法，等最后想把这些东西凑到一个最终版时肯定不是一个轻松愉快的事。所以一个条理清晰的版本控制过程是很有必要的，其实这个问题在科研人员意识到以前就困扰程序员很久了：一个软件如果拆成多个部分去协作或者同一段代码的反复调试与修改都会让开发者不胜其烦，然后就出现了版本控制软件。换句话讲，如果在管理论文时也使用版本控制，那么会不会更方便一些呢？下面我们来看一下一篇典型理工科论文目录下应该有的东西：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文
&lt;ul&gt;
&lt;li&gt;实验计划 - 实验日志 文本格式存储&lt;/li&gt;
&lt;li&gt;实验原始数据 - excel表格 仪器导出的原始数据 图片&lt;/li&gt;
&lt;li&gt;探索性数据分析 - 根据原始数据所做的分析 如描述性统计图表 相关分析 主成分分析 rmd文档&lt;/li&gt;
&lt;li&gt;最终图表 - 论文中出现的图（需要版本控制）pdf 或 jpg&lt;/li&gt;
&lt;li&gt;实验相关论文 - Endnote library 或 bibtex&lt;/li&gt;
&lt;li&gt;论文草稿（需要版本控制）doc 或 Rnw&lt;/li&gt;
&lt;li&gt;论文终稿 doc 或 pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;仔细想想，如果论文是一盘菜，计划就是菜谱，数据就是原材料，图表是最直观的亮点，最终上桌的那一份跟炒给自己吃的完全不是一个概念。但中间这些半成品是很烦人的，食之无味，弃之可惜，版本控制软件就是来解决这个问题的。&lt;/p&gt;
&lt;h2 id=&#34;问题解决&#34;&gt;问题解决&lt;/h2&gt;
&lt;p&gt;先来看看版本控制软件（以git&amp;amp;github为例）的功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;版本控制&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这是废话，但该类软件围绕的核心就是解决同一文件不同版本的问题。一般来说，一篇论文会有两条主分支，一条是成品分支，用来提交；另一条是草稿分支，记录论文生成的详细步骤，当草稿修改满意，就可以合并到主分支去发布，发布后有问题可以继续从主分支上跳出到草稿分支继续去修改。事实上，版本控制软件会帮助你记录每一次修改的内容并制作快照，这样不论你想要任何时候的版本或修改的内容都可以在版本控制软件的流程图或历史记录中轻松找到。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;备份还原&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其实到了今天还不用诸如dropbox或skydrive或googledrive的研究僧应该比较少见了，再不济也会每天给自己发个邮件备份一下，但用版本控制软件（以分布式的git为例）备份数据还是有优势的：&lt;strong&gt;comment&lt;/strong&gt;。我现在主要用dropbox，自然研究文件夹也放在上面，借助同步功能，我从不担心数据丢失的问题，但其自带的恢复与还原功能对免费用户而言只能恢复30天内的版本，如果你本地dropbox端同时&lt;a href=&#34;http://weizhifeng.net/git-with-dropbox.html&#34;&gt;启用git&lt;/a&gt;，每次comment都会有一个快照备份到云端，这样你就可以任意恢复到之前的版本。与网盘的被动备份不同，使用版本控制软件会让你养成主动总结的习惯，每天push到客户端一次并在comment中记录日志会让工作井井有条。还原的话也很简单，在图形界面中浏览修改记录，找到想要回滚的那个镜像切过去就OK了。相信用过虚拟机的对这种快照备份的便捷应该不陌生。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分支管理&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我并不建议深入研究诸如git，svn详细的命令参数并熟悉命令行操作，他们都有成熟的图形界面，直观易懂。&lt;a href=&#34;http://nyuccl.org/pages/GitTutorial/&#34;&gt;这里&lt;/a&gt;有一个简明的教程，而对于仅是拿来用的人而言你只要了解下面几个命令就OK了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pull 把服务器端内容拖到本地端&lt;/li&gt;
&lt;li&gt;push 把本地端内容推送到服务端&lt;/li&gt;
&lt;li&gt;add 把当前文件夹内文件加入到版本库里&lt;/li&gt;
&lt;li&gt;tag 在某个主干或分支上任意一点加标签，可通过show命令在标签间快速跳转对比&lt;/li&gt;
&lt;li&gt;branch 在主干上开条分支&lt;/li&gt;
&lt;li&gt;head 把工作节点在主干与不同分支间切换&lt;/li&gt;
&lt;li&gt;merge 把分支合并到主干上&lt;/li&gt;
&lt;li&gt;checkout 容易理解，从某个位置把想要的内容取出编辑，修改完了再comment到版本库里&lt;/li&gt;
&lt;li&gt;comment 每次add或modify后都需要comment提交，comment之后才可以选择push到主干或者分支&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其实说白了很简单，主干是按时间走的，分支是平行的。所谓分支管理就是如何把分支的内容裁掉，然后剩下的主干就是你的成品；同时，如果你不想修改主干，只是做一些尝试，可以随时从主干上开个分支单干，也可使随时回滚。这样有了分支主干，版本号什么的就没太有必要了，通过图形界面的节点跳转就可以，还能看到comment。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;协作修改&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其实上面的功能如果你自己有不错的使用习惯都可以简化实现，但一遇到协作问题可能就有问题了，例如张三给你修改了段落a，李四修改了段落b，王五a跟b都修改了，而你只需要最终的一个版本，这时候就麻烦一些了。在git里，你可以通过diff命令去发现不同版本的区别，如果修改的地方不一样，用merge命令可以直接合并这些到一个最终版（例如合并张三与李四的修改）。但如果遇到王五的情况，版本控制软件会提示冲突，这时候就需要使用者自己抉择使用哪个版本，我的建议是先把所有版本都存成分支，然后随机保留其中一个分支到主干，通过diff寻找与另一个分支的不同，把意见合并后修成新版本comment到主干，按照一个一个分支去解决矛盾，当然每解决一个分支都得comment一下到主干，这样最终你只会有一个主干版本，其余的矛盾版本都被记录到版本库的history里去了，可以随时查看修改原因。写到这里你也发现了，对一篇论文提供详尽的修改意见与日志记录就是协作的精髓，因为每个协作者都可以看到并做出新的修改。&lt;/p&gt;
&lt;h2 id=&#34;另外的问题&#34;&gt;另外的问题&lt;/h2&gt;
&lt;p&gt;不过看到这里很多人可能会奇怪，产生这么多的快照会不会很消耗硬盘空间，其实不会，版本控制软件必然对于压缩有很高的要求，如果你熟悉增量备份的概念，那么这种压缩就不会太成问题。&lt;/p&gt;
&lt;p&gt;git可以很好的支持word文档，pdf文档，但如果你的协作者也能接受的话，我更推荐使用RStudio中的knitr包配合自带的git接口与project功能来管理你的论文。通俗的讲，就是用Rnw文档作为草稿文档而输出的pdf作为提交文档，对应的版本管理只针对Rnw文档就可以了，如果你熟悉R与latex排版并用R来给出论文的图表，那么从论文写作到修改，你都可以只控制一个Rnw文档，而且我也建议&lt;a href=&#34;http://blog.sciencenet.cn/blog-41174-447716.html&#34;&gt;可重复研究的思想&lt;/a&gt;能够在科研界迅速普及，这对科技进步是至关重要的。&lt;/p&gt;
&lt;p&gt;写readme文档或日志习惯是很重要的，即使你不打算使用版本管理，那我也建议对特定项目留存日志方便溯源。以写论文为例，实验数据是一方面，文献阅读是另一方面，很多人文献读了不少但一写文章就忘了出处还要重新查询，所以要活用endnote或其他文献管理软件的分类与标签及笔记功能，最好把笔记单独分主题保存为文本文档，摘录出关键点，这样不论你写到哪个部分，想到哪个点就可以在笔记或日志中快速搜索到并溯源。其实这是从资料角度的版本管理，你需要的是看过资料后的感想与启示，这个东西提炼出来资料本身存个档就好了。很多人习惯于在文献上进行标注，只要你回头还查的到也无所谓，但实际情况往往是看过当时很有感触，很快就忘记了，所以把重要的东西提炼成笔记很重要，只要能查到你也没必要去记实际内容。&lt;/p&gt;
&lt;p&gt;关于最开始的计划，最好也有个电子版存档，毕竟ctrl + F就能找到关键点，而且对于一个实验室而言，长久的记录实验计划与数据的电子版本身就是一笔财富，我很少去翻都发黄的早期实验记录本，但有电子版的话我会很乐意去提取想要的信息。&lt;/p&gt;
&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;
&lt;p&gt;其实电脑不是个好东西，他每天占据了你大量的时间，软件就是用来缩短你蹲在屏幕前时间的，如果你能把所有资料整理的井井有条，那么你会有很多时间去发展个人兴趣，回归生活。磨刀不误砍柴工，养成习惯的好处是长久的。&lt;/p&gt;
&lt;h2 id=&#34;扩展阅读&#34;&gt;扩展阅读&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;阮一峰对&lt;a href=&#34;http://www.ruanyifeng.com/blog/2008/12/a_visual_guide_to_version_control.html&#34;&gt;版本控制软件的图解&lt;/a&gt;与&lt;a href=&#34;http://www.ruanyifeng.com/blog/2012/07/git.html&#34;&gt;分支管理策略&lt;/a&gt;及&lt;a href=&#34;http://www.ruanyifeng.com/blog/2012/08/how_to_read_diff.html&#34;&gt;diff命令的详解&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;谢益辉的&lt;a href=&#34;http://yihui.name/knitr/&#34;&gt;knitr包&lt;/a&gt;与对&lt;a href=&#34;http://cos.name/2012/06/reproducible-research-with-knitr/&#34;&gt;可重复研究的推崇&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://johnmacfarlane.net/pandoc/&#34;&gt;Pandoc&lt;/a&gt;各种文件格式互转的利器，适合给老板看doc，自己Rnw或tex&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;阳志平关于&lt;a href=&#34;http://www.yangzhiping.com/tech/github.html&#34;&gt;github的介绍&lt;/a&gt;，我就不去重新发明轮子了&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.phdcomics.com/&#34;&gt;Phdcomic&lt;/a&gt;是一个讲述博士生活的漫画站，除了xkcd与Abstruse Goose外我仍旧关注的漫画&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>最小一乘法的解为什么是中位数？</title>
      <link>https://yufree.cn/cn/2013/03/31/median/</link>
      <pubDate>Sun, 31 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2013/03/31/median/</guid>
      <description>&lt;p&gt;之前写过一篇最小二乘法，为神马不是差的绝对值，当时讨论时对最小一乘的基本思想不太了解，只知道不好寻优。后来想想，数值分析里没有解析解的方程多如牛毛也能用一些方法逼近最优值，想来求解也不困难，本来这一页也就翻过去了。However，最近在统计之都上看到了一篇介绍统计学思想的&lt;a href=&#34;http://www.johnmyleswhite.com/notebook/2013/03/22/modes-medians-and-means-an-unifying-perspective/&#34;&gt;文章&lt;/a&gt;，顿时感觉醍醐灌顶，对回归问题也有了新的认识，摘要如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;统计是一种总结的学问，也就是用少量信息反应大量信息的知识&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;给定一组数让你用一个数描述，最大的矛盾就在于这个数如何处理与原有数据不一致的矛盾，毕竟会丢失信息&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这个过程可以分解为选定描述差异算法与最小化这个差异两步，最后给出的数要具有这两重代表含义&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;选用众数（modes）描述事实上是一个最小零乘问题，也就是二元描述，要么对，要么错&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;选用中位数（medians）描述事实上是一个最小一乘问题&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;选用平均值（means）描述事实上是一个最小二乘问题&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;详细的内容不再赘述，文章通俗易懂，这里要处理的是问题时为什么最小一乘的解是中位数？这一点在文中一笔带过并没有解释，不知道是不是因为这个问题弱爆了，反正我想了半天才搞明白，本文就是对这个思考过程的整理：&lt;/p&gt;
&lt;h1 id=&#34;从最小二乘入手&#34;&gt;从最小二乘入手&lt;/h1&gt;
&lt;p&gt;因为我对最小二乘比较熟，那么我首先要考虑为什么平均值是最小二乘的解，答案呼之欲出。最小二乘法的表示方式可直接求导，结果就是平均值，换句话讲，最小二乘法得到的回归线是要经过均值点的，下面我用一个例子来说明这个问题&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;library&lt;/span&gt;(UsingR)
&lt;span style=&#34;color:#a6e22e&#34;&gt;data&lt;/span&gt;(galton)
lm1 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;lm&lt;/span&gt;(galton&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;child &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; galton&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;parent)
newGalton &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;data.frame&lt;/span&gt;(parent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rep&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;NA&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1e+06&lt;/span&gt;), child &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rep&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;NA&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1e+06&lt;/span&gt;))
newGalton&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;parent &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rnorm&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1e+06&lt;/span&gt;, mean &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mean&lt;/span&gt;(galton&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;parent), sd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sd&lt;/span&gt;(galton&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;parent))
newGalton&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;child &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; lm1&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;coeff[1] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; lm1&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;coeff[2] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; newGalton&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;parent &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rnorm&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1e+06&lt;/span&gt;, sd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sd&lt;/span&gt;(lm1&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;residuals))
&lt;span style=&#34;color:#a6e22e&#34;&gt;smoothScatter&lt;/span&gt;(newGalton&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;parent, newGalton&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;child)
sampleLm &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;vector&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, mode &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;list&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(i in &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;) {
sampleGalton &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; newGalton&lt;span style=&#34;color:#a6e22e&#34;&gt;[sample&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1e+06&lt;/span&gt;, size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;, replace &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; F), ]
sampleLm[[i]] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;lm&lt;/span&gt;(sampleGalton&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;child &lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt; sampleGalton&lt;span style=&#34;color:#f92672&#34;&gt;$&lt;/span&gt;parent)
}
&lt;span style=&#34;color:#a6e22e&#34;&gt;for &lt;/span&gt;(i in &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;) {
    &lt;span style=&#34;color:#a6e22e&#34;&gt;abline&lt;/span&gt;(sampleLm[[i]], lwd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, lty &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)    
}
&lt;span style=&#34;color:#a6e22e&#34;&gt;abline&lt;/span&gt;(lm1, col &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;red&amp;#34;&lt;/span&gt;, lwd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://yufree.github.io/blogcn/figure/demo.png&#34; alt=&#34;plot of chunk demo&#34;&gt;&lt;/p&gt;
&lt;p&gt;注释我就不写了，这是C站数据分析课讲解最小二乘的一个例子，当我们用预先回归出的模型随机生成一组数据，然后从里面反复重采样100次，每一次得到一个线性模型，把所有的模型叠加，最后我们看到的图形最为稳健的部分就是平均值所在地。也就是说，最小二乘法的内在含义就是平均值回归，用均值代表整体。同时你会发现，这个解是唯一不变的，因为均值的求法不会得到两个答案。那么从这里出发能不能解决最小一乘的问题呢？先别急，先看一个答案不唯一的表示法。&lt;/p&gt;
&lt;h1 id=&#34;回顾最小零乘&#34;&gt;回顾最小零乘&lt;/h1&gt;
&lt;p&gt;最小零乘的本质就是找一个数，跟数据中一致就是0，不一致就是1。从数据集的角度看就是找到一个数，使一致的1累计最少，那答案脱口而出：众数。但你应该很快就反应过来了，众数可能不唯一，所以最小零乘给出的答案不唯一，这对回归而言是灾难性的，因为不唯一的描述是不确定不可重复的。OK，在这个基础上，我们可以讨论最小一乘了。&lt;/p&gt;
&lt;h1 id=&#34;最小一乘与中位数&#34;&gt;最小一乘与中位数&lt;/h1&gt;
&lt;p&gt;从刚才的论述中，我希望读者可以明白两件事&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;回归事实上就是一种简化版的总结，算法是用来支持总结的，不必须唯一&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为了求解方便，我们倾向于使用表述上分歧小且求解方法稳定的算法，平均值或者说最小二乘很符合这一点&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那么为什么要提最小一乘呢？都用最小二乘不就完了吗？注意第一点，最小二乘仅仅是一种算法，他没有太多的特殊性，也并不完美，一个异常值就足以毁掉一组看似不错的数据，想想统计学里著名的安斯库姆四重奏,这个算法算不上稳健的。而反观中位数却往往可以规避异常值问题，那么这又是如何实现的呢？&lt;/p&gt;
&lt;p&gt;我们从最基础的问题开始构建，考虑5个点1,2,3,4,10, 如果找一个数代表这三个数你会选择什么？众数的话哪个都可以，均值的话是4，中位数是3。其实哪个都可以，但如果有个实际背景的话我想更多人会觉得3差不多，有点代表性，毕竟对于10有点信心不足。先不管这些，让我们想想最小一乘算法是如何实现的，如果这5个数分布在一个数轴上，找一个数使其绝对值最小怎么找？很简单分下组，最大值与最小值一组，第二大与第二小一组，以此类推。如果存在一个数满足到所有数的距离绝对值最小，那么它一定位于每一组之间，因为只有这样才能保证其到两端距离最短，这样到了最中间的那个数就直接考虑去中间那个数，这样距离为零，差的绝对值的和自然最小了。同时我们会发现如果我们两个两个的加入数，这个算法也是稳定的，也就是可以推广，这样异常值不过是其中的一组数，求解的结果对特定一组数据并不敏感，这就保证了稳健性。OK，那这个数是什么呢？没错，中位数。&lt;/p&gt;
&lt;p&gt;说到这里你会觉得少点什么，没错，这好像只对奇数管用，偶数个数怎么办？其实在处理偶数时，我们最早学到的中位数概念是一种误导，没必要取均值。放到数轴上看，在最中央的两个数之前的任何一个数都可以最为最小一乘的解，那么这就是开始我说到的问题了，寻优结果不唯一。目前可以使用的最小一乘算法应该都无法规避这个问题，但相比众数，起码可以给出一个范围了，同时我们也看到，这个解法具备一定的可编程性，所以也可以拿来用。不过正如谢益辉在其硕士论文中所提到的，对中位数敏感一样可以造成回归上信息的缺失，所以也请把最小一乘看成一个普通青年的算法，最小二乘也是，至于众数……最好别用。而这些在建模上都是要反复考虑才能去选择的，不要盲目追新。&lt;/p&gt;
&lt;p&gt;OK，如果你能理解数轴，最小一乘的算法及其与中位数的关系对你已经不陌生了，如何实现的问题就交给学统计的来做吧，我们只需要知道调用相关函数就可以了。如果你觉得最小一乘是不是只是一个求解特例，那恭喜你，直觉不错，最小一乘是分位数回归的特例，后面还有很多新知识。&lt;/p&gt;
&lt;p&gt;从特殊到一般，从一般到很一般，这个过程的乐趣不是一般的特殊。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>最小二乘法？为神马不是差的绝对值 </title>
      <link>https://yufree.cn/cn/2012/10/12/mean/</link>
      <pubDate>Fri, 12 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2012/10/12/mean/</guid>
      <description>&lt;p&gt;最近搞了点机器学习的东西，因为0基础所以老老实实上了斯坦福的公开课，这期间解决了我一直想不清楚的一个问题：最小二乘法的统计学解释。&lt;/p&gt;
&lt;p&gt;当我们遇到一个原理的时候，实用主义者会认为可以应用就可以了，但总有些吃饱了没事干的人去问个为什么，他们连显而易见的常识都不放过，更别说想最小二乘法这种看起来并不那么理所当然的东西。对方法的本质进行探索是很重要的，这直接关系方法的泛化与推演程度。而本质又是什么？想来有两种：公理与统计学规律。前者的代表就是几何学，基本是个纯演绎体系，后者的代表就是一切说不明白但又很显然的道理，而为了让这个显然更精准和科学一些，我们需要一些统计学的知识，算是个归纳体系。逻辑上演绎而不是归纳体系更符合科学的严谨性，这也是证伪的核心，但这不是说统计得到的规律意义不大，相反，当今多数研究的科学性更多是由统计学意义而不是反例来支配的，就连黎曼猜想这种大手笔的数学证明过程也不乏统计方法的应用。但说到底本质的东西就是无法再从这里往前推的知识或原因，我们周知的世界就是由这些东西根本支配。但这里没有说所有的事物都能找到一个说的清楚的原因，强加因果是很荒谬的，现在很多事件过度强调找原因事实上很幼稚，原因不都是一下就说得清的。好了，不废话了，回到那个最直接的问题&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;数据拟合中，为什么要让模型的预测数据与实际数据之差的平方而不是绝对值和最小来优化模型参数？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;首先，我们来考虑数据拟合的实际状况：当我们寻找模型来拟合数据时，偏差是不可避免的存在的。对一个拟合良好的模型而言，这个偏差整体应该是符合正态分布的，这里可能你会问为什么是正态分布？其实这就是个假设，你用什么分布就要在后续的计算中运用相应分布的概率密度函数，而偏差这种东西符合什么分布最靠谱呢？如果你喜欢扔硬币的话就知道硬币一面出现的概率就是0.5，你扔多次某一面出现的次数的概率就是个二项分布，这是离散的，你扔硬币的次数趋向正无穷再来看这个分布就是正态分布。这之间的证明过程涉及斯特林公式神马的，其实这个推导是在一定条件下完成的，想了解的自行放狗。如果你认同这种0.5概率的扔硬币，那么可以假想理想的偏差也是跟硬币某一面出现的概率分布差不多就行了，至于再深入考虑为什么，那就基本是形而上学的东西了，自便。&lt;/p&gt;
&lt;p&gt;现在，我们已经知道偏差符合正态分布，那么下一步就是理解另外一种函数——极大似然函数。在模型拟合中，极大似然函数的本质就是让我们用来拟合数据的模型与每一个数据点的更为相符，这就要求偏差的大小应该是基本一致，或者说符合正态分布，那么偏差大小基本一致与不一致怎么区别呢？这里我们用偏差出现的概率相乘的大小来表示。因为概率大小都在0到1之间并符合期望为x的正态分布，两个偏差值越接近中心期望x，乘积越大。极大似然函数就是用来表示这一关系的，当然在这里联乘的形式可以取对数改为概率求和，如果你还有印象的话，正态分布的概率密度函数是欧拉数的幂函数形式，而幂中有一个负号有一个平方，平方就是偏差的平方，负号则将原来求最大值变成了求最小值，这时候反过来看这个极大似然函数的求解其实就是最小二乘法。&lt;/p&gt;
&lt;p&gt;本质上来说，模型拟合都可以用极大似然函数求最值来表示，如果你能想办法把你想键的模型转为一个寻优问题，那就可以通过求导等数学方法来解决了，但千万要注意：并不是所有的模型都可以有最优解，有些只有局部最优，有些则压根找不到，需要足够聪明的人转为对偶的凸函数或其他可解的问题才能寻优，此外，数学上证明了的NP-hard问题就别尝试了，更不要尝试NPC问题了。&lt;/p&gt;
&lt;p&gt;好了，现在我大概说明白了为什么用平方和了，本质上就是正态分布的概率密度函数所致，那么为什么不是绝对值的和呢？简单说绝对值的和无法转化为一个可解的寻优问题，既然无法寻优如何得到恰当的参数估计呢？就这么简单。&lt;/p&gt;
&lt;p&gt;关于最小二乘，刘未鹏在讲解&lt;a href=&#34;http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/&#34;&gt;贝叶斯定律&lt;/a&gt;时也提到过，他从贝叶斯定律而不是极大似然的角度给出了与本文基本一致的解释,可谓殊途同归。&lt;/p&gt;
&lt;p&gt;OK，到此结束，概率论就是个坑啊！&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>所谓数据处理软件</title>
      <link>https://yufree.cn/cn/2012/09/08/data-analysis-software/</link>
      <pubDate>Sat, 08 Sep 2012 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2012/09/08/data-analysis-software/</guid>
      <description>


&lt;p&gt;要是猜得不错，国内本科生最先接触的图表可能就是柱形图、折线图、饼图、散点图，稍微进一步就是线性回归，明白&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;超过几个九就可以，多数大学毕业生的数据处理水平过个n年也还是停留在这个水平。到了研究生阶段就更乐呵了，不少大仙都是把excel当原始数据收集软件用（当然不少分析软件兼容excel导入也是个问题），做个线性回归就上origin、sigmaplot……统计分析更是到处求spss、sas的破解版……个别人张口闭口matlab，却也只是做做回归……我要是excel的设计者，估计先被气死再被气活然后思考下人生再来个自我了断。&lt;/p&gt;
&lt;p&gt;先列举下excel可以做的东西，放心都不给教程，总不能让F1键当摆设吧。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线性回归&lt;/li&gt;
&lt;li&gt;非线性回归&lt;/li&gt;
&lt;li&gt;多元线性回归&lt;/li&gt;
&lt;li&gt;描述性统计&lt;/li&gt;
&lt;li&gt;单因素方差分析&lt;/li&gt;
&lt;li&gt;双因素方差分析&lt;/li&gt;
&lt;li&gt;假设检验（F、t、z检验）&lt;/li&gt;
&lt;li&gt;抽样&lt;/li&gt;
&lt;li&gt;生成随机数&lt;/li&gt;
&lt;li&gt;傅立叶分析&lt;/li&gt;
&lt;li&gt;规划求解（包括线性规划、非线性GRG、演化）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果你的统计学水平能达到考研数一的水平的话，那么你就会发现excel完全可以配合你的知识来使用，够用。当然你说excel是收费的，没错，但多数高校都采购过excel，这样即便你在文章中注明使用excel也不存在问题，倒是上面提到的那些破解版不见得满大街都有卖的，随意在文中注明风险不小。要明确一点，多数数据处理软件不是免费的，但经典的算法都是公开的，所以有时候可以说我在数据处理中用了什么算法而绕开软件，其实诸如Scilab、R、freemat……等数据处理软件都是免费的，当然不是每个人都习惯那些操作界面，但前面这句纯粹胡说八道，对于对数据没概念的人学习任何一个软件都是一样的，那些看似高端的话等你实际把玩过这些软件再说吧。如我一般的穷学生，别人写好了软件让你用还懒的看quick start之类的小教程那不是高端，是虚伪。多数非计算学科的科研工作中涉及的数据处理量根本就是毛毛雨，本来用的次数就有限还挑三拣四是很无聊的，更不用说多数人的数据处理模式根本就是照着文献搬的，美其名曰工具就是这么用，根本没有基本的数据分析意识，多数的数据分析都是为了寻找原始数据或现象中的规律或验证自己的假说。&lt;/p&gt;
&lt;p&gt;根据假设选工具，根据结果给结论，这是最基本的，文献大都默认了这一点，所有算法都有自己的适用范围，不加区别的乱用，甚至文章中都不提使用原因是很多研究工作的通病，因为有前人的文献，所以很多时候审稿人也会默认这样是可行的，但可行不代表无疑。有个思想实验叫中文房间，其中一个延伸版就是外面的人给房间中的人递中文纸条，房间里的人完全不懂中文但有一本中文百科大全，他只需要对着长得差不多的字去看这个条目对应的可回答的项目并抄下来然后传递出去就可以了，这时候外面的人自以为跟房间里交流的不错，事实上里面的那个是个水货，完全不懂中文。在今天的话题里，中文纸条就是数据处理的过程，房间里就是不少研究生的真实写照，房间外就是审稿人，那本百科全书就是文献。其实在国内，有不少审稿人自己也是搬着本百科全书在理解。也许你会说这只是一个小部分，我只要说明白了问题不必解决每个细节的合理性，但这样就完全隐藏了你自己的真实水平，出来混都是要还的，这绝不是危言耸听。我最反感两个人实际都不懂却非要讨论一个专业话题，要么就是互相吹捧，要么就是互相谩骂，不幸的是，国人在这件事上乐此不疲，很少听到有营养的讨论。&lt;/p&gt;
&lt;p&gt;此外，也就人说excel图不漂亮，其实图表的格式都是可以自定义的，所谓的不漂亮只是默认格式不漂亮。又有人说了，默认不漂亮就是大问题，我没必要去伺候一个工具。事实上学精一个工具要强于泛泛学习多个工具，软件的设计大同小异，学通一个，一通百通。况且所谓的默认格式漂亮也只是假象，更真实的情况可能是别人只教你这样用且告诉你这样出图写文章障碍少。为了方便，很多人放弃了更重要的东西——质疑。这样做不被质疑，但反过来想，被质疑才是知识进步的方式，总是不质疑也就总是做些边边角角的研究，创新性从一开始就被实用主义打败了。结果似乎是皆大欢喜：文章发了，职称升了，工资涨了。但这样做研究跟那些白领、公务员本质上没什么不同——学识不长，人脉关系网疯涨，社交水平不断提高，自我感觉层次长了不少。&lt;/p&gt;
&lt;p&gt;坦言之，我不反对实用主义，也不反对掌握方法比内容更重要的观点。但实用主义不能是为达目的不论手段的借口，信息时代方法固然重要，但没有内容的骨架也是毫无价值，你可以搞个智能手机天天搜索，但这不是全部。kk在《失控》里说机器在向人进化的同时人也在向着机器进化，但人永远不会成为当前的机器，看不清方向很正常，但总是看得清方向同样可怕。并不是分清了轻重缓急就一定要把重的与急得做完了而放弃轻的缓的，要知道，进化并没让我们比两万年前的判断能力改变多少，我们会犯错，更重要的是，犯错也许创造未来。&lt;/p&gt;
&lt;p&gt;最后附上wiki上一个数据处理软件的对比&lt;a href=&#34;http://en.wikipedia.org/wiki/Comparison_of_numerical_analysis_software&#34;&gt;列表&lt;/a&gt;，免费的很多哦亲！&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>从高维诅咒到因子分析</title>
      <link>https://yufree.cn/cn/2012/08/08/frombtoc/</link>
      <pubDate>Wed, 08 Aug 2012 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2012/08/08/frombtoc/</guid>
      <description>&lt;p&gt;调用无尽的参数来描述一个物体是有必要的吗？可以从另一个角度来看看待这个问题：我有一个苹果，我有必要为了说明它是个苹果而去学习植物学甚至是遗传组学来断定这货是个苹果吗？&lt;/p&gt;
&lt;p&gt;当然没有必要，这倒不是说你对这个苹果的认识不够深刻，只是在辨认苹果这个概念所对应的实体时我们会套用一个类似柏拉图理型世界的玩意来识别并忽视其所谓不完美的部分来认定，这种抽象的概念在柏拉图看来是完美且合理的。且不论这里面的理想成分有多少是主观的，这里需要注意的是其实所谓“完美”的概念所描绘的东西现实世界可能并不存在。好了，在这里我们会遇到一个问题，这样的描述其实是增加了一个维度，而这个维度对于事物的描绘可能是唯一的。因此，我只依靠这个维度去判断区别物体不是更好吗？&lt;/p&gt;
&lt;p&gt;这里停一下，前文明明说用无尽的参数描述是有必要的，怎么又回到没必要了呢？因为有必要的前文已经说过了，高维诅咒可以帮助我们设置密码，此外前文留的尾巴的真正意义是“无尽”这个词，从0到1间有多少有理数？这个数会比1到2之间的多还是少？比0.5到1之间的呢？好了，不折腾了，这个问题思考下去就是希尔伯特那个第一问题——连续统假设。这是个不可从内部证明的东西，其实我倒希望很多这样的问题包括黎曼猜想，如果能用对角线方法证明这个问题不可证明就好了，至少不像费马定理那样最后的证明搞得非专业的俺一点都看不懂。这个“无尽”问题是个无底洞，很多经典的悖论就是在无尽上挖的坑，例如芝诺的乌龟悖论，其实说谎者悖论的本质也是涉及了无穷推衍甚至变成循环推衍而让人心驰神往，但无尽的推衍不代表无尽的时间，谜题的设置有时候就是让人感觉到不可能而不去思考的，这些问题存在了几千年但不影响人类对世界的认识，为什么？因为这不妨碍采集捕猎男欢女爱？这是实用主义的观点但却很真实：这个世界就是带着问题或者说谜题展现到每个人面前的，而我们在这方面的认知水平与2万年前区别不大，都是得过且过。为什么呢？我们的认知的程度更多与生存下去所需的知识水平相对应，那种思考无穷的东西换不来饭吃，可能有那么一两个这样的基因都被自然选择抹掉了，但在现在的生活条件下会不会再现呢？谁知道呢，未来无限可能。&lt;/p&gt;
&lt;p&gt;好了，前面的尾巴收拾掉了，来谈谈我们现有的认知水平。我们的认识如果真如柏拉图所言是看到一个个完美理型的影子的话就会有个问题：何为完美？几何或许给了我们一些答案，到同一点距离相同点的轨迹所构成的圆？平行的两条线？等边三角形？或是物理上的光速与绝对零度？不知道。这是个纯主观的东西，让盲人去想象彩虹是不现实的，让别人去接受你的完美也很困难。但每个人都可以构建自己的完美，这就是亚里士多德的观点：归纳与演绎。我们认识世界本质上是一种抽象提取特征的过程，我们不需要知道太多的细节，把握特征就可以整理思路，归类事物，也就是说我们的唯一性建立在抽象的基础上，而抽象过程不会是随机的，其目的性可能就是实用或者说可交流。事实上，知识的认可要比其本身更有意义，那要保证每个人都认可你的认识，我们要拥有同一套密码本，那就是所谓的知识。这些知识的产生过程既要不依赖于人而存在又要可保证可被交流，这如何实现？这问题可能我们自己永远答不上来，我们的大脑进化到今天本身就回答了这个问题，至于这是怎样的一个过程，我们能做的就只有像寻找背景辐射那样搜寻我们认识事物共通的一些线索。在这里从维度角度上看，我想讨论的是维度的降低过程。&lt;/p&gt;
&lt;p&gt;当两个相互陌生的人看到一块石头，他们如何就这个石头进行交流呢？首先得告诉对方看到了什么，这里语言就为我们提供了便利，一些既定的概念就可以用来交流，那么这些概念如何抽象出来的呢？我们的感官可能是罪魁祸首，例如颜色、质地、重量、口味、气味……通过一些感官上的综合我们可以形成一个特征谱，而这个特征谱可能是唯一的，因此为了交流我们会对特征谱进行模糊化，保留我们所认为最根本的特征来传递这个信息，这个过程就是一个降维的过程。而当要描述物体的唯一性时也很简单，将维度升上去，加几个区别的特性就好。有了抽象我们可以归类物体，有了具象我们可以区别物体，这也许就是交流的起点。上篇文章实际只是说了具象的过程，今天的主角是抽象过程。所谓因子分析的东西就是这样一个抽象工具。&lt;/p&gt;
&lt;p&gt;但其实真正熟悉因子分析的人可能察觉到我在这里用因子分析不太合适，因为因子分析是从显性变量中提取隐性变量的过程，而且它能不能降维还取决于变量是否独立之类的假设。的确，如果变量间独立那就谈不上降维而仅仅是个信息处理能力的问题了，但所谓变量独立这种事在现实世界恐怕并不多见，而这也是抽象过程的一部分前提。当然，其实面对因子分析更麻烦的地方在与其与主成分分析经常混在一起讨论，这里就不涉及这两种过程了，但值得明确的是主成分分析属于一种描述性统计，属于让数据自己说话而因子分析则事先有自己的一个假设来进行验证，具体区别可参考wiki上的解释：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The differences between principal components analysis and factor analysis are further illustrated by Suhr (2009):
PCA results in principal components that account for a maximal amount of variance for observed variables; FA account for common variance in the data.
PCA inserts ones on the diagonals of the correlation matrix; FA adjusts the diagonals of the correlation matrix with the unique factors.
PCA minimizes the sum of squared perpendicular distance to the component axis; FA estimates factors which influence responses on observed variables.
The component scores in PCA represent a linear combination of the observed variables weighted by eigenvectors; the observed variables in FA are linear combinations of the underlying and unique factors.
In PCA, the components yielded are uninterpretable, i.e. they do not represent underlying ‘constructs’; in FA, the underlying constructs can be labeled and readily interpreted, given an accurate model specification.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其实关于因子分析再深了我也讲不了了，只是说这是一种统计学工具，而这种工具在我看来是很有说服力的，同时我也认为统计学工具对于了解世界认识世界是极为关键的，至少这种工具可以让我们从纯粹的自然选择中逃离出来一探自我与所谓完美的东西。但所谓统计似乎也逃不出先验东西的存在，而且似乎正是有了先验的东西统计学变的更实用了，下一篇从因子分析到垃圾邮件就会讨论这个问题以及解决因子分析这个尾巴，至于时间吗，可能遥遥无期吧^.^&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>从密码强度到高维诅咒</title>
      <link>https://yufree.cn/cn/2012/07/09/fromatob/</link>
      <pubDate>Mon, 09 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2012/07/09/fromatob/</guid>
      <description>&lt;p&gt;很多网站在设置密码时会提醒你使用大小写与特殊字符以提高密码强度，这样做对吗？对了一半。&lt;/p&gt;
&lt;p&gt;首先，这句话对是因为它提示很多人避免使用简单密码。即便出现了CSDN密码泄漏问题，很多人依然固执的使用生日、电话号码、邮箱、姓名甚至是888888或123456来作为密码。这种密码很容易被破解不是因为它使用的字符简单，而是它本身有规律。就像福尔摩斯利用词频破解跳舞小人的密码一样，本身有规律的字符是可以根据经验来猜测的，这样就会有人将这些规律总结，最后通过一个“字典”提供给想破解密码的人，而破解者会优先考虑使用这个字典中的数据进行猜测，这样就会极大的提高命中率，应该说这是经验性的但却很实用。也许有人会说怎样寻找这种规律，很简单，你去下载一个CSDN泄漏的密码包去归纳总结下就知道了。但就算没有泄漏，想想自己经常用的密码就知道了。但多数负责任的网站通过脚本会直接拒绝你设置密码时使用简单密码，方法与破解一致，写个脚本检查你的密码是否与“字典”中简单密码是否一致就可以，当然“字典”是由无数本的。&lt;/p&gt;
&lt;p&gt;其次，这句话不对的地方在于对于一个不用字典而使用暴力破解方式的人而言，666666跟aB#(d3  没有任何区别。每一个字符的可能性跑不出你的键盘上的键盘数×2（因为有上档键），也就是不超过100个，那么6位密码的可能性就是$$100^6$$种可能，这对于现代计算机而言就是几毫秒的事，但密码位数多了就不一样了，在微软的&lt;a href=&#34;https://www.microsoft.com/zh-cn/security/pc-security/password-checker.aspx&#34;&gt;密码检查器&lt;/a&gt;中你不断输入1就会发现密码强度开始虽然是弱的，但最终会变为best，也就是说对于暴力破解，一个简单的长密码要比复杂的短密码安全的多。顺便说一下微软也提供了所谓强密码的&lt;a href=&#34;http://www.microsoft.com/zh-cn/security/online-privacy/passwords-create.aspx&#34;&gt;建立原则&lt;/a&gt;，感兴趣看一下。&lt;/p&gt;
&lt;p&gt;最后，今天真正想说的就是长密码。现在我们重新来看一下长密码安全的原因，很简单，密码必须要由字符构成，而字符的种类与字符串的长度是密码的基本属性。把这一过程再抽象一下就是我有一个集合，集合里有若干元素，每个元素有若干可能。好了，有点数学常识的就会意识到了，这不就是一个多维向量吗。而密码的安全性无过于就是多维向量的唯一性不被猜测出而已，维度数就是字符长度，每一维的属性值就是字符的种类。那么结果呢？结果就是维度越高唯一性就会越强。换句话讲，当你描述一个物体的特性越多时，这个物体就越不容易被认错。沿着这个思路走，在笛卡尔三维空间里，我们知道一个球的空间坐标后或许就可以确定这个物体是什么了，但我们并不能很好的“描述”这个球，或者说多数情况下“唯一性”只是我们主要用来区别其他物体的一个模糊概念，但从不同角度说的话会有很多判别标准，或者说为了描述一个概念，我们可以调用无数的分类特征，但这有必要吗？&lt;/p&gt;
&lt;p&gt;没有，这就是今天要说的&lt;a href=&#34;http://en.wikipedia.org/wiki/Curse_of_dimensionality&#34;&gt;高维诅咒&lt;/a&gt; 。因为通常多数人不理会那个链接而真正理会那个链接的人又不屑于看我写的东西，我还是解释一下这个基本概念，这个词是“用来描述当（数学）空间维度增加时，分析和组织高维空间（通常有成百上千维），因体积指数增加而遇到各种问题场景”。还不够直观？打比方我去采样测定污染物，在某个10平方米的地方每隔1米取样也就100个样但如果这是个10维空间，我要在除了长、宽以外的维度的8个维度上取样就悲剧了，需要取10的10次方个点，这是什么意思呢？如果污染物只存在某100个点上那在二维空间里我都能测到还可以画一个浓度分布图（这其实多了一个维度）。但如果考虑10维空间，那这些点分布实在太稀疏了，根本看不出什么有规律的分布来，这就是高维诅咒的现象。也许你说这没什么不好，没有两片相同的树叶吗，但当我们真正去了解这个世界时，我们多数情况下是依赖自己的感觉与前人流传的知识，而所谓的知识大都来自于一种总结，这种归纳式的理论有利于我们将其内容应用到以后的生活中（这当然是一种实用主义的说法而已），而感觉也是在总结抽象一些鲜活的实例为知识而已。这样，如果我们打算去了解一个或一类事物并保证从中抽象出的知识是可重复（也就是可实用，当然我也知道罗素的养鸡场悖论，但自从哥德尔之后我们应该可以活得更豁达一些了）的，我们要收集足够的样本，但过于详细的描述却使这项工作变得异常复杂并且远离直观且根本就达不到统计规律所需要的样本数，那这个现象就没用了吗？&lt;/p&gt;
&lt;p&gt;不会，高维诅咒对于密码设置很有意义，正因为现在多数人都会在设置密码时使用大小写或特殊字符，所以对于字典不全的破解者而言根本不用区分，把每一个字符当成一个维度，把可能的变化当成维度上的数去试就好了，在大型计算机的帮助下很快就可以了，但维度高了就不一样了，计算量是级数级的增长，当然也许对未来的计算机也不算什么了。但维度过高对于网站也是一种负担，因为存储数据也是要占用服务器资源的，而且多数人也不会去考虑搞一个很长的密码（当年DES密码设计时就有类似考虑而限制了密钥长度）。其实，这些都只是需求对技术的妥协，也许有一天，这一切都不是问题。但是，无论如何，只要我们需要交流或信息流通，加密与解密就必须跟上要求，否则我们就得适应三体人的生活。&lt;/p&gt;
&lt;p&gt;那么倒数第二个问题的答案会不会是有必要呢？下一篇打算讨论下这个问题——从高维诅咒到因子分析。希望不会像诺莫图那样一停停一年到现在也没谱。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
