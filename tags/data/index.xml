<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data on Miao Yu | 于淼 </title>
    <link>https://yufree.cn/tags/data/</link>
    <description>Recent content in data on Miao Yu | 于淼 </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://yufree.cn/tags/data/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>自定义大学排行榜</title>
      <link>https://yufree.cn/cn/2023/01/17/custom-ranking-us/</link>
      <pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2023/01/17/custom-ranking-us/</guid>
      <description>&lt;h2 id=&#34;比赛介绍&#34;&gt;比赛介绍&lt;/h2&gt;
&lt;p&gt;这是和鲸社区与 COS 统计之都联合举办的云讲堂实战系列活动第一场，本活动旨在从实际问题出发训练某综合性数据分析技能，优秀选手将通过统计之都&lt;a href=&#34;https://space.bilibili.com/22035559/channel/collectiondetail?sid=70922&amp;amp;ctype=0&#34;&gt;云讲堂&lt;/a&gt;报告展示自己的成果。第一场的实际任务是自定义大学排行榜，主要训练指标生成、可视化与可重复性报告的撰写等技能。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cosx.org/&#34;&gt;COS 统计之都&lt;/a&gt; 是一个旨在推广与应用统计学知识的网站和社区。由谢益辉创办，现任理事会主席为常象宇，现由世界各地的众多志愿者共同管理维护。&lt;/p&gt;
&lt;h2 id=&#34;背景介绍&#34;&gt;&lt;strong&gt;背景介绍&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;对数据科学社区而言，生成综合指标进行排名其实是经常会遇到的问题，例如什么样的文章要靠前推荐？什么商品更适合某类客户需求？而在研究工作中，很多时候也需要设计综合指标来辅助决策。例如，空气质量指数就是一个涵盖六种污染物浓度分指数的综合指标，而其公布的数值其实是对六个指标取最大值，也就是说，如果你看到空气质量指数同样是100的两个城市，一个可能是因为颗粒物超标，另一个可能是臭氧超标，这里的设计原则是木桶原理，取最差的那一个。社会科学中很多指标的生成往往也暗含了不同的选择标准或指标权重，例如消费者物价指数CPI的计算就会涉及多种商品的价格加权，美国CPI里食品比重不到8%而法国则在15%左右。&lt;strong&gt;这些加权或设计原理往往需要相关专业知识来背书，因此在常规的数据科学训练里很少被提及，但实战中却又特别需要&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;要想训练综合指标的设计与生成，最简单的方法就是通过真实数据来研究一个通用主题。大学排行就是这样一个适合你练手的项目&lt;/strong&gt;，任何一所大学都能找到自己排名靠前的某个排行榜来作为招生的宣传，考生与家长也经常喜欢用排名来印证自己的选择是合理的。同时，好或者不好其实是一个很难定义的东西，国际上较为公认的大学排行有四个：QS、USNews、泰晤士报还有上海软科的世界大学排名，他们一般会构建一些量化子指标分别打分，最后算总和进行排名。&lt;/p&gt;
&lt;p&gt;但对个体而言，其实看重的点并不一样，例如大多数排名会算论文发表量，但如果学生不喜欢做学术，带这个指标的排名其实对他意义不大；从纯结果看，学生毕业后 10 年的年均收入对于大多数打工人可能很重要，但排名里可能完全不考虑。此外，大学排行里其实也掺杂了很多商业或可优化因素，例如泰晤士报排名很多年来都是牛津排第一，而 USNews 排名美国大学时会考虑录取率，这就搞的很多有钱的私校在申请季大量投广告来拉低录取率。&lt;/p&gt;
&lt;p&gt;在奥巴马时代，美国政府意识到排名的乱象已经形成了误导，于是教育部就出钱收集了大量的大学相关数据免费公开到网上，这样感兴趣的家长和学生可以根据自己对未来的规划来排名申请大学。这个项目运行到现在不说是大获成功吧，也可以说门可罗雀。虽然数据每年都更新且全部透明公开带 API 跟文档，但利用率其实不高。 因此，可以利用这部分数据来训练综合指标的设计与生成，打造一个你认为合理的美国大学排行榜。&lt;/p&gt;
&lt;p&gt;首先，这是个竞赛，是个有奖竞赛，获奖者会受邀到统计之都云讲堂做在线报告。&lt;/p&gt;
&lt;p&gt;其次，这个竞赛需要的数据量偏大，但和鲸社区提供了计算资源，可以直接在线进行数据分析。&lt;/p&gt;
&lt;p&gt;再次，这个竞赛可以让你了解下美国教育系统，特别是联邦学生贷款的现状，可以从中发现一些问题，如果参赛者有兴趣，可以将其投稿给专业期刊作为自己的学术成果发表或作为研究项目列入简历。&lt;/p&gt;
&lt;p&gt;这里给一个简单思路示例。知名数据分析网站&lt;a href=&#34;https://fivethirtyeight.com/&#34;&gt;538&lt;/a&gt;就针对这部分数据进行过&lt;a href=&#34;https://fivethirtyeight.com/features/the-economic-guide-to-picking-a-college-major/&#34;&gt;分析并撰写了报告&lt;/a&gt;，通过毕业后收入对不同专业进行了排名，发现很多有意思的结果：从收入看STEM（科学、技术、工程、数学）这个传统认为收入比较高的专业里S，也就是科学收入并不高；心理学很流行但其实工资并不高；精算行业收入很高，但失业率也很高等，这些结果都有助于你去构造一个大学排名指标，而且这篇文章的代码在GitHub上&lt;a href=&#34;https://github.com/fivethirtyeight/data/tree/master/college-majors&#34;&gt;有&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据读取示例&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;如果用R可使用&lt;a href=&#34;https://www.btskinner.io/rscorecard/index.html&#34;&gt;rscorecard&lt;/a&gt;包来通过API读取数据&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-R&#34; data-lang=&#34;R&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## 在 https://api.data.gov/signup/ 注册API key&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;sc_key&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;YOUR_API_KEY_HERE&amp;#39;&lt;/span&gt;)
df &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sc_init&lt;/span&gt;() &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#a6e22e&#34;&gt;sc_filter&lt;/span&gt;(region &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, ccbasic &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;c&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;21&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;22&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;23&lt;/span&gt;), locale &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;41&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;43&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#a6e22e&#34;&gt;sc_select&lt;/span&gt;(unitid, instnm, stabbr) &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#a6e22e&#34;&gt;sc_year&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;latest&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;%&amp;gt;%&lt;/span&gt; 
    &lt;span style=&#34;color:#a6e22e&#34;&gt;sc_get&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;时程安排&#34;&gt;&lt;strong&gt;时程安排&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;报名 ddl：1 月 27 号 23:59&lt;/li&gt;
&lt;li&gt;报告提交ddl：2 月 10 号 23:59&lt;/li&gt;
&lt;li&gt;作业汇报交流会：2 月 18 号 19:00（腾讯会议号请关注统计之都公众号或和鲸活动页面更新）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;奖项设置&#34;&gt;奖项设置&lt;/h2&gt;
&lt;p&gt;TOP1：优秀奖。将获得 800 元稿酬、电子证书、鲸奇徽章。
TOP2-4：创意奖。将获得 200 元稿酬、电子证书。
其他有效提交：参与奖。将获得《现代科研指北》纸质书 1 本、电子证书。&lt;/p&gt;
&lt;h2 id=&#34;报告要求&#34;&gt;&lt;strong&gt;报告要求&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;这个竞赛没有标准答案与指标，你需要提交的是在线的 Jupyter Notebook 项目，需要包含从数据清洗到指标生成及可视化的全过程且包括理由。也就是说，你需要提交的是一份具有可读性的&lt;strong&gt;可重复性报告&lt;/strong&gt;，要说明指标设计的原因、有效性、灵敏度及延展性，并生成相关图表或可视化面板。&lt;/p&gt;
&lt;p&gt;当然，你也可以只针对一个或几个指标进行可视化与深入讨论，结合新闻与相关研究撰写挖掘其中隐含的例如歧视、冷热门专业、学费/收益比等现象，给出你认为值得关注的结论。&lt;/p&gt;
&lt;p&gt;祝玩得愉快！&lt;/p&gt;
&lt;h2 id=&#34;面向人群&#34;&gt;&lt;strong&gt;面向人群&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;想要提高数据分析实战能力的学生、老师等研究人员&lt;/li&gt;
&lt;li&gt;对指标设计感兴趣的数据分析从业者&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;主办方&#34;&gt;主办方&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;COS 统计之都&lt;/strong&gt;：专业、人本、正直的统计学服务平台。
&lt;strong&gt;主页&lt;/strong&gt;：https://cosx.org/&lt;/p&gt;
&lt;p&gt;活动链接：&lt;a href=&#34;https://www.heywhale.com/home/activity/detail/63b7ecd6555e5f7e505374af&#34;&gt;https://www.heywhale.com/home/activity/detail/63b7ecd6555e5f7e505374af&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>解卷积</title>
      <link>https://yufree.cn/cn/2022/05/21/deconvolution/</link>
      <pubDate>Sat, 21 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2022/05/21/deconvolution/</guid>
      <description>&lt;p&gt;解卷积本来是指反解卷积，但在色谱质谱联用里特指从多物质混合的色谱图里分离出单独物质的色谱。不用说，这跟卷积原始概念已经八杆子打不着了，就是个学科小领域里造词或者填词赋予新含义的案例。要搞清楚这个问题，得从卷积说起。&lt;/p&gt;
&lt;p&gt;卷积跟加减乘除一样，就是一种函数运算方法，定义上就是两个函数经过一通操作变成第三个函数。这个一通操作里涉及两个要点：1）要对其中一个函数要进行反转位移之后2）对两个函数的乘积求和或求积分。这个反转与位移发生在积分的方向上，先卷后积。&lt;/p&gt;
&lt;p&gt;估计很多人看到这个定义会感觉莫名其妙，因为这个定义似乎没啥实际意义，凭啥要卷？又凭啥要积？这就涉及具体学科知识了。不过数学上倒也有个方便的解释，那就是函数卷积的傅立叶变换是函数傅立叶变换的乘积。傅立叶变换可以理解为一种时域与频域的转换，这样时域上卷积就是频域乘积，进行卷积运算相当于在另一个域里做乘积，傅立叶变换是可逆的，只要你能在一个域里找到乘积的物理意义，那么卷积运算就不难理解了。同时这也给解卷积提供了思路，既然可以搞成乘积形式，那就可以对原始数据做变换求卷积就简单多了。&lt;/p&gt;
&lt;p&gt;可能有人又要问了，为啥我要对一个函数做变换？大概率还是因为计算上的方便，例如取对数后乘法就变加法了。又或者让你求正弦函数，你要是不做个泰勒展开那就只能查表了，别忘了我们基于逻辑门的计算机CPU也就能做个加法运算，其余数学运算都要还原成二进制逻辑运算才能进行。至于说傅立叶变换，可以理解为把一个线性函数转为不同频率正弦或余弦函数的叠加，这样做一个好处在于三角函数的导数与积分还是三角函数，然后关注相位变化就可以了。所以说我们并不是吃饱了撑的搞出一些数学变换或运算方法，很多时候是因为进行这些运算或变换能解决实际问题。不过，这是我学了高数很久之后才在实践中意识到的，感觉现在数学教育还是过于抽象了。&lt;/p&gt;
&lt;p&gt;不过卷积也可以用幂级数乘法来理解，两个幂级数相乘后同幂次的系数怎么算？打比方 &lt;code&gt;$x^2+2x+1$&lt;/code&gt; 与 &lt;code&gt;$x+1$&lt;/code&gt; 求乘积的一次方系数，这就要是前者的一次与后者的零次乘积与前者零次与后者一次的和，也就是3，计算对齐过程就是个卷积。其实在概率论里，两个独立变量的和的概率密度函数就是这两个独立变量的卷积，例如两个骰子掷出某个点数和的概率就是两个独立变量概率密度函数的卷积，当然我们固定了和，所以两个变量其实求卷积时并不独立了。到这里其实我们大概能感受到，卷积一定是要在两个函数自变量的特定和上的一个属性。&lt;/p&gt;
&lt;p&gt;到这里为止，我们只知道了卷积运算因为具备一些特性，可能会是一种有实际意义的运算，但实际中为啥要用还是还是要看具体问题。在信号处理里，我们固定信号输入函数跟这个信号自身在时间上的衰减函数，他们的卷积就是时间上输出的信号，这个信号既包含了输入的信号，也累积叠加了衰减效果，这个应该不难理解，时间点x上我们看到的输出信号就应该是前面t = 0时刻信号衰减到t=x时的信号累积上t=1时刻信号衰减到t=x的信号，以此类推。所以我们看到的输出本来就是个卷积后的结果。&lt;/p&gt;
&lt;p&gt;这里积不难理解，但卷在哪？卷在那个衰减函数上，因为你实际用的是输入t=0去乘了衰减函数t=x的数，如果我们把这两个函数对齐，就需要先对衰减函数在积分的时间轴t上做镜像反转，然后平移到t=x处对齐，这就是卷的过程。此处卷积连接的输入与衰减函数也是通过时间和恒定来连接的，也就是如果你能构筑两个函数，其自变量和在某个维度上是恒定的，那么他们就适合在这个维度上做卷积运算且应该有具体的物理意义，例如某种累积效应。不过卷积是翻转加平移的，如果只平移不反转，在乘积上求和或者积分也可能有意义，例如求相关性。这里的意义是需要看需求的，构建运算不难，但跟物理世界相联系是需要下功夫理解的。&lt;/p&gt;
&lt;p&gt;上面那个是一维卷积，如果输入输出都是个二维图像，那么卷积运算就相当于对图像做滤镜，跟输入做卷积的就是所谓的卷积核或滤镜。这个卷积核在卷积神经网络里其实就对应了图像的某个局部特征，或符合某种模式后可以穿过滤镜形成有效信号。当卷积核是3*3的矩阵，那么当图像是出现某种模式时，训练神经网络过程会找到这个对应的卷积核。一张图片可能有很多特征，那么也就可以训练出很多卷积核，当进行图片分类预测时，我们可以训练出一个基于卷积核的全连接层来输出预测分类。当然这里激活层池化层等技术或模型构架就不考虑了，但卷积运算确实是卷积神经网络的一个核心，专门用来抽数据中的特征。至于说为啥是卷积运算而不是其他运算，其实基于其他运算的也有，但本质上要处理的基本问题是图像抽特征的方法，否则1400万像素的图片你要训练的参数就是个天文数字，但训练几万个卷积核就容易多了，可以理解为一种降维的思路。&lt;/p&gt;
&lt;p&gt;说白了卷积运算就是一种特殊滤波器或放大器，但具体到分析化学里，问题就不一样了。分析化学的光谱分析里也涉及卷积，具体说就是我们仪器上测到的信号是真实信号跟某些卷积核做卷积出来的，当然这不是说噪音，而是诸如衍射过程这种仪器来源的有固定模式的信号，此时解卷积其实就是个模式识别过程。到这里卷积的概念还算是符合原有定义的，但具体到学科小领域就完全变味了。&lt;/p&gt;
&lt;p&gt;质谱色谱联用测到的数据是一个三维向量：一个维度是时间，一个维度是荷质比，一个维度是响应。早期可视化手段基本就是直接把荷质比这个维度上的所有响应求和形成总离子流图。但因为存在共流出现象，这种图上你数出来的色谱峰数目跟物质数目是对不上的，很多单独的峰其实是多个物质在不同荷质比上的响应叠加出来的，这导致总离子流图的峰形长得怎么说呢，比较畸形。&lt;/p&gt;
&lt;p&gt;时间来到1974年，Biller 跟 Biemann 发表了一篇题为 &lt;em&gt;Reconstructed mass spectra, a novel approach for the utilization of gas chromatograph&amp;mdash;mass spectrometer data&lt;/em&gt; 的论文，提出了一种重构出单独质谱峰的算法，这个算法受限于年代，非常简单，就是提出用每个扫描循环离子响应最大值来分离不同物质，然后合并对应的扫描得到比较干净的谱图。然后到了1976年，Dromey 等人搞出了一个非常复杂的利用相似峰形建模提取独立物质质谱的方法，这个方法效果不错，但因为计算上太复杂基本属于原汤化原食，就这批人在用。等到了1992年，Colby在JASMS上发表了题为&lt;em&gt;Spectral deconvolution for overlapping GC/MS components&lt;/em&gt;的文章，其实这篇文章的核心在于对1974年的方法进行改进，具体来说就是每个扫描循环里面只找十个峰，然后合并谱图重构出总离子流图，进而提高质谱分离的分辨率。因为这活是90年代做的，94年就有人将其程序化了，但很不幸同时也就把解卷积这个词给带到这个小领域里了。&lt;/p&gt;
&lt;p&gt;不过这里我们很清楚此处的信号根本就没有卷积，解卷积也无从谈起，但因为Colby这篇文章，后面三十年大家就都默认了解卷积等同于重构质谱峰的意义了。不过在蛋白质质谱领域，解卷积还有另外一个概念，就是把多电荷峰反解回分子量，此处最流行的是一种利用最大似然度求解的maxent算法，同样也跟卷积的原始概念毫无关系。在中文环境里，研究人员还有仪器厂商的销售尤其喜欢这个词，因为听上去高大上显专业，但我估计他们中的大多数可能都不知道这个概念本身其实是用错了，单纯以讹传讹，三人成虎。&lt;/p&gt;
&lt;p&gt;但更可悲的是其实错不错都不重要了，很多一线科研人员的数学功底基本上都归零了，傅立叶变换、卷积、二重积分、delta方法等概念全都谁教的还给谁了。论文里的数据分析都是对着其他论文照猫画虎，经常是驴唇不对马嘴，要是继续这么自己造词搞小圈子，那跟八股文写茴字也没啥区别了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>漫谈 base R 与 ggplot2 </title>
      <link>https://yufree.cn/cn/2021/12/18/base-r-ggplot2/</link>
      <pubDate>Sat, 18 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/12/18/base-r-ggplot2/</guid>
      <description>


&lt;p&gt;这几天心里颇不宁静。纽约的单日新冠确诊数又出了新纪录，工作邮箱里好几封通知群聚感染的邮件，很多还是完全免疫甚至打了加强针的。感觉新变种可以自立山头看作新病毒了，而面世仅一年的疫苗似乎特异性已经不那么乐观了。不过更强的传染性倒不见得有更强的症状，但大概率这病毒是打算跟人类长期共存了，这就有点抗生素与致病菌的剧本了。我理解国内清零政策很大程度是因为放开后医疗系统会崩溃，不过长期清零对于一种不断变异增强传染性的病毒而言是否具有可持续性我想已经在被论证了，同样完全躺平现在看也不是什么好的应对策略，解决问题不能非此即彼，总是要根据实际情况动脑子的。说到这我就来填一个很早就想填的坑，是关于R绘图系统的。&lt;/p&gt;
&lt;p&gt;不知道从什么时候开始，tidyverse 跟六角贴纸开始满天飞了，在我看来工具是用来解决问题而不是创造问题的，围绕工具形成的文化现象或类似价值观的东西属于副产品，但推广价值观这类副产品显然要比推广软件容易，因此到一定程度总会出现一些反客为主的声音。我对此不置可否，只是想说工具一定要能解决问题才有意义，在解决问题上，将问题清楚描述出来要比直接问某某工具咋做某某图更重要。&lt;/p&gt;
&lt;p&gt;我接触 R 是在十年前，那个时候提到 R 的绘图系统，更多指的是 base R 与 grid/Lattice 这两套系统，前者强调的是最高程度的自定义自由度而后者侧重的则是一个函数给出想要的图形。显然，前者更适合原始意义的绘图比较适合开发者而后者更适合具体应用场景的用户，后者的一个显著优势是默认出图就足够漂亮而显著缺点则是自定义比较费劲。用户从来都是最难伺候的，总有用户既想要最大程度的默认好看又需要一定程度的自定义（说的就是科研狗），此时 ggplot 就出现了。&lt;/p&gt;
&lt;p&gt;ggplot 里面的 gg 代表的是图形语法，本来 ggplot 就是 Hadley 理论转实践的尝试，原始的包在08年就不更新了，现在 CRAN 上也没有了，后面 ggplot2 里的那个2就是致敬最原始的版本 ggplot 。ggplot2 最显著的优点就是用图形语法结合了 base R 与 grid 系统的理念，默认足够好看，自定义掌握了一套通用层层叠加语法后也很容易上手。不过，我也得替 grid/Lattice 说句话，他们也是支持自定义的，Lattice 到今天也处于活跃开发状态，并且跟 R 一起发布。不过他们的自定义显然没有上升到价值观层面，跟 base R 的逻辑更像，支持公式化的数据表达，支持对象化操作，也可以通过 &lt;code&gt;update&lt;/code&gt; 这个函数来有限调节自定义的细节。不过有句老话说“革命不彻底就是彻底不革命”，用户从来不会过多理会开发历史，基本都是颜狗。ggplot2 也是依赖 grid 包来构建的，包括了所有 Lattice 的优点但学起来更容易，所以很快用户就倒向了这套绘图系统。不过，我们现在常看到的 base R 与 ggplot 的对比很大程度是从可视化结果上来的，但别忘了 ggolot 也是 R 语言的产物，理论上研究清楚了 grid 包也可以搞出类似的东西，真正的区别是用户的使用逻辑。&lt;/p&gt;
&lt;p&gt;base R 的使用逻辑更像是白纸画图，从坐标轴到图像都是可以随意自定义的，与之对应的就是需要用户了解一大堆底层命令。base R需要用户对可视化有很具体的了解，例如先用纸笔草图画出雏形，然后通过排列组合基本的作图元素重现在绘图区里。举个最简单的例子，想在条形图上加个误差线，就需要分解为具体的两部分：1）误差线长度与起点坐标，2）然后从起点坐标平行y轴画一条线段，线段末端可以把原有箭头末端里箭头的角度从锐角改成90度垂直。然后我们就能看到误差线了。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;iris&amp;quot;)
mean &amp;lt;- by(iris$Sepal.Length,iris$Species,mean)
sd &amp;lt;- by(iris$Sepal.Length,iris$Species,sd)
temp &amp;lt;- barplot(mean,ylim=c(0,max(mean)+2*max(sd)))
arrows(temp,mean+sd, temp, mean-sd, angle=90, code=3, length=0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/12/18/base-r-ggplot2/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这个简单到不能再简单的功能偏巧默认的条形图 &lt;code&gt;barplot&lt;/code&gt; 里没有，函数文档里那个误差线又不是平头的，所以很多用户马上认为 base R 做不了。单就这个例子而言，其实条形图本来就不应该有误差线，真正需要表示不确定性范围应该用箱形图或提琴图来展示真实分布，但你要是去读学术论文会发现时至今日还是有大量图用条形图加误差线，有的还要加星号表示显著差异。这些从作图角度而言完全就是缝合怪，但始作俑者后人不断。很多用户只知道最终要看到什么样的图，但完全不关心图背后的统计学意义，然后就到处问哪个函数能做这样的图，追求形似。这是我认为现在还需要学 base R 作图的一个重要原因，那就是用户一定要有把图形元素拆解回原始数据的能力，知道自己再干什么而不是盲目追求炫酷，这太浮躁。&lt;/p&gt;
&lt;p&gt;在 ggplot 这套系统里，图片实际被拆解成了三个部分：数据、映射与可视化方式。数据最好是一个数据框，映射解决变量对应在图片上的维度例如哪个是x？哪个是y？哪个分组用颜色/大小/形状来表示，可视化方式就是指定出图的具体形式。其实这个逻辑在 base R 里也是成立的，只不过最后这个不同可视化方式会对应不同函数，然后里面参数一大堆名字还不一样。ggplot则是把这些都规范到一种形式里去了，而且可以通过加号这个函数层层叠加可视化方式，而各自可视化方式内部也可以重新进行坐标映射。此外，ggplot事实上也支持在作图过程中执行一定的计算，例如平滑或者汇总，这类计算都归到&lt;code&gt;stat_&lt;/code&gt;系列的函数里了。如果打算自定义某个维度，可以统一用 &lt;code&gt;scale_XXX_manual&lt;/code&gt; 来进行修改，这里&lt;code&gt;XXX&lt;/code&gt;对应的是你映射的维度，例如 &lt;code&gt;scale_fill_manual&lt;/code&gt; 对应的就是自定义填充颜色。注意，这里是映射后的图片里的维度，而映射则一般都是在可视化方式的 &lt;code&gt;aes&lt;/code&gt; 里定义的，用来把数据中的某个维度指定到图片里的维度里。此外，ggplot 自然也支持根据分组信息的多图可视化，这里就是&lt;code&gt;facet_grid&lt;/code&gt;函数来统一管理，这里 ggplot 的价值观是每一张图都要尽可能清楚展示数据，且一幅图讲一个故事，然后通过坐标对其进行比较。也就是说，ggplot里画双坐标轴这种图就不太推荐，虽然也可以自定义后来个形似，但其实双坐标轴图在可视化方式里面的地位大概跟饼图或3D柱形图差不多，属于人厌狗嫌那个组的。所有的双坐标轴图理论上都可以并应该拆成两幅图来描述两件不同的事，如果两件事相同，那么双坐标轴总可以转为单坐标轴。&lt;/p&gt;
&lt;p&gt;从我个人经验而言，如果你掌握了 base R 作图，转到 ggplot 作图非常轻松，会省掉一大堆需要自定义的东西，统一用他们的函数体系来画就行。反之，如果掌握了 ggplot 的语法，学 base R 应该也很轻松，因为base R绘图函数里的映射与可视化方式也都有现成的包或函数，只是可能参数名字乱一些。说 ggplot 对新手友好，一大部分指的是默认美观，还有一部分原因就是从设计上就阻止了很多类似3D柱形图或双坐标轴的存在。但搞笑的是在爆栈网上从来不缺人去问这类图如何在ggplot体系里实现，也不缺大神给出解决方法。因此，我一度认为限制一定的自由度其实也对培养良好的可视化习惯有帮助，但现实却是很多用户既没学到ggplot里的可视化原则，又产生了对 base R 莫名其妙的优越感，一抬手就是管道化教条式的层层叠加代码，一行代码能做到的非学习牛津大学贝利学院优秀毕业生、大英帝国爵级司令勋章获得者、大不列颠及北爱尔兰联合王国内阁常任秘书汉弗莱·阿普比的语言风格去卡形式，这就距离解决问题比较遥远了。&lt;/p&gt;
&lt;p&gt;其实 base R 与 ggplot 之争的背后存在一个代码风格统一的问题，如果选 ggplot ，确实代码更容易读，但问题是很多初学者的水平大概就在复制代码改变量名的水平，完全不知道代码的意义，这样层层堆出来的代码甚至会重复定义，也没啥可读性。究其原因，代码风格应该是初学者到了中级水平才应该考虑的问题，初学者最应该了解的是可视化的逻辑，然后结合自己具体的问题去练习并累积经验。不过，很多初学者都是excel打底的，脑子里全是哪个对话框画什么样的图这种思维，这种情况不论是转 base R 还是 ggplot ，他们脑子里的预期都是一个函数出图解决问题，根本不关心图是如何画出来的，这就很容易变成教条化的用户，记住步骤但不知道原因。实话说很多基于ggplot体系的作图包本质上就是把需要用户自定义的部分自己强制定义一遍然后就上线了，这种包完全迎合了某些领域的独特可视化品味，后面跟了一批教条化的用户，这样的默认可用的软件包我觉得并不利于数据分析人员理解自己的数据。&lt;/p&gt;
&lt;p&gt;但凡学用编程语言进行可视化，起码是要知道自己在做什么的。看到一张漂亮的图，可以尝试分析图片元素，然后尝试自己将其组合起来。有这种想法后，base R 也好，ggplot 也好，学明白一个就基本也会另一个了，甚至说迁移到 python 的 matplotlib 或其他交互式作图系统都不困难。但如果搞不清楚原理，那么换一个软件就只能继续到网上复制现成的代码。我倒不是鄙视从网上复制现成的代码，毕竟这事我也没少做，但总要有个学习的过程才能掌握。&lt;/p&gt;
&lt;p&gt;软件优劣之争在我看来很多都是鸡同鸭讲，很多比较都是在特殊应用场景下才有明显区别。但每个人的最终应用场景毕竟是不同的，解决问题的意义显著大于跟工具分高下的意义。显然编程语言绘图的能力范围更多受限于使用者的能力而非工具本身，因此这样的工具优劣争论还是少一些吧，争到最后大概率都成了人身攻击。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>辛普森悖论</title>
      <link>https://yufree.cn/cn/2021/08/27/simpson-paradox/</link>
      <pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/08/27/simpson-paradox/</guid>
      <description>&lt;p&gt;曾经有两本书伴我度过了无数漫漫长夜，毕竟每次鼓起勇气看不超过10页就会睡过去，一本是&lt;a href=&#34;https://yufree.cn/cn/2020/09/01/metaphor/&#34;&gt;前面&lt;/a&gt;写过的 &lt;strong&gt;metaphors we live by&lt;/strong&gt;，另一本就是《悖论简史》。这种书的一大特色就是读的时候如果不带脑子看不懂，带脑子就头疼，但可气的是写的还挺有趣。&lt;/p&gt;
&lt;p&gt;基础科研的很多突破都是来自于悖论或者说反例，不过这里的悖论属于理论悖论，大概率是理论本身有问题，需要新的理论来解释观察与实验。还有很多悖论属于错觉，本身不是悖论仅仅因为解释上的片面出现，其实魔术就可以化为这一类，很多魔术手法展示的现象完全是违背常理的，但了解手法后就会发现其实是利用了一些惯性思维产生的错误解释。真正的悖论是语义学上的，例如“这句话是错的”就是一个语义悖论，如果认为这句话是错的那就应该是对的，但如果认为这句话是对的其又描述了一个自己是错误的判断，这种带有自指的悖论属于无解。其实数学领域的第三次危机本质上也要通过语义学划定语义解释范围来凑合解决，这属于逻辑自身的漏洞。&lt;/p&gt;
&lt;p&gt;辛普森悖论属于某种程度上的错觉悖论。其本质就是说存在一种分组方法，让&lt;code&gt;$\frac{A_1+B_1}{C_1+D_1} &amp;gt; \frac{A_2+B_2}{C_2+D_2}$&lt;/code&gt;，然后&lt;code&gt;$\frac{A_1}{C_1}&amp;lt;\frac{A_2}{C_2}$&lt;/code&gt; 并且 &lt;code&gt;$\frac{B_1}{D_1}&amp;lt;\frac{B_2}{D_2}$&lt;/code&gt;，乍看之下会感觉莫名其妙，因为数学上找这么一组数太简单了（睡不着觉别数羊，就去构造辛普森悖论，比数羊效果好多了）。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$\frac{1+3}{5+4} = \frac{4}{9} &amp;gt; \frac{4}{10} = \frac{2+2}{8+2}$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$\frac{1}{5}&amp;lt;\frac{1}{4}$&lt;/code&gt; 并且 &lt;code&gt;$\frac{3}{4}&amp;lt;\frac{1}{1}$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;从数学角度看完全不存在悖论，因为&lt;code&gt;$\frac{A}{B}+\frac{C}{D} \neq \frac{A+B}{C+D}$&lt;/code&gt;，所以&lt;code&gt;$\frac{A}{B}$&lt;/code&gt;与&lt;code&gt;$\frac{C}{D}$&lt;/code&gt;的数值比较关系无法传递到&lt;code&gt;$\frac{A+B}{C+D}$&lt;/code&gt; 的比较里。&lt;/p&gt;
&lt;p&gt;但我们要加个语境就完全不同了。例如，这里我们把上面的数扩大十倍，某种化学品暴露组一共90人患病40人，对照组一共100人患病40人，此时研究人员会得出暴露组发病率比对照组高的结论。然而，如果暴露组里有50名男性发病10人，对照组80名男性发病20人，我们会发现男性对照组发病率高于暴露组；而同时女性40人里发病30人，对照组里20人发病20人，还是暴露组低于对照组。&lt;/p&gt;
&lt;p&gt;同样的数据，如果环境科学家看到会说这是一种致病污染物需要禁止而药厂则认为这是一种对不同性别都有效的预防性药物，两边成果都足以发表业内很好的杂志上也都说得通，但解读出的含义完全相反。由于人体本身就有回归到正常的现象，确实存在一些病吃药七天恢复不吃药一星期恢复，所以很多我们研究发现的效应如果不能强到药到病除而需要通过不断细分数据来发现效应，那么大概率就会存在辛普森悖论。&lt;/p&gt;
&lt;p&gt;但问题现在的很多病的新药或常见保健品就是这个表现水平，徘徊在吃不死人的底线之上通过安慰剂效应或信仰来起效果，这就很尴尬了。保健品大多数吃了虽然没用但也不会有害，跟食物差不多效果但价钱就不一样了。而且如果一个人得了慢性病，大概率本来身体状态就是起起伏伏，此时你吃保健品就会形成一个错觉：身体状态好就会认为保健品起效了而不好则会认为没吃够或者哪天忘了吃了。保健品本身就起到了信仰的效果，功劳都是它的罪过都是自己的，有这份心什么成不了？&lt;/p&gt;
&lt;p&gt;其实我跟安慰剂效应有很深的渊源，小学每年都有体测，那时候我跑步不行，就开始动歪脑筋，反正体测又不尿检不如用兴奋剂来提高成绩。但问题我家哪有兴奋剂啊，回去一通翻箱倒柜发现一桶咖啡，我之前也没喝过心想这玩意也算兴奋剂吧？结果冲了一碗就去体测，当时感觉甜甜的还挺好喝，效果也还不错，成绩有明显提升。这事过去快一年等到下一次体测来的时候我又想起这玩意了，这次又翻出来仔细读标签才发现是“咖啡伴侣”但伴侣两个字很不好认，也就是喝的是植脂末跟奶粉，这次成绩就崩了。好多年后我才喝到真正的咖啡，说实话，还不如咖啡伴侣好喝。&lt;/p&gt;
&lt;p&gt;后来跟父母说起这事就奇怪，家里又没人喝咖啡为啥要搞一罐咖啡伴侣？答案也不难猜，这是过年走亲戚送来送去留下来的礼物，他们买的时候估计也当成咖啡了，包装洋气而且比真咖啡便宜多了。这里能促进成绩的其实只是服用了兴奋剂的信念而非兴奋剂，也就是当一件事决定因素心理影响更大时，药物的药效反而成了玄学了。当然重申一下，体测别动歪脑筋，人本身的潜力要远大于外界刺激，平时加强锻炼才是正道。否则，凭运气赚来的，早晚都要凭实力输出去，这就是所谓的回归现象。&lt;/p&gt;
&lt;p&gt;接着说辛普森悖论，这里我们已经看到，数学上比较整体比值与局部比值是毫无意义的，各种情况都会出现。然而，如果我们给数字赋予含义，那么就会出现不同专业基于不同立场给出的完全相反但又都解释得通的结论。也就是说，从绝对的数学计算上，这就是个鸡同鸭讲毫无意义的比较，但赋予背景后，现实中又确实存在明确的问题，例如前面说的那种化学品究竟是应该推广还是禁用？这个问题咋解决？&lt;/p&gt;
&lt;p&gt;此时我们就不得不进入因果推断的领域了，我们必须对化学品、性别及疾病这三者关系建模，这也算某种三体问题了，考虑方向其实一共就23种关系：&lt;/p&gt;
&lt;h2 id=&#34;三者都没关系&#34;&gt;三者都没关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品 性别 疾病&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三者里只有一组两两关系&#34;&gt;三者里只有一组两两关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品-&amp;gt;性别 疾病&lt;/li&gt;
&lt;li&gt;化学品&amp;lt;-性别 疾病&lt;/li&gt;
&lt;li&gt;化学品 性别-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;化学品 性别&amp;lt;-疾病&lt;/li&gt;
&lt;li&gt;性别 化学品-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;性别 化学品&amp;lt;-疾病&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三者里有两组两两关系&#34;&gt;三者里有两组两两关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品-&amp;gt;性别-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;化学品-&amp;gt;性别&amp;lt;-疾病&lt;/li&gt;
&lt;li&gt;化学品&amp;lt;-性别-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;化学品&amp;lt;-性别&amp;lt;-疾病&lt;/li&gt;
&lt;li&gt;性别-&amp;gt;化学品-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;性别-&amp;gt;化学品&amp;lt;-疾病&lt;/li&gt;
&lt;li&gt;性别&amp;lt;-化学品-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;性别&amp;lt;-化学品&amp;lt;-疾病&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三者里有三组两两关系&#34;&gt;三者里有三组两两关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品&amp;lt;-性别&amp;lt;-疾病&amp;lt;-化学品&lt;/li&gt;
&lt;li&gt;化学品&amp;lt;-性别&amp;lt;-疾病-&amp;gt;化学品&lt;/li&gt;
&lt;li&gt;化学品&amp;lt;-性别-&amp;gt;疾病&amp;lt;-化学品&lt;/li&gt;
&lt;li&gt;化学品&amp;lt;-性别-&amp;gt;疾病-&amp;gt;化学品&lt;/li&gt;
&lt;li&gt;化学品-&amp;gt;性别&amp;lt;-疾病&amp;lt;-化学品&lt;/li&gt;
&lt;li&gt;化学品-&amp;gt;性别&amp;lt;-疾病-&amp;gt;化学品&lt;/li&gt;
&lt;li&gt;化学品-&amp;gt;性别-&amp;gt;疾病&amp;lt;-化学品&lt;/li&gt;
&lt;li&gt;化学品-&amp;gt;性别-&amp;gt;疾病-&amp;gt;化学品&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们的专业知识或者语义本身就可以缩小待检验的模型，例如性别是先天的，所以凡是有化学品决定性别的都可以排除掉，疾病也不可能决定性别跟化学品，此时我们就剩下六种模型了：&lt;/p&gt;
&lt;h2 id=&#34;三者都没关系-1&#34;&gt;三者都没关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品 性别 疾病&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三者里只有一组两两关系-1&#34;&gt;三者里只有一组两两关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品&amp;lt;-性别 疾病&lt;/li&gt;
&lt;li&gt;化学品 性别-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;性别 化学品-&amp;gt;疾病&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三者里有两组两两关系-1&#34;&gt;三者里有两组两两关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品&amp;lt;-性别-&amp;gt;疾病&lt;/li&gt;
&lt;li&gt;性别-&amp;gt;化学品-&amp;gt;疾病&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;三者里有三组两两关系-1&#34;&gt;三者里有三组两两关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;化学品&amp;lt;-性别-&amp;gt;疾病&amp;lt;-化学品&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里假如我们发现性别确实会同时影响化学品跟疾病，那么就必须考虑控制性别后的净效应。假如性别只影响化学品不影响疾病（例如女性喜欢用化妆品暴露量更大），那么那么我们则不需要控制性别可以直接考察化学品对疾病的影响。如果性别也不影响化学品，那么也可以直接考察化学品对疾病的影响。这就是三种化学品跟疾病有关系的模型。如果化学品跟疾病本就没关系，甚至性别也跟疾病没关系，例如这是一种细菌性传染病，那从一开始研究就没有因果关系支持，做出的结果就属于玄学了。但一定不要直接排除掉这些可能，不论常识还是专业知识都排除不了就需要逐一考察。&lt;/p&gt;
&lt;p&gt;通过分析这三种模型，我们会发现要首先检验性别跟疾病是不是有关系，确定了这一条，后面就知道是不是要考虑按性别分组了。此时辛普森悖论从实际意义上就解决了，靠的其实还是我们自己赋予的实际语义与专业知识，但别忘了专业知识可能本身就是错的，实际语义可能存在二义性，这些才是搞出悖论的根基。&lt;/p&gt;
&lt;p&gt;不过如果我们看回原始版辛普森悖论，里面的三体是性别、学院及录取率，要解决这个问题就还是要把23种可能性全列出来然后排除掉学院决定性别、录取率决定性别这些语义跟常识上不存在的可能性，然后就可以知道需要检验的是什么了，但这里可以看出辛普森原版悖论可排出的可能性要比我的例子少，因此需要检验的模型就更多，跟性别有关决定录取率的模型有关的有五个。我们来看下其中两个有三组两两关系的模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;性别-&amp;gt;学院&amp;lt;-录取率&amp;lt;-性别&lt;/li&gt;
&lt;li&gt;性别-&amp;gt;学院-&amp;gt;录取率&amp;lt;-性别&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;学院决定录取率还是录取率决定学院这个是不太好说的，因为这两者可能并不是谁决定谁的，可能被学校同时控制，例如学校会决定学院规模与最低录取率之类，此时就不是三体问题，因为另一个变量学校又出现了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;性别-&amp;gt;学院&amp;lt;-学校-&amp;gt;录取率&amp;lt;-性别&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此时因果图出现一个对撞结构，你要是需要控制学院就开了后门无法正确估计性别的作用，此时就不应控制任何变量直接看两者关系。但真实世界哪有这么简单，学校这个因素通常我们根本观察不到，所以也不好假设其与学院的关系，如果学院可以影响学校决策，此时就还是要控制学院。也就是说辛普森悖论是否有解其实完全要依赖实际存在的因果关系。&lt;/p&gt;
&lt;p&gt;Judea Pear 曾经写过一份辛普森悖论的技术&lt;a href=&#34;http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf&#34;&gt;报告&lt;/a&gt;，里面给出了一种无限层级的因果图，他起了名叫做辛普森悖论机。在那张图里，如果你顺次控制混杂因素与对撞因素，那么原因与结果就会反复出现需要控制变量与不需要控制变量的情况。也就是说，想解决悖论，重要的是解决背后的因果图，不同语境逻辑或因果图下对其他变量控制与否是可以完全不同的。我们最终只能接受一个相对语境下的正确答案，绝对语境下那个数学问题其实并没有意义。&lt;/p&gt;
&lt;p&gt;也就是说，辛普森悖论其实暗示了我们不同自洽逻辑下可以出现不唯一的正确解，而这个正确与否取决于你对其背后因果关系模型的假设，而这种假设可以不唯一。好消息是自然科学中检验假设很大一部分可以通过实验来证实或证伪，但坏消息却是我们日常交流用的语言与语义可能天然就无法将因果关系描述清晰，能量化的数值无物理意义而有物理意义的数值无法量化，这就导致我们只能看到模型下的真实而无法验证模型本身。这部分内容在科学哲学里就涉及实在论与非实在论了，也是催眠利器。&lt;/p&gt;
&lt;p&gt;或许我们所谓解决悖论的方法不过就是引入一套模型屏蔽掉会产生悖论的讨论，但这反而说明了悖论的无解与我们自然语义天然存在漏洞。至于说不同学科根据自己学科利益来报道结果倒也不用太担心，这类弱效应的成果最多搞出一堆保健品与智商税产品，基本上无害。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>平行世界的统计推断</title>
      <link>https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/</link>
      <pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/</guid>
      <description>


&lt;p&gt;最近重新看了下之前对p值的笔记，突然对零假设充满了陌生感。在p值的语境里，当我们看到数据D在零假设下发生的概率低就会做出数据D不支持零假设的判断，这是一个条件概率等价替换的问题：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$p(H0|D) = p(D|H0)$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这个当然是有问题的，我真正关心的是零假设是否成立而不应该是数据出现在零假设下概率。而这在假设检验的设计中转化成了零假设成立下出现观察数据的概率，这里最大的不对劲在于数据D不支持零假设依然无法判断零假设是否成立。对于实验科学，我们能收集数据D，但检验零假设似乎没什么道理，零假设是来自于随机过程，我们真正关心的从来都是产生差异的过程，现在却要用随机过程来检验产生差异与否，总感觉哪里不对。&lt;/p&gt;
&lt;p&gt;不过重读了丁鹏老师的&lt;a href=&#34;https://cosx.org/2019/05/recheck-the-lady-tasting-tea/&#34;&gt;文章&lt;/a&gt;后，我意识到当年Fisher搞出的统计推断的背景的女士品茶实验其实是穷举所有可能，因为品茶结果是离散的，所以穷举空间是有限的，此时发生各种情况的概率是离散的，这也就形成了Fisher精确检验的基础。这里面容易被忽视的点在于品茶实验中品茶顺序与结果是无关的，也就是形成正确答案的顺序与答案无关，也就是说如果数据D是真理，那么随机化过程不影响真理对错。&lt;/p&gt;
&lt;p&gt;那么回到前面的问题，Fisher这个p值设计实际上巧妙的规避了&lt;code&gt;$p(H0|D)$&lt;/code&gt;与&lt;code&gt;$p(D|H0)$&lt;/code&gt;的区别，因为在这个离散概率的语境下，空假设提供的实际是一个虚拟的有限平行宇宙，当某个假设成立时，数据一定会支持假设而不成立时数据在有限平行宇宙里只会以很低的概率出现，&lt;code&gt;$p(H0|D)$&lt;/code&gt;与&lt;code&gt;$p(D|H0)$&lt;/code&gt;的讨论在这里就没意义了，虚拟的有限平行宇宙实际是给出了所有可能性空间。也就是说，产生差异这件事被零假设转化成了虚拟的有限平行宇宙，其中有一个宇宙产生了差异，那么这个宇宙下待检验的假设就可以被认为是证实的。&lt;/p&gt;
&lt;p&gt;换句话说，Fisher的零假设是包含了所有假设在内的假设宇宙，在某些宇宙里出现某些数据是正常且合理的，但这些宇宙相比所有假设宇宙的空间非常小，那么在这些宇宙里数据背后对应的规律应该就是真实的。如果假设宇宙本来就不多（当然这个只会在离散结果的条件下出现），那么事实上就形成了统计功效不足的问题。前面我意识到的不对劲其实是误会，因为我脑子里还放着与零假设对应存在的备择假设这个东西，所以我会纠结是不是检验错了。但事实上，备择假设本就是零假设的一部分，其独特性是通过在可能性空间的低概率来表征的。从这个逻辑上看，Fisher的精确检验并不存在现在普遍使用的零假设备择假设这套体系里的问题，而且只要转化一下，Fisher精确检验一样可以用在更广的领域。&lt;/p&gt;
&lt;p&gt;这里最大的问题在于可能性空间的低概率跟数据存在规律性需要是一个东西，这也是Fisher精确检验的核心。现在很多对p值的质疑可能来自于对可能性空间认识的偏误，对于很多人而言，可能性空间是数学意义上的无限空间，但对实验结果而言，在一定观测精度下可能性空间其实是有限的而观测的数据又是一定存在于这个空间之内的，这就导致概率低下进而我们会认为规律存在。举个例子，如果观察我开门这个动作100次，那么我每次都用右手这个事件对比&lt;code&gt;$2^{100}$&lt;/code&gt;这个可能性空间而言概率很低但存在，这件事可能让观察者得出我是右利手的规律性结论。这里统计推断只负责告诉你概率，怎么解释是观察者自己决定的事，p值0.05就是个方便决策的阈值但统计学更关心概率怎么计算。&lt;/p&gt;
&lt;p&gt;也就是说现在对p值的质疑是集中在后面决策步骤上的，但这个决策标准其实本来应该各个学科根据自己的学科规律可能性空间来制定，不能简单把锅甩给统计学家，毕竟他们对不同学科规律可能性空间其实并不了解。很多时候重复性不好的本质是所谓规律性在可能性空间里并不稀有，随机过程就会发生，这个时候应该做的是对规律性给出更严格的定性定量要求与描述，还有种可能就是本来就是伪规律，是噪音被当成了规律，天知道科研人员为了混饭吃会不会把其实不稀有的偶发事件当成规律来报道，这时候应该被质疑的其实应该是实验者而不是统计学决策工具。&lt;/p&gt;
&lt;p&gt;我不太清楚后面是怎么把p值从离散分布推广到0到1之间的连续均匀分布的，但现在我倒是有兴趣看下p值本身在平行宇宙里的分布了。如果有规律的事实在实验限定的空间里发生，其p值的分布应该会与随机过程产生的p值不一样。这里我不打算采用多次随机抽样，因为此时分布事实上是已知的，此时进行随机实验其实是在假设分布存在且成立的条件下判断事实。相反，我会随机生成一组数据但保留这一组数据当成既成事实，但随机化分组过程来检验p值的分布，此时应该更符合事实存在后对假设的判断这个思路，这个应该更贴近Fisher精确检验的思想。这里我们考虑三种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;真实差异固定&lt;/li&gt;
&lt;li&gt;完全是随机数&lt;/li&gt;
&lt;li&gt;固定的真实差异加上随机数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第一种情况是规律完全成立；第二种是完全无规律；第三种是可观察或可测量的数据。生成三组数据后我们对其分组（简单二分）进行10000次随机化操作，然后进行t检验，记录并观察p值。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
# 真实差异
x &amp;lt;- c(rep(100,260),rep(200,260))
# 随机差异
xr &amp;lt;- rnorm(520)
# 考虑误差的真实差异
xm &amp;lt;- c(rep(100,260),rep(200,260))+rnorm(520)
p &amp;lt;- pr &amp;lt;- pm &amp;lt;- c()
for(i in 1:10000){
        # 随机化分组
        g &amp;lt;- factor(sample(c(1,2),520,replace = T))
        p[i] &amp;lt;- t.test(x~g)$p.value
        pr[i] &amp;lt;- t.test(xr~g)$p.value
        pm[i] &amp;lt;- t.test(xm~g)$p.value
}
# 探索p值分布
sum(p&amp;lt;0.05)/length(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0481&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(p&amp;lt;0.5)/length(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5084&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(p&amp;lt;0.9)/length(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8978&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(pr&amp;lt;0.05)/length(pr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0514&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(pr&amp;lt;0.5)/length(pr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4994&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(pr&amp;lt;0.9)/length(pr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9014&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(pm&amp;lt;0.05)/length(pm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0493&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(pm&amp;lt;0.5)/length(pm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5023&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(pm&amp;lt;0.9)/length(pm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8991&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(3,2))
hist(p,breaks = 20)
hist(p,breaks = 100)
hist(pr,breaks = 20)
hist(pr,breaks = 100)
hist(pm,breaks = 20)
hist(pm,breaks = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这个结果非常有意思，第一个能看到的现象是如果数据本身存在规律性，那么p值的分布是一个离散分布。这个分布介于0到1之间，越接近0的部分越密集，越接近1的的部分越稀疏，但是如果计算小于0.05，0.5，0.9的比例情况，会发现这种稀疏分布依旧符合均匀分布的概率分布特征。如果数据不存在规律性，那么p值的分布就是很均匀的。如果数据混合了规律性与噪音，依然会显示出这种离散分布特征。下面我用qq图来观察下这个分布跟均匀分布的区别：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
ref &amp;lt;- runif(10000)
par(mfrow=c(1,1))
qqplot(ref,p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqplot(ref,pr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqplot(ref,pm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;可以看到，如果数据本身存在规律性，其随机化分组后的p值虽然跟均匀分布很接近，但qq图上确实会表现出前密后舒的螺旋延伸状态。&lt;/p&gt;
&lt;p&gt;我虽然不清楚统计学上有没有对这个p值分布的研究，如果没有我先管它叫 MY Distribution ，谁让我名字缩写就是MY，这个“我的分布”可能对实验学科非常有意义。&lt;/p&gt;
&lt;p&gt;这里为了区别我再做一个仿真，这次我不是对分组随机而是对采样随机：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
# 固定分组
g &amp;lt;- factor(c(rep(1,260),rep(2,260)))
p &amp;lt;- c()
for(i in 1:10000){
        # 随机化采样
        x &amp;lt;-  sample(c(rep(100,260),rep(200,260))+rnorm(520),520)
        p[i] &amp;lt;- t.test(x~g)$p.value
}
# 探索p值分布
sum(p&amp;lt;0.05)/length(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0406&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(p&amp;lt;0.5)/length(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5344&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(p&amp;lt;0.9)/length(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9356&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(p,breaks = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(p,breaks = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqplot(ref,p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这里我们同样能看到这种蛇形走位，而且似乎随机化样品看到的趋势更明显。但同样的，这里我的模拟逻辑还是保持数据不变，只是随机化过程。&lt;/p&gt;
&lt;p&gt;实验学科已经被多重检验问题困扰了很久了，通常演示p值分布很多人是喜欢从一个已知分布里反复抽样形成差异，此时的p值分布是有偏的：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
pvalue &amp;lt;- NULL
for (i in 1:10000){
  a &amp;lt;- rnorm(10,1)
  b &amp;lt;- a+1
  c &amp;lt;- t.test(a,b)
  pvalue[i] &amp;lt;- c$p.value
}
# 探索p值分布
hist(pvalue,breaks = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(pvalue,breaks = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/05/21/statistical-inference-in-parallel-world/index_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;然后因为随机抽样得到的是p值均匀分布，所以很多错误发现率的控制算法都是在想办法区分这两种分布。现在常用的BH矫正、Q值法或者Bonferroni法都依赖检验数量与前面说的分布差异来决定判断标准或者控制整体错误率。这个看上去很合理，因为你检验多了出现随机相关的可能性就是高了。然而这个过程又很不合理，因为我们测量时有时候并不知道测量的维度是不是跟分组有关系，只是顺手测了，这类测量浪费了大量的统计功效。这也是传统实验学科跟组学实验学科经常扯皮的地方，传统做一对一的控制实验，如果测到了一个有意义的信号就可以发表，但技术进步后，因为我同时测了其他其实无意义的信号，那些有意义的信号被掩盖在随机相关里了。前面所说的多重检验问题的处理思想通常是保留强信号，这样本身有规律的弱信号就被自动忽略了。也许我们可以认为效应比较弱的信号不如效应比较强的信号，但只要存在规律性，作为研究人员就一定要去搞清楚是咋回事而不是用统计学工具给自己做挡箭牌。&lt;/p&gt;
&lt;p&gt;这样，前面那个p值分布的意义就很明确了：如果规律会造成数据异质性而我们的分组过程就是试图发现这种规律性，那么不可避免的会在p值分布上造成离散分布的状态。相反，随机相关则不会呈现出这种p值的离散分布而是均匀分布。这个均匀与离散分布的差异如果能用一个统计量来描述，那么我们事实上就能根据这个统计量（暂且命名为统计量MY）区别出真实规律与随机相关。我现在能想到的构建方法非常原始，就是对直方图概率密度曲线的0.9到1这一段找最大值与最小值，如果比值超过一个阈值就认为有规律性，否则就认为是随机数据。但应该有更数学化的构建方法。&lt;/p&gt;
&lt;p&gt;在这个语境下，我们就不用搞这些p值的阈值矫正了，直接对每一次假设检验进行分组随机化模拟过程，然后生成p值分布。如果其MY值表示为离散均匀分布，那么这一组假设检验的规律性就是有保证的，如果指示为均匀分布，那么这组数据本身就可以判定为无法检测规律而排除。这样我们可以对数据在进行统计推断前做一个规律性测试，只有通过了规律性测试的数据才值得进行统计推断。而且，只要单次统计推断给出小于0.05的p值，我们就可以直接相信，因为那些可能出现随机相关的数据已经被我们排除掉了。&lt;/p&gt;
&lt;p&gt;不过，到这里我的知识水平已经到头了，因为这个MY值如何构建我是不知道的，现阶段我能想到的解决方案一个是直接用眼看，一个是动用图像识别的机器学习算法识别这类图像，也就是让机器看。不过这个思路应该问题不大，说白了就是利用模拟探索产生真实数据的可能性空间或平行宇宙而不是利用分布产生仿真数据，后者其实是先定在了某一种宇宙之中，这里基本延续了Fisher精确检验的思路，计算上也是可行的。&lt;/p&gt;
&lt;p&gt;其实我也不是凭空想思考这个问题，前两天中午有个报告里演讲者提到了Fisher精确检验跟因果分析的关心，我当时忙活着做午饭没听明白她到底讲了啥，但Fisher精确检验的思想确实听明白了。然后吃着午饭就想到了这个随机化分组是不是影响p值的问题，一开始我用的是直方图的默认输出，结果发现p值总是均匀分布感到很丧气，但因为在模拟中本来是1000次多输了一个0就打算看看更精细的直方图，这才看到了这个被掩盖的p值离散分布。我应该不是第一个看到这个分布的，但将其用到替换多重检验的错误率控制上应该是个比较有前景的应用，欢迎读者来给我拍砖，要是没人写过论文（不大可能）也可以拿去写论文，我知道的已经都写出来了，欢迎引用或合作。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>暴露组学中的数据挑战</title>
      <link>https://yufree.cn/cn/2021/03/27/exposome-challenge/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/03/27/exposome-challenge/</guid>
      <description>&lt;p&gt;最近做了次闭门分享，是关于暴露组学中存在的数据挑战，也借机整理了下暴露组学的资料，这里也留个底。&lt;/p&gt;
&lt;p&gt;暴露组学认为生物表型或疾病是环境与基因在时间尺度上交互作用的结果，也就是：&lt;/p&gt;
&lt;p&gt;$$P(t) = G(t) * E(t)$$&lt;/p&gt;
&lt;p&gt;而暴露组学更多关注环境影响及其与基因的交互作用，毕竟单纯的基因影响已经被基因组那边搞得差不多了。&lt;/p&gt;
&lt;p&gt;暴露组学(exposome)最早是2005年C. P. Wild 在期刊 Cancer Epidemiol Biomarkers Prev 的&lt;a href=&#34;(https://cebp.aacrjournals.org/content/14/8/1847.short)&#34;&gt;社论&lt;/a&gt;上提出的。2010年，Rappaport 与 Smith 在 Science 上发表了题为 Environment and Disease Risks 的展望&lt;a href=&#34;https://science.sciencemag.org/content/330/6003/460&#34;&gt;文章&lt;/a&gt;，认为暴露不应限制在直接接触到的化学物质，也要考虑更广义的暴露，例如微生物暴露与生活压力等。&lt;/p&gt;
&lt;p&gt;2015年，美国环保署举办了 &lt;a href=&#34;https://sites.google.com/site/nontargetedanalysisworkshop/&#34;&gt;Non-Targeted Analysis Workshop&lt;/a&gt; 来讨论环境与生物介质中外源化合物的标准筛选方法、标准品制备与谱数据库的开发，后来演变成了涉及来自学术界、政府、公司近30家实验室的 ENTACT(EPA&amp;rsquo;s non-targeted analysis collaborative trial) 项目。ENTACT 项目的参与单位包括八家政府机构 (California Dept. of Public Health, California Dept. of Toxic Substances Control, Eawag, EPA, NIST, Pacific Northwest National Laboratory, Research Centre for Toxic Compounds in the Environment, US Geological Survey)，五家公司（AB Sciex, Agilent, Leco, Thermo, Waters）与十五家学术机构（Colorado School of Mines, Cornell Univ., Duke Univ., Emory Univ., Florida International Univ., Icahn School of Medicine at Mt. Sinai, North Carolina State Univ., San Diego State Univ., Scripps Research Institute, Univ. of Alberta, Univ. of Birmingham, Univ. of California at Davis, Univ. of Florida, Univ. of Washington, WI State Laboratory of Hygiene）。在环境领域要想做 NTA 最好去这些地方，因为参与 ENTACT 项目的机构定期会测盲样进行方法比对，基本可以接触到 NTA 最顶尖的技术，相信以后相关环境标准也会脱胎于这个项目。欧洲也有个类似的项目叫做 &lt;a href=&#34;https://ec.europa.eu/programmes/horizon2020/en&#34;&gt;Horizon 2020&lt;/a&gt;，国内目前还是野生游击队状态，不论学术界还是业界大都在炒概念，落地案例有限。&lt;/p&gt;
&lt;p&gt;除了化学污染物，暴露组学也会涉及到社会科学的研究方法。2016年，Global Burden of Disease (GBD) 项目估计全球59.9%的死亡来自各类外部风险，16%的&lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/29056410/&#34;&gt;全球死亡&lt;/a&gt;来自于水、大气、土壤污染，其造成的健康相关开支每年约4.6万亿美金（16%的全球经济产出）。基于双胞胎的&lt;a href=&#34;https://www.nature.com/articles/s41588-018-0313-7&#34;&gt;研&lt;/a&gt;&lt;a href=&#34;https://www.nature.com/articles/ng.3285&#34;&gt;究&lt;/a&gt;也发现遗传因素大概能解释49%的人类特质，剩下的部分就可能来自各类广义上的暴露。&lt;/p&gt;
&lt;p&gt;基于化学污染物的暴露组学在方法学方面主要借鉴全基因组关联分析研究（GWAS）发展为&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0010746&#34;&gt;全暴露组关联分析研究（EWAS）&lt;/a&gt;。但广义的暴露组学涉及的学科非常广，不同学科背景的研究人员可能会使用完全不同的研究思路，整合这些跨学科知识是非常困难的，学科术语墙造成的交流障碍及精细化专业分工导致的研究人员的视野狭隘经常让合作举步维艰。最简单的例子就是当你真的系统性发现某种暴露的意义是远大于另一种暴露时，这对研究重要性比较弱的那部分科研人员的事业发展是毁灭性的，经济或者利益集团在某种程度上已经阻碍了综合性科学问题的探索。&lt;/p&gt;
&lt;p&gt;暴露组学核心科学问题有两个：窗口期与组学。因为健康相关的暴露问题大都是慢性且长期的暴露，需要根据时间尺度变化来推导影响，遗传因素当然也会有时序表达问题但更为保守些，环境因素变化影响可能更大，很多暴露的影响存在窗口期，例如人们孕前或幼儿时期的暴露可能敏感度更高。目前在数据分析方法上，时间序列分析可用来研究被观察者时间尺度上的变化，显著性差异分析可以在实验设计中研究被观察者在单暴露因素下的变化，所谓窗口期，就是找出时间序列分析中被观察者在单暴露因素下的变化时间段。分布滞后模型 Distributed Lag Models (DLM)常被用来讨论时序数据中相关系数与回归因子在时间尺度上的变化。&lt;/p&gt;
&lt;p&gt;而组学涉及更多的是多暴露问题，也就是暴露组学研究并不只关注一种暴露而是系统性关注多种暴露及其相互影响，这点提高了研究的复杂度。现在多暴露数据来源大体可以分为三类：问卷、生物样品及环境样品。问卷数据可以是流行病学调查报告、基于行为的人群画像、基于邮编的社会经济地位或医院的电子病例还有心理学量表；生物样品可以是人的血样、尿样、粪便、头发、牙齿、指甲、汗液等；环境样品则可以是室内灰尘饮用水这类比较个人化的样品分析，也可以是遥感数据、环境监测或被动采样技术下拿到的区域数据流，还可以是更大尺度上的气候变化模型的预测值。这里单人单样本单时间点的暴露组维度可以上万，毕竟就算描述一个小分子，我们能给出的分子描述符也可以成千上万，暴露组涉及成千上万的小分子与各类其他指标，这里降维是必须要做的，不然单是描述暴露组都成问题。不过暴露组并不像遗传信息那样比较稳定，暴露组的动态变化是一定存在的，不同疾病的相关暴露组是不一样的，提高动态数据中信噪比的难度不小。当前的数据分析思路就是通过构建整体影响指标来指代不同污染物的综合加权影响，但又要保证可以回溯出单一污染物的影响。用统计学语言来说就是构建潜在变量，计算不同暴露在该变量上的投影。说到这里可能你会认为不就是因子分析，但我们能拿到的真实数据并不总是连续的，有些还存在严重的缺失问题，对此加权分位数加和回归 Weighted Quantile Sum (WQS) regression 提供了一种思路。&lt;/p&gt;
&lt;p&gt;从数据挑战上看，除了关键的术语壁垒问题，另一个挑战就是高质量的数据采集与管理，这看上去像是技术问题但所有洗过数据的人都知道其中存在多少莫名其妙的问题，这里行业内一定要同一标准，否则大量资源会被浪费在高噪音数据中。高维数据处理其实可以借鉴机器学习的一些思路与方法，但一定要先理解实际问题，因为现在仪器能采集的信号实在太多，最好的降维就是利用专业知识排除掉噪音。缺失值与宏模型训练也是一个挑战，一个人的暴露组数据几乎一定是不全的，你可能只有A的血样与环境样品而有B的尿样与调查问卷，这里很有可能A跟B的数据都包含了足够预测某种疾病的信息，这里就需要训练一个模型的模型来处理暴露组不全的预测问题。此外，暴露组与基因组给出的结论可能在传统的病理学研究看来是离经叛道甚至不合逻辑的，如何跟传统学科对接也是要处理的问题。当然，前面的讨论还都是建立在有数据的前提之下，暴露组还面临个人隐私泄露相关的医学伦理问题，恐怕只能依赖密码学的发展提供工具。&lt;/p&gt;
&lt;p&gt;在当前的阶段，讨论暴露组学更多还是在厘清科学问题并搭建方法与模型的阶段，虽然研究目标很明确但涉及问题过于复杂。我现在其实在为回国找工作，问了一圈高校的环境学院，能单独配高分辨质谱做相关研究的地方一只手就能数过来，现在已经基本放弃去环境学院打算找医学院或交叉学科研究机构了。不过这个方向我还是很看好的，越是复杂的问题，解决起来才有挑战性，如果以后继续科研这条路，我会设计一个上万人的队列研究，追踪一批人至少十年收集一组可重复进行各类研究的样品与数据，希望能得出些许靠谱点的结论。只希望技术发展再快一点，把追踪的成本降到合理范围，用高质量数据回答科学问题而不是像现在这样天天证（chao）明（zuo）概念。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>jupyter与容器技术</title>
      <link>https://yufree.cn/cn/2021/01/28/container/</link>
      <pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/01/28/container/</guid>
      <description>&lt;p&gt;我们现在评价科研成果，经常喜欢用当前的共识这个表述，但前沿领域的共识不见得是对的。更严格说，所有涉及观点站队问题的讨论都不属于科学讨论，面对质疑经过合格训练的科学家都不会简单采信某一方观点而是要去看原始数据或在自己的实验室重现，让事实说话要比空对空有意义的多。不过重复实验在现代社会能不能拿到经费是存疑的，在当前经费获取的方式下，重复研究别人的成果是费力不讨好的，更多人重复他人实验是为了基于此进行进一步研究，如果重复不出有些课题组会尝试其他路径，有些就去联系原作者询问实验细节。&lt;/p&gt;
&lt;p&gt;我博士阶段末期就收到过关于一组植物愈伤组织实验重复的询问，当时我特意去重复了实验还发了操作过程照片过去，结果那边还是重复不出来。我到现在也搞不清楚是哪里的问题，因为我重复后结果跟发表的论文是一样的，但对方咋做的就不知道了，而且他那边其实对我的体系进行了本地化修改，暴露物也换掉了，但在我看来应该不影响结果。因为这段往事让我意识到可重复性里面可能包含了质疑者与原作者都没意识到但很重要的步骤，而这个步骤直接导致了重现性不好，这就是真实科研或者说实验学科会遇到的问题。好比一份菜谱给出来，两个厨子炒出了两份口味不同的菜，如果菜谱符合要求，那么一定是存在菜谱外的东西影响了结果。这种情况在化学实验里不常见，因为化学实验体系通常比较简单，但生物实验就非常常见了，同样成分的培养基经常一个能养活细胞，另一个养啥死啥，这也是为什么生物论文实验里用的材料是要标注厂家信息的，因为很多实验对材料的要求只能用厂家品控来保证重复性。&lt;/p&gt;
&lt;p&gt;实验的可重复性眼下可控性里操作空间还是有的，但眼下实验完成后的数据处理与共享部分如果透明化，那么会极大挤压学术不端的空间。虽然同样一组数据在不同的分析方法或统计模型下可能得到完全不同的结论，但只要你能把数据如何分析的过程及原始数据共享出来，那么也是可接受的。打比方要做一组高维数据的机理与预测模型，你用随机森林选出一组重要变量并基于此构建了逻辑通顺的机理模型，隔天用同样的数据别人用线性混合模型又选出一组变量也构建了逻辑通顺的机理模型，但这两个模型降维上用了完全不同的策略与假设，导致两组模型虽然都说得通但都可能只解释了真相的一部分，这个也属于常见现象。&lt;/p&gt;
&lt;p&gt;当前期刊一般都会要求上传原始数据，但这是不够的，博士阶段我读了一篇论文看到了里面一种计算方法很有意思，但他们给的是数学公式，我是在matlab上重现后才能去验证。这个操作对于实验学科的人而言门槛过高，绝大多数实验学科研究人员的数据分析水平不会超过调用别人写好函数处理数据的阶段，指望自己写需要自学很多东西，这虽然应该是合格研究人员应具备的素质，但难度还是有的。我现在读很多论文可以感到明显的割裂感，就是实验部分与数据分析是分工来做的，这就导致很多描述非常不准，例如简单利用p值来说明结果而意识不到p值本身的问题，很多数据分析方法描述非常奇怪，明明一两句就能说清楚但却自己定义了一大堆东西来绕弯，很明显对分析方法原理没搞清楚。&lt;/p&gt;
&lt;p&gt;这属于无效协作，实验方与数据分析方想按照分工原理来提高效率，但最后展示出来的则是一团乱麻。一篇论文一定要有一个人能同时理顺实验与数据分析的所有步骤，这看上去很不合理但没办法，从我跟单位里所谓专业统计分析人员打交道的体验来看，那种强调要把数据做成他们那种行是样品列是特征的标准格式的要求是荒谬的，真实数据要转化到那种标准格式是要进行大量假设的，这部分实验方通常不了解，数据处理方又不管，最后给出的结果常常导致实验方无法验证而数据处理方又对数据中普遍存在的异常值与确实值大为恼火。在我看来根本就不能割裂开实验与数据分析，这两步需要同一个人来做，而且职业做实验的一定会被自动化仪器取代而职业处理标准数据分析的也一定会被自动化软件取代，唯独互相连接的人是无法被技术取代的。&lt;/p&gt;
&lt;p&gt;当前的解决方案对于实验人员而言就是保留完整的数据分析脚本，这个本来很正常的需求因为图形化商业软件的流行而被认为很不友好。说句不好听的，这就是被图形界面给惯坏了而忘了科研中对重复性的要求，而且大多数专业图形界面的数据分析软件其实也会记录操作步骤，你的每一步点击都会在一份记录中被保留，所以数据分析步骤与图形界面并不矛盾。不过从实际数据分析角度，如何给出脚本确实是个技术问题而jupyter项目则在一定程度上解决了这个问题。&lt;/p&gt;
&lt;p&gt;在介绍Jupyter项目之前，我想说如果你的数据分析完全依赖 R 与 Python，那么自带 RStudio 服务器版的 Rocker 镜像配合 Rmarkdown 文档就已经可以实现数据分析的完全可重现性了，甚至 Rmarkdown 文档本身就可以作为完整数据分析步骤的良好载体而附加在论文附件里来保证可重复性。当然如果你懂一点R包开发，把分析方法作为模版嵌到一个R包里也是没问题的。今天想说 jupyter 项目，纯粹是因为我最近考古发现现在 jupyter 已经从 Python 的轻量级在线开发环境成长为多语言支持的平台了，其部署上也非常容易。当然，其对学术写作生态的支持对比 Rmarkdown 的生态还是弱了非常多，更偏探索，当然依赖pandoc的核心都可以互相转换，但感觉学术界，特别是实验学科目前对R的接受度更高，毕竟学术数据分析所需要的统计工具 R 基本都有现成的，Python 在这方面虽然机器学习的包更全，但实际研究里需要的工具就不完整。R 包的社区里存在大量即懂专业知识又懂统计分析的人，会给出很多研究人员直接用的函数，当然很多开发者并不太在意效率问题。Python的社区里也有科研人员，但实验学科的不多，整体社区偏软件工程偏通用计算问题。不过理想状态是两种语言都掌握，一种做到开发级，另一种做到应用级对于绝大多数科研数据分析问题就都能处理了。&lt;/p&gt;
&lt;p&gt;Jupyter 项目最开始就是专门为 python 设计的，后来逐渐发展壮大，可以支持更多的语言，前提是你要把对应的核装上保证交互通信畅通。这里顺道也捋一捋 Python 的安装问题，现在人装软件都是全家桶，你单装一个 Python后面一样有大量的依赖问题，因此大多数都是去装一个anaconda，这是基于Python的数据处理和科学计算平台发行版，但其实也支持R。可以把 anaconda 理解为 TeX Live 之于 TeX 排版系统的关系，作为发行版，基本要囊括编程语言本身、集成开发环境（IDE）、常用包及包的管理器。例如，TeX live里就会打包排版引擎、参考文献处理引擎、文档格式转换、字体、常用宏包、包管理器 tlmgr、文档编辑器 TeXworks 等几乎所有你可能用到的工具与文档。anaconda 里你可以装 Jupyter notebook作为IDE，也可以装PyCharm作为IDE，甚至可以装RStudio，其包管理用的是 conda，用法上类似 Python 的 pip 安装，但 conda 不仅仅支持python，你是可以用 conda 来装 R 包的，而且其解决依赖问题也比较智能（pip其实也可以做到）。不过，因为 anaconda 本身也是个公司，有付费产品，免费产品界面也有点花哨，所以很多有洁癖的人会选 miniconda 这种精简版。&lt;/p&gt;
&lt;p&gt;Jupyter 经典版就是交互式笔记本，也是最早得以流行的核心功能，用户可以在代码块里写代码，然后运行代码块直接看到结果。熟悉 R 的会发现这跟 knitr 的功能类似，不同点在于 knitr 对于代码块的控制更多，侧重一次编译出带结果的成品文档，而Jupyter notebook 侧重实时输出结果或再现结果，更接近 REPL 但不算是个好的开发工具。在实际写文档时，相信多数人会选择调用包里的函数而不是现写一个，所以 Jupyter notebook 在交互要求高时也还算不错。虽然大多数人用 ipython 作为解释器，但通过 IRkernel 这个R包并初始化后其代码块也可以执行 R。在 RStudio 里，也可以创建类似的 R Notebook，并用 reticulate 包来使用 python。Jupyter 笔记本是完全采用网页界面形式进行交互（其实 RStudio 也是），笔记本的下一代产品是 JupyterLab ，这个界面更接近一个全功能的IDE了，也是基于 notebook的，可以装各种插件来提高效率，所以现在上手可以直接从 JupyterLab 开始。&lt;/p&gt;
&lt;p&gt;Jupyter项目中最吸引我的是Jupyter hub，前面的在线笔记本是一个人用的，Jupyter hub可以将笔记本发布到网上供多人使用。在R里面我们做网络应用一般用Shiny，后台跑的是R，如果借助Jupyter hub，我们也可以把一个交互式应用放到网上，这里可以用带有Jupyter hub的docker镜像进行快速部署，也可以借助k8s部署到集群上。如果你只打算在一台服务器上做一个轻量级的在线应用或分享一个笔记本，用户不超过100人，可以直接用The Littlest JupyterHub 来部署，这个应该是个很好的教学工具平台，不过R里也有learnr包作为对比。&lt;/p&gt;
&lt;p&gt;另一个值得关注的项目是binder，在这个项目里你可以直接分享一个笔记本，在这个&lt;a href=&#34;https://mybinder.org/&#34;&gt;网站&lt;/a&gt;，只需要告诉binder你的 Github 库地址就可以，当然这个库里得有笔记本。这个项目相当于给了公共计算资源，你的GitHub笔记本会被生成一个 docker 镜像，然后借助dockerhub展示给用户，这个项目目前是免费的，也支持R，请不要滥用。&lt;/p&gt;
&lt;p&gt;有了jupyter项目与容器技术，科研数据分析的流程就可以实现在线化与可重复性，其实如前所述单纯R的生态也可以做到。这样研究成果发表时应该同时附带对应的数据分析流程报告且最好这份报告也支持在线验证，这样就很容易从技术上堵掉图片误用这类说不清道不明的漏洞。上传原始数据并同时上传数据处理脚本，所有处理过程都让电脑来完成，这样审稿只需要关注处理方法是否合理就可以了，因为这里面从数据到结果没有可以人为干涉的空间。我相信重要的发现一定是可重复的，那么起码要保证数据到结果之间100%的重现性。&lt;/p&gt;
&lt;p&gt;在验证学术问题上，实验数据比专家口水更管用，代码自动化重现要比手动点击靠谱。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>中介分析</title>
      <link>https://yufree.cn/cn/2020/12/25/mediation/</link>
      <pubDate>Fri, 25 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2020/12/25/mediation/</guid>
      <description>&lt;p&gt;最近看到有研究开始把中介分析引入到环境健康分析里来了，考虑到实验学科的数据分析能力长期落后统计分析，我这里也做个中介，把这种方法通过问题导向方式解释下。&lt;/p&gt;
&lt;p&gt;首先说下中介分析的背景，中介分析现在可以划分到因果分析中，但历史比因果分析要长，其解决的基本问题就是回归的解释问题。打个比方，一个人的身高影响因素可能来自于遗传，也可以来自后天营养水平，如果我仅仅研究遗传那部分，那么可以列一个公式：&lt;/p&gt;
&lt;p&gt;$$Y(身高) = X（遗传）+\epsilon$$&lt;/p&gt;
&lt;p&gt;遗传最简单的就是用父母平均身高，这个回归公式可以说是线性回归的起点。然而，父母身高虽然可以作为一个综合指标可以用来做预测，但我们并不知道机理，也就是究竟是什么样的遗传机制决定了身高。今天我们组学技术泛滥，可以在分子水平去定量身高的遗传影响机制，假设我们关注的是基因，也就有了下面这两个公式：&lt;/p&gt;
&lt;p&gt;$$Y(身高) = M_1（基因1）+ M_2(基因2)+&amp;hellip;+M_n(基因n)+\epsilon$$&lt;/p&gt;
&lt;p&gt;$$X(遗传) = M_1（基因1）+ M_2(基因2)+&amp;hellip;+M_m(基因m)+\epsilon$$&lt;/p&gt;
&lt;p&gt;这里基因就成了解释遗传影响身高的中介，要知道这个中介身份不是任意找的，是学科知识引导的，你只有认可基因决定遗传这个生物学遗传学的说法，才能构成这个身高-基因-遗传这个中介关系。也正是因为这一步依赖理论逻辑，所以可以纳入因果分析之中考量，用来回答因果机制问题。这里果就是身高，因的笼统指标就是父母身高，机制指标就是各种基因。这个结构范式很容易用到其他领域，用来解释两个综合指标间发生联系的具体机制，例如在暴露组学中，发现某种污染物与某种疾病间的联系是比较容易的，但解释污染物导致疾病的分子机制就需要毒理学研究。但一个很可能出现的情况就是污染物A影响了代谢物甲，代谢物甲会影响疾病1，但我们却不能直接观察到污染物A与疾病1的关系，这里的盲点在于代谢物甲的调节机制可能是非线性的，但如果不发现这个调节机制我们对污染的评价就是不全面的，这里就需要中介分析。&lt;/p&gt;
&lt;p&gt;在中介分析的语境下，我们听到更多的是直接效应与间接效应，此时有可能同时存在X对Y的直接影响与X通过M影响Y的间接效应。当然你也可以理解为X是一个综合因素，我们提出的M只能解释X的一部分，剩下解释不了的都成了X的直接效应。但无论如何，这个中介物一定要是比X解释性更强的因素而不能是更综合的，否则你没法分析机理，打比方身高你可以用牛奶饮用量作为X，用牛奶里某种维生素作为M，但要是你打算解释维生素如何影响身高，就不能把牛奶饮用量当中介物，统计模型上是没问题的，但因果逻辑上属于胡说八道，除非你打算提出一个新理论说维生素通过改变牛奶成分影响身高，不出意外会被审稿人花式玄学调侃。&lt;/p&gt;
&lt;p&gt;那么中介分析怎么做，其实你问题想明白了解决思路也就比较清晰了，首先做直接的回归：&lt;/p&gt;
&lt;p&gt;$$Y = X+\epsilon$$&lt;/p&gt;
&lt;p&gt;然后，用你的M去解释下X：&lt;/p&gt;
&lt;p&gt;$X = M+\epsilon$&lt;/p&gt;
&lt;p&gt;之后，把M跟X结合起来去解释下Y：&lt;/p&gt;
&lt;p&gt;$Y=X+M+\epsilon$&lt;/p&gt;
&lt;p&gt;从第一个公式，你可以知道X是否影响Y；从第二个公式，你可以知道中介M有没有找对；第三个公式，你可以知道控制了M后X是否还有直接影响。这里所谓的是否有影响就是看回归系数是否与0有显著差异。在最后一个回归上，如果加入M后，X没直接影响了，那么这个中介就能完全解释Y了，也就不用考虑X了。然而，如果存在直接影响，那么就可以进一步评价直接与间接影响大小了。这三个回归做完，基本就清楚中介效应是否存在了。当然，这个框架没法处理我前面说的非线性问题或交互作用，不过基本就能厘清传统中介分析怎么做了。你可以得到中介效应（ME）与直接效应（DE），总效应自然就是第一个回归给出的结果，其实也就是前面两个效应的加和。不过真实实验数据，M与X可能同时被其他因素影响，X与Y也可能被其他因素影响，这些因素可能已知，可能未知。数据的收集也很难符合X跟M都随机的要求。&lt;/p&gt;
&lt;p&gt;此时就要引入因果分析了，其实就是对传统模型泛化，把直接效应与间接效应再各分为控制的（controlled）与自然的（natural）来将交互作用纳入到效应考察之中。这里需要额外做四个假设：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对控制效应与自然效应，要保证X与Y之间没有混杂因素，也就是共果&lt;/li&gt;
&lt;li&gt;对控制效应与自然效应，要保证M与Y之间没有混杂因素&lt;/li&gt;
&lt;li&gt;对自然效应，要保证X与M之间没有混杂因素&lt;/li&gt;
&lt;li&gt;对自然效应，要保证M与Y间没有由X引发的混杂因素&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个数据要求就比较高了，特别是最后两个，但如果假设没问题，可以用 Pearl’s mediation formulas 去估计总直接效应与纯间接效应或总间接效应与纯直接效应，此时X与M的交互作用会被总效应吸收。不过这些计算交给软件去做就可以了，你只需告诉模型是否要考虑交互作用，或者可以都试试。另外，中介效应需要做敏感性分析，一般用bootstrap来做。&lt;/p&gt;
&lt;p&gt;如果你真打算用中介分析，一定要搞清楚你的模型假设是啥，要对数据结构有充分理解，这也是所有因果分析都需要注意的。对数据假设越多，模型系统偏差可能就越多或越少，不要为了用中介分析去用中介分析，有时候很简单的模型或许粗糙，但结论却更可靠，只要你清楚你在干什么。&lt;/p&gt;
&lt;p&gt;参考资料&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.publichealth.columbia.edu/research/population-health-methods/causal-mediation&#34;&gt;因果中介分析的介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/mediation/index.html&#34;&gt;R的中介分析包：mediation 包&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/doing-and-reporting-your-first-mediation-analysis-in-r-2fe423b92171&#34;&gt;基于鸢尾花数据做的中介分析案例&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>降维可视化</title>
      <link>https://yufree.cn/cn/2020/10/28/dimension-reduction/</link>
      <pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2020/10/28/dimension-reduction/</guid>
      <description>&lt;p&gt;高维数据的降维可视化是其探索性分析的起点，无法直观显示的模式或规律很难令人信服，当然能直观显示并不代表对应模式或规律的真实存在。这里我们关注的是高维度样本间的模式或规律而不是维度间的关系，因此可视化也是基于样本间关系的，特别是其异质性或相似性。虽然是三维空间生物，我们的视觉捕捉的是二维图像然后大脑利用透视阴影等信息处理为立体信息，但在可视化问题上搞透视跟阴影经常会产生误导，所以这里只讨论降维到2D平面的方法。样本间的关系在二维平面上无所谓就是用距离来描述，离得近表示样本相似，离得远表示样本差异大，这是降维可视化理论上的出发点，但一定注意很多常用降维方法并不保证这个出发点，需要理解算法搞清楚适用环境与目的，不要为了使用降维而降维。这里讨论下降维可视化常见方法及其在科研中的应用。&lt;/p&gt;
&lt;h3 id=&#34;主成分分析pca&#34;&gt;主成分分析（PCA）&lt;/h3&gt;
&lt;p&gt;主成分分析是科研中最常用的降维可视化手段，其基本原理是方差最大化来选择新的投影方向，新的投影是原有多维数据维度的线性组合。主成分分析的具体实现可以是特征值分解，也可以是奇异值分解（SVD），其中特征值分解是早期主成分分析的主要实现手段而奇异值分解则可以有更广的应用范围，例如不用去算协方差矩阵就可以给出奇异值及容易并行化等。&lt;/p&gt;
&lt;p&gt;主成分分析的降维可视化就是利用方差累积解释度较高（例如超过80%）的维度来展示样品，因为主成分分析可以降序给出各主成分的方差解释度，当降序后前几维方差累积解释度较高时就可以用较少的前几个维度或主成分来可视化较高维度的数据了。至于说各主成分上原始维度的投影及其物理意义就需要数据背后的知识来支持了，新的维度可被看成潜在变量，因此主成分分析可被认为因子分析的一种，用来揭示高维数据背后潜在正交变量。同时主成分分析对数据的要求是存在线性共相关，要是这条不满足，主成分分析无法实现降维。而正交变量有没有物理或生物意义主成分分析是不管的，解释不通可能说明数据不适合主成分分析的线性假设。&lt;/p&gt;
&lt;p&gt;另外需要注意为了各维度线性组合权重类似，各维度要标准化到同一尺度。不满足正态分布的可能还要进行对数处理，不过是否进行对数处理需要考虑维度间是加性还是乘性，因为你对数转化后主成分分析重组的维度描述的是维度间累乘的结果，你的每一步数据处理都影响对可视化的解释。举个例子，如果你给了数据点的长宽高三个维度，那么对数转化后映射出的主成分就是样品的体积差异，说到底还是要理解你数据的物理意义来降维。为了追求统计方法的使用范围而进行的对数化处理是削足适履，统计方法应针对复杂实际问题提出解决方案而不是反过来荒唐地要求事实符合统计假设。&lt;/p&gt;
&lt;p&gt;在分析化学中，测定的物质信号一般与其浓度或含量成正比，此时要不要取对数要明白物质浓度的生物学意义。生物学效应一般符合剂量效应曲线（更本质驱动的是化学反应动力学过程在酶促反应中的表现），很多时候是S型或线性的，此时剂量是要取对数。如果想看到表型上的多维度效应的线性组合，在分子层面需要对浓度信号做对数转换的，这就给了降维时对数处理合理的物理意义。同样的，如果两个分子存在于同一个路径或代谢通路上，其效应是累乘的，此时想在降维中保存样品中这类信息的作用就要做两次对数转化。但如果你不是考察生命过程或考察过程的剂量效应曲线是其他形态的，那就要仔细思考取对数的合理性，你思考越多后面的问题越少。&lt;/p&gt;
&lt;h3 id=&#34;独立成分分析ica&#34;&gt;独立成分分析（ICA）&lt;/h3&gt;
&lt;p&gt;矩阵分解并不只是有主成分分析这一种，如果我们放开对成分间正交的限制，也可以用来提取独立成分。独立成分分析里独立的定义就比较艺术了，不同算法可能使用了不同统计量，但不论哪一种都是用来保证从高维数据中提取相似度高的维度然后压缩为一个新维度。事实上独立成分分析更符合实验科学中对样品描述的假设，但无奈主成分分析还是当前主流。&lt;/p&gt;
&lt;h3 id=&#34;非负矩阵分解&#34;&gt;非负矩阵分解&lt;/h3&gt;
&lt;p&gt;独立成分分析与主成分分析都会引入负数，负数在数学上无可厚非但科学上经常讲不通，因此也有专门针对非负矩阵的分解方式，此时拆出来的新维度之可能是原有维度的加性组合。如果你确定只想可视化数据中的加性组合，可以考虑这种降维方法。&lt;/p&gt;
&lt;h3 id=&#34;多维标度分析mds&#34;&gt;多维标度分析（MDS）&lt;/h3&gt;
&lt;p&gt;多维标度分析的降维思路是定义一个stress函数，最小化低维度坐标距离与高维度样本距离，然后这组坐标就可以捕捉到高维度样本间的关系了。当高维度样本距离是欧式距离时，最小化这个函数等同于求解主成分分析，所以你可以把主成分分析看成多维标度分析的一个特例。但如果距离不是欧式距离或者你对数据进行了非参转化，那么多维标度分析就可以看成一种低维展示样品的技术了。这里可以定义做几维的映射，而且需要的是距离矩阵而非原始数据就可以进行降维，这都是多维标度分析的应用场景更广。当然，跟主成分分析类似，降维都是要丢信息的，可以通过一些统计量来评价降维后是否反映了原始数据的信息。&lt;/p&gt;
&lt;h3 id=&#34;t-sne&#34;&gt;t-SNE&lt;/h3&gt;
&lt;p&gt;数据样本间的关系可以是线性的，也可以是非线性的，在非线性条件下用主成分分析降维效果不会太好，此时可以尝试 t-SNE。t-SNE 是一种基于流形分析的最优化方法，不过这里最优化的是目标是尽可能保持数据间原有的局部关系，MDS或PCA则侧重了全局映射。在 t-SNE中，样本点间的欧式距离用来计算样本间相似度并认为其符合正态分布，优化目标是让新的低维坐标点间距离分布概率与样本相似度距离分布概率接近，而t-SNE的最终输出其实依赖了图论里输出的算法。t-SNE是有些参数要调的同时也依赖随机点，所以每次生成的低维映射会有不同。如果你选这种方法，那就默认你更关注样本间的局部非线性关系。&lt;/p&gt;
&lt;h3 id=&#34;umap&#34;&gt;UMAP&lt;/h3&gt;
&lt;p&gt;UMAP主要是为了克服t-SNE算起来比较慢的问题，其核心算法对不同样本点临近考虑一个距离半径，如果半径跟其他样本点重合就合并为一组，在UMAP中这个半径并非固定的，而是根据样本本身周围样本点的稀疏情况来决定。这样当高维度样本点间关系确定后通过低维输出就可以了，这部分跟 t-SNE 是差不多。t-SNE 与 UMAP 都是依赖流形分析来解决非线性问题，都用到了拓扑结构的思想。不过还是那句话，不能为了用这种方法而用，你写论文方法部分总不能写这玩意高大上我才用吧，虽然不否认有些编辑确实好这口。&lt;/p&gt;
&lt;h3 id=&#34;自组织映射网络som&#34;&gt;自组织映射网络（SOM）&lt;/h3&gt;
&lt;p&gt;自组织映射网络也出现在一些科研实验数据中，这个降维方法的源头在人工神经网络。传统人工神经网络是反馈学习的，自组织映射网络则是依赖竞争学习的。SOM的输出是自定义的，一般都定义成一个二维平面网格，样本来了通过神经元函数去计算其跟哪个网格输出最接近，相似的样品自然会输出到相似的网格上，可视化出来后就可以在自定义的布局上看到自组织出的样品分布。如果你抽取出神经元来看不同维度的加权情况也可以探索下原始维度在布局上的影响。到SOM降维可视化就已经从捕捉相似性到映射相似性了，相应的对读者的数据分析能力要求也越来越高或越来越黑箱，进入魔术状态就不好了。&lt;/p&gt;
&lt;p&gt;其实除了上面说到的，还有很多非线性降维可视化手段，例如Locally Linear Embedding、Neighbourhood Component Analysis、Autoencoders、Kernel principal component analysis等，但如果你搞明白前面这些，后面这些一来不难理解，二来也会发现可能根本就用不上。科研用降维手段不是炫技，就是解决最开始那个探索样品间关系的问题，而探索也只是形成假设的第一步。诚然如果选错了方法可能错过一些信息，但如果样本间异质性如果足够，上述方法都或多或少能展示出来。如果异质性需要很精细调参数才能展示，更大的可能有两个：样本量不够或现象本身可重复性差，不应在这种数据上浪费资源，重新采样可能是更好的选择。&lt;/p&gt;
&lt;h3 id=&#34;参考资料&#34;&gt;参考资料&lt;/h3&gt;
&lt;p&gt;1 &lt;a href=&#34;https://compgenomr.github.io/book/dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d.html&#34;&gt;Computational Genomics with R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2 &lt;a href=&#34;https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction&#34;&gt;Nonlinear dimensionality reduction-wikipedia&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>线性模型</title>
      <link>https://yufree.cn/cn/2020/10/12/linear-model/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2020/10/12/linear-model/</guid>
      <description>&lt;p&gt;线性模型的基本形式就是因变量是由自变量作用加和而成，在这个语境下，其实把自变量改为变量，放宽独立性限制，也能将一些非线性部分，例如高幂次的自变量及变量间的乘积或交互作用考虑进去，这样，线性模型几乎可以覆盖绝大多数科研中常用的假设检验与模型。在实际问题的抽象上，只要可以把目标数值的变动用其他数值的拆解或组合表示出来，那么可以粗略认为标准化后其他数值的回归系数可用来比较不同数值间的贡献，而对于该系数的显著性检验则可以说明该系数的影响是否显著。&lt;/p&gt;
&lt;p&gt;打个比方，流行病学里常说的某种疾病发病率或风险比在考虑了人群性别、年龄、BMI、吸烟史等的影响后发现某污染物起了显著影响，这就是说在一个目标变量为病发病率或风险比的线性模型中，性别、年龄、BMI、吸烟史作为协变量而污染物作为自变量，模型拟合结束后发现污染物的系数经假设检验为显著差异于零，也就是没影响。这里，协变量与自变量在回归上是平等的，可以把协变量理解为控制变量，如果你考察吸烟的影响，那么吸烟与否就是自变量，包含污染物在内其他项就成了协变量。不过所有考察变量选择的原则在于其理论上或经验上被认为与目标变量有关系且无法通过随机采样、配对等手段消除影响，这种情况对于观测数据比较常见。&lt;/p&gt;
&lt;p&gt;当线性模型的自变量只有一项时，其实考察的就是自变量与响应变量间的相关性。当自变量为多项时，也就是多元线性回归，考察的是你自己定义的“自变量”与“协变量”还有响应变量的关系。如果自变量间不能互相独立，那么最好将独立的部分提取出来作为新的变量，这种发现潜在变量的过程归属于因子分析，可以用来降维。自变量本身存在随机性，特别是个体差异，这种随机性可能影响线性模型自变量的系数或斜率，也可能影响线性模型的截距，甚至可能同时影响，此时考虑了自变量的随机性的模型就是线性混合模型。线性混合模型其实已经是层级模型了，自变量的随机性来源于共同的分布。如果自变量间存在层级，例如有些变量会直接影响其他变量，那么此时线性模型就成了决策/回归树模型的特例了。如果层级关系错综复杂，那不依赖结构方程模型是没办法搞清楚各参数影响的。然而模型越复杂，对数据的假设就越多，对样本量的要求也就越高。同时，自变量或因变量有些时候也要事先进行连续性转换，这就给出了logistics回归、生存分析等特殊的回归模型。科研模型如果是依赖控制实验的，那么会在设计阶段随机化绝大部分变量，数据处理方面到线性混合模型就已经很少见了。但对于观测数据，线性混合模型只是起点，对于侧重观察数据的社会科学研究，样本量与效应大小是结论可靠性的关键，精细的模型无法消除太多的个体差异。&lt;/p&gt;
&lt;p&gt;高维数据是线性模型的一大挑战，当维度升高后，变量间要么可能因为变异来源相似而共相关，要么干脆就是随机共相关。在某些场景下，高维数据可能都没有目标变量，需要先通过探索性数据分析找出样本或变量间的组织结构。这种场景下应通过变量选择过程来保留独立且与目标变量有潜在关系的变量。也就是说，变量选择的出发点是对数据的理解，优先考虑相关变量而非简单套用统计分析流程。当然，统计方法上也有变量选择的套路，评判标准可能是信息熵或模型稳健度的一些统计量，可以借助这些过程来简化模型或者说降维。对于线性模型而言，就是均方误、Mallow’s $C_p$、AIC、BIC还有调节R方等，可借助回归模型软件来完成。&lt;/p&gt;
&lt;p&gt;回归或模型拟合都存在过拟合的风险，所谓过拟合，就是模型对于用来构建模型的数据表现良好，但在新数据的预测性上却不足的情况。与过拟合对应的是欠拟合，此时拟合出的模型连在构建模型的数据验证上表现都不好。这里的表现可以用模型评价的一些指标，其实跟上面进行变量选择的指标是一样的，好的模型应该能捕捉到数据背后真实的关系，也因此在训练数据与新数据上表现一致。&lt;/p&gt;
&lt;p&gt;在统计学习领域里，工程实践上最简单的验证过拟合与欠拟合的方法就是对数据进行切分，分为用来构建模型的训练集与验证模型预测性能的检测集，更细的分法则将检测集分为可在模型调参过程中使用多次的检测集与最后最终评价模型的一次性验证集，三者比例大概6:3:1，也可根据实际情况来定。也就是说，模型的构建不是一次性完成的，而是一个反复调整模型参数的过程来保证最终的模型具备良好的预测性与稳健度。&lt;/p&gt;
&lt;p&gt;在技术层面上，调参过程有两种基本应对方法，第一种是重采样技术，第二种是正则化，两种方法可以组合使用。重采样技术指的是通过对训练集反复采样多次建模来调参的过程。常见的重采样技术有留一法，交叉检验与bootstrap。留一法在每次建模留一个数据点作为验证集，重复n次，得到一个CV值作为对错误率的估计。交叉检验将训练集分为多份，每次建模用一份检验，用其他份建模。bootstrap更可看作一种思想，在训练集里有放回的重采样等长的数据形成新的数据集并计算相关参数，重复多次得到对参数的估计，计算标准误。在这些重采样技术中，因为进行的多次建模，也有多次评价，最佳的模型就是多次评价中在验证集上表现最好的那一组。&lt;/p&gt;
&lt;p&gt;正则化则是在模型构建过程中在模型上对参数的效应进行人为减弱，用来降低过拟合风险。具体到线性模型上，就是在模型训练的目标上由单纯最小化均方误改为最小化均方误加上一个对包含模型参数线性组合的惩罚项，这样拟合后的模型参数对自变量的影响就会减弱，更容易影响不显著，如果自变量过拟合的话就会被这个正则化过程削弱。当惩罚项为模型参数的二次组合时，这种回归就是岭回归；当惩罚项为模型参数的一次绝对值组合时，这种回归就是lasso；当惩罚项为一次与二次的组合时，这种回归就是弹性网络回归。实践上正则化过程对于降低过拟合经常有神奇效果，同时正则化也可作为变量选择的手段，虽然岭回归无法将系数惩罚为0，但lasso可以，这样在参数收缩过程中也就同时实现了变量选择。&lt;/p&gt;
&lt;p&gt;为了说明实际问题，有时候单一形式的模型是不能完全捕捉数据中的变动细节的，我们可以在工程角度通过模型组合来达到单一模型无法达到的预测性能。模型组合的基本思想就是对同一组数据生成不同模型的预测结果，然后对这些结果进行二次建模，考虑在不同情况下对不同模型预测结果给予不同的权重。这种技术手段可以突破原理限制，而最出名的例子就是人工神经网络里不同神经元采用不同核函数的做法了。&lt;/p&gt;
&lt;p&gt;对于科研数据的线性回归，还有两个常见问题，一个是截断问题，另一个是缺失值处理。截断问题一般是采样精度或技术手段决定的，在数值的高位或低位无法采集高质量数据，此时可以借助截断回归等统计学方法来弥补。另一种思路则是在断点前后构建不同的模型，这样分别应对不同质量的数据。对于数据缺失值的问题，统计学上也提供了很多用来删除或填充缺失值的方法，填充数据不应影响统计推断，越是接近的样本，就越是可以用来填充缺失值，当然这个思路反着用就是个性化推荐系统模型的构建了。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
