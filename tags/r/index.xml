<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Miao Yu | 于淼 </title>
    <link>https://yufree.cn/tags/r/</link>
    <description>Recent content in R on Miao Yu | 于淼 </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 26 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://yufree.cn/tags/r/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1997-2020中国研究生数据分析</title>
      <link>https://yufree.cn/cn/2021/12/26/1997-2020-graduate-edu-data/</link>
      <pubDate>Sun, 26 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/12/26/1997-2020-graduate-edu-data/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/cn/2021/12/26/1997-2020-graduate-edu-data/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;最近还在处理《现代科研指北》的书稿，可以确定的是正式出版应该不会用这个名字，新名字我还没想好。书里很多内容与当前在线版有不伤害本质的区别与删节，主要是为了照顾读者情绪与国内出版要求。目前可以认为在线版是开源项目而出版版是基于在线版的二次创作。书稿中原有一段是分析1997-2017年教育部研究生数据的，眼下要拖稿到2022年了，所以我把数据更新到了2020年（因为21年数据还没出），然后就发现很多结论已经验证了，很多新问题又出来了。因为书稿本身内容已经很多了，很多分析就放在这里，前一版我用的是Excel，这版我就全换成R了。&lt;/p&gt;
&lt;div id=&#34;数据来源&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;数据来源&lt;/h2&gt;
&lt;p&gt;原始数据是教育部放在自己&lt;a href=&#34;http://www.moe.gov.cn/jyb_sjzl/moe_560/2020/&#34;&gt;网站&lt;/a&gt;上的，我这里只用了很小一部分。因为教育部按年发布，不同年代间报表有差异，很多统计方法也换掉了，这里我尽量洗了一下。有差异部分会在后面提及。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 研究生数据
graduate &amp;lt;- read.csv(&amp;#39;https://raw.githubusercontent.com/yufree/sciguide/master/data/graduate.csv&amp;#39;, check.names = F)
## 教职数据
faculty &amp;lt;- read.csv(&amp;#39;https://raw.githubusercontent.com/yufree/sciguide/master/data/faculty.csv&amp;#39;, check.names = F)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;研究生规模&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;研究生规模&lt;/h2&gt;
&lt;p&gt;研究生扩招是继高考扩招后另一个“知识改变命运”的国民级叙事，高考现在已经扩不动了，而且因为现在国家对中等职业教育有了硬性规定，适龄人口里大概一半人可能要读职业教育类学校，而剩下的一半多基本都被普通高中吸收了，高等教育入学率已经基本平稳，真考不上的那可能确实就是不适合读书了。然而，选拔性考试压力只会推迟而不会消失，在高考的选拔作用有限后，研究生入学考试就成了下一个战场。&lt;/p&gt;
&lt;p&gt;首先，自然是讨论下研究生规模，这个群体一直是在增长的，而且最近几年增长的更快了。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;graduate2 &amp;lt;- graduate[graduate$category == &amp;#39;Total&amp;#39;,]
# 人数
library(showtext)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: sysfonts&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: showtextdb&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;showtext::showtext_auto()
par(mfrow=c(1,2))
plot(graduate2$year,graduate2$`Enrolment(Master)`,xlab = &amp;#39;年份&amp;#39;,ylab = &amp;#39;人数&amp;#39;,pch=19,col=&amp;#39;black&amp;#39;,ylim=c(min(graduate2$`Admitted(Master)`),max(graduate2$`Enrolment(Master)`)),main=&amp;#39;硕士研究生&amp;#39;)
points(graduate2$year,graduate2$`Graduates(Master)`,pch=19,col = &amp;#39;red&amp;#39;)
points(graduate2$year,graduate2$`Admitted(Master)`,pch=19,col=&amp;#39;blue&amp;#39;)
segments(graduate2$year[-c(22:24)],graduate2$`Admitted(Master)`[-c(22:24)],graduate2$year[-c(22:24)]+3,graduate2$`Graduates(Master)`[-c(1:3)])
legend(&amp;#39;topleft&amp;#39;,legend = c(&amp;#39;在校生&amp;#39;,&amp;#39;录取人数&amp;#39;,&amp;#39;毕业人数&amp;#39;), col = c(&amp;#39;black&amp;#39;,&amp;#39;blue&amp;#39;,&amp;#39;red&amp;#39;),pch=19)

plot(graduate2$year,graduate2$`Enrolment(Doctor)`,xlab = &amp;#39;年份&amp;#39;,ylab = &amp;#39;人数&amp;#39;,pch=19,col=&amp;#39;black&amp;#39;,ylim=c(min(graduate2$`Entrants(Doctor)`),max(graduate2$`Enrolment(Doctor)`)),main=&amp;#39;博士研究生&amp;#39;)
points(graduate2$year,graduate2$`Graduates(Doctor)`,pch=19,col = &amp;#39;red&amp;#39;)
points(graduate2$year,graduate2$`Entrants(Doctor)`,pch=19,col=&amp;#39;blue&amp;#39;)
segments(graduate2$year[-c(20:24)],graduate2$`Entrants(Doctor)`[-c(20:24)],graduate2$year[-c(20:24)]+5,graduate2$`Graduates(Doctor)`[-c(1:5)])
legend(&amp;#39;topleft&amp;#39;,legend = c(&amp;#39;在校生&amp;#39;,&amp;#39;录取人数&amp;#39;,&amp;#39;毕业人数&amp;#39;), col = c(&amp;#39;black&amp;#39;,&amp;#39;blue&amp;#39;,&amp;#39;red&amp;#39;),pch=19)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/12/26/1997-2020-graduate-edu-data/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这里很多人会立即注意到2017年开始硕士研究生突然出现了扩张，这是个统计问题，教育部2017年换了统计方法，研究生招生、在校生指标内涵发生变化。招生包含全日制和非全日制研究生；在校生、授予学位数包含全日制、非全日制研究生和在职人员攻读硕士学位学生在校生的方法。&lt;/p&gt;
&lt;p&gt;目前，我们已经有250万在校硕士生与接近50万的在校博士生，每年录取人数已经达到接近100万的硕士与10万的博士，硕博比基本稳定在10:1。然后我们看下延期现象，我把录取人数与硕士三年后毕业人数跟五年后博士人数做了连接，可以看出硕士毕业基本不怎么延期，但博士那边连线斜率最近几年都是负的，也就是说延期现象很普遍。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;graduate3 &amp;lt;- graduate2[!is.na(graduate2$`Estimated Graduates for Next Year (master)`),]
par(mfrow=c(1,2))
plot(graduate3$year[-1],graduate3$`Estimated Graduates for Next Year (master)`[-5],xlab = &amp;#39;年份&amp;#39;,ylab = &amp;#39;人数&amp;#39;,pch=19,col=&amp;#39;black&amp;#39;,xlim = c(2017,2020), ylim=c(min(graduate3$`Graduates(Master)`),max(graduate3$`Estimated Graduates for Next Year (master)`)),main=&amp;#39;硕士研究生&amp;#39;)
points(graduate3$year[-1],graduate3$`Graduates(Master)`[-1],pch=19,col = &amp;#39;red&amp;#39;)
segments(graduate3$year[-1],graduate3$`Estimated Graduates for Next Year (master)`[-5],graduate3$year[-1],graduate3$`Graduates(Master)`[-1],pch=19)
plot(graduate3$year[-1],graduate3$`Estimated Graduates for Next Year (Doctor)`[-5],xlab = &amp;#39;年份&amp;#39;,ylab = &amp;#39;人数&amp;#39;,pch=19,col=&amp;#39;black&amp;#39;,xlim = c(2017,2020), ylim=c(min(graduate3$`Graduates(Doctor)`),max(graduate3$`Estimated Graduates for Next Year (Doctor)`)),main=&amp;#39;博士研究生&amp;#39;)
points(graduate3$year[-1],graduate3$`Graduates(Doctor)`[-1],pch=19,col = &amp;#39;red&amp;#39;)
segments(graduate3$year[-1],graduate3$`Estimated Graduates for Next Year (Doctor)`[-5],graduate3$year[-1],graduate3$`Graduates(Doctor)`[-1],pch=19)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/12/26/1997-2020-graduate-edu-data/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 平均延期概率
mean(graduate3$`Graduates(Master)`[-1]/graduate3$`Estimated Graduates for Next Year (master)`[-5])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8885&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(graduate3$`Graduates(Doctor)`[-1]/graduate3$`Estimated Graduates for Next Year (Doctor)`[-5])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.363&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;不过，很多学科硕士实际上是两年，而博士因为存在硕博连读很难直接说多少年毕业。然而，教育部最近五年也给出了下一年预计毕业人数，这样我们就可以直接对比毕业人数。上图可以看出最近四年的预计毕业人数与实际毕业人数状况，对于硕士而言11%学生不能按期毕业而博士则是64%，也就是说现在不论读硕士还是读博士都有超过1/10的概率无法按期毕业，每年延期毕业的硕士差不多等于当年读博士的人数，也算是一种讽刺。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;分学科状况&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;分学科状况&lt;/h2&gt;
&lt;p&gt;下面我们看一下不同学科间的差异，这里可以明显看出，扩招在不同学科间是不一样的，硕士的扩招里专业学位、管理学、医学与工学学位的增长是很快的，但文科学位增长不大，军事学哲学甚至在下降，理学增长其实有限。然而，在博士的扩招趋势里，理学、工学与医学是最大的学科，增长也快，其余的学科在过去十年里没怎么扩招。&lt;/p&gt;
&lt;p&gt;对比硕士博士学科差异，可以看出硕士里的管理学经济学硕士多但管理学经济学博士却不多，这说明这些学科硕士大概就挺容易就业的。硕士增长不多博士增长多，例如理学则属于基础学科，需要博士学位才能就业。但如果两者增长都很快，例如农学、艺术，可能是国家需求高，更可能是都不好找工作。另外，硕士专业学位的快速增长可以说明目前国家已经识别到硕士学位的应用方向需求，同时，专业博士学位也出现了，这说明应用方向对知识需求也越来越高。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;graduate2 &amp;lt;- graduate[graduate$category != &amp;quot;Total&amp;quot;&amp;amp;graduate$category !=&amp;quot;Of Which: Female&amp;quot;,]
graduate2 &amp;lt;- graduate2[complete.cases(graduate2[,c(1:11)]),]
library(ggplot2)
ggplot(graduate2,aes(year,`Graduates(Master)`,color = category)) + 
        geom_point()+
        facet_wrap(facets = vars(category),scales = &amp;#39;free&amp;#39;)+
        ggtitle(&amp;#39;硕士&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/12/26/1997-2020-graduate-edu-data/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(graduate2,aes(year,`Graduates(Doctor)`,color = category)) + 
        geom_point()+
        facet_wrap(facets = vars(category),scales = &amp;#39;free&amp;#39;)+
        ggtitle(&amp;#39;博士&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/12/26/1997-2020-graduate-edu-data/index_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;教职规模&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;教职规模&lt;/h2&gt;
&lt;p&gt;看完了研究生状况，自然就该看下国内教职的状况，这里我主要用了教育部的研究生导师数据集。不过，当前国内教职没有博士学位已经基本不可能了，所以就不讨论硕士毕业后就业状况了，大概率是无法留在学术圈的。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,2))
plot(faculty$year[faculty$category == &amp;#39;Total&amp;#39;],faculty$total[faculty$category == &amp;#39;Total&amp;#39;],pch=19, main=&amp;#39;教职数&amp;#39;,ylim=c(0,max(faculty$total)),xlab = &amp;#39;年份&amp;#39;, ylab = &amp;#39;人数&amp;#39;, type = &amp;#39;o&amp;#39;)
points(faculty$year[faculty$category == &amp;#39;Professors&amp;#39;], faculty$total[faculty$category == &amp;#39;Professors&amp;#39;],pch=19,col=&amp;#39;blue&amp;#39;, type = &amp;#39;o&amp;#39;)
points(faculty$year[faculty$category == &amp;#39;Asso. Professors&amp;#39;],faculty$total[faculty$category == &amp;#39;Asso. Professors&amp;#39;],pch=19,col=&amp;#39;red&amp;#39;, type = &amp;#39;o&amp;#39;)
points(faculty$year[faculty$category == &amp;#39;middle&amp;#39;],faculty$total[faculty$category == &amp;#39;middle&amp;#39;],pch=19,col=&amp;#39;orange&amp;#39;, type = &amp;#39;o&amp;#39;)
legend(&amp;#39;topleft&amp;#39;,legend = c(&amp;#39;总数&amp;#39;,&amp;#39;教授&amp;#39;,&amp;#39;副教授&amp;#39;,&amp;#39;中级职称&amp;#39;), col = c(&amp;#39;black&amp;#39;,&amp;#39;blue&amp;#39;,&amp;#39;red&amp;#39;,&amp;#39;orange&amp;#39;),pch=19)
plot(faculty$year[faculty$category == &amp;quot;Supervisors of master&amp;#39;s degree prog.&amp;quot;],faculty$total[faculty$category == &amp;quot;Supervisors of master&amp;#39;s degree prog.&amp;quot;],pch=19, main=&amp;#39;教职数&amp;#39;,ylim=c(0,400000),xlab = &amp;#39;年份&amp;#39;, ylab = &amp;#39;人数&amp;#39;, type = &amp;#39;o&amp;#39;)
points(faculty$year[faculty$category == &amp;#39;Supervisors of doctoral programmes&amp;#39;], faculty$total[faculty$category == &amp;#39;Supervisors of doctoral programmes&amp;#39;],pch=19,col=&amp;#39;blue&amp;#39;, type = &amp;#39;o&amp;#39;)
points(faculty$year[faculty$category == &amp;#39;Supervisors of doc. &amp;amp; mas. Degree programmes&amp;#39;],faculty$total[faculty$category == &amp;#39;Supervisors of doc. &amp;amp; mas. Degree programmes&amp;#39;],pch=19,col=&amp;#39;red&amp;#39;, type = &amp;#39;o&amp;#39;)
legend(&amp;#39;topleft&amp;#39;,legend = c(&amp;#39;硕士导师&amp;#39;,&amp;#39;博士导师&amp;#39;,&amp;#39;硕士博士导师&amp;#39;), col = c(&amp;#39;black&amp;#39;,&amp;#39;blue&amp;#39;,&amp;#39;red&amp;#39;),pch=19)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/12/26/1997-2020-graduate-edu-data/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;由于本科生从1999年开始扩招，教职数也一直增加。不过，高级职称的增加速度明显低于教职总数的增加速度而中级职称教职在快速增加。教授与副教授都在增长，甚至教授人数比副教授还要多。如果我们看下导师资格，会发现硕士导师增长飞快而博士导师数目基本是不变的，大概在2万左右。需要注意的是最近国内教职正在改革，改为预聘长聘的方法，所以以后硕士博士导师可能区别不大了。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;faculty2 &amp;lt;- faculty[faculty$category == &amp;#39;Total&amp;#39;, ]
par(mfrow=c(1,1))
col = RColorBrewer::brewer.pal(8,&amp;#39;Set2&amp;#39;)
plot(faculty2$year,faculty2$`30 Years &amp;amp; Under`,pch=19, main=&amp;#39;教职数&amp;#39;,ylim=c(0,110000),xlab = &amp;#39;年份&amp;#39;, ylab = &amp;#39;人数&amp;#39;,col=col[1], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`31-35years`,pch=19,col=col[2], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`36-40years`,pch=19,col=col[3], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`41-45years`,pch=19,col=col[4], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`46-50years`,pch=19,col=col[5], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`51-55years`,pch=19,col=col[6], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`56-60years`,pch=19,col=col[7], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`61 Years &amp;amp; Over`,pch=19,col=col[8], type = &amp;#39;o&amp;#39;)
legend(&amp;#39;topleft&amp;#39;,legend = c(&amp;#39;30岁以下&amp;#39;,&amp;#39;30-34&amp;#39;,&amp;#39;35-39&amp;#39;,&amp;#39;40-44&amp;#39;,&amp;#39;45-49&amp;#39;,&amp;#39;50-54&amp;#39;,&amp;#39;55-59&amp;#39;,&amp;#39;60岁以上&amp;#39;), col = col,pch=19)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/12/26/1997-2020-graduate-edu-data/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;下面就是教职年龄分布图了，这里需要注意，1997-2013年的教职年龄分布数据分段为30岁（含）以下，31-35，36-40，41-45，46-50，51-55，56-60，60岁以上。&lt;/p&gt;
&lt;p&gt;这里我们可以看到1999年大学扩招后35-39岁段出了个教职高峰，也就是那个年代基本博士毕业就能拿到教职，后面每隔五年我们会看到这个教职高峰的平移，现在已经移到55-59岁了。如果退休年龄不延长的话，大概未来五到十年我们会看到一个退休导致的教职空窗期，现在一年高级教职大概会有两三万，遇到这个高峰会翻倍。不过大概率退休年龄会延长，所以真正能遇到这个空窗期的人大概现在还在读中学或者说现在就要至少是个中级职称去排队。再晚上几年，中国教职就会一个萝卜一个坑了。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,3))
faculty2 &amp;lt;- faculty[faculty$category == &amp;#39;Professors&amp;#39;, ]
plot(faculty2$year,faculty2$`30 Years &amp;amp; Under`,pch=19, main=&amp;#39;教授&amp;#39;,ylim=c(0,80000),xlab = &amp;#39;年份&amp;#39;, ylab = &amp;#39;人数&amp;#39;,col=col[1], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`31-35years`,pch=19,col=col[2], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`36-40years`,pch=19,col=col[3], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`41-45years`,pch=19,col=col[4], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`46-50years`,pch=19,col=col[5], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`51-55years`,pch=19,col=col[6], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`56-60years`,pch=19,col=col[7], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`61 Years &amp;amp; Over`,pch=19,col=col[8], type = &amp;#39;o&amp;#39;)
legend(&amp;#39;topleft&amp;#39;,legend = c(&amp;#39;30岁以下&amp;#39;,&amp;#39;30-34&amp;#39;,&amp;#39;35-39&amp;#39;,&amp;#39;40-44&amp;#39;,&amp;#39;45-49&amp;#39;,&amp;#39;50-54&amp;#39;,&amp;#39;55-59&amp;#39;,&amp;#39;60岁以上&amp;#39;), col = col,pch=19)

faculty2 &amp;lt;- faculty[faculty$category == &amp;#39;Asso. Professors&amp;#39;, ]
plot(faculty2$year,faculty2$`30 Years &amp;amp; Under`,pch=19, main=&amp;#39;副教授&amp;#39;,ylim=c(0,80000),xlab = &amp;#39;年份&amp;#39;, ylab = &amp;#39;人数&amp;#39;,col=col[1], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`31-35years`,pch=19,col=col[2], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`36-40years`,pch=19,col=col[3], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`41-45years`,pch=19,col=col[4], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`46-50years`,pch=19,col=col[5], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`51-55years`,pch=19,col=col[6], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`56-60years`,pch=19,col=col[7], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`61 Years &amp;amp; Over`,pch=19,col=col[8], type = &amp;#39;o&amp;#39;)
legend(&amp;#39;topleft&amp;#39;,legend = c(&amp;#39;30岁以下&amp;#39;,&amp;#39;30-34&amp;#39;,&amp;#39;35-39&amp;#39;,&amp;#39;40-44&amp;#39;,&amp;#39;45-49&amp;#39;,&amp;#39;50-54&amp;#39;,&amp;#39;55-59&amp;#39;,&amp;#39;60岁以上&amp;#39;), col = col,pch=19)

faculty2 &amp;lt;- faculty[faculty$category == &amp;#39;middle&amp;#39;, ]
plot(faculty2$year,faculty2$`30 Years &amp;amp; Under`,pch=19, main=&amp;#39;中级职称&amp;#39;,ylim=c(0,30000),xlab = &amp;#39;年份&amp;#39;, ylab = &amp;#39;人数&amp;#39;,col=col[1], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`31-35years`,pch=19,col=col[2], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`36-40years`,pch=19,col=col[3], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`41-45years`,pch=19,col=col[4], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`46-50years`,pch=19,col=col[5], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`51-55years`,pch=19,col=col[6], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`56-60years`,pch=19,col=col[7], type = &amp;#39;o&amp;#39;)
points(faculty2$year,faculty2$`61 Years &amp;amp; Over`,pch=19,col=col[8], type = &amp;#39;o&amp;#39;)
legend(&amp;#39;topleft&amp;#39;,legend = c(&amp;#39;30岁以下&amp;#39;,&amp;#39;30-34&amp;#39;,&amp;#39;35-39&amp;#39;,&amp;#39;40-44&amp;#39;,&amp;#39;45-49&amp;#39;,&amp;#39;50-54&amp;#39;,&amp;#39;55-59&amp;#39;,&amp;#39;60岁以上&amp;#39;), col = col,pch=19)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/12/26/1997-2020-graduate-edu-data/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;如果我们仔细分析这个1999年扩招造成的特殊年龄分布，会发现处于这个年龄段的基本都是教授，也就是20年前还是可以熬年限的。但现在显然都堵在副教授上了，副教授教授里的年轻人这些年都在增加，而中级职称的人还在快速增长。数据上看40-44岁如果不能上教授或副教授，后面基本也上不去了，目前国家的人才项目基本也是卡在40或45左右的。&lt;/p&gt;
&lt;p&gt;从就业上看，每年博士毕业生10万，但中级职称每年增加两万左右，高级职称刚毕业（包含博士后）现在也有可能拿到，大概每年也是两三万的量级。也就是说，目前教职市场最多可以吸收一半的博士毕业生。因为大学不扩招了，所以教职市场很难出现大扩充，又因为博士还在扩招，往后应该会看到更多的博士毕业生直接输送到社会。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;小结&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;小结&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;扩招依然进行中，研究生群体在300万左右，每年还要增加百万量级&lt;/li&gt;
&lt;li&gt;硕士会有十分之一延期，博士则超过六成，读博要做好年龄规划&lt;/li&gt;
&lt;li&gt;不同学科间硕士博士扩招情况不同，硕士偏重职业化与应用，博士扩招侧重理工科&lt;/li&gt;
&lt;li&gt;教职方面存在1999年大学扩招后出现的一个人口红利，未来会有个每年四五万的教职空窗期&lt;/li&gt;
&lt;li&gt;目前教职扩充吸收的主要是中级职称，大概吸收当年五分之一的博士毕业生&lt;/li&gt;
&lt;li&gt;40-44岁拿到的职称基本就是退休职称了&lt;/li&gt;
&lt;li&gt;现在会有一半博士不做教职，未来会有更多，早做职业化打算&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>漫谈 base R 与 ggplot2 </title>
      <link>https://yufree.cn/cn/2021/12/18/base-r-ggplot2/</link>
      <pubDate>Sat, 18 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/12/18/base-r-ggplot2/</guid>
      <description>


&lt;p&gt;这几天心里颇不宁静。纽约的单日新冠确诊数又出了新纪录，工作邮箱里好几封通知群聚感染的邮件，很多还是完全免疫甚至打了加强针的。感觉新变种可以自立山头看作新病毒了，而面世仅一年的疫苗似乎特异性已经不那么乐观了。不过更强的传染性倒不见得有更强的症状，但大概率这病毒是打算跟人类长期共存了，这就有点抗生素与致病菌的剧本了。我理解国内清零政策很大程度是因为放开后医疗系统会崩溃，不过长期清零对于一种不断变异增强传染性的病毒而言是否具有可持续性我想已经在被论证了，同样完全躺平现在看也不是什么好的应对策略，解决问题不能非此即彼，总是要根据实际情况动脑子的。说到这我就来填一个很早就想填的坑，是关于R绘图系统的。&lt;/p&gt;
&lt;p&gt;不知道从什么时候开始，tidyverse 跟六角贴纸开始满天飞了，在我看来工具是用来解决问题而不是创造问题的，围绕工具形成的文化现象或类似价值观的东西属于副产品，但推广价值观这类副产品显然要比推广软件容易，因此到一定程度总会出现一些反客为主的声音。我对此不置可否，只是想说工具一定要能解决问题才有意义，在解决问题上，将问题清楚描述出来要比直接问某某工具咋做某某图更重要。&lt;/p&gt;
&lt;p&gt;我接触 R 是在十年前，那个时候提到 R 的绘图系统，更多指的是 base R 与 grid/Lattice 这两套系统，前者强调的是最高程度的自定义自由度而后者侧重的则是一个函数给出想要的图形。显然，前者更适合原始意义的绘图比较适合开发者而后者更适合具体应用场景的用户，后者的一个显著优势是默认出图就足够漂亮而显著缺点则是自定义比较费劲。用户从来都是最难伺候的，总有用户既想要最大程度的默认好看又需要一定程度的自定义（说的就是科研狗），此时 ggplot 就出现了。&lt;/p&gt;
&lt;p&gt;ggplot 里面的 gg 代表的是图形语法，本来 ggplot 就是 Hadley 理论转实践的尝试，原始的包在08年就不更新了，现在 CRAN 上也没有了，后面 ggplot2 里的那个2就是致敬最原始的版本 ggplot 。ggplot2 最显著的优点就是用图形语法结合了 base R 与 grid 系统的理念，默认足够好看，自定义掌握了一套通用层层叠加语法后也很容易上手。不过，我也得替 grid/Lattice 说句话，他们也是支持自定义的，Lattice 到今天也处于活跃开发状态，并且跟 R 一起发布。不过他们的自定义显然没有上升到价值观层面，跟 base R 的逻辑更像，支持公式化的数据表达，支持对象化操作，也可以通过 &lt;code&gt;update&lt;/code&gt; 这个函数来有限调节自定义的细节。不过有句老话说“革命不彻底就是彻底不革命”，用户从来不会过多理会开发历史，基本都是颜狗。ggplot2 也是依赖 grid 包来构建的，包括了所有 Lattice 的优点但学起来更容易，所以很快用户就倒向了这套绘图系统。不过，我们现在常看到的 base R 与 ggplot 的对比很大程度是从可视化结果上来的，但别忘了 ggolot 也是 R 语言的产物，理论上研究清楚了 grid 包也可以搞出类似的东西，真正的区别是用户的使用逻辑。&lt;/p&gt;
&lt;p&gt;base R 的使用逻辑更像是白纸画图，从坐标轴到图像都是可以随意自定义的，与之对应的就是需要用户了解一大堆底层命令。base R需要用户对可视化有很具体的了解，例如先用纸笔草图画出雏形，然后通过排列组合基本的作图元素重现在绘图区里。举个最简单的例子，想在条形图上加个误差线，就需要分解为具体的两部分：1）误差线长度与起点坐标，2）然后从起点坐标平行y轴画一条线段，线段末端可以把原有箭头末端里箭头的角度从锐角改成90度垂直。然后我们就能看到误差线了。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;iris&amp;quot;)
mean &amp;lt;- by(iris$Sepal.Length,iris$Species,mean)
sd &amp;lt;- by(iris$Sepal.Length,iris$Species,sd)
temp &amp;lt;- barplot(mean,ylim=c(0,max(mean)+2*max(sd)))
arrows(temp,mean+sd, temp, mean-sd, angle=90, code=3, length=0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/12/18/base-r-ggplot2/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这个简单到不能再简单的功能偏巧默认的条形图 &lt;code&gt;barplot&lt;/code&gt; 里没有，函数文档里那个误差线又不是平头的，所以很多用户马上认为 base R 做不了。单就这个例子而言，其实条形图本来就不应该有误差线，真正需要表示不确定性范围应该用箱形图或提琴图来展示真实分布，但你要是去读学术论文会发现时至今日还是有大量图用条形图加误差线，有的还要加星号表示显著差异。这些从作图角度而言完全就是缝合怪，但始作俑者后人不断。很多用户只知道最终要看到什么样的图，但完全不关心图背后的统计学意义，然后就到处问哪个函数能做这样的图，追求形似。这是我认为现在还需要学 base R 作图的一个重要原因，那就是用户一定要有把图形元素拆解回原始数据的能力，知道自己再干什么而不是盲目追求炫酷，这太浮躁。&lt;/p&gt;
&lt;p&gt;在 ggplot 这套系统里，图片实际被拆解成了三个部分：数据、映射与可视化方式。数据最好是一个数据框，映射解决变量对应在图片上的维度例如哪个是x？哪个是y？哪个分组用颜色/大小/形状来表示，可视化方式就是指定出图的具体形式。其实这个逻辑在 base R 里也是成立的，只不过最后这个不同可视化方式会对应不同函数，然后里面参数一大堆名字还不一样。ggplot则是把这些都规范到一种形式里去了，而且可以通过加号这个函数层层叠加可视化方式，而各自可视化方式内部也可以重新进行坐标映射。此外，ggplot事实上也支持在作图过程中执行一定的计算，例如平滑或者汇总，这类计算都归到&lt;code&gt;stat_&lt;/code&gt;系列的函数里了。如果打算自定义某个维度，可以统一用 &lt;code&gt;scale_XXX_manual&lt;/code&gt; 来进行修改，这里&lt;code&gt;XXX&lt;/code&gt;对应的是你映射的维度，例如 &lt;code&gt;scale_fill_manual&lt;/code&gt; 对应的就是自定义填充颜色。注意，这里是映射后的图片里的维度，而映射则一般都是在可视化方式的 &lt;code&gt;aes&lt;/code&gt; 里定义的，用来把数据中的某个维度指定到图片里的维度里。此外，ggplot 自然也支持根据分组信息的多图可视化，这里就是&lt;code&gt;facet_grid&lt;/code&gt;函数来统一管理，这里 ggplot 的价值观是每一张图都要尽可能清楚展示数据，且一幅图讲一个故事，然后通过坐标对其进行比较。也就是说，ggplot里画双坐标轴这种图就不太推荐，虽然也可以自定义后来个形似，但其实双坐标轴图在可视化方式里面的地位大概跟饼图或3D柱形图差不多，属于人厌狗嫌那个组的。所有的双坐标轴图理论上都可以并应该拆成两幅图来描述两件不同的事，如果两件事相同，那么双坐标轴总可以转为单坐标轴。&lt;/p&gt;
&lt;p&gt;从我个人经验而言，如果你掌握了 base R 作图，转到 ggplot 作图非常轻松，会省掉一大堆需要自定义的东西，统一用他们的函数体系来画就行。反之，如果掌握了 ggplot 的语法，学 base R 应该也很轻松，因为base R绘图函数里的映射与可视化方式也都有现成的包或函数，只是可能参数名字乱一些。说 ggplot 对新手友好，一大部分指的是默认美观，还有一部分原因就是从设计上就阻止了很多类似3D柱形图或双坐标轴的存在。但搞笑的是在爆栈网上从来不缺人去问这类图如何在ggplot体系里实现，也不缺大神给出解决方法。因此，我一度认为限制一定的自由度其实也对培养良好的可视化习惯有帮助，但现实却是很多用户既没学到ggplot里的可视化原则，又产生了对 base R 莫名其妙的优越感，一抬手就是管道化教条式的层层叠加代码，一行代码能做到的非学习牛津大学贝利学院优秀毕业生、大英帝国爵级司令勋章获得者、大不列颠及北爱尔兰联合王国内阁常任秘书汉弗莱·阿普比的语言风格去卡形式，这就距离解决问题比较遥远了。&lt;/p&gt;
&lt;p&gt;其实 base R 与 ggplot 之争的背后存在一个代码风格统一的问题，如果选 ggplot ，确实代码更容易读，但问题是很多初学者的水平大概就在复制代码改变量名的水平，完全不知道代码的意义，这样层层堆出来的代码甚至会重复定义，也没啥可读性。究其原因，代码风格应该是初学者到了中级水平才应该考虑的问题，初学者最应该了解的是可视化的逻辑，然后结合自己具体的问题去练习并累积经验。不过，很多初学者都是excel打底的，脑子里全是哪个对话框画什么样的图这种思维，这种情况不论是转 base R 还是 ggplot ，他们脑子里的预期都是一个函数出图解决问题，根本不关心图是如何画出来的，这就很容易变成教条化的用户，记住步骤但不知道原因。实话说很多基于ggplot体系的作图包本质上就是把需要用户自定义的部分自己强制定义一遍然后就上线了，这种包完全迎合了某些领域的独特可视化品味，后面跟了一批教条化的用户，这样的默认可用的软件包我觉得并不利于数据分析人员理解自己的数据。&lt;/p&gt;
&lt;p&gt;但凡学用编程语言进行可视化，起码是要知道自己在做什么的。看到一张漂亮的图，可以尝试分析图片元素，然后尝试自己将其组合起来。有这种想法后，base R 也好，ggplot 也好，学明白一个就基本也会另一个了，甚至说迁移到 python 的 matplotlib 或其他交互式作图系统都不困难。但如果搞不清楚原理，那么换一个软件就只能继续到网上复制现成的代码。我倒不是鄙视从网上复制现成的代码，毕竟这事我也没少做，但总要有个学习的过程才能掌握。&lt;/p&gt;
&lt;p&gt;软件优劣之争在我看来很多都是鸡同鸭讲，很多比较都是在特殊应用场景下才有明显区别。但每个人的最终应用场景毕竟是不同的，解决问题的意义显著大于跟工具分高下的意义。显然编程语言绘图的能力范围更多受限于使用者的能力而非工具本身，因此这样的工具优劣争论还是少一些吧，争到最后大概率都成了人身攻击。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>因果结构提取</title>
      <link>https://yufree.cn/cn/2021/03/15/pc-algorithm/</link>
      <pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/03/15/pc-algorithm/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/cn/2021/03/15/pc-algorithm/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;因果关系能否直接从数据中获取这个问题对很多人而言答案是否定的，相关不代表因果都成了说烂了的老梗。根源上人认识世界只能通过可感知的现象，背后的规律都是在抽象意义上自洽但现实表现都含有噪音。说夸张点普朗克尺度已经界定了测量手段的极限，有些理论可能就是永远无法实证但数学上自洽的。过去的一个世纪是实验与测量技术大突破的100年，无数现代仪器或仅仅就是传感器为各类科学研究提供了大量的现象数据，也营造了数据无所不能的幻象。说是幻象是因为数据背后不仅仅有规律，也有内生的噪音，很多研究痴迷于换用不同的数据模型来提高预测性，但却忽略数据信噪比，当信噪比很低时任何结论都会不靠谱，不同模型有矛盾的预测结果无法说明模型的优劣而仅仅就是现象本身方差太大，所谓的信号或者规律其实是内生噪音的随机性导致的。&lt;/p&gt;
&lt;p&gt;不过因果推断就是尝试解决这个问题的。最近有一个暴露组学的数据&lt;a href=&#34;https://github.com/isglobal-exposomeHub/ExposomeDataChallenge2021/blob/main/README.md&#34;&gt;挑战&lt;/a&gt;，里面给出了一组人群数据，涵盖了暴露组、基因组、表观基因组、代谢组的数据，用来寻找包括哮喘在内的多种表型差异。第一眼看上去就像是系统生物学的研究，但因为有暴露组跟基因组数据，就又像是想解决一个历史性的科学问题，那就是健康究竟是基因还是外界暴露说了算。不过表观基因组跟代谢组的数据则成了这个模型的关键问题，因为这两组数据都受基因跟外界暴露的调控，而最终的结果可能就是疾病发病率。简单说就是下面这样一个模型：&lt;/p&gt;
&lt;p&gt;外界环境 -&amp;gt; 暴露组 -&amp;gt; 表观遗传组/代谢组 -&amp;gt; 健康状态&lt;/p&gt;
&lt;p&gt;遗传 -&amp;gt; 基因组 -&amp;gt;&lt;/p&gt;
&lt;p&gt;这就是因果关系图了，因果分析有两个应用场景，一个是效应估计，就是找出各自的贡献；另一个则更关键，就是提取因果关系。像上面这个数据挑战需要解决的就是前者，因为背后的模型基本是固定的。但这里要明确这个模型逻辑上通但测量上不一定测的准，基因组也可能被环境调控，同时很多暴露影响也有遗传上的共同因素例如住在祖屋里，这就是前面说的内生噪音，因此不论是模型推断还是拟合，不确定性是一定存在的，很小的效应就是会看不出来或者不同模型给不同答案，Gelman 一直强调的 Type M 错误是科研中一定要注意的，效应太小就是实际意义不大，甚至科学上都会因为重现性差而没意义。&lt;/p&gt;
&lt;p&gt;但其实我更好奇的是，假如没有这个模型，我们能不能从一堆数据中提炼出一个因果结构。我找来找去发现了PC算法，用来发现数据中存在的因果关系，这里记录一下。&lt;/p&gt;
&lt;p&gt;首先，我们先仿真一组因果数据：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;E &amp;lt;- 6*rnorm(1000,10,1)+rnorm(1000)
G &amp;lt;- rnorm(1000,50,2)+rnorm(1000)
M &amp;lt;- E*0.7+G*0.3+rnorm(1000)
Epi &amp;lt;- E*0.1+G*0.9+rnorm(1000)
Asthma &amp;lt;- 0.6*M+0.4*Epi+rnorm(1000)
data &amp;lt;- cbind.data.frame(E,G,M,Epi,Asthma)
cor(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               E        G      M    Epi Asthma
## E       1.00000 -0.02042 0.9647 0.2565 0.8551
## G      -0.02042  1.00000 0.1314 0.8550 0.3549
## M       0.96474  0.13144 1.0000 0.3733 0.9159
## Epi     0.25647  0.85500 0.3733 1.0000 0.5888
## Asthma  0.85509  0.35488 0.9159 0.5888 1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairs(data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/03/15/pc-algorithm/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor.test(Asthma,E)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s product-moment correlation
## 
## data:  Asthma and E
## t = 52, df = 998, p-value &amp;lt;2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.8375 0.8709
## sample estimates:
##    cor 
## 0.8551&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor.test(Asthma,G)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s product-moment correlation
## 
## data:  Asthma and G
## t = 12, df = 998, p-value &amp;lt;2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.2995 0.4079
## sample estimates:
##    cor 
## 0.3549&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里E代表暴露组、G代表基因组、M代表代谢组、Epi代表表观组、Asthma代表疾病，仿真中设定疾病六成可以被代谢组解释而四成可以用表观组解释，然而代谢组里七成是环境影响而三成是基因影响，表观组则是一成环境影响而九成基因影响。这里我们可以看到如果进行简单回归，基本就是一团乱码。&lt;/p&gt;
&lt;p&gt;下面我们用PC算法拟合一下试试：&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(pcalg)
suffStat &amp;lt;- list(C=cor(data), n = nrow(data))
pc.fit &amp;lt;- pc(suffStat, gaussCItest, p = ncol(data), alpha = 0.01, verbose = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Order=0; remaining edges:20
## x= 1  y= 2  S=  : pval = 0.519 
## x= 1  y= 3  S=  : pval = 0 
## x= 1  y= 4  S=  : pval = 1.2e-16 
## x= 1  y= 5  S=  : pval = 0 
## x= 2  y= 3  S=  : pval = 2.987e-05 
## x= 2  y= 4  S=  : pval = 0 
## x= 2  y= 5  S=  : pval = 1.067e-31 
## x= 3  y= 1  S=  : pval = 0 
## x= 3  y= 2  S=  : pval = 2.987e-05 
## x= 3  y= 4  S=  : pval = 3.173e-35 
## x= 3  y= 5  S=  : pval = 0 
## x= 4  y= 1  S=  : pval = 1.2e-16 
## x= 4  y= 2  S=  : pval = 0 
## x= 4  y= 3  S=  : pval = 3.173e-35 
## x= 4  y= 5  S=  : pval = 4.855e-101 
## x= 5  y= 1  S=  : pval = 0 
## x= 5  y= 2  S=  : pval = 1.067e-31 
## x= 5  y= 3  S=  : pval = 0 
## x= 5  y= 4  S=  : pval = 4.855e-101 
## Order=1; remaining edges:18
## x= 1  y= 3  S= 4 : pval = 0 
## x= 1  y= 3  S= 5 : pval = 0 
## x= 1  y= 4  S= 3 : pval = 2.175e-46 
## x= 1  y= 4  S= 5 : pval = 3.191e-101 
## x= 1  y= 5  S= 3 : pval = 2.84e-18 
## x= 1  y= 5  S= 4 : pval = 0 
## x= 2  y= 3  S= 4 : pval = 1.207e-38 
## x= 2  y= 3  S= 5 : pval = 1.84e-72 
## x= 2  y= 4  S= 3 : pval = 0 
## x= 2  y= 4  S= 5 : pval = 0 
## x= 2  y= 5  S= 3 : pval = 4.245e-101 
## x= 2  y= 5  S= 4 : pval = 1.425e-31 
## x= 3  y= 1  S= 2 : pval = 0 
## x= 3  y= 1  S= 4 : pval = 0 
## x= 3  y= 1  S= 5 : pval = 0 
## x= 3  y= 2  S= 1 : pval = 1.174e-94 
## x= 3  y= 2  S= 4 : pval = 1.207e-38 
## x= 3  y= 2  S= 5 : pval = 1.84e-72 
## x= 3  y= 4  S= 1 : pval = 1.184e-65 
## x= 3  y= 4  S= 2 : pval = 1.006e-69 
## x= 3  y= 4  S= 5 : pval = 4.762e-71 
## x= 3  y= 5  S= 1 : pval = 5.469e-142 
## x= 3  y= 5  S= 2 : pval = 0 
## x= 3  y= 5  S= 4 : pval = 0 
## x= 4  y= 1  S= 2 : pval = 8.164e-77 
## x= 4  y= 1  S= 3 : pval = 2.175e-46 
## x= 4  y= 1  S= 5 : pval = 3.191e-101 
## x= 4  y= 2  S= 1 : pval = 0 
## x= 4  y= 2  S= 3 : pval = 0 
## x= 4  y= 2  S= 5 : pval = 0 
## x= 4  y= 3  S= 1 : pval = 1.184e-65 
## x= 4  y= 3  S= 2 : pval = 1.006e-69 
## x= 4  y= 3  S= 5 : pval = 4.762e-71 
## x= 4  y= 5  S= 1 : pval = 2.8e-195 
## x= 4  y= 5  S= 2 : pval = 7.757e-101 
## x= 4  y= 5  S= 3 : pval = 5.666e-140 
## x= 5  y= 1  S= 2 : pval = 0 
## x= 5  y= 1  S= 3 : pval = 2.84e-18 
## x= 5  y= 1  S= 4 : pval = 0 
## x= 5  y= 2  S= 1 : pval = 4.455e-179 
## x= 5  y= 2  S= 3 : pval = 4.245e-101 
## x= 5  y= 2  S= 4 : pval = 1.425e-31 
## x= 5  y= 3  S= 1 : pval = 5.469e-142 
## x= 5  y= 3  S= 2 : pval = 0 
## x= 5  y= 3  S= 4 : pval = 0 
## x= 5  y= 4  S= 1 : pval = 2.8e-195 
## x= 5  y= 4  S= 2 : pval = 7.757e-101 
## x= 5  y= 4  S= 3 : pval = 5.666e-140 
## Order=2; remaining edges:18
## x= 1  y= 3  S= 4 5 : pval = 2.984e-295 
## x= 1  y= 4  S= 3 5 : pval = 3.844e-29 
## x= 1  y= 5  S= 3 4 : pval = 0.577 
## x= 2  y= 3  S= 4 5 : pval = 2.05e-08 
## x= 2  y= 4  S= 3 5 : pval = 4.093e-267 
## x= 2  y= 5  S= 3 4 : pval = 0.4735 
## x= 3  y= 1  S= 2 4 : pval = 0 
## x= 3  y= 1  S= 2 5 : pval = 4.25e-304 
## x= 3  y= 1  S= 4 5 : pval = 2.984e-295 
## x= 3  y= 2  S= 1 4 : pval = 1.062e-28 
## x= 3  y= 2  S= 1 5 : pval = 3.746e-09 
## x= 3  y= 2  S= 4 5 : pval = 2.05e-08 
## x= 3  y= 4  S= 1 2 : pval = 0.1595 
## x= 3  y= 5  S= 1 2 : pval = 1.519e-51 
## x= 3  y= 5  S= 1 4 : pval = 1.218e-71 
## x= 3  y= 5  S= 2 4 : pval = 0 
## x= 4  y= 1  S= 2 3 : pval = 1.981e-08 
## x= 4  y= 1  S= 2 5 : pval = 0.1367 
## x= 4  y= 2  S= 1 3 : pval = 0 
## x= 4  y= 2  S= 1 5 : pval = 3.064e-224 
## x= 4  y= 2  S= 3 5 : pval = 4.093e-267 
## x= 4  y= 5  S= 1 2 : pval = 7.358e-24 
## x= 4  y= 5  S= 1 3 : pval = 1.651e-120 
## x= 4  y= 5  S= 2 3 : pval = 7.409e-36 
## x= 5  y= 3  S= 1 2 : pval = 1.519e-51 
## x= 5  y= 3  S= 1 4 : pval = 1.218e-71 
## x= 5  y= 3  S= 2 4 : pval = 0 
## x= 5  y= 4  S= 1 2 : pval = 7.358e-24 
## x= 5  y= 4  S= 1 3 : pval = 1.651e-120 
## x= 5  y= 4  S= 2 3 : pval = 7.409e-36 
## Order=3; remaining edges:10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pc.fit,main=&amp;#39;Demo&amp;#39;,labels=c(&amp;quot;E&amp;quot;,&amp;quot;G&amp;quot;,&amp;quot;M&amp;quot;,&amp;quot;Epi&amp;quot;,&amp;quot;Asthma&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required namespace: Rgraphviz&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2021/03/15/pc-algorithm/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;怎么说呢，结果还算满意，虽然细节上还有点问题（例如E跟Epi间效应比较弱就没被算法发现），基本符合我们仿真的设定，这也算是一种内生噪音的后果吧，就算是仿真也可能搞出随机相关来。&lt;/p&gt;
&lt;p&gt;现在我们来说下PC算法究竟啥原理。本质上PC算法一直在反复做条件独立性假设检验，这里我们指定的就是 Gaussian 检验，算的其实就是多变量间的偏相关性然后通过假设检验决定连接。至于方向性，越是结果其内生噪音应该越大。&lt;/p&gt;
&lt;p&gt;不过这种直接依赖数据的因果结构发现适用范围非常有限，前面这个例子我用了线性模拟与检验才会有个说得过去的结果，要是本来不是这种分布，结果意义就不大了。什么？你问真实数据？，真实数据E跟G加起来上万维，M跟Epi的维度也是这个量级，信噪比不高的话根本啥都找不出来，找出来的因果结构说不定还是Epi在G上面，所以我一直以来就认为组学数据分析的出路在于提高信噪比而不是盲目往里送维度。但降维就要丢信息，如何权衡有效信息量很重要，而这个有效的判定则是根据实际问题来，不同问题的降维策略应该是不一样的，这就需要研究人员根据实际情况决策了。&lt;/p&gt;
&lt;p&gt;科学问题不等同因果问题或相关问题，因果律也只是研究工具的一种，噪音才是科学求证的公敌。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MS/MS annotation by paired mass distances analysis</title>
      <link>https://yufree.cn/en/2021/01/17/ms-ms-annotation-by-paired-mass-distances/</link>
      <pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2021/01/17/ms-ms-annotation-by-paired-mass-distances/</guid>
      <description>
&lt;script src=&#34;https://yufree.cn/en/2021/01/17/ms-ms-annotation-by-paired-mass-distances/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Last year I make a poster presentation for MS/MS annotation by paired mass distance(PMD) analysis. It’s already been included as &lt;code&gt;pmdanno&lt;/code&gt; function in pmd package. Here I will explain the principle of PMD annotation.&lt;/p&gt;
&lt;p&gt;Firstly, you need a spectra database. Here I use HMDB MS/MS spectra database as an example. Then you will get a list with each compound as element. The list should have a element of spectra with mz and ins, an element of name, an element of prec for precursor ions. I have included this database in rmwf package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remotes::install_github(&amp;#39;yufree/rmwf&amp;#39;)
# remotes::install_github(&amp;#39;yufree/pmd&amp;#39;)
library(rmwf)
data(&amp;quot;qtof&amp;quot;)
str(qtof)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 4
##  $ name   : chr [1:5062] &amp;quot;HMDB0000014&amp;quot; &amp;quot;HMDB0000014&amp;quot; &amp;quot;HMDB0000014&amp;quot; &amp;quot;HMDB0000014&amp;quot; ...
##  $ mz     : num [1:5062] 227 227 227 227 227 ...
##  $ msms   :List of 5062
##   ..$ : num 116
##   ..$ : num [1:3] 5 111 116
##   ..$ : num [1:15] 0.07 16.03 16.1 27.01 42.01 ...
##   ..$ : num [1:15] 0.07 16.03 16.1 27.01 42.01 ...
##   ..$ : num 116
##   ..$ : num [1:3] 5 111 116
##   ..$ : num [1:136] 1.98 2.01 2.01 2.02 2.02 2.02 3.99 3.99 4.03 4.03 ...
##   ..$ : num [1:36] 1 3.99 3.99 5 8.01 ...
##   ..$ : num [1:3] 30 44 74
##   ..$ : num [1:6] 18 18 36 83 101 ...
##   ..$ : num [1:15] 1.98 9.98 11.96 18.01 26.02 ...
##   ..$ : num [1:6] 18 18 36 83 101 ...
##   ..$ : num [1:15] 1.98 9.98 11.96 18.01 26.02 ...
##   ..$ : num [1:3] 30 44 74
##   ..$ : num 1
##   ..$ : num [1:10] 1 10 34 43 44 ...
##   ..$ : num [1:3] 1 18 19
##   ..$ : num [1:45] 0.98 1 9.98 15.01 16.03 ...
##   ..$ : num [1:190] 0.93 0.98 1 1 1.06 2.02 2.02 2.04 2.95 2.95 ...
##   ..$ : num [1:105] 1 1 1.01 1.06 2 2.01 2.02 2.95 2.98 3.01 ...
##   ..$ : num [1:703] 0.03 0.04 0.62 0.93 0.93 0.97 0.99 1 1 1 ...
##   ..$ : num [1:3] 46 71 117
##   ..$ : num [1:36] 2.02 8.01 9.98 12 12 ...
##   ..$ : num [1:21] 0.08 9.04 12 17.03 29.03 ...
##   ..$ : num [1:21] 0.08 9.04 12 17.03 29.03 ...
##   ..$ : num [1:3] 46 71 117
##   ..$ : num [1:36] 2.02 8.01 9.98 12 12 ...
##   ..$ : num 17
##   ..$ : num 27
##   ..$ : num [1:6] 15 27 27 42 42 ...
##   ..$ : num 27
##   ..$ : num [1:6] 15 27 27 42 42 ...
##   ..$ : num 17
##   ..$ : num [1:3] 1.01 59.01 60.02
##   ..$ : num [1:3] 1.01 59.01 60.02
##   ..$ : num [1:6] 18 37.1 55.1 212 249.1 ...
##   ..$ : num 212
##   ..$ : num 212
##   ..$ : num 212
##   ..$ : num 212
##   ..$ : num [1:6] 18 37.1 55.1 212 249.1 ...
##   ..$ : num 132
##   ..$ : num 132
##   ..$ : num [1:3] 27 132 159
##   ..$ : num 132
##   ..$ : num [1:3] 27 132 159
##   ..$ : num 132
##   ..$ : num 17
##   ..$ : num 132
##   ..$ : num 132
##   ..$ : num 132
##   ..$ : num 132
##   ..$ : num [1:3] 1 17 18
##   ..$ : num 9.45
##   ..$ : num 9.45
##   ..$ : num 194
##   ..$ : num [1:3] 55.1 194 249.1
##   ..$ : num [1:3] 55.1 194 249.1
##   ..$ : num 194
##   ..$ : num [1:10] 0.95 17.06 18.01 24.95 42.01 ...
##   ..$ : num [1:15] 0.95 1.01 1.01 2.02 17.06 ...
##   ..$ : num [1:10] 0.95 17.06 18.01 24.95 42.01 ...
##   ..$ : num [1:3] 1 36 37
##   ..$ : num [1:2485] 0.03 0.03 0.03 0.04 0.04 0.04 0.04 0.04 0.04 0.04 ...
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num [1:3] 18 26 44
##   ..$ : num [1:3] 18 26 44
##   ..$ : num 17
##   ..$ : num [1:3] 18 28 46
##   ..$ : num [1:3] 18 28 46
##   ..$ : num 26
##   ..$ : num 26
##   ..$ : num [1:6] 2.02 17.03 25.98 27.99 43.01 ...
##   ..$ : num [1:6] 2.02 17.03 25.98 27.99 43.01 ...
##   ..$ : num [1:3] 1 180 181
##   ..$ : num [1:16836] 0.02 0.03 0.03 0.03 0.04 0.04 0.05 0.05 0.05 0.06 ...
##   ..$ : num [1:15] 6.09 18.01 19.12 20.88 21.91 ...
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num 46
##   ..$ : num [1:28] 1.01 1.98 8.06 15.94 15.99 ...
##   ..$ : num [1:28] 1.01 1.98 8.06 15.94 15.99 ...
##   ..$ : num 212
##   ..$ : num 212
##   ..$ : num [1:3] 18 225 243
##   ..$ : num [1:3] 18 225 243
##   ..$ : num [1:3] 18 225 243
##   ..$ : num [1:3] 18 225 243
##   ..$ : num 212
##   ..$ : num 212
##   ..$ : num [1:3] 2.02 44.03 46.04
##   ..$ : num [1:3] 2.02 44.03 46.04
##   ..$ : num 16
##   .. [list output truncated]
##  $ msmsraw:List of 5062
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 112 228
##   .. ..$ intensity : num [1:2] 70 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 112 117 228
##   .. ..$ intensity : num [1:3] 100 25.8 50.5
##   ..$ :&amp;#39;data.frame&amp;#39;: 6 obs. of  2 variables:
##   .. ..$ masscharge: num [1:6] 66 93 135 210 226 ...
##   .. ..$ intensity : num [1:6] 15.5 100 15.2 12 14.2 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 6 obs. of  2 variables:
##   .. ..$ masscharge: num [1:6] 66 93 135 210 226 ...
##   .. ..$ intensity : num [1:6] 15.5 100 15.2 12 14.3 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 112 228
##   .. ..$ intensity : num [1:2] 70 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 112 117 228
##   .. ..$ intensity : num [1:3] 100 25.8 50.5
##   ..$ :&amp;#39;data.frame&amp;#39;: 814 obs. of  2 variables:
##   .. ..$ masscharge: num [1:814] 45.2 45.4 45.4 45.8 46 ...
##   .. ..$ intensity : num [1:814] 2.75 2.38 1.81 2.23 2.07 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 890 obs. of  2 variables:
##   .. ..$ masscharge: num [1:890] 44.9 44.9 44.9 45.7 45.7 ...
##   .. ..$ intensity : num [1:890] 0.927 1.514 1.947 0.402 0.34 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 108 138 182
##   .. ..$ intensity : num [1:3] 72.5 100 48.7
##   ..$ :&amp;#39;data.frame&amp;#39;: 7 obs. of  2 variables:
##   .. ..$ masscharge: num [1:7] 65 92.1 120 138.1 148 ...
##   .. ..$ intensity : num [1:7] 12.29 8.8 6.49 7.67 100 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 6 obs. of  2 variables:
##   .. ..$ masscharge: num [1:6] 65 92 110 120 122 ...
##   .. ..$ intensity : num [1:6] 88.3 42.8 11.8 24.6 13.5 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 7 obs. of  2 variables:
##   .. ..$ masscharge: num [1:7] 65 92.1 120 138.1 148 ...
##   .. ..$ intensity : num [1:7] 12.31 8.81 6.51 7.71 100 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 6 obs. of  2 variables:
##   .. ..$ masscharge: num [1:6] 65 92 110 120 122 ...
##   .. ..$ intensity : num [1:6] 88.3 42.8 11.8 24.6 13.5 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 108 138 182
##   .. ..$ intensity : num [1:3] 72.6 100 48.6
##   ..$ :&amp;#39;data.frame&amp;#39;: 6 obs. of  2 variables:
##   .. ..$ masscharge: num [1:6] 166 200 209 243 244 ...
##   .. ..$ intensity : num [1:6] 1.05 3.46 1.31 100 12.76 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 14 obs. of  2 variables:
##   .. ..$ masscharge: num [1:14] 122 156 165 166 167 ...
##   .. ..$ intensity : num [1:14] 7.75 10.08 4.93 30.33 3.05 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 6 obs. of  2 variables:
##   .. ..$ masscharge: num [1:6] 227 228 229 245 246 ...
##   .. ..$ intensity : num [1:6] 59.52 5.77 2.16 100 10.01 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 53 obs. of  2 variables:
##   .. ..$ masscharge: num [1:53] 97 98.1 100 101 105.1 ...
##   .. ..$ intensity : num [1:53] 13.87 1.2 1.36 1.07 8.73 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 72 obs. of  2 variables:
##   .. ..$ masscharge: num [1:72] 79.1 81.1 82 85 91.1 ...
##   .. ..$ intensity : num [1:72] 2.94 2.13 3.41 3.7 6.6 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 26 obs. of  2 variables:
##   .. ..$ masscharge: num [1:26] 91.1 93.1 94.1 97 99 ...
##   .. ..$ intensity : num [1:26] 18.02 8.07 8.51 81.98 9.6 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 228 obs. of  2 variables:
##   .. ..$ masscharge: num [1:228] 95.1 95.1 95.3 96.2 96.9 ...
##   .. ..$ intensity : num [1:228] 11.61 7.12 2.62 8.8 1.87 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 110 156 210 227
##   .. ..$ intensity : num [1:4] 25.43 43.54 9.51 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 10 obs. of  2 variables:
##   .. ..$ masscharge: num [1:10] 83.1 93 95.1 110.1 122.1 ...
##   .. ..$ intensity : num [1:10] 12.8 10.7 11 100 13.1 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 7 obs. of  2 variables:
##   .. ..$ masscharge: num [1:7] 81 93 110 154 163 ...
##   .. ..$ intensity : num [1:7] 14.4 11.1 100 79.3 11 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 7 obs. of  2 variables:
##   .. ..$ masscharge: num [1:7] 81 93 110 154 163 ...
##   .. ..$ intensity : num [1:7] 14.4 11.1 100 79.3 11 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 110 156 210 227
##   .. ..$ intensity : num [1:4] 25.46 43.52 9.52 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 10 obs. of  2 variables:
##   .. ..$ masscharge: num [1:10] 83.1 93 95.1 110.1 122.1 ...
##   .. ..$ intensity : num [1:10] 12.8 10.7 11 100 13.1 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 119 136
##   .. ..$ intensity : num [1:2] 37.4 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 92 107 134
##   .. ..$ intensity : num [1:3] 6.71 26.83 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 8 obs. of  2 variables:
##   .. ..$ masscharge: num [1:8] 64 65 68 90 92 ...
##   .. ..$ intensity : num [1:8] 6.91 37.74 8.31 8.91 74.77 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 92 107 134
##   .. ..$ intensity : num [1:3] 6.69 26.8 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 8 obs. of  2 variables:
##   .. ..$ masscharge: num [1:8] 64 65 68 90 92 ...
##   .. ..$ intensity : num [1:8] 6.91 37.73 8.33 8.95 74.78 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 119 136
##   .. ..$ intensity : num [1:2] 37.4 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 58.1 59.1 118.1
##   .. ..$ intensity : num [1:3] 100 30.7 83
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 58.1 59.1 118.1
##   .. ..$ intensity : num [1:3] 100 30.7 82.9
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 97 134 346
##   .. ..$ intensity : num [1:4] 100 37.9 29.9 66.8
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 348
##   .. ..$ intensity : num [1:2] 20.5 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 348
##   .. ..$ intensity : num [1:2] 83.8 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 348
##   .. ..$ intensity : num [1:2] 20.4 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 348
##   .. ..$ intensity : num [1:2] 83.8 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 97 134 346
##   .. ..$ intensity : num [1:4] 100 37.9 29.8 66.8
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 268
##   .. ..$ intensity : num [1:2] 31.2 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 268
##   .. ..$ intensity : num [1:2] 100 57.1
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 107 134 266
##   .. ..$ intensity : num [1:3] 12.8 100 15.9
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 119 136 268
##   .. ..$ intensity : num [1:3] 7.81 100 10.61
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 107 134 266
##   .. ..$ intensity : num [1:3] 12.8 100 15.9
##   ..$ :&amp;#39;data.frame&amp;#39;: 5 obs. of  2 variables:
##   .. ..$ masscharge: num [1:5] 136 137 268 269 270
##   .. ..$ intensity : num [1:5] 47.69 2.16 100 9.67 1.29
##   ..$ :&amp;#39;data.frame&amp;#39;: 5 obs. of  2 variables:
##   .. ..$ masscharge: num [1:5] 94 119 120 136 137
##   .. ..$ intensity : num [1:5] 1.52 17.18 1.37 100 5.09
##   ..$ :&amp;#39;data.frame&amp;#39;: 6 obs. of  2 variables:
##   .. ..$ masscharge: num [1:6] 119 136 137 268 269 ...
##   .. ..$ intensity : num [1:6] 0.3 100 5.31 68.77 6.91 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 268
##   .. ..$ intensity : num [1:2] 31.2 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 136 268
##   .. ..$ intensity : num [1:2] 100 57
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 119 136 268
##   .. ..$ intensity : num [1:3] 7.77 100 10.65
##   ..$ :&amp;#39;data.frame&amp;#39;: 242 obs. of  2 variables:
##   .. ..$ masscharge: num [1:242] 46.8 47 47.9 48.7 48.7 ...
##   .. ..$ intensity : num [1:242] 1.127 0.999 0.384 0.973 1.434 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 90.1 99.5 111
##   .. ..$ intensity : num [1:3] 100 14.41 9.51
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 90.1 99.5 111
##   .. ..$ intensity : num [1:3] 100 14.42 9.54
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 136 330 330 330
##   .. ..$ intensity : num [1:4] 33.33 5.51 9.21 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 107 134 328
##   .. ..$ intensity : num [1:4] 26.43 9.91 100 53.85
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 107 134 328
##   .. ..$ intensity : num [1:4] 26.41 9.89 100 53.89
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 136 330 330 330
##   .. ..$ intensity : num [1:4] 33.32 5.47 9.22 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 5 obs. of  2 variables:
##   .. ..$ masscharge: num [1:5] 60.1 85 102.1 103 162.1
##   .. ..$ intensity : num [1:5] 12.9 25.8 20.4 47.1 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 9 obs. of  2 variables:
##   .. ..$ masscharge: num [1:9] 57 58.1 59.1 60.1 61 ...
##   .. ..$ intensity : num [1:9] 5.23 42.66 14.31 39.56 2.91 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 5 obs. of  2 variables:
##   .. ..$ masscharge: num [1:5] 60.1 85 102.1 103 162.1
##   .. ..$ intensity : num [1:5] 13.3 26.1 19.8 47.4 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 485 obs. of  2 variables:
##   .. ..$ masscharge: num [1:485] 46.3 46.5 47.5 47.7 48.8 ...
##   .. ..$ intensity : num [1:485] 0.3 0.343 0.314 0.279 0.414 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 1000 obs. of  2 variables:
##   .. ..$ masscharge: num [1:1000] 45.1 45.2 45.2 45.3 45.6 ...
##   .. ..$ intensity : num [1:1000] 1.61 1.78 4.23 1.78 1.01 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 84.1 130.1
##   .. ..$ intensity : num [1:2] 100 38.1
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 84.1 130.1
##   .. ..$ intensity : num [1:2] 100 38.1
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 85 111 129 173
##   .. ..$ intensity : num [1:4] 100 10.71 26.83 8.71
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 85 111 129 173
##   .. ..$ intensity : num [1:4] 100 10.71 26.79 8.71
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 137 154
##   .. ..$ intensity : num [1:2] 100 95.4
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 91.1 109.1 119 137.1
##   .. ..$ intensity : num [1:4] 14.92 1.38 16.93 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 8 obs. of  2 variables:
##   .. ..$ masscharge: num [1:8] 65 79.1 81.1 91.1 94 ...
##   .. ..$ intensity : num [1:8] 6.071 1.277 2.066 100 0.501 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 11 obs. of  2 variables:
##   .. ..$ masscharge: num [1:11] 41 53 63 65 77 ...
##   .. ..$ intensity : num [1:11] 0.67 0.579 1.187 49.753 0.563 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 16 obs. of  2 variables:
##   .. ..$ masscharge: num [1:16] 39 41 51 53 55 ...
##   .. ..$ intensity : num [1:16] 3.35 2.27 3.34 1.22 0.634 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 9 obs. of  2 variables:
##   .. ..$ masscharge: num [1:9] 55 70 72 73 73.9 ...
##   .. ..$ intensity : num [1:9] 4.8 10.91 11.81 3.3 6.31 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 9 obs. of  2 variables:
##   .. ..$ masscharge: num [1:9] 55 70 72 73 73.9 ...
##   .. ..$ intensity : num [1:9] 4.83 10.89 11.76 3.3 6.26 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 447 obs. of  2 variables:
##   .. ..$ masscharge: num [1:447] 45.3 45.9 46.1 46.6 47 ...
##   .. ..$ intensity : num [1:447] 0.144 0.126 0.12 0.117 0.111 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 286 obs. of  2 variables:
##   .. ..$ masscharge: num [1:286] 45.3 45.8 48.5 48.6 49.4 ...
##   .. ..$ intensity : num [1:286] 11.93 13.92 8.81 10.23 12.22 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 8 obs. of  2 variables:
##   .. ..$ masscharge: num [1:8] 59 76 96.9 116 137.9 ...
##   .. ..$ intensity : num [1:8] 9.45 100 16.33 21.13 29.02 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 67.1 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 100 14.4
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 60.9 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 100 25.5
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 67.1 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 100 14.4
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 60.8 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 58.1 104.1
##   .. ..$ intensity : num [1:2] 100 25.5
##   ..$ :&amp;#39;data.frame&amp;#39;: 14 obs. of  2 variables:
##   .. ..$ masscharge: num [1:14] 85 86.1 87 102.9 111 ...
##   .. ..$ intensity : num [1:14] 30 4.2 40.2 11.7 100 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 14 obs. of  2 variables:
##   .. ..$ masscharge: num [1:14] 85 86.1 87 102.9 111 ...
##   .. ..$ intensity : num [1:14] 30.06 4.24 40.25 11.67 100 ...
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 112 324
##   .. ..$ intensity : num [1:2] 50.7 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 112 324
##   .. ..$ intensity : num [1:2] 100 33.2
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 97 139 322
##   .. ..$ intensity : num [1:4] 100 52.45 7.51 60.36
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 97 139 322
##   .. ..$ intensity : num [1:4] 100 51.85 6.81 58.96
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 97 139 322
##   .. ..$ intensity : num [1:4] 100 52.48 7.46 60.4
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 79 97 139 322
##   .. ..$ intensity : num [1:4] 100 51.86 6.83 58.94
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 112 324
##   .. ..$ intensity : num [1:2] 50.6 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 2 obs. of  2 variables:
##   .. ..$ masscharge: num [1:2] 112 324
##   .. ..$ intensity : num [1:2] 100 33.3
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 58.1 60.1 104.1
##   .. ..$ intensity : num [1:3] 19.1 38.1 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 3 obs. of  2 variables:
##   .. ..$ masscharge: num [1:3] 58.1 60.1 104.1
##   .. ..$ intensity : num [1:3] 19.1 38.1 100
##   ..$ :&amp;#39;data.frame&amp;#39;: 4 obs. of  2 variables:
##   .. ..$ masscharge: num [1:4] 134 207 223 223
##   .. ..$ intensity : num [1:4] 4.2 13 3 100
##   .. [list output truncated]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This database has included all of the 5062 Q-ToF spectra from 1259 compounds in HMDB. We only considered the peaks larger than 10% of the base peak and calculated all of the paired mass distances within the spectra. For example, for compound HMDB0000014, the MS/MS spectra should be (112.1, 228.1) with intensity (69.97, 100). Then the PMD spectra for annotation should be 116 for this compounds.&lt;/p&gt;
&lt;p&gt;For the PMD annotation, we will also compute the PMDs of input spectra. Then we compare the input PMDs with the database. Here we need three parameters to refine the candidates. The first parameter is ppm for mass accuracy of precursor ions. The second parameter is the range of precursor ions, the default setting should be 1.1 to include M+H or M-H. The third parameter is the pmd length percentage cutoff for annotation. 0.6(default) means 60 percentage of the pmds in your sample could be found in certain compound pmd database. The fourth parameter is the relative intensity cutoff for input spectra for pmd analysis, default 0.1 for 10 % of the base peak.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this is the sepctra of HMDB0034004
file &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;challenge-msms.mgf&amp;quot;, package = &amp;quot;rmwf&amp;quot;)
# pmd msms annotation
anno &amp;lt;- pmd::pmdanno(file,db=qtof)
unique(anno$name)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;HMDB0034004&amp;quot; &amp;quot;HMDB0003217&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;enviGCMS::plotanno(anno)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The score rule for pmd annotation is that the candidates will be ordered according to the overlapped pmd numbers. In this case, if two candidates have 3 and 4 pmd overlapped with the input spectra, the latter one will be the first candidate.&lt;/p&gt;
&lt;p&gt;Such annotation could be used for MS1 annotation. However, without precursor ion to refine the candidates. It’s better to find the M+H or M-H in advance. In this case, the input spectra should be processed by isotope, adducts or neutral loss detection by pmd of 1.006Da, 22.98Da, etc. Then the following step should be the same as MS2 pmd annotation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R语言中的网络可视化</title>
      <link>https://yufree.cn/cn/2020/06/24/r-network-analysis/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2020/06/24/r-network-analysis/</guid>
      <description>


&lt;p&gt;这个问题反复掉坑，在这总结下，省的记吃不记打。&lt;/p&gt;
&lt;p&gt;所谓网络可视化，一定是有节点与节点间连线组成，节点一般指代一个样品或特性，连线则代表了样品间或特性间的关系。也就是说，网络的最小单元就是一个两点连线，也就是起点与终点，虽然描述一个网络很直观，但具体到数据结构上就存在一些问题。常规样本数据一般是每一行代表一个样品，每一列代表一个描述样品的维度，样品或维度间的关系并不能展示在原始数据结构里，所以我们需要将样品-维度的数据框转成描述网络的起点-终点数据结构才好可视化。但事实上，你更应该需要一种描述网络的数据类型，然后根据类型定义可视化方法，也就是将原始数据转为网络数据类型，这种类型定义也方便了除可视化外其他针对网络的分析方法开发与使用。&lt;/p&gt;
&lt;p&gt;在 R 中，有两个包提供了描述网络的基础类型定义，一个是 &lt;code&gt;network&lt;/code&gt;包，另一个是&lt;code&gt;igraph&lt;/code&gt;包。这两个包允许用户生成一个专门描述网络的对象，也定义了该对象类型的绘图方法，也就是说，如果你可以直接 &lt;code&gt;plot&lt;/code&gt; 一个网络对象，实现快速可视化。很多网络可视化的工具，例如 &lt;code&gt;ggnetwork&lt;/code&gt;包或&lt;code&gt;ggraph&lt;/code&gt;包或&lt;code&gt;GGally&lt;/code&gt;包的&lt;code&gt;ggnet2&lt;/code&gt;函数，都支持输入的对象为 &lt;code&gt;network&lt;/code&gt;包或&lt;code&gt;igraph&lt;/code&gt;包里定义的网络类型。不过，这里面&lt;code&gt;ggraph&lt;/code&gt;包还可以基于&lt;code&gt;tidygraph&lt;/code&gt;包使用&lt;code&gt;tbl_graph&lt;/code&gt;对象来描述网络关系，几乎完全覆盖了&lt;code&gt;igraph&lt;/code&gt;包的内容，当然，装这个就得装 &lt;code&gt;tidyverse&lt;/code&gt;全家桶。&lt;/p&gt;
&lt;p&gt;不论哪一种对象类型，网络对象一定可以抽象出两张表，一张表保存节点的属性，另一张表保存节点间连线的属性。而对于可视化而言，节点属性表其实就是原始数据框，而连线属性表则要保存我们计算出的节点间关系，例如节点间相关性、距离等。然后很自然每一种对象类型都设计了独立的针对节点与边的赋值方法，有了这些方法就可以自定义一些节点或边的属性方便可视化。不过，这里很多人手头只有数据框，也就是只有节点那张表，关系的表还是要自己生成的，在图论里这叫做邻接矩阵（adjacency network）。最简单就是一个相关矩阵或距离矩阵，不过也不一定就是方阵或三角阵，这里面坑多我就不展开说了，多数可视化包都支持你导入一个邻接矩阵或者更原始的起点终点数据框来生成网络对象，然后你可以对节点与边进行属性定义，在可视化时指定需要可视化的属性就可以了。很多人（其实就是我）卡在第一步数据导入就放弃了，但过了第一步能生成网络对象后后面就特别容易进行后续分析。当然，至于说网络可视化的具体布局，其实是存在一些预先设定好的美观布局的，你可以根据需求进行调整，但不要误导读者，做好图例。&lt;/p&gt;
&lt;p&gt;这里特别提一下&lt;code&gt;qgraph&lt;/code&gt;包，这个包几乎依赖了上面所有提过的包，但这在应用学科中并不少见，例如这个包主要是为心理测量学设计的。但很有意思的是，如果你去搜索 R 语言的网络可视化教程，基本都会找到心理测量学或社会科学背景的人写的东西，而且质量很高，例如&lt;a href=&#34;https://kateto.net&#34;&gt;Katya Ognyanova&lt;/a&gt;的博客，&lt;a href=&#34;http://sachaepskamp.com/&#34;&gt;Sacha Epskamp&lt;/a&gt; 的博客，&lt;a href=&#34;https://cvborkulo.com&#34;&gt;Claudia van Borkulo&lt;/a&gt; 的博客还有&lt;a href=&#34;https://psych-networks.com/&#34;&gt;这里&lt;/a&gt;，特别最后一个总结并追踪相当多近些年网络科学的主题。打个比方，通常我们说节点间的关系，一般就是想到相关性，但两个节点间也可以用是否独立来构建联系而相关只是独立与否的一种，偏相关行不行？或者如果计算二元而非连续特征值（社会科学里定量研究常用）间的独立性就需要用到 Ising 模型。同时构建出的网络是不是稳定也需要正则化例如 lasso 或重采样来对变量间关系进行调整，去掉不稳定的联系。另外，如何检测一个网络中的社群？有哪些算法？其实背后也是潜在变量分析的影子，这些主题在心理学领域被挖得很深。&lt;/p&gt;
&lt;p&gt;如果跳到生物信息学领域，有一个 &lt;code&gt;WGCNA&lt;/code&gt;包用的特别多，但据我观察很多写教程的人都没搞清楚原理与模型假设。&lt;code&gt;WGCNA&lt;/code&gt;包是基于巴拉巴西的无尺度网络构建的，基本原理就是先对所有基因构建两两相关性矩阵，然后从相关性矩阵中探索出共表达的基因模块，相当于把几千维的基因降维到不到十个且最好能联系上生物学意义例如某个通路啥的，具体计算则是每个模块进行主成分分析（其实是SVD分解），然后用第一个主成分作为这个模块的代表对你的研究分组进行差异分析，找出哪个模块有影响然后解释。这里面核心步骤里有两个坑，第一个在相关性矩阵到基因模块这里，第二个在主成分分析那边。第一个坑是因为其探索模块用了无尺度网络的假设，首先得去选一个幂级数来计算邻接矩阵，这个幂级数是拟合无尺度网络的度分布搞出来的，很多数据本身不符合无尺度网络的度分布，所以硬套这个假设是不合适的。第二个坑跟第一个有关系，只有模块内部第一个主成分可解释方差很高才能这么用，但由于第一个坑很多人用了默认值，第二个坑也就只用了第一个主成分，很多时候方差解释连三分之一都不到，虽然能讲故事，但明显是有偏的。当然这个包里也是涵盖了很多对于用户而言天书级的概念，很多人不求甚解套默认值也把文章给发了，完全就当神奇降维盒子在用了。其实说白了网络分析是另一层意义上的因子分析，起一个降维作用，只是降维方式不是简单的线性组合而是引入了图论的一些统计量罢了，但我看到很多人用起来套代码，解释上完全就是胡说八道，特别是代谢组学里会出现套基因组学的分析方法而不验证假设盲目追求自动化。不过我也看到了很多基于图论的生物信息学文章，很多想法非常超前但引用非常少且真正生产数据的人基本看不懂或不看，这就还不如心理测量学那边研究人员的学科内科普做得好。&lt;/p&gt;
&lt;p&gt;说个题外话，其实我能知道很多包的问题不是跟开发者打过交道，而是我查过很多包的源码，非统计与计算机科学背景开发者写的包其实是沉默的大多数，他们一般只会用基础R包函数来实现自己想要的功能，如果没有就会去依赖其他包，很少用 &lt;code&gt;Rcpp&lt;/code&gt;，不开并行计算，基本不关心速度，用S3 对象而不是S4，这倒是科研编程的日常状态。有时候读他们的源码有种见字如面的感觉：有的人明显是其他语言转过来的，有次读一个包的代码怎么看怎么别扭，后来发现这个开发者的母语是 java，很多定义方式都是那边传过来的。有的人注释掉的代码其实有另一重意思，源码里保留了很多进化遗迹，类似化石。有的人严谨，每个函数都写测试，文档明显打磨过语言。有的人飘逸，通篇找不到注释，很多编码风格都不一致，感觉是爆栈网复制过来的。有的人很明显是&lt;code&gt;tidyverse&lt;/code&gt; 风格出现后才开始学的 R ，对基础函数用法非常不熟。非统计与计算机科学开发者的代码通常存在很多不严谨的地方，没有经过软件工程的训练，更多是为了解决特定目的而快速实现的，不过很多代码展示的想象力非常丰富多彩。&lt;/p&gt;
&lt;p&gt;好了，说这么多还是要给点最直观的例子。下面我就手工生成几个网络并做下基础可视化，这里我不会用最常见的那种起点终点数据结构，因为这个东西是需要从原始数据生成的，很少有原始数据本身就是这种关系结构，而且此处我也不涉及 &lt;code&gt;ggplot2&lt;/code&gt; 风格的绘图包，用基础绘图系统来做，函数统一为&lt;code&gt;plot&lt;/code&gt;，当然不同的对象类型会有不同的绘图参数。&lt;/p&gt;
&lt;div id=&#34;network-版&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;network&lt;/code&gt; 版&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(110)
library(network)
# 生成一个3节点网络
net &amp;lt;- network.initialize(3)
# 画出来
plot(net,vertex.cex=10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 添加一条边
add.edge(net,2,3) 
# 画出来
plot(net,vertex.cex=10, displaylabels=T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 添加两个点
add.vertices(net,2)
# 画出来
plot(net,vertex.cex=10, displaylabels=T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 模拟一个5*12的数据框
df &amp;lt;- matrix(rnorm(60),5)
# 用邻接矩阵直接生成网络
dfcor &amp;lt;- cor(df)
# 去掉低相关性边
dfcor[dfcor&amp;lt;0.5] &amp;lt;- 0
netcor &amp;lt;- as.network(dfcor,matrix.type = &amp;#39;adjacency&amp;#39;)
plot(netcor)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 增加节点/边属性
set.vertex.attribute(netcor, &amp;quot;class&amp;quot;, length(netcor$val):1)
set.edge.attribute(netcor,&amp;quot;color&amp;quot;,length(netcor$mel):1)
# 可视化属性
plot(netcor,vertex.cex=5,vertex.col=get.vertex.attribute(netcor,&amp;quot;class&amp;quot;),edge.col=get.edge.attribute(netcor,&amp;#39;color&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-1-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;igraph-版&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;igraph&lt;/code&gt; 版&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(110)
library(igraph)
# 生成一个3节点网络
net &amp;lt;- graph.empty(n=3, directed=TRUE)
# 画出来
plot(net)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 添加两条边
new_edges &amp;lt;- c(1,3, 2,3)
net &amp;lt;- add.edges(net, new_edges)
# 画出来
plot(net)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 添加两个点
net &amp;lt;- add.vertices(net, 2)
# 画出来
plot(net)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 模拟一个5*12的数据框
df &amp;lt;- matrix(rnorm(60),5)
# 用邻接矩阵直接生成网络
dfcor &amp;lt;- cor(df)
# 去掉低相关性边
dfcor[dfcor&amp;lt;0.5] &amp;lt;- 0
net &amp;lt;- graph.adjacency(dfcor,weighted=TRUE,diag=FALSE)
plot(net)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-2-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 增加节点/边属性
V(net)$name &amp;lt;- letters[1:vcount(net)]
E(net)$color &amp;lt;- &amp;quot;red&amp;quot;
E(net)[ weight &amp;lt; 0.7 ]$width &amp;lt;- 2
E(net)[ weight &amp;lt; 0.7 ]$color &amp;lt;- &amp;quot;green&amp;quot;
# 可视化属性
plot(net)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-06-24-r-network-analysis_files/figure-html/unnamed-chunk-2-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;网络可视化只是网络分析的基础，很多基于网络稳定性分析还有网络群组分析都是可以基于更基础的概率图模型来进行，这些分析都有明确的背景问题来源，但涉及的知识点非常多，从统计物理到图论到随机过程，不过如果你带着自己的问题去探索，总会有新的发现。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Daily check for coronavirus data</title>
      <link>https://yufree.cn/en/2020/04/17/daily-check-for-coronavirus-data/</link>
      <pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/en/2020/04/17/daily-check-for-coronavirus-data/</guid>
      <description>


&lt;p&gt;As employee of health system, every week I will check our lab for leaks or instrumental issues. Meanwhile, I am using the following code to check daily increasing cases in US state-county level every morning.&lt;/p&gt;
&lt;p&gt;I think three signals are crucial for community level data and every community and people should decide their own timeline for normal live to survive in this pandemic and potential recession in the following months or years:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Positive rate of testing is decreasing to ~1%. In this case, the testing ability should be enough to screen all the potential infected people.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Daily increasing hospitialized number is less than the sum of dischared patients and dead people. In this case, the medical resources should reach the peak.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Daily death numbers reached the peak and decrease for two weeks. In this case, the susceptible individuals in the community shoud be either infected or isolated and we could start to consider the cease of lockdown of local community.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;state&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;STATE&lt;/h1&gt;
&lt;p&gt;Data source is New York Times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

file &amp;lt;- read.csv(&amp;#39;https://github.com/nytimes/covid-19-data/raw/master/us-states.csv&amp;#39;,stringsAsFactors = F)
file$date &amp;lt;- as.Date(file$date)

# Check States with max case larger than 10000 and more than 100 deaths
df &amp;lt;- file%&amp;gt;%
    group_by(state)%&amp;gt;%
    mutate(change=c(0,diff(cases)),change2=c(0,diff(deaths)))%&amp;gt;%
    filter(max(cases)&amp;gt;10000 &amp;amp; max(deaths)&amp;gt;100)%&amp;gt;%
    ungroup() 

df %&amp;gt;%
    ggplot(aes(x=date,y=change,fill=state)) +
    geom_point() +
    geom_smooth() +
    facet_wrap(~state,scales = &amp;#39;free&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-04-17-daily-check-for-coronavirus-data_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;%
    ggplot(aes(x=date,y=change2,fill=state)) +
    geom_point() +
    geom_smooth() +
    facet_wrap(~state,scales = &amp;#39;free&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-04-17-daily-check-for-coronavirus-data_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;county&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;COUNTY&lt;/h1&gt;
&lt;p&gt;Data source is New York Times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;file &amp;lt;- read.csv(&amp;#39;https://github.com/nytimes/covid-19-data/raw/master/us-counties.csv&amp;#39;,stringsAsFactors = F)
file$date &amp;lt;- as.Date(file$date)

library(tidyverse)
# Check Counties with max case larger than 10000 and more than 100 deaths
df &amp;lt;- file%&amp;gt;%
    group_by(county,state)%&amp;gt;%
    mutate(CaseChange=c(0,diff(cases)),DeathChange=c(0,diff(deaths)))%&amp;gt;%
    filter(max(cases)&amp;gt;10000 &amp;amp; max(deaths)&amp;gt;100)%&amp;gt;%
    ungroup() 

df %&amp;gt;%
    ggplot(aes(x=date,y=CaseChange,fill=state)) +
    geom_point() +
    geom_smooth() +
    facet_wrap(~county+state,scales = &amp;#39;free&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-04-17-daily-check-for-coronavirus-data_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;%
    ggplot(aes(x=date,y=DeathChange,fill=state)) +
    geom_point() +
    geom_smooth() +
    facet_wrap(~county+state,scales = &amp;#39;free&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-04-17-daily-check-for-coronavirus-data_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## NYC zipcode
## covid19nyc &amp;lt;- read.csv(&amp;#39;https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;test&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Test&lt;/h1&gt;
&lt;p&gt;Data source is &lt;a href=&#34;https://covidtracking.com&#34;&gt;covidtracking.com&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;daily &amp;lt;- read.csv(&amp;#39;https://covidtracking.com/api/v1/states/daily.csv&amp;#39;,stringsAsFactors = T,header = T)
daily$date &amp;lt;- as.Date(daily$dateChecked)

dd &amp;lt;- daily %&amp;gt;%
    group_by(state)%&amp;gt;%
    mutate(posrateChange=positiveIncrease/totalTestResultsIncrease,recin = c(0,-diff(recovered)))%&amp;gt;%
    ungroup()
# pos rate changes
dd %&amp;gt;%
    filter(state==&amp;#39;NY&amp;#39;)%&amp;gt;%
    ggplot(aes(x=date,y=posrateChange)) +
    geom_point() +
    geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-04-17-daily-check-for-coronavirus-data_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# hospitialized/death/recover daily increase in NY
dd %&amp;gt;%
    filter(state==&amp;#39;NY&amp;#39;)%&amp;gt;%
    tidyr::pivot_longer(col=c(&amp;#39;hospitalizedIncrease&amp;#39;,&amp;#39;deathIncrease&amp;#39;,&amp;#39;recin&amp;#39;),names_to = &amp;#39;condition&amp;#39;,values_to = &amp;#39;count&amp;#39;) %&amp;gt;%
    ggplot(aes(x=date,y=count,color=condition)) +
    geom_point() +
    geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-04-17-daily-check-for-coronavirus-data_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# hospitialized/death/recover daily increase in States with more than 10000 positive cases
dd %&amp;gt;%
    group_by(state) %&amp;gt;%
    filter(positive&amp;gt;10000) %&amp;gt;%
    ungroup() %&amp;gt;%
    tidyr::pivot_longer(col=c(&amp;#39;hospitalizedIncrease&amp;#39;,&amp;#39;deathIncrease&amp;#39;,&amp;#39;recin&amp;#39;),names_to = &amp;#39;condition&amp;#39;,values_to = &amp;#39;count&amp;#39;) %&amp;gt;%
    ggplot(aes(x=date,y=count,fill=condition)) +
    geom_point(aes(col=condition)) +
    geom_smooth()+
    facet_wrap(~state,scales = &amp;#39;free&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/en/2020-04-17-daily-check-for-coronavirus-data_files/figure-html/unnamed-chunk-3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Those numbers are real people.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>3D立体图</title>
      <link>https://yufree.cn/cn/2020/04/15/magic-eye/</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2020/04/15/magic-eye/</guid>
      <description>&lt;p&gt;小学时突然有一阵流行3D立体图，很多人拿着花花绿绿的铅笔盒到处显摆，然后一伙人聚到一起讨论看到了什么。我那时不知道怎么看，听别人描绘的神奇图景十分沮丧，小伙伴还把能看出来跟超能力啥的挂了钩。所以，我，AKA热血日漫业余爱好者，从小学起，就觉得自己不是拯救地球的料了，就算被人画到漫画里，也是个连专属招式都没有的战五渣。当然现在更不是，甚至都不认为地球需要拯救了，人类说到底只是在自救。&lt;/p&gt;
&lt;p&gt;虽然我记性差，但这个从未看到3D立体图的心结时不时会炸个尸，毕竟童年的遗憾才叫遗憾，现在的遗憾我都已经学会自欺欺人了。中学时期读过一本《趣味物理学》，里面也提到了裸眼3D立体图，当时我费了半天劲也还是没看出来。我第一次真正看到3D立体图是一个读研究生时期的晚上，偶然看到网上有人贴了一张标着刘红石作品的3D立体图，其实本来还是没希望看到但突然在某个距离上我看到了其中的一个大茶壶。这个图很神奇，你要是能看到，就一直能看到了，然后眨眨眼同样视角又消失了。刘红石是一位中学老师，自己凭兴趣制作了大量3D立体图，如果有人为中国3D立体图立传，那他一定占据重要&lt;a href=&#34;http://www.liuhs.com/&#34;&gt;篇章&lt;/a&gt;。我后来来回尝试几次，总结出了看到的方法：打比方你的屏幕离你20厘米，要聚焦去看23到25厘米的距离，你当然不能透视屏幕，但通过欺骗大脑，你就能看到3D图像出现在23到25厘米的地方，好像屏幕陷了进去成了一个舞台。童年的心愿通过一种类似自欺欺人的方法实现了，不得不说也是一种成长。&lt;/p&gt;
&lt;p&gt;其实原理上3D立体图并不难，就是人的视觉会自动识别景深，这些图的产生原理就是找一个基础模块反复重叠组成一张大图，然而景深的出现则是依赖在基础模块对应的像素点进行偏移，当你眼睛聚焦在屏幕后面时，其实本质上是让你的大脑重叠两个基础模块，因为基础模块在景深上有偏移，重叠后你会感觉图片凹或凸了一块，因为大脑还是认为两个点没有偏移而增加了深度差异。同理，因为聚焦点在图片后面，所以你看到图就像是窗子里往外看，会因为有景深而显得更广阔些。当然，如果你理解了原理还是看不到也没关系，我当年也这样的，安心等待你的惊喜一刻。&lt;/p&gt;
&lt;p&gt;其实这玩意也是个&lt;a href=&#34;https://eyeondesign.aiga.org/the-hidden-history-of-magic-eye-the-optical-illusion-that-briefly-took-over-the-world/&#34;&gt;舶来品&lt;/a&gt;，上世纪60年代，研究人员就首先提出随机点立体图来研究视觉，特别是立体视觉形成的过程。但这个研究是用两张图来进行的，到了70年代才出现单张图的立体视觉，由神经科学家 Christopher Tyler 提出。不过这种图首次出现在大众视野是在上世纪90年代，当时在美国一家出售调试计算机模拟器的英国公司雇员 Baccei 苦于宣传手段来推销新产品，这时他见到了摄影师 Ron Labbe 工作室里出现的立体图，这种新颖的设计马上就吸引了他的注意力。 Baccei 马上就去买了《Stereo World》的杂志潜心研究，很快他就把自己需要推广产品型号“M700”制作成了立体图并投放了广告，很快就形成了一股流行趋势。&lt;/p&gt;
&lt;p&gt;刚开始 Baccei 是为了卖公司产品，结果很快他就发现卖这种3D立体图比卖产品要赚的多。例如他们将这种图卖给了美国航空飞机杂志 《American Way》，然后美国航空就说如果能首先看出来的乘客就可以获得香槟一瓶，当然也是大获成功。之后他们与一家日本公司合作，而日本人为了发音方便就给这种图起了个英文名：magic eye，魔眼，时至今日这个词依旧很流行，也成为Baccei与设计师 Cheri Smith 后来公司的名字。1993年， Baccei 在麻省开办设计公司专门出售这种图，第一本相关书籍的初版30000本马上售罄，出版商加印了50万份来满足市场需求。但这个潮流到了1995年就开始降温了，最初一张图要25刀，然后逐渐降价到5刀。此时市面上有了芭比娃娃、毛绒玩具以及电子宠物，这种费力还不一定能看到的图自然也会退潮。同时，也出现了光栅立体画这种变个角度就可以看到不同图像的技术玩具，更低的娱乐门槛且可以彩色显示（魔眼无法还原色彩），魔眼消退几乎也是必然。&lt;/p&gt;
&lt;p&gt;后来互联网崛起，娱乐走向了虚拟化，魔眼这种图再也没有重新崛起而是很稳定地成为了一种小众兴趣。然而，裸眼3D技术并未就此停止，任天堂的3DS掌上游戏机就采用了这种技术，我曾经尝试玩过恶魔城，观感跟魔眼其实类似，也是找一个距离，在距离范围内欺骗大脑，然后你就一直可以看到3D效果的游戏场景了。不过，似乎玩家对此并不买账，反而是阉割后的2DS卖的不错。其他裸眼3D技术还有一些，例如全息投影之类，不过现在裸眼3D的竞争对手可能是增强现实，特别是跟智能眼镜或手机摄像头结合的那种。&lt;/p&gt;
&lt;p&gt;但我还是对魔眼耿耿于怀，因为我想自己做出来，毕竟也是个科研向伪程序员。最简单就是找包，但这次我发现这类小众应用会有C++版、JavaScript版、Java版、python版、甚至Matlab版都有，甚至都有反解原图的程序，但就是木有R版生成的软件。其实早在2017年我就动了念头写个R版的魔眼，但里面那个位移部分的算法当时没搞清楚，然后因为这个图的生成存在图片逐行扫描的问题，把图片向量化完了找位移像素然后重新组合的算法我想想脑袋就大，最后索性就引入了 Rcpp 写 for 循环，但无奈功力太弱，生成的图能感觉是魔眼，但里面的图像怎么看都不对，所以这事就扔一边了，一扔三年。&lt;/p&gt;
&lt;p&gt;前两天我重新整理了诺谟图，那篇的初版是2011年写的，好容易填了个九年的坑，这个三年的坑就想也顺道解决。不同于三年前的是我这几年对 python 的使用经验在增长，虽然没到熟练运用的程度但能看懂代码，所以这次就直接尝试把python版的&lt;a href=&#34;https://flothesof.github.io/making-stereograms-Python.html&#34;&gt;魔眼&lt;/a&gt;翻译到 Rcpp 里去，而且我先在 python 里进行了调试，搞清楚了每一步到底在干什么，创造轮子或许费事但照虎画猫的事我常干。不过这次我终于搞清楚了三年前似是而非的魔眼图是怎么回事，其实就是行与列搞反了，把三年前的代码输入矩阵做个转制其实就可以了。不过这次也是踩了不少 Rcpp 的坑，什么编译完了没输出啥的，但做完了回头看其实我需要的函数特别简单，只是基础太差来回绕弯了。这个包就叫做 &lt;code&gt;magiceyer&lt;/code&gt;，我先扔到 &lt;a href=&#34;https://github.com/yufree/magiceyer&#34;&gt;Github&lt;/a&gt; 上，有个很简单的&lt;a href=&#34;https://yufree.github.io/magiceyer/articles/MagicEye.html&#34;&gt;使用说明&lt;/a&gt;。其实可以认真扩展下成为一个更完整的包，不过我心愿已了，就这样吧。&lt;/p&gt;
&lt;p&gt;在我看来，这种图具备一种延迟特性，无论是谁不用程序反解突然给一张魔眼图都得停一下集中注意力才能看出来，这个特性有一定的加密前景，且可以藏到二维码里。技术不论好坏就看使用场景，但可以作为一种抽签方法，因为其实能否看出来比较随机，除了极少数爱好者估计不会有人专门训练这个。如果配合我之前搞的回归残差藏信息的方法，估计会是一个不错的谜题，如果有人在无背景知识条件下全部逐层解开，想来也是很有成就感的事。&lt;/p&gt;
&lt;p&gt;我一直有种感觉，现在网络时代人们的生活少了些类似魔眼图的东西，也许我们可以获得更优质的信息，但乐趣却更多与感官刺激或情绪释放相联系，有些探索的乐趣正被一种“网上都有”的想法抹杀，网上的确都有，但探索的乐趣跟答案有没有不是一个概念。如果远离了键盘鼠标跟视网膜屏幕，还有没有什么小玩意或兴趣可以让你不在乎外面的风雨压力呢？或者说除了自己情绪的释放与让别人知道你很不错，还有没有眼前一亮？可能是锤炼一道拿手菜，可能是手抄一段杂志里有意思的话，可能是对着一套古腾堡机械装置发呆，可能是打一套太极拳，如果你从未看出来立体效果或许可以去看看魔眼图，重要的不是别人期望你去做或该做什么或你期望做什么去迎合自己的人设，仅仅就是想去做而已。&lt;/p&gt;
&lt;p&gt;那就去做，留给人类享受生活的时间可能不多了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-15-magic-eye_files/magiceye.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>新冠传播中的机会平等</title>
      <link>https://yufree.cn/cn/2020/04/07/covid-19-community/</link>
      <pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2020/04/07/covid-19-community/</guid>
      <description>


&lt;p&gt;首先，虽然纽约是目前的重灾区，但生活其中也就那样。现在已经对各类甩锅与追责没了一丝兴趣，只是些憋在家里郁闷情绪的释放途径而已，这对个人认知失调的恢复是有用的，对疫情控制毫无价值。取笑、傲慢、愤怒、仇恨、谄媚、悲痛…你不能用人际交往中的情绪去跟病毒讨价还价，病毒不会因为个人、市场或群体情绪的波动而变弱或变强，那种一切尽在掌握的虚拟掌控感只是过去两百年科技发展的一个有毒的副产品，这个时间借鸡下蛋的行为值得钉在人类发展的耻辱柱上促人反思。瘟疫流行是一种短板效应，任何的疏忽与大意都会让前面的努力白费，那种觉着自己暂时没事就真没事的人值得拥有一次自然选择的机会。&lt;/p&gt;
&lt;p&gt;我比较关心病毒传播中社区差异，而这个问题平时由于较大的人口流动几乎不能研究。但居家令出了后就不一样了，在居家令的背景下，你当然还是无法保证所有人不流动，但风险意识会让多数人留在家里。在纽约市，人口流动减弱后我们就可以看到社区间交流的程度了，纽约是存在明显社区隔离的，特别是种族间隔离，不同社区间相隔也就是一条街，但区间可能几乎没有交流。那么，理论上纽约的社区间新冠病毒也会存在斑块化，相邻街区也许人口密度接近，但因为社交隔离病毒的传播力也应该有明显差异。那么如何找人口基数差不多的街区呢？这里我用邮编来替代，因为过去设计邮编就是按人数来的，不过时过境迁现在邮编已经不能反映当前居住人口了，但似乎也没有别的方法（其实有，通信商与大型互联网公司可以做到，但我搞不到这些数据）。好，我们来攒一下数据。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidycensus)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ────────────────────────────────── tidyverse 1.3.0 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✓ ggplot2 3.3.0     ✓ purrr   0.3.3
## ✓ tibble  3.0.0     ✓ dplyr   0.8.5
## ✓ tidyr   1.0.2     ✓ stringr 1.4.0
## ✓ readr   1.3.1     ✓ forcats 0.5.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census_api_key(&amp;quot;USE YOUR KEY&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## To install your API key for use in future sessions, run this function with `install = TRUE`.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 纽约市各邮编确诊数据
covid19nyc &amp;lt;- read.csv(&amp;#39;https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv&amp;#39;)
# 提取2018年人口普查数据
zip &amp;lt;- get_acs(geography = &amp;quot;zcta&amp;quot;,
variables = c(medincome = &amp;quot;B19013_001&amp;quot;, 
              population = &amp;quot;B01003_001&amp;quot;,
              asian = &amp;quot;B02001_005&amp;quot;,
              black=&amp;quot;B02001_003&amp;quot;,
              white=&amp;quot;B02001_002&amp;quot;),
year = 2018)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Getting data from the 2014-2018 5-year ACS&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;zipnyc &amp;lt;- merge(covid19nyc,zip,by.x = &amp;#39;MODZCTA&amp;#39;,by.y=&amp;#39;GEOID&amp;#39;)
# reshape
zipnyc2 &amp;lt;- tidyr::pivot_wider(zipnyc[,-c(3,4,5,8)],names_from = variable, values_from = estimate) %&amp;gt;%
        mutate(rate=Positive/population,arate=asian/population,brate=black/population,wrate=white/population)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里我收集了纽约市不同邮编的发病人数，除以2018年人口调查的人数就是发病率。然后我计算了亚裔、非裔与白人的人口比例。下面我们就先看一下图。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 社区发病率与收入
ggplot(zipnyc2,mapping = aes(medincome,rate))+geom_point()+geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-07-covid-19-community_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 社区发病率与亚裔比例
ggplot(zipnyc2,mapping = aes(arate,rate))+geom_point()+geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-07-covid-19-community_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 社区发病率与非裔比例
ggplot(zipnyc2,mapping = aes(brate,rate))+geom_point()+geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-07-covid-19-community_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 社区发病率与白人比例
ggplot(zipnyc2,mapping = aes(wrate,rate))+geom_point()+geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-07-covid-19-community_files/figure-html/unnamed-chunk-2-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;看起来收入超过9万美金，其社区发病率就会逐渐下降。非裔人口比例高，发病率会有所提高，白人则有个比例越高发病率越低的趋势。亚裔不明显，因为比例其实一直都不高。图上看你会感觉这个趋势很弱，但如果单纯线性回归其系数都是显著差异于0的，这里我就不去纠结p值的问题了。我们进一步看一下收入与族裔差异。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 白人比例与收入
ggplot(zipnyc2,mapping = aes(medincome,wrate))+geom_point()+geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-07-covid-19-community_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 黑人比例与收入
ggplot(zipnyc2,mapping = aes(medincome,brate))+geom_point()+geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-07-covid-19-community_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 亚裔比例与收入
ggplot(zipnyc2,mapping = aes(medincome,arate))+geom_point()+geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-07-covid-19-community_files/figure-html/unnamed-chunk-3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;这里我们就有了个三角关系：族裔比例-收入-发病率。别的我就不说了，白人比例越高，收入越高，发病率越低。也就是说，虽然病毒传播是平等的，但经济收入差异会影响发病率，那么什么原因导致的呢？&lt;/p&gt;
&lt;p&gt;个人防护用品其实全纽约都买不到，图上这个社区范围算不上能吃特供的人群，所以差异不应该在防护。非裔相比白人也许更喜欢互动，但保守的亚裔却没有出现白人那样的明显趋势。那么剩下的就是隔离了，也就是说，高收入的人隔离做得好。不对，应该这么说，高收入的人有条件隔离。要知道纽约的封城不是说完全封，而是非必要工种封，那么什么是必要工种？医护当然是，但更多的是那些 paycheck by paycheck 的工作，例如公交司机、环卫还有维修工等，这些人时薪很高但其实工时不长，结果就是他们的活动可能并未减少甚至更加繁忙，其得病风险也会比那些在家的人高。但这些人从来都没在媒体里出现过，你去采访个医生，他可以告诉你他们有多苦，你去采访个环卫工人，他们说话的逻辑都可能不是很通。媒体从来都只关心能发声的群体。他们从事最危险但必要的工作，而最后的赞誉与他们无关。这里有篇&lt;a href=&#34;https://graphics.reuters.com/HEALTH-CORONAVIRUS/USA/qmypmkmwpra/&#34;&gt;报道&lt;/a&gt;发现，中低收入者在居家令发出后其实并未减少活动，相比3%左右的病死率，吃不上饭风险可能更大。&lt;/p&gt;
&lt;p&gt;当然，我不是说医护不该称赞，只是想说贫穷是更大的流行病，不解决贫穷，瘟疫就一直有藏身之处，他们不会出现在台面上的数据表里，但他们会存在。危险的工作总要有人做或被机器取代，但在那一天到来之前，不能忽视隔离背后的机会平等问题，在这个问题里我们可以看到族裔可能有机会不平等，收入也可能有机会不平等，甚至职业也会造成机会不平等。这里的平等是被传染的机会而不是其他，有人说病毒对所有人是公平的，其实真相可能更露骨与残酷。&lt;/p&gt;
&lt;p&gt;我很清楚这里面有很多逻辑漏洞，数据支持也不完整，但至少这个基于网络开放数据的探索是可重复的。这个时代，虽然突发瘟疫的特效药可能短期不存在，但如果你有一个问题，就应该有开放数据可以正面或侧面回答，不要总是等答案。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R语言会议幻灯片读后感</title>
      <link>https://yufree.cn/cn/2019/06/28/r-conf-19/</link>
      <pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2019/06/28/r-conf-19/</guid>
      <description>&lt;p&gt;今年的R会议幻灯片我直到这周才看，而且幻灯片体积着实不小，我这边下载好久才搞下来。先声明，读后感纯属个人感受，不代表统计之都主编意见，而且因为有些分会场报告没有上传，所以感受是有偏的。&lt;/p&gt;
&lt;p&gt;相比去年，今年计算机与统计方面的内容进一步弱化，强调应用场景的数据分析越来越多，R语言会议里出现了更多语言的应用与主题，感觉成了整体数据科学爱好者的会议而不仅仅局限于R或学术界，大概就像是分析化学里的匹斯堡会议，学术届工业届都会参与。今年有本科生与研究生的会场，也符合会议一贯传帮带的模式，给年轻人更多机会。&lt;/p&gt;
&lt;p&gt;今年的大会报告很精彩，印象最深的是吴喜之老师的报告，有大局观，看到了当前统计学教育里的很多问题，例如统计显著性问题及对一些统计知识的误用，特别是对回归模型可解释性，吴老师认为线性模型好解释的看法是皇帝的新衣，浪费了大把老师学生的精力。我们确实是通过在线性模型里增加协变量来控制他们的影响，不过如同吴老师所言，这不代表控制了变量就独立了，相反，如果变量本来是独立的根本无需把他们放到一个多元模型里，创建多元模型就是因为变量之间不独立，如果不独立你去费力解释其中一个模型的系数就显得只见树木不见森林，用单变量思维解释多变量共同作用结论很难说多么靠谱。&lt;/p&gt;
&lt;p&gt;这个问题我也想过，不过是从另一个角度。对于一个模型，如果是预测特征特别多的话，本质上是把信号淹没在随机性的海洋里了，所以考虑了正则化过程的模型才会在实际应用中表现出色。实际科研中我见到很多人把相关性很强的变量或无关变量共同加入到模型里，且不论模型要不要被解释，这种单纯想通过提高变量数目来提高模型预测效能的思路就是错的。大量的检验其实是无关检验，用上FDR反而把信号给压没了，共相关的变量在模型构建时应该给予更小的自由度。然而这些思考我在很多号称搞数据的人那边完全看不到，很多时候他们只是在尝试算法与调参，并不关心实际问题内部的逻辑，用算法的抽象掩盖对实际数据的无知。这个问题对于有实际问题背景的人而言不算严重，但对于只接受过统计学或计算机科学教育的人而言是非常严重的
，他们通常高估算法的通用性沉浸于精巧模型的调试，正如易丹辉老师的报告所言，很多时候描述性统计就能发现问题而没有背景知识是看不出来的。&lt;/p&gt;
&lt;p&gt;如吴老师所言，统计思维其实就是科学思维。统计学是最接近实证科研的学科，统计量的构建、模型的假设还有推断的标准都是构建在数学物理原理之上的，你不会对蚂蚁与大象取平均体重是因为这个量没有实际意义而不是不能算出来，每一个统计量背后都有数理背景。如果不了解这个，天天关心模型调参或强制让数据符合模型是没意义的，做过水处理的同学应该知道污染物吸附上有个弗里德里希吸附公式，你如果强制拟合是一定能拟出来的，即便你的机理是其他的。重要的不是算法而是问题，这一点在这次R会议很多报告中都提到了，但有些报告也确实不太注意这点。&lt;/p&gt;
&lt;p&gt;热点主题中最突出的可能就是数据伦理方法的讨论。这个去年讨论的还不多，今年已经有相关大会报告与分会场了，律师、工程师、科研工作者都提出了自己视角下的观点，例如软件协议应该提供不同意条款时的浏览模式，个人数据收益权应该可以自主控制而不是让渡给服务商，可信计算环境及向个人付费获得数据收益权等等。这些问题不是今天才有的，但技术的发展让这些问题已经很现实地关联到个人权益了，以后影响可能更深，我觉得数据伦理相关的博弈规则设计会是有识之士未来能大展身手的地方，这不是一个单向优化问题而是在生活的便利与个人隐私取得平衡的一个权衡问题，如果规则合适会双赢，不当就会出现封闭与双输。&lt;/p&gt;
&lt;p&gt;今年覃文锋总结了下R社区的现状与趋势，值得关注。另外由于科研背景的报告不少，关于开放科学及OSF也有介绍，开放科学可以看作可重复性危机的一个解决方案，做科研的应该去了解这些趋势。今年依旧有关于热门技术主题例如物联网、区块链的报告，虽然其实跟R语言关系不大了，但只要跟数据分析相关其实都适合讨论。我看到的比较有闪光点的报告其实是金融主题的，有些概念挺有意思，例如凯利公式还有金融对话机器人等。心理学与生物信息学的专场感觉运作上挺成熟了，报告也照顾了外学科人士。从幻灯片制作角度看，大都是一本正经的，其实可以学习谢大那样加些动图调节气氛的，开会应该是很轻松的交流场合，公式太多其实演示过程中没有人会仔细看的。&lt;/p&gt;
&lt;p&gt;今年有个趋势是英文幻灯片很多，报告英文用中文讲这个一般是为了方便有外籍人士参会的场景，但对于本土化的R语言会议其实最好不要设置语言门槛。虽然科研口对英文没什么障碍，但业界或爱好者水平其实中文更方便理解，R会议应该是为了交流而来，希望不要搞成分会场之间存在术语墙而互相无法交流的状况。我个人外出参会经常是瞎转，经常可以从其他学科中发现有意思的理论或想法，但对于存在术语墙的小学科圈子我个人并不喜欢，一伙人互捧是没啥意思的，抢蛋糕从来都是外来户。&lt;/p&gt;
&lt;p&gt;如果说所有报告里你只有时间看一篇，那就读下吴喜之老师的吧，绝对不亏。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R语言会议幻灯片读后感</title>
      <link>https://yufree.cn/cn/2018/06/07/r-conf-18/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2018/06/07/r-conf-18/</guid>
      <description>&lt;p&gt;这两天抽空看了11届R语言会议的公开幻灯片，信息量还是比较大的，在技术这块那些放在我待办事项里的东西还没来得及看，下一代技术就已经出来了。有张幻灯片说13年的 opencpu 是老古董了，但这才5年啊，这技术估计还没进到大学讲义里就有人说已经凉了。不过实话说，现在的软件文档比5年前友好太多了，现在入门的痛苦比当年小了一大截。&lt;/p&gt;
&lt;p&gt;只说 R 社区，knitr 与 Rmarkdown 的出现切实降低了开发者与学习者之间的认知鸿沟。现在我如果发现一个新包，都直接看小品文，这在5年前是可遇不可求的。而这些小品文多半都是 Rmarkdown 直接生成的网页。同时，现在开发一个新包，用 pkgdown 可以一键生成介绍网站。也许很多人觉得这都是一些连接转换工作，很多事 pandoc 之前就做了，但我觉得这些工作意义很大，因为给你土豆、青椒与茄子你没吃过是做不出地三鲜的。&lt;/p&gt;
&lt;p&gt;这个时代不缺新技术，我每次觉得自己想法很好时上网搜总能翻出类似想法的上世纪文献，但我奇怪的是为啥都提出了二十多年后面人没接着做？问了一些人得到的答案要么是压根不知道，要么是知道了也没能力复制出来，要么就直接看不懂。很显然，技术或想法也是需要营销手段来推广的，没人用的想法跟不存在在今天是同义词。&lt;/p&gt;
&lt;p&gt;但营销想法是个异常痛苦的事，你会写软件远远不够，想出现在网上要学建站，前端、后端、数据库是座山。建好了框架还得填内容，你得学会说“人话”，也就是懂点配色跟教育心理，起码得知道同理心才能用初学者的眼光来写内容。文字内容显然不够，你最好有幻灯片、案例文档与不超过5分钟的讲座视频。这些东西全学下来不是不现实，而是不经济。好在现在我们有大量说“人话”的生产工具，基本默认配置就够用了，只有站在别人肩上才能看得远，现和水泥砌墩子太耽误事。&lt;/p&gt;
&lt;p&gt;不过也正是这些技术让后发优势越来越明显，坑都被填了就可以更关注要解决的问题了。当年ghost还原没出来时，重装个系统得沐浴更衣拿出半天时间来折腾各种意外与软件配置，后来小学生都知道小事重启大事重装了，因为可以一键还原了。我知道很多技术出身的人对这个很不屑，好比我当年翻烂了《纽摄》深知一定要开手动对焦来拍照，但问题后来发现还不如手机全自动模式来的漂亮。这种不屑跟清末留辫子的遗老遗少在情感上没啥区别，技术高的人的高更多在于他们把技术带给了所有人而不是少数人，正因为后生可畏才更不能倚老卖老，老老实实当后生多好，用那个被废弃的校训说就是：气有浩然、学无止境。&lt;/p&gt;
&lt;p&gt;技术与概念都可能过时也最终一定会过时，这个倒没什么可担心的，新技术框架一定是会比上一代更说“人话”的，所以其实迁移成本并不大。我看了下，谷歌的tensorflow 算是把深度学习这块统一了半边天，配个说“人话”的 keras ，以后构建深度学习模型可能会非常快。在机器学习方面，涉及数据量大需要分布式计算的就考虑 spark 体系，如果数据量上不了10G，老老实实学 caret 的框架也够了。前端方面，shiny 已经足够简单了，新工具例如 plumber 或者 fiery 适用于需要 API 调用或大规模并发调用等场景，按需求来。大炮打蚊子的事我经常干，干多了就会觉得自己特别傻，明明我选工具，结果让工具选我算怎么回事。&lt;/p&gt;
&lt;p&gt;曾经我也为了逻辑上完备性去学一些工具，受挫最大的是个名为结构方程模型的鬼玩意，一共六个字，三个名词，颇有“古道西风瘦马”的意味。但问题是不论怎么看都觉得不靠谱，假设太多，输出完全不知道是什么意思。我印象中模型都是现实的简化并帮助理解现实，这玩意不简化也就算了，输出还不一定符合现实，还得再找个理由去解释模型的输出，这种鸡生蛋蛋生鸡的玩具我是无福消受。不过我也知道很多人用也是按套路来，并不真的理解，这时候就真害人害己了，遇到个看不见皇帝新衣的孩子是要闹笑话的。&lt;/p&gt;
&lt;p&gt;又扯远了，上面是对幻灯片里软件这块的总结。另一个感受就是这个R语言会议其实更像数据科学会议了，因为很多报告是与R无关而与数据分析有关的。这是个很有意思的现象，我不清楚国内这个学科有没有自己的会议，但有没有都无所谓了，R语言会议似乎已经占领这个生态位了。我记得第一次参加R语言时，谢大准备了一个报告，第一个教室讲完了跑到第二个教室重讲一遍，那时候规模很小。等我16年去的时候人数就过千了，而且真的是涉及到方方面面了。现在这个会议每年还会在全国各地轮转，学科发展潜力巨大。更可贵的是这个会并不挂靠协会，更多是学生在传承，从参会规模跟覆盖学科来看，R语言会议很有可能在中国成为数据科学最重要的会议。&lt;/p&gt;
&lt;p&gt;同时，我发现很多报告最后都有招人或留微信的情况，所以这个会会类似匹斯堡会议一样是个业界也会非常关注的招人场合。匹斯堡会议对于分析化学的学生是个找工作的神会，很多公司都是当场收简历面试发录取一条龙，R语言会议既然学生主办，就可以办招聘专场，培养下一代的数据人才。&lt;/p&gt;
&lt;p&gt;另外，我也看到了些同行，这次生物信息学似乎没有专场但又几个跟环境健康的专场，发的期刊跟讲的内容我都很熟。但比较遗憾的是感觉他们是按照自己学科会议报告准备的，并没有特别强调数据分析的作用。结论固然是吸引人的，但展示过程与思路也很重要，不仅要站在别人的肩上，也要让别人能放心站到你的肩上。&lt;/p&gt;
&lt;p&gt;从内容上看，很多报告因为注意到了听众面比较广而更多是展示结果，讲的比较浅显。但同会场其他报告人却可能讲的很深，这种状况对于学术会议一般不会出现，因为有同行评议与分会主席来控制报告水平相对一致，但R语言会议这样也挺好，会有利于听众现场学习。我之前去也碰到过这种情况，水平差距有时比较可观，但我相信参会者都是因为兴趣来参加并认真准备的，这里面的差距不是个人决定的，而是学科决定的。&lt;/p&gt;
&lt;p&gt;我很明显感觉到有米的行业水平会高一些，例如量化金融、机器学习与人工智能，几乎是在讲最前沿的东西，很多来自业界的一手信息与案例。但到了技术辐射学科，例如公共卫生、医疗等行业，更多的就是新方法的R语言应用。很明确学科的技术水平也是个经济学问题，哪行平均工资高，哪行概率上就能吸引更聪明的人，学科就会领先。当学科内竞争激烈后会自然流向技术辐射或应用学科来进行降维打击，最终整体提高所有学科的平均水平。这可以说学术界内先富带动后富的实证。&lt;/p&gt;
&lt;p&gt;技术辐射学科之内，来自学术界的并不如来自业界的水平高。起码从展示效果上，业界明显有优势但明显术语用的不对，不过也可能是很多学科用术语构建了隔离墙来抱团取暖。不得不承认很多公司在做的东西不仅有用而且如果发文章可能也不错，理工科学生其实都可以去看看，肯定能开拓眼界。这个会有很多创业公司，活力很高，按我去年的估算，国内学术界因为扩招要分流一半到三分之二的博士到业界，如果真有本事，可能业界在以后会解决更多的实际问题。说到底，留在学术界做博后等机会是把命运交给别人来选择，对于不喜欢出头的国人而言也算正常，只是浪费掉自己的选择权始终是件很奢侈的事。&lt;/p&gt;
&lt;p&gt;其实，幻灯片公开并不是今年才有的，我一般都会过一遍。也不只是R语言会议，RStudio的会议不仅有幻灯片还有视频，公开会议幻灯片与视频在R这块一直都是很好的传统。国内原来只有精品课程网站可以看课件，很多会议报告报告人都不会留底，这算是不同学科的风格吧。技术类学科公开有利于推广，基础学科公开会被说不严谨，没有同行评议什么的。但我从内心是希望大家都公开的，用公开审稿替代匿名审稿，我们要解决的是问题，这都什么年代了还担心优先权，还是那句话，没人用的想法跟不存在在今天是同义词。如果每天的交流促进科学问题的解决，那么为什么要把这种交流搞得一年一次呢？&lt;/p&gt;
&lt;p&gt;这个时代有很多红利，对于国内而已，R语言会议的组织形式与内容都是一种很特殊的存在，也是一种清流红利，能用到什么程度全看参与者个人了，我估计组织方都不一定想得到，顺势而为就可以了。（这次是公众号体）&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
