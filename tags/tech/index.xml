<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tech on Miao Yu | 于淼 </title>
    <link>https://yufree.cn/tags/tech/</link>
    <description>Recent content in tech on Miao Yu | 于淼 </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Feb 2022 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://yufree.cn/tags/tech/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>竭泽而渔式非目的分析</title>
      <link>https://yufree.cn/cn/2022/02/17/exhaustive-nontargeted-analysis/</link>
      <pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2022/02/17/exhaustive-nontargeted-analysis/</guid>
      <description>&lt;p&gt;在高分辨质谱分析领域，偏生物样本的代谢组学与偏环境样本的非目的分析都是近期比较火的研究方向。这里关注的待测物一般是分子量1500Da或2000Da以下的小分子，相比蛋白质组学而言看起来是简单了但其实分析逻辑复杂度更高。从定性角度来说，基于质谱的定性要依赖精确质量数与其分子碎片的特异性，蛋白质组学的麻烦在于分子量大了以后精确质量数测定受同位素与多电荷影响谱图解析会有困难，实践里都是打碎了测肽段，然后利用算法重新拼出蛋白质序列，当然自上而下的测序现在也有些新思路，更多依赖仪器进步。到了小分子部分，蛋白质组学里存在的同位素与多电荷问题相对少一点，但这里就不存在肽段的概念了，也就是小分子测定里没有类似碱基或氨基酸的结构单元，此时单纯依赖高分辨质谱只能得到分子式，但要想知道结构，必须要有碎片信息。&lt;/p&gt;
&lt;p&gt;这里我先明确下为什么我不用所谓“非靶标”来翻译non-target 或untargeted 这个词。首先非靶标跟非目的就不是一回事，目的分析更多针对研究目的，靶标更多侧重已知物，前者是研究概念而后者是分析概念。在具体研究中，应该使用目的而非靶标，例如我想找一种已知物的代谢物，已知物我们有标准品而代谢物没有，那么这算靶标还是非靶标？没有标准算非靶标的话事实上你又知道其与已知物可能就存在一个质量差，也正是利用质量差来找代谢物；算靶标你又没有标准，只是有个筛选概念。但如果直接算目的性分析就没有问题，因为我研究目的就是代谢物，真正的非目的分析应该是完全意外的发现或基于效应引导分析的发现，例如田振宇老师对6PPD-quinone的发现。&lt;/p&gt;
&lt;p&gt;现在很多人名义上说是非目的分析，实际做的却是全扫描配合DDA/DIA之后去跟质谱数据库做比对。坦白说这跟你买了标准在自己实验室搞个本地数据库做比对从结果上看区别不大，后者无论如何也不能叫做非目的分析，因为逻辑上你根本就不打算测数据库之外的物质，你的目的实际上是讨论已知数据库里的物质。在我看来，能在研究开始前就得到标准谱图的物质都不能算到非目的分析里，只是高阶版目的分析或筛选，基于已知代谢物或污染物的下游分析例如代谢通路分析事实上都是在对已有的研究进行低质量验证工作，结论都只能基于已知代谢通路来讨论，谈不上有什么新发现。当然，新发现从来都是很困难的，能把已知通路影响做出来也算不错，目前很多人对真正非目的分析的可行性表示了怀疑，更多去关注已知通路的评价，也就是事实上做的都是目的性分析。这样的结论从分析角度更可靠，但似乎除了展示些花里胡哨的系统性影响外也很难有新假说提出，做的结果大都是看上去高端大气但读了也不知道具体在说啥，其他学科的人也始终对此持怀疑态度而应用受限。&lt;/p&gt;
&lt;p&gt;我觉得这个问题还是有解的，非目的分析关注的重点应该是信息收集而不是物质鉴定。如果你关注物质鉴定，那么肯定掉到数据库匹配的目的性分析老路上。但如果关注信息收集而把物质鉴定剥离出去，思路就会很明确，那就是最大限度收集样品里的小分子信息。看上去跟物质鉴定一样，但实际操作是不一样的，因为你需要保存的结果不是鉴定出的物质列表而是可能的谱图，前者是一个终点而后者可以反复去跟更新的数据库匹配来进行探索发现工作。举例而言就是你说我测到了一万个峰，鉴定出了一百个物质，但我更关心的是这一万个峰里不论有没有名字究竟有多少物质。很多所谓鉴定物质只有个名字而没有任何文献报道其功能，实质上跟没鉴定出来区别不大，你给他起名字叫物质XYZ都可以，真实研究关注的是其分子功能，但这个很多非目的分析的文章反而不去讨论，只去关注可鉴定物质间的关系，结果大概率还是其他通路研究的一个远程验证。&lt;/p&gt;
&lt;p&gt;那么具体在质谱上如何操作呢？第一步毫无疑问是全扫描，这里只说软电离部分，硬电离属于后面分子碎片的讨论。理论上软电离全扫描拿到的应该是物质的分子离子峰，但理论跟现实的距离好比卖家秀跟买家秀，看上去相似但其实根本不是一回事。即便是最常见的ESI源，单个物质软电离下会同时出现同位素峰、加合物峰、中性丢失峰还有一些莫名其妙的寡聚体与各种峰的多电荷峰。我下载了HMDB的LC-MS质谱数据，算了下大概平均每个物质的所谓MS1标准谱图也有26个峰，而GC-MS这种硬电离的平均峰数为90。这就是现实跟理论的差距，LC-MS的全扫描软电离数据里甚至还有碎片离子峰可以用来做定性，相关研究可以查阅广州工业大学薛靖川老师的工作。&lt;/p&gt;
&lt;p&gt;但我们的目的是收集物质信息，从信息角度一个物质我只需要一个峰，其余的峰给的信息都是冗余的。只要我找到单一物质的一个峰且最好是分子离子峰，那么我把这个峰送去做二级质谱得到结构信息，那么也就完成了信息收集。也就是说，如果我对每一个潜在物质找到一个峰，那么实际上样品里所有可以测到的物质信息我就都可以收集到二级谱图了，哪怕现在鉴定不了，起码我也可以根据谱图相似性做功能性推测。这也就是标题所说的“竭泽而渔”式非目的分析。&lt;/p&gt;
&lt;p&gt;那么怎么去掉冗余峰呢？常见思路就是给分别找出同位素峰与加合物，然后排除掉，这里就需要一个事先定义的质量差列表。不过我前面也说了，这还是目的性分析的思路，利用经验法则来排除。但真实样品里加合物是非常复杂的，套用已知质量差的列表会引入假阳性，例如样品里本没有某种加合物，但你去套这个列表就会误把不同的物质鉴定为加合物关系。我的思路很简单，既然只关注样品里存在的加合物，那么我就去算样品里的单独保留时间区间里的质量差，然后计算下各种质量差的出现频率，频率高的就标注为潜在加合物，这样就降低了假阳性的概率。其实，在真实样品中经常会发现很多不常见的高频率质量差，但也不用过度解释，直接归为未知加合物就可以了。然后，通过峰强度的相关性计算出潜在加合物间的相关性峰网络，这个网络大概率就来自于同一物质。之后，我们就可以根据一些自定义的规则来选择其中的一个峰作为潜在的先导离子，例如选强度最高的。这样我们就把全扫描的数据处理成了独立物质峰数据，一个峰代表一个物质，算是“竭泽而渔”，有些研究人员则称为伪目的分析（pseudo targeted analysis）。从我经验而看，如果20分钟全扫描可以得到一万个峰，那么独立物质峰个数大概只有一千这个量级。&lt;/p&gt;
&lt;p&gt;那么是不是这一千个峰就可以用目的性分析的方式作为先导离子来采集二级质谱呢？如果只进样一次是不行的，因为PRM模式下20分钟扫一千个先导离子灵敏度完全不够，二级谱质量非常差。这里需要一个技巧，那就是根据先导离子的保留时间写一个多次进样脚本，让每次进样都测定样品中一部分互相不干扰灵敏度的先导离子。从我经验而言，一千个先导离子大概需要10次重复进样就可以覆盖所有独立物质峰。这样在样品进样阶段，我们就收集了所有可能的物质结构信息（二级谱图），至于啥时候能鉴定出来，这个一来可以等后面出现更全的数据库，二来其实对很多研究而言，给个名字并不重要，例如我们发现很多物质都是甲氧基化的，那么可以直接去验证相关酶的活性而不是去费力研究所有甲氧基化物质具体是什么。生物信息重要的是过程而不是名字。&lt;/p&gt;
&lt;p&gt;前面介绍的就是我最近发表在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的PMDDA工作流，整体流程如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yufree/yufree.cn/master/static/images/PMDDAnew.png&#34; alt=&#34;PMDDAnew&#34;&gt;&lt;/p&gt;
&lt;p&gt;那么效果如何呢？我比对了利用已知加合物质量差列表的CAMERA或RAMClustR，这两个软件都有提供MS1谱的功能，可以直接给出潜在物质的精确质量数，我就用这些数据计算出先导离子荷质比并同样进行重复进样。最后我同时对比了PMDDA/CAMERA/RAMClustR 选择先导离子后采集的二级谱图在GNPS上的分子网络图（可理解为潜在物质）。结果如下图，可以看出这种基于样品自身信息选择先导离子的方法要全面优于基于已知质量差列表的方法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yufree/yufree.cn/master/static/images/comparepclust.png&#34; alt=&#34;comparepclust&#34;&gt;&lt;/p&gt;
&lt;p&gt;同时我其实也对比了常见的DDA（一年前的预印本里有，后来因为加入iterative DDA在正式论文里被移除掉了），DDA的问题是牺牲了灵敏度去采集二级谱图而其先导离子虽然依赖样品自己，但选到的峰其实质量非常差，这就出现了DDA测到的很多物质在MS1数据里压根就找不到的现象。这里我要说下很多现在发表的工作经常把MS1的数据与MS2的数据分开来讨论，其背后的小心思就在于MS2里好不容易发现的能解释得通的物质在MS1里完全找不到或数据不支持结论，这就是DDA滥用的后果。其实DDA倒也可以把MS1里选择的先导离子作为优先离子，但问题是如果你关注的就是MS1里能测到的物质，PRM就是能收集所有信息灵敏度最高的方法了，DDA肯定会把扫描时间浪费在MS1里没有的离子上。我还额外测试了iterative DDA，这项技术需要仪器支持，每次扫描只会扫前一次扫描里没有扫到的先导离子，效果跟PMDDA差不多，但让我比较吃惊的是PMDDA跟iterative DDA测到的MS1里的物质很多是不重叠的。也就是说，如果你真想竭泽而渔，那么最好的方法就是在样本量允许情况下同时进行PMDDA与PMDDA辅助的iterative DDA。&lt;/p&gt;
&lt;p&gt;这里我要补充下文章里没详细描述的一部分经验内容，那就是“竭泽而渔”的效果。从独立分子网络或者说潜在化合物的数目上看，即使使用PMDDA，先导离子里有二级谱图生成的也不多，大概只有一半多一些，而DDA收集的二级谱图从数目上看是足够多的，但问题是它选的先导离子并不存在于MS1的峰表里。形成这个现象的原因可能是我的MS1峰列表排除掉了样品中峰响应小于空白中同一峰响应三倍的峰，但进行这个峰过滤的原因在于这样的峰通常峰形很差，不能拿来定量，但DDA却会因为这些峰符合top10的选择原则就给送去做二级了。所以，我很难说“竭泽而渔”的PMDDA一定能拿到比DDA更多的二级谱图，但从其与MS1质量较高的峰表的对应效果而言还是有优势的。而且，我还发现传统DDA的重复进样可重现性非常差，下图展示了三次DDA进样同一样本时其选择的先导离子，基本一针一个样，这使得我更怀疑所有用DDA作出的研究成果了。有意思的是仪器厂家的软件根本就不提供这个功能，所以做科研必须要用开源软件且具备自己验证的能力，否则就是仪器厂家的免费甚至付费复读机。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yufree/yufree.cn/master/static/images/ddams1file.png&#34; alt=&#34;ddams1file&#34;&gt;&lt;/p&gt;
&lt;p&gt;PMDDA的另一优点在于不需要仪器配合，你完全可以用三重四极杆质谱来采集二级谱信息。这里的关键就是要通过数据分析提取MS1全扫数据通过多次进样来采集高质量的二级谱信息。不过，大连化物所许国旺老师也有类似思路的研究，这里最大区别就在于先导离子的选择策略。此外我对于研究的可重复性要求非常高，研究里用的样品是标准参考物质，数据可以直接从网上下载，所有处理代码是开源的，除了在&lt;a href=&#34;https://github.com/yufree/xcmsrocker/pkgs/container/xcmsrocker&#34;&gt;xcmsrocker&lt;/a&gt;镜像里有模版脚本外，我还在GitHub上建了一个&lt;a href=&#34;https://github.com/yufree/pmdda&#34;&gt;仓库&lt;/a&gt;保存了本项研究中所有分析细节，包括所有论文里图片的生成过程，而论文在预印本期间就已被多次引用。实际上我现在所在的实验室就用的PMDDA工作流来给其他课题组测样，可靠的工作流要经得起公开透明的各种验证并方便迁移到其他人的研究环境里，但很遗憾很多研究看着很好却因为过渡依赖商用软件很难实现本地化与验证。&lt;/p&gt;
&lt;p&gt;如果有问题或对此感兴趣也欢迎随时联系我，我会提供技术支持。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>质谱组学数据的批次效应</title>
      <link>https://yufree.cn/cn/2022/01/27/ms-data-batch-effect/</link>
      <pubDate>Thu, 27 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2022/01/27/ms-data-batch-effect/</guid>
      <description>&lt;p&gt;组学数据的一个显著特征就是需要同时测定一个样本里成千上万的信号，这个信号可以是基因、蛋白或代谢物。但这是说给外行听的，真实数据对于基因组来说一个基因对应测定的可能是十几个碱基序列片段或测序信号，对蛋白组来说一个蛋白的数据需要从众多肽段重组出来（也可以直接topdown测定，勿杠），对代谢组而说一个代谢物能产生十几个包括同位素、加合物、寡聚体在内的质谱峰。但凡分析化学背景出身的研究人员都会特别关心测量上的质量问题，因为如果本来就测得不对，后面的统计方法或可视化方法再怎么天花乱坠都属于空中楼阁。&lt;/p&gt;
&lt;p&gt;不过，真实情况要复杂很多。就算一个样品只需要20分钟分析，50个样品需要加上空白与混合样进行质量控制，一般每10个样品就需要进一针空白与混合样，这样实际要进样60个，考虑上每次进样前后都需要一些混合样来稳定柱效，这一批样品大概要跑24小时。每天50个样品连续跑一周也就能跑三百多个真实样品，但凡用过色谱的都知道一根柱子跑上几千个样品后基本也就该换了，连轴转的实验室基本半年一换。而且，做过分析的都知道，连轴转要比断断续续做对仪器稳定性来说更好，但即使再好，同一个样品隔一周测一次也能看到明显的变化，有时候是信号逐渐减弱，有时候则是响应突然断崖式跌一个数量级。从我个人经验而言，即使连续进样的数据也会出现类似状况，更不用说应对那些队列数据。&lt;/p&gt;
&lt;p&gt;队列数据收集起来不是一次成型的，所以样品天然就存在不同批次。今天采血的大哥拔针偏慢，下次采血的大姐喜欢快速收针，很容易造成样品天然背景就不一样。这时候即便你把样品混到一起随机进样，看到的也是基线高高低低的总离子流。不过这都不算事，因为队列数据经常不是同一时间收集的，所以要么攒到最后一起测，要么凑一波几十个样先测，前者需要保证样品储存不能产生影响，后者需要保证不同时间分析的仪器出信号要稳定。巧了，这两个保证现实中一个都做不到，即使零下八十摄氏度保存代谢物也会出现差异（我在实验室验证过），而仪器前面说了，隔一周都不够稳就不用说隔几个月甚至几年了。在论文里大家都会说是稳定的或者换个方式说“尽最大可能保证稳定”，至于不稳定的部分，学术界目前都是回避讨论。所以，用流行病学样本里的分子证据来证实某种疾病的机理是很困难的，一般都需要实验室条件下用老鼠或细胞实验来反复验证。我称之为测不准原理，不懂测量分析的研究人员容易沉浸于理论的逻辑自洽中，真实数据里的因果要比假设的复杂得多。&lt;/p&gt;
&lt;p&gt;好了，既然提到批次效应，我们就要想办法去掉这个影响。做目的性分析出身的人往往很不屑，因为他们有内标法。内标法一般是用稳定同位素标记过的物质在样品进样前加入，所有测到的目标物的响应都要去除内标的响应，这样如果分析上的波动是来自于仪器的不稳定，那么内标控制后的响应就可以排除掉这部分影响。不过内标法有一个前提，那就是标记物质与待测物在仪器上响应因子要一样，原汤化原食。但到了非目的分析，开全扫描模式，你根本不知道待测物是什么，此时加入内标并用内标矫正所有峰响应就属于胡闹了。不同物质的离子化过程是有差异的，在同一样品基质下，有些是信号增强，有些则是信号减弱，你要是用信号减弱的校准信号增强的，那么增强的信号就更强了。我到现在也不是很明白为啥有些非目的分析的研究中还在用内标，不是说内标没用，而是内标能校准的东西太少了。&lt;/p&gt;
&lt;p&gt;真正有用的混合样，也就是每一批次的样品都取出10微升来混成一个样，这样的样品应该会包含所有样品里的物质。那么，我们只要隔10个样品进一针混合样，然后就可以得到混合样里每一个物质的变化趋势了，然后只需要将这个变化趋势去除掉，剩下的就是样品物质的真实生物学变化了。这里需要注意的是这个变化趋势要是明显的才有矫正的意义，举例来说A物质在整个进样序列里的混合样中一直下降，那么对多个混合样里A物质的响应与其进样顺序做回归分析就会看到一条斜率为负的回归线，此时代入样品的进样顺序就会给出在这个回归线上的响应。所谓去除批次效应，就是用样品的真实响应去扣除掉进样顺序回归产生的响应。因为每个物质的质谱响应行为不同，所以回归线也不一样，有线性的，也有非线性的，但这个回归线的物理意义就是来自于进样顺序或批次的影响。因此，如果你进样序列里有混合样，那么后面发现批次效应就可以用这种方式进行原汤化原食的矫正。&lt;/p&gt;
&lt;p&gt;不过，有些实验数据过于莽撞，根本就没加入空白或混合样，这样有没有办法进行矫正呢？也有，但不全有。如果我们知道样本分组的话，那么数据还有救。所谓的样品，不过是一堆存在异质性的数据，如果我们的目标是寻找样品分组差异，那么所有不同于分组差异的显著性趋势都可以归类为某种批次效应。这样对多个物质而言，我们就先构建一个模型：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$响应 = 分组差异 + 其他差异$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这样我们就得到一个其他差异的初始值矩阵，然后我们对这个其他差异的矩阵进行主成分分析或svd分解，这样就可以提取出其他差异中的主要趋势，然后我们构建下面的模型：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$响应 = 其他差异主成分+分组差异+剩下的其他差异$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;拟合这个模型，然后继续在剩下的其他差异中找主要趋势，这里需要设计一个显著性的统计量，当统计量不再显著后就停止寻找，此时的模型就成了：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$响应 = 分组差异+其他差异所有主成分$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;此时，我们只提取分组差异就可以了，这个迭代过程就是替代变量的构建过程，这个方法可以在没有混合样但知道样品分组的条件下进行批次效应控制，效果非常好。那有朋友就会问了，如果我不知道分组差异呢？那我反问一个问题，你怎么知道存在批次效应呢？此时样品的波动究竟是来自于批次效应还是样品本来的差异根本就没法知道。如果你确定来自于批次效应，那么其实直接构建下面的模型：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$响应 = 生物学差异+批次$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;就可以了，但你至少也要知道批次或进样顺序，如果都不知道，我甚至都不知道你有什么理由认为存在批次效应。对于非目的分析，批次效应的难点在于不同物质的效应不一样，没办法用单一方法进行矫正，但这不代表盲目矫正就是对的。通常而言，如果你已经看到了批次效应，那么这个趋势就应该是可建模识别的，否则你根本说不清楚究竟是批次效应还是样本异质性导致出的现象。现在出现了很多自动化的批次效应检测矫正工具，且不论其中大部分都是重复造轮子，很多工具设计者本身就没有理解批次效应的复杂性简单进行函数套用来自动化决策过程，这种工程思维对软件开发是没问题的，但对科研是很不负责的。话虽这么说，这样的事其实我也没少干，这也是最近我在反思的问题，工具永远不能替代思考，否则人会陷到机械流程中无法自拔。&lt;/p&gt;
&lt;p&gt;此外，现在很多研究人员会只做目的性分析的代谢组学，也就是只去测定已知代谢物的数据并进行讨论。其实这是个聪明的选择，因为非目的分析通常会被物质鉴定卡住，做到最后其实也只能给出已知的在谱库里的数据，这只是另一种形式的目的性分析。不过，组学数据如果最后只能用已知信息来给出结论，那么额外测定的信息其实毫无价值。这可以部分解释为什么基因组或测序总能发现新东西而做蛋白的都喜欢光源冷冻电镜，代谢组学则始终在方法学层面打转，不是小分子里没有生物信息，也不是提取不出来，而是提取出来数据库里没有就给扔了。而且数据库匹配现在也是玄学，很多化合物标准品的谱图长一个样，放到生物样品里加上基质效应加上不同仪器状态就变另一个样了，考虑到代谢组学样品前处理跟没有差不多，就算你测到了已知物可能都无法匹配出来。&lt;/p&gt;
&lt;p&gt;这些现实存在的问题都需要解决，只进行“聪明的选择”永远也回答不了最难的科学问题，只是现在的我有点怀疑：是不是有些东西就是永远也测不准呢？如果是的话，那科研就一定要想办法与这种永恒的不确定性共存并寻求发展，这也是过去两三百年科学家们一直在做的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fountain 标记语言</title>
      <link>https://yufree.cn/cn/2021/10/25/fountain/</link>
      <pubDate>Mon, 25 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/10/25/fountain/</guid>
      <description>&lt;p&gt;这是去年留的一个坑，眼看着挖土填是来不及了，就灌水来填了。&lt;/p&gt;
&lt;p&gt;标记语言里最经典的是 HTML，但最出圈的应该是 Markdown。所谓标记语言，可近似理解为纯文本+标记，而标记是用来定义文本的结构、格式或属性。一款标记语言要有自己的标记环境，例如 HTML 喜欢用一对尖括号来进行标记，尖括号里的内容则指示了特定的结构、格式或属性。例如 &lt;code&gt;&amp;lt;p&amp;gt;段落&amp;lt;/p&amp;gt;&lt;/code&gt; 就对纯文本&amp;quot;段落&amp;quot;进行了标记，标记内容为&lt;code&gt;p&lt;/code&gt;，也就是段落的意思。&lt;/p&gt;
&lt;p&gt;绝大多数现代浏览器能识别 HTML ，把标记内容按一定样式可视化出来，当然样式也是可以自己定义的。但HTML或CSS对习惯读纯文本的人而言写起来还是比较繁琐的，所有就有了 Markdown 这种简化版标记语言，把成对尖括号这种反人类设计改成更直观的井号、星号还有方括号这些标记，然后开发了一个引擎，把这些标记翻译成 HTML 这类已经有固定标准的标记语言，最后被浏览器渲染成网页。&lt;/p&gt;
&lt;p&gt;我猜测最初 Markdown 的开发者只想提供一个极简的标记语言，因此只转译了例如标题、加粗、斜体、引用、列表、链接等很少的 HTML 标记，用20%的努力解决了80%的场景。因为最终展示的也是 HTML ，所以如果写文档的人想要更多的功能，可以在 Markdown 直接混写 HTML 标记语言，反正最后也都能展示出来。&lt;/p&gt;
&lt;p&gt;不过 Markdown 后来因为其简单易用出圈了，原来只有前端工程师才涉及的标记语言现在几乎各行各业的人都在用。学习 Markdown 可能几分钟就能搞定，但用起来感觉神清气爽。如果一个文字工作者被 Word 与各种在线编辑器折磨一段时间后，那么他不是已经用上了Markdown，就是在寻找这类工具的路上。例如我学Markdown完全是因为早就习惯了另一种标记语言：Emacs 内置的 Org-mode，我曾经用这玩意写过一段时间的日记，当然叫它日记算是抬举它了，一年也没几篇，后来因为电脑硬盘挂了就彻底没了。印象中 Org-mode 写维基特别好用，但后来转到 Markdown 也无外乎发现了我其实能用到的功能并不多，先写起来比纠结学哪个工具更重要。另一原因则是不管 Vim 还是 Emacs，我都因为用得少记不住快捷键，此时找个不依赖编辑器又简单易用的标记语言就成了自然需求。&lt;/p&gt;
&lt;p&gt;当然，但凡提标记语言就不能不提另一个山头：TeX。这个可以理解为排版用的标记语言，不同于转换为网页，TeX的终点是排版好的dvi文档，当然现在更流行的是PDF文档。这里就不多说 TeX 那一坨发展史了，反正本来就是某研究人员不爽传统排版而自己搞出来的标记语言，其发展初期长期被学术鄙视链顶端率先接触计算机的物理与数学研究人员采纳，而其他学科普通研究人员喝到计算机技术外溢到学术投稿系统的汤时，满大街都已经是微软 Office 的天下了。&lt;/p&gt;
&lt;p&gt;我接触 TeX 应该是读了王垠的一篇博文，不得不说这是个超级大坑，但也得承认 TeX 标记语言如果搞明白了，软件编译流程基本也就能想明白了。不过 TeX 终究也没火起来，它对标的是所见即所得的全功能编辑器，学习门槛比 Markdown 高太多。现在就算是研究人员大概也就还是搞数学跟计算机相关的学科出于对公式编辑器的喜爱还在用，再有就是对排版要求极高的个人，而出版社则反而不用，他们早就被 Word 给驯化了。&lt;/p&gt;
&lt;p&gt;不过，自从谢大的 knitr 包横空出世，编程语言与标记语言的混搭却意外得到了很多关注。早期 R 语言社区存在 Sweave 文档，此时混搭的是 R 与 LaTeX 文档，但前面说了，LaTeX 这个坑一般人是不愿意跳的，谢大的 knitr 包则选择了 Markdown 作为标记语言，这样一下就把门槛放平了。这里要说明的是 knitr 包很多人认为就是实现 RMarkdown 文档的编译，其实不全面，knitr 包也支持 Rnw 这种用 LaTeX 做标记语言的文档的编译，而且 knitr 包也不仅仅支持 R，其他编程语言配置好了也能进行文档编译，反而是针对 R 与 Markdown 的部分单独被 rmarkdown 包承包了。而 Python 社区现在则基本都是 Jupyter notebook 的天下了，当然 Jupyter 也支持 R、Julia等编程语言与 Markdown 作为标记语言。&lt;/p&gt;
&lt;p&gt;当然，有编程语言与标记语言的混排需求的人还是少数，更多人直接选了Markdown 编辑器来作为日常写作工具。例如我现在想记一些不涉及编程的东西最先开的是 Typora 这个编辑器，很多人也喜欢用支持 Markdown 的笔记软件。好了，终于要到 Fountain 标记语言了。&lt;/p&gt;
&lt;p&gt;我第一次知道 Fountain 标记语言是在Joplin这个笔记软件的选项里看到了其对这种标记语言的支持，当时就出于好奇查了一下，发现这是专门给编剧写剧本的。但我显然不是编剧，毕竟生活这出戏不用编就已经足够荒唐魔幻了，用不着虚构。但仔细了解了 Fountain 标记语言后，我发现这个格式对一个场景非常有用，而恰好这个场景也可能会是很多人可能用到的。&lt;/p&gt;
&lt;p&gt;首先允许我来个现炒现卖，剧本其实是个很神奇的排版方式：一页典型剧本开头要有场号，然后要有这场戏的梗概，之后就是内容，例如两个人的对白或转场。每一页英文标准剧本会严格限制字体与纸张大小，大概对应一分钟的戏。好了，知道这些就够了，毕竟我们也不是这个专业的。&lt;/p&gt;
&lt;p&gt;这里神奇的地方在于一页纸对应一分钟的戏，那么对于研究人员来说就可以用来准备讲稿，特别是学术会议。其实写还是按原来的写，Fountain 标记语言跟Markdown几乎一致，只不过是限制的更多，这样你导出的PDF文档每页大概就是一分钟的量，15分钟的报告也就需要15页稿子，有点像高中语文的1000字稿纸，方便练习。&lt;/p&gt;
&lt;p&gt;其实我去年开会就尝试过，稿子写了一上午，结果练习一次就发现自己实在是不习惯念稿，我更习惯看着幻灯片讲，一看稿子就分神。不过这可能是我个人问题，练习十遍能讲出十个版本，加上讲稿就成了十一个，我个人做报告会进入一种精神高度集中状态，经常临时给出新的想法或者故事线，练的多后反而对熟悉的东西没有兴致去讲好，影响报告效果。所以我这里推荐 Fountain 标记语言但实话说我自己反而用不好。&lt;/p&gt;
&lt;p&gt;另一个适合Fountain标记语言的场景是现在视频制作时的文案，固定字体与纸张大小后就会固定视频长度，而且可以利用剧本的转场或场景转换来让视频内容的丰富些，也就是把叙事从文字陈述改为文字陈述加上场景转换加上现场布置加上动作，这样文案就会更立体些，我称之为立体叙事或镜头叙事。这样的镜头叙事对于传达信息会更生动些，对于老师备课也有启发。这些东西落实到剧本上，体验感与互动感会比单纯的文案有所增强。只要保证好一页纸对应一分钟的内容，创作也会更好把握节奏。同样，我也尝试过，但也发现我根本不适合准备文案，更适合临场发挥。这里只是分享给有需要的朋友。&lt;/p&gt;
&lt;p&gt;这里需要说明的是你需要专门的软件来支持Fountain标记语言的实时预览来控制创作节奏，可以考虑用joplin，毕竟专业的剧本软件都是收费的。另一个问题就是Fountain标记语言的节奏控制是针对英文的，切到中文每一页不一定对应一分钟，但只要你固定了字号与格式，基本也可以用。&lt;/p&gt;
&lt;p&gt;另外，我也推荐去读下剧本，这样可以更好体会编剧对于镜头语言的控制，中文的有个花生剧本可以免费用但不支持Fountain标记语言，好处是有不少经典电影剧本可以看。我不是搞编剧的，但很明显编剧的镜头语言与场景控制能力在其他行业里也可以用得上，至于如何用或者用好，自己去体验下就知道了。&lt;/p&gt;
&lt;p&gt;其实我到最后也没具体去聊Fountain标记语言的语法，这不重要，都是给个关键词10分钟就能搜索掌握到的。但要是掌握了剧本这种创作形式，可能打开一个新世界。&lt;/p&gt;
&lt;p&gt;ONE PIECE 就在新世界的尽头。淡出～&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>看门人分子</title>
      <link>https://yufree.cn/cn/2021/06/17/gatekeeper/</link>
      <pubDate>Thu, 17 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/06/17/gatekeeper/</guid>
      <description>&lt;p&gt;今年ACS春季会议我做了一个口头报告，提出了“看门人分子”的概念。这大概是我又双叒叕挖坑的一个开端，现在预印本已经放到&lt;a href=&#34;https://chemrxiv.org/engage/chemrxiv/article-details/60c9e3b13fc2cb674c000d4e&#34;&gt;网上&lt;/a&gt;了，这里做下解释。&lt;/p&gt;
&lt;p&gt;这里我关注的科学问题来自于污染物暴露研究，如果一个人暴露给一种污染物，那么就存在导致某种健康状态的可能。在暴露组学研究中，污染物暴露通常作为某种疾病的预测变量，也就是有下面的关系：&lt;/p&gt;
&lt;p&gt;$$健康 = f(污染物，协变量)$$&lt;/p&gt;
&lt;p&gt;然而在代谢组学中，我们会认为健康状态是由代谢物水平变化直接导致的，也就是下面的关系：&lt;/p&gt;
&lt;p&gt;$$健康 = g(代谢物)$$&lt;/p&gt;
&lt;p&gt;这里没有放协变量是因为很多协变量在分子水平上本就可以被某些代谢物来指示，例如性激素水平大概就能指示性别，此时你把性别放到模型里会造成共线性，降低模型稳健度。&lt;/p&gt;
&lt;p&gt;观察这两个等式我们会发现，如果代谢物本身可以体现健康状况，那么污染物也就有可能是通过影响代谢物来影响健康状态。注意，此处代谢物是可以被替换为其他来自人体样本的分子信息的，例如蛋白质、组蛋白、基因甲基化水平等。但本质上是一个环境信息传递到分子生物学信息再传递到健康效应的信息流。考虑上遗传作用也就是：&lt;/p&gt;
&lt;p&gt;$$环境暴露/遗传 \rightarrow 内源响应 \rightarrow 健康$$&lt;/p&gt;
&lt;p&gt;此时我们会发现，遗传影响与环境影响其实都会体现在生物样本的分子信息里，当这三部分都能被观察到时，我们应该可以从分子水平解释环境或遗传对健康影响的机理。遗传那部分不是我关注的，后面只讨论环境影响，这里放上遗传是为了让这个模型逻辑上通顺些，实际不同健康状态的主导影响也是不一样的。&lt;/p&gt;
&lt;p&gt;环境流行病学会关注那些受环境影响更大的疾病，但描述暴露与疾病关系却通常是毒理学家在做，这里经常出现毒理学上证明有影响但流行病学数据看不出来（大多数）或者流行病学看到了影响但毒理学证据不充分（少数）。这里的不一致在我看来主要是因为毒理学所采用的控制实验体系过分简化了环境暴露，很多毒理学上用到的剂量现实生活中很难出现，或者说虽然剂量水平相当，但因为缺乏暴露途径或存在其他拮抗暴露影响的机制。更重要的在于很多时候评价指标如果太过宏观就会不够灵敏，而描述暴露跟健康的数值都可以算相对宏观的指标。&lt;/p&gt;
&lt;p&gt;因此，要搞清楚环境究竟如何影响健康，我们最好是直接测量人群样品的分子水平信息，这就是代谢组学跟蛋白质组学还有表观遗传组学在做的事。但人群样本不可能做到随机化，我们不太容易拿到个体水平的环境信息，例如我可以知道A地当天室外空气细颗粒物浓度，但不太容易拿到某个人当天暴露的细颗粒物浓度，最简单就是如果这个人自己在家做饭，那么他的细颗粒物暴露量是远高于室外浓度的。&lt;/p&gt;
&lt;p&gt;其实，外在暴露的变动范围其实是远大于内源响应的，室外温度可以从零下几十度到四五十度，但人的体温波动不会特别大。从这个角度出发，体内的代谢物分子可以分成两类，一类是对暴露敏感的用来响应与感知外界变化，另一类则是不敏感的用来维持生命系统。可以预想到，人体代谢平衡或者主要代谢通路更多是不敏感的那一类分子在维持，如果暴露真的造成了健康状态改变，那么肯定是先影响敏感分子然后突破敏感分子的信息传播阈值影响到了不太敏感的主代谢通路。那么，这部分敏感的分子就有必要找出来。&lt;/p&gt;
&lt;p&gt;这里我们就会发现一个问题，当进行组学研究时，我们是不预设敏感度的，也就是仪器能测到啥我们就报道啥，但这里就涉及一个统计学多重检验的问题，我们会在完全不敏感的代谢分子上浪费大量的统计功效。毕竟现在错误发现率的控制都是基于检验数的，检验数越多，判断出现差异的要求就越高。例如我去进行污染物A对代谢组影响的研究，结果测到了1000种代谢物，然后要做1000次假设检验才能知道哪种代谢物跟污染物有关，此时就要做错误发现率控制。不过其实从一开始我们就知道不会有太多出现差异的，但这部分知识没法反应到数据分析方法里。&lt;/p&gt;
&lt;p&gt;现在常用的一个做法就是降维或聚类，把趋势类似的代谢物组合为一个新的潜在变量，然后去跟污染物进行相关分析。且不论到头来你怎么再把影响回溯到具体的物质，这个思路并不解决我们收集了一大堆无关信息。我的想法就比较简单直接，在进行污染物与代谢物之间分析之前，先对代谢物内部相关性进行分析。在设定的相关性阈值之下，代谢物会形成代谢网络，绝大多数情况你会看到一个主干网络与零星分布的小代谢网络，这里我们把这类代谢物叫做大陆代谢物，但更多的代谢物是独狼或孤岛，根本无法与其他代谢物产生任何联系。此时，生物信息的流动是构建在代谢网络上的，因此我们只关注大陆代谢物。这是一个基于概率的假设，不可否认会丢掉一些有用信息，但却能显著降低研究难度。&lt;/p&gt;
&lt;p&gt;从我做的案例来看，常规代谢物检测在一个较高相关性阈值下90%以上的代谢物属于孤岛，这样我们通过区分孤岛与大陆就实现了一次基于信息含量的降维。而面对剩下的代谢物，我们会逐一检查其与暴露物的相关性。可以预见，并非所有代谢物都会与暴露物有关系，这样我们就能筛到直接与暴露物打交道的代谢物。因为这些代谢物一方面跟污染物有联系，另一方面又跟其他代谢物有联系，那么很有可能污染物的信息流的接受者就是他们。换句话说，这些代谢物是污染物影响代谢网络的最前线，所以我给他们起名为看门人（gatekeeper）分子。下面是一个简单的看门人分子示意图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yufree/presentation/gh-pages/figure/gkc.png&#34; alt=&#34;gatekeeper&#34;&gt;&lt;/p&gt;
&lt;p&gt;理论上会有两种看门人分子，一种是直接横在暴露物与代谢网络之间的，另一种则是互相为看门人分子。但不论哪一种，看门人分子都应该属于对暴露物比较敏感的，而且在真实数据中，我发现看门人分子可以响应不止一种暴露物。这个发现就说明代谢物层面有可能存在一个相对通用的看门人机制，其实本来环境毒理学就有外源代谢通路的说法，但如果能从小分子层面描述可能更有利于阐明影响机制。&lt;/p&gt;
&lt;p&gt;不过看门人分子只处理了环境暴露跟代谢物的关系，我们最开始的那个问题，也就是影响健康的机制还没有讨论，只是把讨论范围缩小到了具体的代谢物上。在我做的案例中，环境暴露无法直接发现跟健康的关系，但当我用这些暴露物的看门人分子来研究与健康的关系时，可以直接观察到看门人分子与健康状态的关系。也就是说，看门人分子要比直接用环境暴露研究健康影响更灵敏。这倒不难理解，毕竟环境影响的测量不确定性本来也比代谢物测量不确定性要高。&lt;/p&gt;
&lt;p&gt;其实看门人分子这个概念的本质还是基于网络分析的，基因组里比较流行的WGCNA这个包的核心也是发现网络结构后对每个独立网络结构做svd分解，然后取第一个主成分来代表网络进行下一步分析。不过，看门人模型从一开始就是直接针对具体的代谢物来建模的，通过筛选网络结构来找出有实际意义的代谢物，形成假设方便后续检验。也就是有下面的关系：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yufree/yufree.cn/master/static/images/EMDG.png&#34; alt=&#34;EMDG&#34;&gt;&lt;/p&gt;
&lt;p&gt;这个分析方法的重点就在于我把代谢组限定到了看门人分子这个子集，这些代谢物可能更适合进行环境影响相关的疾病研究（起码初步的结果是这样）。同时，看门人分子不仅仅可以是小分子代谢物，还可以是蛋白质或其他生命大分子。这样看门人分子实际就成为了可以把各种组学连在一起的概念，而且这部分分子要比其他分子对环境因素更敏感，更可能是健康防护的第一条防线。&lt;/p&gt;
&lt;p&gt;关于这个概念我已经做过四次报告了。从反馈上看，做生物信息的把这个方法看成了利用信息量降维的手段；做统计的第一反应是这玩意怎么看着像因果分析；做生命科学的认为这个概念比花里胡哨的分析方法简单，容易设计实验验证；做流行病的则强调一定要把协变量给考虑进去且考虑其在预防医学上的意义，特别要考虑灵敏度问题；做环境的更关心能不能直接给出一个基于人体代谢物的看门人分子数据库。我之所以费劲写这篇介绍就是因为预印本被很多不同背景的人改的一言难尽，所以干脆拿母语把核心概念重写出来算了。&lt;/p&gt;
&lt;p&gt;从我个人角度跟能力，其他方向我做不了但我乐意与之交流，学科间应该有更多的互动而不是互相睥睨。不过，这个概念倒是有可能跟我之前所说的反应组学进行联动。当前的网络结构是利用相关性来构建的，但基于质量差的反应组也可以用来构建网络，而且解释性可能更好些。其实我并不喜欢炒作概念，主要原因是很多新概念只是构建了学科间的壁垒，但看门人分子这个隐喻却有利于学科间的融合，唯一的问题就是当前研究人员是否乐意打破学科壁垒了。&lt;/p&gt;
&lt;p&gt;ps. 我本来起的名字是守门员（goalkeeper），至于为啥改成看门人，那又是个关于政治正确的故事，不提也罢。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>jupyter与容器技术</title>
      <link>https://yufree.cn/cn/2021/01/28/container/</link>
      <pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/01/28/container/</guid>
      <description>&lt;p&gt;我们现在评价科研成果，经常喜欢用当前的共识这个表述，但前沿领域的共识不见得是对的。更严格说，所有涉及观点站队问题的讨论都不属于科学讨论，面对质疑经过合格训练的科学家都不会简单采信某一方观点而是要去看原始数据或在自己的实验室重现，让事实说话要比空对空有意义的多。不过重复实验在现代社会能不能拿到经费是存疑的，在当前经费获取的方式下，重复研究别人的成果是费力不讨好的，更多人重复他人实验是为了基于此进行进一步研究，如果重复不出有些课题组会尝试其他路径，有些就去联系原作者询问实验细节。&lt;/p&gt;
&lt;p&gt;我博士阶段末期就收到过关于一组植物愈伤组织实验重复的询问，当时我特意去重复了实验还发了操作过程照片过去，结果那边还是重复不出来。我到现在也搞不清楚是哪里的问题，因为我重复后结果跟发表的论文是一样的，但对方咋做的就不知道了，而且他那边其实对我的体系进行了本地化修改，暴露物也换掉了，但在我看来应该不影响结果。因为这段往事让我意识到可重复性里面可能包含了质疑者与原作者都没意识到但很重要的步骤，而这个步骤直接导致了重现性不好，这就是真实科研或者说实验学科会遇到的问题。好比一份菜谱给出来，两个厨子炒出了两份口味不同的菜，如果菜谱符合要求，那么一定是存在菜谱外的东西影响了结果。这种情况在化学实验里不常见，因为化学实验体系通常比较简单，但生物实验就非常常见了，同样成分的培养基经常一个能养活细胞，另一个养啥死啥，这也是为什么生物论文实验里用的材料是要标注厂家信息的，因为很多实验对材料的要求只能用厂家品控来保证重复性。&lt;/p&gt;
&lt;p&gt;实验的可重复性眼下可控性里操作空间还是有的，但眼下实验完成后的数据处理与共享部分如果透明化，那么会极大挤压学术不端的空间。虽然同样一组数据在不同的分析方法或统计模型下可能得到完全不同的结论，但只要你能把数据如何分析的过程及原始数据共享出来，那么也是可接受的。打比方要做一组高维数据的机理与预测模型，你用随机森林选出一组重要变量并基于此构建了逻辑通顺的机理模型，隔天用同样的数据别人用线性混合模型又选出一组变量也构建了逻辑通顺的机理模型，但这两个模型降维上用了完全不同的策略与假设，导致两组模型虽然都说得通但都可能只解释了真相的一部分，这个也属于常见现象。&lt;/p&gt;
&lt;p&gt;当前期刊一般都会要求上传原始数据，但这是不够的，博士阶段我读了一篇论文看到了里面一种计算方法很有意思，但他们给的是数学公式，我是在matlab上重现后才能去验证。这个操作对于实验学科的人而言门槛过高，绝大多数实验学科研究人员的数据分析水平不会超过调用别人写好函数处理数据的阶段，指望自己写需要自学很多东西，这虽然应该是合格研究人员应具备的素质，但难度还是有的。我现在读很多论文可以感到明显的割裂感，就是实验部分与数据分析是分工来做的，这就导致很多描述非常不准，例如简单利用p值来说明结果而意识不到p值本身的问题，很多数据分析方法描述非常奇怪，明明一两句就能说清楚但却自己定义了一大堆东西来绕弯，很明显对分析方法原理没搞清楚。&lt;/p&gt;
&lt;p&gt;这属于无效协作，实验方与数据分析方想按照分工原理来提高效率，但最后展示出来的则是一团乱麻。一篇论文一定要有一个人能同时理顺实验与数据分析的所有步骤，这看上去很不合理但没办法，从我跟单位里所谓专业统计分析人员打交道的体验来看，那种强调要把数据做成他们那种行是样品列是特征的标准格式的要求是荒谬的，真实数据要转化到那种标准格式是要进行大量假设的，这部分实验方通常不了解，数据处理方又不管，最后给出的结果常常导致实验方无法验证而数据处理方又对数据中普遍存在的异常值与确实值大为恼火。在我看来根本就不能割裂开实验与数据分析，这两步需要同一个人来做，而且职业做实验的一定会被自动化仪器取代而职业处理标准数据分析的也一定会被自动化软件取代，唯独互相连接的人是无法被技术取代的。&lt;/p&gt;
&lt;p&gt;当前的解决方案对于实验人员而言就是保留完整的数据分析脚本，这个本来很正常的需求因为图形化商业软件的流行而被认为很不友好。说句不好听的，这就是被图形界面给惯坏了而忘了科研中对重复性的要求，而且大多数专业图形界面的数据分析软件其实也会记录操作步骤，你的每一步点击都会在一份记录中被保留，所以数据分析步骤与图形界面并不矛盾。不过从实际数据分析角度，如何给出脚本确实是个技术问题而jupyter项目则在一定程度上解决了这个问题。&lt;/p&gt;
&lt;p&gt;在介绍Jupyter项目之前，我想说如果你的数据分析完全依赖 R 与 Python，那么自带 RStudio 服务器版的 Rocker 镜像配合 Rmarkdown 文档就已经可以实现数据分析的完全可重现性了，甚至 Rmarkdown 文档本身就可以作为完整数据分析步骤的良好载体而附加在论文附件里来保证可重复性。当然如果你懂一点R包开发，把分析方法作为模版嵌到一个R包里也是没问题的。今天想说 jupyter 项目，纯粹是因为我最近考古发现现在 jupyter 已经从 Python 的轻量级在线开发环境成长为多语言支持的平台了，其部署上也非常容易。当然，其对学术写作生态的支持对比 Rmarkdown 的生态还是弱了非常多，更偏探索，当然依赖pandoc的核心都可以互相转换，但感觉学术界，特别是实验学科目前对R的接受度更高，毕竟学术数据分析所需要的统计工具 R 基本都有现成的，Python 在这方面虽然机器学习的包更全，但实际研究里需要的工具就不完整。R 包的社区里存在大量即懂专业知识又懂统计分析的人，会给出很多研究人员直接用的函数，当然很多开发者并不太在意效率问题。Python的社区里也有科研人员，但实验学科的不多，整体社区偏软件工程偏通用计算问题。不过理想状态是两种语言都掌握，一种做到开发级，另一种做到应用级对于绝大多数科研数据分析问题就都能处理了。&lt;/p&gt;
&lt;p&gt;Jupyter 项目最开始就是专门为 python 设计的，后来逐渐发展壮大，可以支持更多的语言，前提是你要把对应的核装上保证交互通信畅通。这里顺道也捋一捋 Python 的安装问题，现在人装软件都是全家桶，你单装一个 Python后面一样有大量的依赖问题，因此大多数都是去装一个anaconda，这是基于Python的数据处理和科学计算平台发行版，但其实也支持R。可以把 anaconda 理解为 TeX Live 之于 TeX 排版系统的关系，作为发行版，基本要囊括编程语言本身、集成开发环境（IDE）、常用包及包的管理器。例如，TeX live里就会打包排版引擎、参考文献处理引擎、文档格式转换、字体、常用宏包、包管理器 tlmgr、文档编辑器 TeXworks 等几乎所有你可能用到的工具与文档。anaconda 里你可以装 Jupyter notebook作为IDE，也可以装PyCharm作为IDE，甚至可以装RStudio，其包管理用的是 conda，用法上类似 Python 的 pip 安装，但 conda 不仅仅支持python，你是可以用 conda 来装 R 包的，而且其解决依赖问题也比较智能（pip其实也可以做到）。不过，因为 anaconda 本身也是个公司，有付费产品，免费产品界面也有点花哨，所以很多有洁癖的人会选 miniconda 这种精简版。&lt;/p&gt;
&lt;p&gt;Jupyter 经典版就是交互式笔记本，也是最早得以流行的核心功能，用户可以在代码块里写代码，然后运行代码块直接看到结果。熟悉 R 的会发现这跟 knitr 的功能类似，不同点在于 knitr 对于代码块的控制更多，侧重一次编译出带结果的成品文档，而Jupyter notebook 侧重实时输出结果或再现结果，更接近 REPL 但不算是个好的开发工具。在实际写文档时，相信多数人会选择调用包里的函数而不是现写一个，所以 Jupyter notebook 在交互要求高时也还算不错。虽然大多数人用 ipython 作为解释器，但通过 IRkernel 这个R包并初始化后其代码块也可以执行 R。在 RStudio 里，也可以创建类似的 R Notebook，并用 reticulate 包来使用 python。Jupyter 笔记本是完全采用网页界面形式进行交互（其实 RStudio 也是），笔记本的下一代产品是 JupyterLab ，这个界面更接近一个全功能的IDE了，也是基于 notebook的，可以装各种插件来提高效率，所以现在上手可以直接从 JupyterLab 开始。&lt;/p&gt;
&lt;p&gt;Jupyter项目中最吸引我的是Jupyter hub，前面的在线笔记本是一个人用的，Jupyter hub可以将笔记本发布到网上供多人使用。在R里面我们做网络应用一般用Shiny，后台跑的是R，如果借助Jupyter hub，我们也可以把一个交互式应用放到网上，这里可以用带有Jupyter hub的docker镜像进行快速部署，也可以借助k8s部署到集群上。如果你只打算在一台服务器上做一个轻量级的在线应用或分享一个笔记本，用户不超过100人，可以直接用The Littlest JupyterHub 来部署，这个应该是个很好的教学工具平台，不过R里也有learnr包作为对比。&lt;/p&gt;
&lt;p&gt;另一个值得关注的项目是binder，在这个项目里你可以直接分享一个笔记本，在这个&lt;a href=&#34;https://mybinder.org/&#34;&gt;网站&lt;/a&gt;，只需要告诉binder你的 Github 库地址就可以，当然这个库里得有笔记本。这个项目相当于给了公共计算资源，你的GitHub笔记本会被生成一个 docker 镜像，然后借助dockerhub展示给用户，这个项目目前是免费的，也支持R，请不要滥用。&lt;/p&gt;
&lt;p&gt;有了jupyter项目与容器技术，科研数据分析的流程就可以实现在线化与可重复性，其实如前所述单纯R的生态也可以做到。这样研究成果发表时应该同时附带对应的数据分析流程报告且最好这份报告也支持在线验证，这样就很容易从技术上堵掉图片误用这类说不清道不明的漏洞。上传原始数据并同时上传数据处理脚本，所有处理过程都让电脑来完成，这样审稿只需要关注处理方法是否合理就可以了，因为这里面从数据到结果没有可以人为干涉的空间。我相信重要的发现一定是可重复的，那么起码要保证数据到结果之间100%的重现性。&lt;/p&gt;
&lt;p&gt;在验证学术问题上，实验数据比专家口水更管用，代码自动化重现要比手动点击靠谱。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>诺谟图</title>
      <link>https://yufree.cn/cn/2020/04/08/nomography/</link>
      <pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2020/04/08/nomography/</guid>
      <description>&lt;p&gt;诺谟图（Nomography），又称列线图解法，于1884年由Philbert Maurice d&amp;rsquo;Ocagne (1862-1938)发明来用图形化计算解决复杂函数的求值方法。随着计算机数值计算的发展逐渐淡出人们的视线，但有必要将其拎出来重新讨论一下。一方面是重新演绎直观计算过程有助于消除科学的神秘性并增加趣味性，另一方面则是我个人的一种直觉：诺谟图似乎要在科学的某些领域复活并展示其巨大的威力，此外诺谟图的设计本身就是一种艺术创作，甚至同一个方程会出现两种完全不同的设计方式。这里我先大概翻译转述下Ron Doerfler 写的一篇&lt;a href=&#34;http://myreckonings.com/wordpress/wp-content/uploads/JournalArticle/The_Lost_Art_of_Nomography.pdf&#34;&gt;论文&lt;/a&gt;里我能看懂的部分来讲下原理，然后扩展讨论下历史，最后讲下其在当前统计学里的应用。&lt;/p&gt;
&lt;h2 id=&#34;原理篇&#34;&gt;原理篇&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/ctof.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;最简单的诺谟图是上面的摄氏度与华氏度的转化图,其本质也是一个计算尺，其区别在于计算尺可通过多步计算来实现更多种类的方程计算而诺谟图则侧重于一步解决一个特殊的方程。其实诺谟图的一般形式是通过将含三个或三个以上的变量的方程中的变量用标尺表示，然后通过等值线（isopleth）将所有已知变量连接，这样未知的变量也就知道了。当然不同变量的标尺的位置是通过前期设计得到的，而这种表示方式要比3D图更为直观也易于接受。&lt;/p&gt;
&lt;p&gt;平行线式诺谟图是最简单也是最经典的一种多变量诺谟图，其基本形式与图式如下:&lt;/p&gt;
&lt;p&gt;$$f_1(u) + f_2(v) = f_3(w)$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/mfm.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;code&gt;$m_1$&lt;/code&gt;，&lt;code&gt;$m_2$&lt;/code&gt;，&lt;code&gt;$m_3$&lt;/code&gt;代表的是比例因子，在一条等值线穿过这三条平行线后，根据相似三角形原理我们可以建立如下方程:&lt;/p&gt;
&lt;p&gt;$$\frac{m_1f_1(u)-m_3f_3(w)}{a} = \frac{m_3f_3(w) - m_2f_2(v)}{b}$$&lt;/p&gt;
&lt;p&gt;整理后可得:&lt;/p&gt;
&lt;p&gt;$$m_1f_1(u) + \frac{a}{b} m_2f_2(v) = (1+ \frac{a}{b})m_3f_3(w)$$&lt;/p&gt;
&lt;p&gt;对照最初的基本形式，我们要做的就是消去与&lt;code&gt;$m$&lt;/code&gt;、&lt;code&gt;$a$&lt;/code&gt;、&lt;code&gt;$b$&lt;/code&gt;相关的部分，如下:&lt;/p&gt;
&lt;p&gt;$$m_1 = \frac{a}{b}m_2 = (1+ \frac{a}{b})m_3$$&lt;/p&gt;
&lt;p&gt;整理可得到下面的公式:&lt;/p&gt;
&lt;p&gt;$$\frac{m_1}{m_2}=\frac{a}{b}$$&lt;/p&gt;
&lt;p&gt;$$m_3 = \frac{m_1*m_2}{m_1+m_2}$$&lt;/p&gt;
&lt;p&gt;好了，我们现在知道了比例因子的关系，同时我们也知道想要画的图的大小与我们关心的变量变动范围，而根据已知变量&lt;code&gt;$f_1(u)$&lt;/code&gt;与&lt;code&gt;$f_2(v)$&lt;/code&gt;的变动范围与图的高度我们可以确定 &lt;code&gt;$m_1$&lt;/code&gt;、&lt;code&gt;$m_2$&lt;/code&gt;与&lt;code&gt;$m_3$&lt;/code&gt;，根据 &lt;code&gt;$m$&lt;/code&gt;的取值与图的宽度可以定出直线 &lt;code&gt;$f_3(w)$&lt;/code&gt;的位置，这样由于在图上&lt;code&gt;$f_1(u)$&lt;/code&gt;、&lt;code&gt;$f_2(v)$&lt;/code&gt;与&lt;code&gt;$f_3(w)$&lt;/code&gt;的坐标都是线性均匀分布的，现在就可以用等值线来进行求解了。&lt;/p&gt;
&lt;p&gt;也许看到这里你会说求个线性方程费这么大的劲是没意义的，但线性方程可不仅仅就这一种形式，原则上可以转化为线性方程的方程都可以用诺谟图来求解，例如下面的两种转化&lt;/p&gt;
&lt;p&gt;$$log(cd) = log c + log d$$&lt;/p&gt;
&lt;p&gt;$$log(c^d) = d log c$$&lt;/p&gt;
&lt;p&gt;其实，这样做的同时你也会发现标尺不再是均匀的了，但没关系，因为我们在绘图时依然可以采用均匀的标尺，然后附加一个对数表就够了，要是有耐心可以把标尺按原始数据绘制上去，这都不影响我们计算的准确性。&lt;/p&gt;
&lt;p&gt;上面的理论分析太过枯燥，下面我们用一个例子来展示在处理复杂的工程经验方程时诺谟图的绘制。方程如下:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$(1.2*D + 0.47)^{0.68}*(0.91T)^{3/2} = N$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;我们关心的范围是&lt;code&gt;$1.0&amp;lt;D&amp;lt;8.0$&lt;/code&gt;与&lt;code&gt;$1.0&amp;lt;v&amp;lt;2.0$&lt;/code&gt;，而&lt;code&gt;$u$&lt;/code&gt;与&lt;code&gt;$v$&lt;/code&gt;的长度都设置为11cm而宽度设置为6cm，首先进行方程变形:&lt;/p&gt;
&lt;p&gt;$$0.68 log(1.2D + 0.47) + 1.5 log T = logN − 1.5 log 0.91$$&lt;/p&gt;
&lt;p&gt;然后根据图形长度范围与参数范围求解 &lt;code&gt;$m$&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;$$m_1 =\frac{11}{0.68 log(1.2(8.0) + 0.47) − 0.68 log(1.2(1.0) + 0.47)}= 20.73$$&lt;/p&gt;
&lt;p&gt;$$m_2 =\frac{11}{1.5 log 2.0 − 1.5 log 1.0}= 24.36$$&lt;/p&gt;
&lt;p&gt;$$m_3 = \frac{m_1*m_2}{m_1 +m_2}= 11.20$$&lt;/p&gt;
&lt;p&gt;根据宽度范围求解第三条线的位置&lt;/p&gt;
&lt;p&gt;$$\frac{a}{b}= \frac{m_1}{m_2}= 0.851$$&lt;/p&gt;
&lt;p&gt;$$a + b = 6$$&lt;/p&gt;
&lt;p&gt;$$b = 3.241 cm, a = 2.759 cm$$&lt;/p&gt;
&lt;p&gt;好了，我们开始画图。&lt;/p&gt;
&lt;p&gt;在D轴上我们的起始点是1.0，然后按照下面的公式在11cm的范围中求坐标&lt;/p&gt;
&lt;p&gt;$$20.73 *(0.68 log(1.2D + 0.47) − 0.68 log(1.2(1.0) + 0.47))$$&lt;/p&gt;
&lt;p&gt;然后画&lt;code&gt;$T$&lt;/code&gt;的范围，起始点依旧是1.0。坐标公式如下:&lt;/p&gt;
&lt;p&gt;$$24.36(1.5 log T − 1.5 log 1.0)$$&lt;/p&gt;
&lt;p&gt;最后我们距离D轴2.759cm处垂直基线画一条N轴，起始点通过下面公式求出:&lt;/p&gt;
&lt;p&gt;$$(1.2(1.0) + 0.47)^{0.68}*(0.91(1.0))^{1.5} = 1.230$$&lt;/p&gt;
&lt;p&gt;坐标通过下面公式求出:&lt;/p&gt;
&lt;p&gt;$$11.20*(logN − 1.5 log(0.91))$$&lt;/p&gt;
&lt;p&gt;OK,现在我们可以得到这张诺谟图了。使用的时候我们只需要知道任意两轴上的数做延长或相交，就可以知道第三轴的数了。这个过程完全依赖直尺比划就可以，类似于查表（不知道现在还教不教）。我们可以看出诺谟图在求解复杂方程上是很方便的，甚至可以求解一些无解析解的方程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/expnomo.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;除了平行线型图，N型图可以用来解决带有除法的公式：&lt;/p&gt;
&lt;p&gt;$$f_3(w) = \frac{f_1(u)}{f_2(v)}$$&lt;/p&gt;
&lt;p&gt;例如这种求圆柱体积的图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/nnomo.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;还有涉及四个变量的比例图，原理也是相似三角形。例如下面这种理想气体公式，你需要画两条线，先连接已知的两个变量，此时与中间对角线有个交点，然后连接交点与另一个已知变量并延长，就知道剩下的那一个变量是什么了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/pvnrt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;类似的还有正弦定理：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/sinx.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;利用相似三角形的性质，如平行、垂直、 还可以得到下面的图形：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/tri.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;还有一种图的原理是依赖等腰三角形的，用来解决并联电阻求值问题。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/parallel.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;当然，你也可以将多种图组合求解一个复杂但可拆分的方程。但更本质的方法则是把所有的比例关系都转成行列式结构，通过网格还可以绘制更为复杂但直观的诺谟图，方便工程应用，也有一定的艺术价值。原理请直接读&lt;a href=&#34;http://myreckonings.com/wordpress/wp-content/uploads/JournalArticle/The_Lost_Art_of_Nomography.pdf&#34;&gt;论文&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/complexnomo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;通过对原始变量的转换，例如平移、旋转、剪切、拉伸、投影， 基本上可以把诺谟图应用到各个数值求解的过程中去。 而实际需求的数值范围则是设计图表中最重要的因素，通过一定的转换会让诺谟图更像是一件艺术品。其实日晷就是一个诺谟图应用的很好实例。这里放个Ron Doerfler自己设计的日晷图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/ShadowLinc.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;历史篇&#34;&gt;历史篇&lt;/h2&gt;
&lt;p&gt;说了这么多你可能会好奇，这个东西是怎么出现的？其实我们可以扯的远一些，聊下图（graph）是怎么出现的。也许很多人没意识到，直到1878年，“graph”这个词才被数学家引入到研究中，用来表示化学-代数关系，而在这之前，图更多使用在天文与地理研究之中，例如天象图与地图。值得注意的是历史中图并不是抽象表达而更多是对真实事物的描述，当然几何求解数值方程的方法也算是历史悠久了。古人用的星盘与四分仪就是基于这种方法设计的，这对宗教的传播与未知世界的探索提供了可靠的保障，但这些更多是测量方法，需要借助实物与观察。&lt;/p&gt;
&lt;p&gt;研究中用的图，特别是带有笛卡尔坐标系来展示真实数据的图其实出现的并不早。根据《Blood, Dirt, and Nomograms: A Particular History of Graphs》的记载，最早的图可以追溯到1770年左右，而且有三个起源。第一个起源是英国人 William Playfire 做统计图集时作的图，用来描述英国国债。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/debt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;第二个起源是 James Watts 搞的，用来绘制引擎的压力体积变化，这个图就不是人画的了，直接就是机器画。在那个年代其实计数对于机械过程是个麻烦事，所以就直接给机器接上个机械臂，其变动用机械臂的震动记录在纸上。这种物理信号直接输出的仿真信号图后来广泛应用在地震监测、心率监测等领域，当然会有坐标系来校准。&lt;/p&gt;
&lt;p&gt;第三个起源是 Johann Heinrich Lambert 的实验记录，此君是科学家出身，坚持自然哲学要用观察数据来量化测量，在这个过程中他制作了大量的图。这里要说明的是自然哲学是相对于博物学来说的，那个年代自然哲学侧重今天的物理学研究关注形而上的东西与分析手段而博物学侧重对自然的宏观观察记录，侧重经验感知与综合。国内经常说的科普其实很多是在博物学这个层面上的，例如逛博物馆，野外考察什么的，侧重知识性。博物学很重要但其实国内缺的是自然哲学这块的思维普及而不仅仅是一个个知识点。&lt;/p&gt;
&lt;p&gt;然而，这个概念搞出来后在四十年里几乎没有人关注，原因在于懂这个技术的人还不知道咋用或者需要用的人不知道有这个技术。1795年法国大革命后引入了计量系统，在对公众推广时发现公众对数字无感，这时有个叫 Pounchet 的棉花制造商发现了 William Playfire 的图，他觉得这个图很有前景。其做的第一个应用就是把乘法表给图表化了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/mt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;这个图咋用呢，打比方我想知道6乘7的结果，我就沿着6的横网格找7的竖网格，里面的等值线就可以告诉我乘积在40到50之间，这对于不懂算术的人应该是非常方便了。但他其实已经将原始但两个变量关系图推广到了三变量关系了。在另一方面，法国的工程师们开始关注投影几何对于堡垒建设的应用，例如堡垒需要修建多么高？怎样最快运送沙土？其后继者接触了 Pounchet 的图后突然发现，投影几何其实可以用在这种存在等值线的图之中。&lt;/p&gt;
&lt;p&gt;所谓投影几何，就是将一组几何关系投影到另一个自定义的坐标系里的几何方法，说白了就是坐标变化。投影几何里的经典作品就是下面这个描述拿破仑大撤退的图，在图形可视化领域里也是经典作品。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/napolun.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;有位叫 Lalanne 的法国工程师将 Pounchet 的这种图示方法广泛推广到工程领域，特别是他把等值线跟地图结合，就有了我们现在看到的等值线地形图，这样三维数据就可以二维展示了。其第二个贡献就在于对数坐标轴的引入，从绘图角度，曲线不如直线好画，他发现把数据对数化后曲线可以变直线，这就是投影几何的一种映射。此时，图已经有了计算属性了。&lt;/p&gt;
&lt;p&gt;到了1884年，Philbert Maurice d&amp;rsquo;Ocagne 进一步将图形计算与投影几何结合，这就有了我们前面看到的诺谟图。而诺谟图事实上通过等值线超越了笛卡尔坐标系，多个参数可以几何投影到多条线但保证其函数关系可以被等值线来描述，使用起来也非常方便，完全不需要学习几何学与函数理论，会划线就可以。在第一次世界大战中，几乎每个防空炮兵都有一个诺谟图，根据当时情况来计算开炮的方位与角度。&lt;/p&gt;
&lt;p&gt;如果你认为诺谟图就是工程上可以用就错了，诺谟图其实在生理学里曾经有统治地位。20世纪初，生理学研究人员意识到人体血液是一个及其复杂的系统，里面有各种各样的化合物，其浓度其实存在一个平衡范围，然而将这个系统展示出来就成了大问题，如果两两关系进行绘图，那么其实超过一百幅图。此时，诺谟图闪亮登场。下面就是一副1928年哈佛生理学家 L.J.Henderson 在耶鲁大学作报告时用的图，他认为这张图浓缩了当时人类对血液的所有知识。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/blood.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;确实，诺谟图在浓缩知识上可能是个顶峰，如果我们打算给后人留一些证明文明存在的证据，也许这样一张图比复杂的公式要更实用，也更直观。哲学家 F.S.C.Northrop 认为一个多组分复杂系统可以也只可以用诺谟图来描述其动力学过程。&lt;/p&gt;
&lt;p&gt;然而，时过境迁，今天连知道诺谟图的人都不多了，实验学科已经意识到单变量控制实验与现实的距离，然而多变量的系统或者时髦一点的组学，其实都已经不知道诺谟图这类描述系统的方法了。同当年的机器类似，或许在数据量充盈的今天，我们可以先画出图，然后反推背后的函数，甚至图本身就是解。有没有很眼熟？&lt;/p&gt;
&lt;h2 id=&#34;回归篇&#34;&gt;回归篇&lt;/h2&gt;
&lt;p&gt;现在还有没有诺谟图，答案是肯定的，且如我前面所说，有些模型确实可以可视化为诺谟图，且所有机器学习里的黑箱模型应该都可以可视化为诺谟图且可以以图片形式保存与传递。在统计学里，有一种数据展示方法叫做回归诺谟图。例如下面这一张：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/rnomo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;这张图的用法跟之前说的有点区别，使用的是垂直等值线，也就是说，图里六个变量你根据实际取值画一条垂直线到最上面那条 Log OR 的轴上，然后你会得到六个值，将这六个值的和计算出来算一个坐标，然后在图上画一条垂直线到下面的风险概率里，然后你就知道总体风险了。这个方法非常直观表示了不同变量取值范围及其对风险值的贡献，比单纯放个系数表上去要直观很多，但我估计在目前大学教育对诺谟图的缺失大背景下，能看懂的人估计不多，这个活原理上非常技术但其实不难。我个人曾经见到审稿人因为看不懂图就说图太复杂不如用表的情况，这种智力倒退在专业细化的今天从来没少过，甚至更多了，有些人自己脑子处理不了信息量大的图就去要信息量少但不准确的图，掩耳盗铃。其实很多复杂些的模型例如支持向量机也可以用诺谟图来展示最终&lt;a href=&#34;http://stat.columbia.edu/~jakulin/Int/f309-jakulin.pdf&#34;&gt;结果&lt;/a&gt;。至于能不能做到数据直接生成，我认为并不困难，甚至比数值求解还要简单。&lt;/p&gt;
&lt;p&gt;当然，这个活不需要你自己做。在 CRAN 上，你可以用 &lt;code&gt;hdnom&lt;/code&gt; 包来对多变量生存分析进行诺谟图绘制。而 &lt;code&gt;nomogramEx&lt;/code&gt; 包可用来从诺谟图图片中反向提取方程。不过我看到诺谟图的使用还是线性模型居多，应用也是更多在生存分析方面，以后应该有更多的应用方向与场景。当然，诺谟图设计本身就是个技术与艺术的结合体，如果你想修身养性平复焦虑又不愿意搞那些练字画画织毛衣的事，或许你该拿起笔来，用现在天天更新的一些数据自己设计一张诺谟图，记录下当前的大流行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;扩展阅读资料&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;制作诺谟图的&lt;a href=&#34;http://pynomo.org/wiki/index.php?title=Main_Page&#34;&gt;软件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.nomographer.com/home/index.html&#34;&gt;爱好者网站&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://myreckonings.com/wordpress/wp-content/uploads/JournalArticle/The_Lost_Art_of_Nomography.pdf&#34;&gt;The Lost Art of Nomography&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstor.org/stable/237474&#34;&gt;Blood, Dirt, and Nomograms: A Particular History of Graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://statmodeling.stat.columbia.edu/2006/05/24/nomograms/&#34;&gt;Nomograms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>网速与树莓派</title>
      <link>https://yufree.cn/cn/2019/12/16/rpi/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2019/12/16/rpi/</guid>
      <description>&lt;p&gt;从大概五年前开始，4G网络开始普及，到今天5G报道满天飞，其实改变了很多人的生活方式。有时我在想，如果我们的网速停在3G时代，智能手机可能不会像今天这样流行。我大概看了下手机里装的应用，很多在网速低了后体验会变化很多，从可有可无变成日常打卡，特别是视频应用与游戏。而互联网开始流行短视频与vlog，很大程度上也是因为从制作到观看中没了网速瓶颈：制作视频不用桌面级软件的后期渲染，很多运动相机或云台相机甚至手机直出就很不错，高清直播配合降低的制作成本让视频内容大爆炸；看视频等信息量大的行为也不用先缓存或降低画质了，4k流媒体其实在4G网速下就可以实现，而4k的信息量跟你到电影院看电影的35mm胶片差不多了。也就是说，如果你同样有10美元，去电影院如果不是图个氛围或者去看IMAX，不如直接订阅4k流媒体服务一个月在家里搞个4k投影仪观看来的值，观看体验是差不多的。同样的问题存在于蓝光影碟，一张4k蓝光影碟几十G的容量只要你有网也属于可有可无了，当然蓝光版跟流媒体版的内容区别可能是其价值所在。现在的父母可以拖出一箱子硬盘告诉孩子这是你成长的点点滴滴，未来的父母可能只需要对孩子的数据读取进行授权就可以，很多数码实体可能要消失。我家里的有我几岁时说话的录音带实体还在，但能出声的录音机确实不好找了，今天的日常操作过二十年看可能就是不可思议了。不过，眼下的问题不在于生产，我看了下自己拍了几万张照片了，但留有印象的寥寥无几，有了直播看回放就几乎绝迹了，反正总有新的东西看，高网速下的持续输入与输出正在塑造新的认知趋势并压缩思考的空间。你如果每时每刻都在记录，那就不会有时间与精力回顾，那么记录本身的意义也就不大了，但也许你就是在享受记录这件事本身，这种事直到上个世纪都不常见：自古人们从来都是选择性高成本记录那些值得回忆的瞬间并反复回顾，没了回顾的低成本记录在信息高速传递的今天其实很是异类。&lt;/p&gt;
&lt;p&gt;伴随网速提高，我们会看到更多东西的消逝或数字化。本地数据存储除了隐私需要外其实必要性在减弱，我们需要的展示结果的终端，至于说数据的处理负担，可以交给云服务器来做。通过生物识别，我们的个人数据可以在网上高速调取，虚拟化会成为生活方式的一部分。很多曾经的习惯会从面向个体信息获取与娱乐变成社交或场合价值优先，例如现在的网吧没有完全在个人电脑普及浪潮后消失，反而成了电子竞技爱好者的自留地或社交场所。同样的，这类生活方式会从服务个体变到服务场合气氛，看重场景下的情感需求对于信息载体的脱实向虚是很重要的趋势。很多人去实体店或买实体看重的其实不是信息本身，而是其附带的情感与价值认同，这也决定了很多事物会衰落，但不会完全消失。等VR与AR技术更进一步，可能人真的会搞不清楚什么是真实存在，什么是虚构，社交可能通过虚拟场景远程实现，而网速或者说信息传递速度是这个趋势的关键，当足够快与平滑时，生理感知无法区分，那么真假在某种意义上就可操纵了。&lt;/p&gt;
&lt;p&gt;网速与本地运算是互相矛盾的，网速足够快，本地运算要求就相应降低，这样本来我们网上向本地传数据然后本机运算，当网速够快我们只需要提需求让服务器或云端进行计算，然后传递结果，本地终端只需要具备提供网速支持与结果展示所需要的计算能力。运算能力也许面对需求永远是不够用的，但在哪里算就是另一个问题了，例如现在云输入法的备选词就是在服务器端计算通过网络返回结果而不是调用本地训练好的模型，这样做有两个优点，一个是更新快，另一个是本地运算要求不高。现在很多电子产品都自带一个电脑或单片机，但在信息传递的支持下可以远程做很多事，美其名曰智能家电。很多在线应用也完全可以替代当年的装机必备软件，办公、图片处理、数据报表等软件纷纷云端化与订阅化，网速足够时一套完整办公环境可以随时随地搭建，效率自然会提高很多。我不必为了加个水印去学ps的图层，只要在要搜的关键词后面加上 online 就可以了，屡试不爽。原来我外出都要带个优盘，现在记住密码或带着有指纹的手或刷脸就可以快速重现工作所需的工具场景，做报告的话我甚至不用带电脑，因为幻灯片在线就可以公开查看，我只需要关键词与能联网的搜索引擎就够了，我需要的是带输入输出的终端而不是处理器，真需要处理笔记本的性能面对台式机或服务器反而很尴尬。&lt;/p&gt;
&lt;p&gt;今年回国我本来想把家里电脑升级下，回去后才知道电脑已经半年多没开机了，爸妈所有的查询现在都是通过智能音箱与手机来做，键盘都不用了。我甚至在想如果下次升级电脑，是否需要去搞个高性能笔记本，还是换成树莓派与几年的云计算服务，价格上区别不大甚至更便宜，我个人娱乐手机配合树莓派就搞定了，而买的正版游戏都从未下载到本地过，办公几乎全部走云端主机，多出来的钱升级下网速换个好点的屏幕就挺好。现在来看有点魔幻，对我而言，学生时代笔记本电脑就是我的全部，娱乐、学习、工作、社交全都通过笔记本电脑来做，现在笔记本电脑更多是工作工具了，或者说能联网的终端都可以是工作工具。本机处理数据一般不大，大的数据我都远程登录办公室一台服务器或干脆扔学校集群上算了，以后可能扔云端上，我也没啥有价值的隐私数据，有需要搞个个人NAS也够了。我猜想很多人觉得还是手边有高性能设备好，但希望他们去尝试下树莓派之类的产品，计算能力日常使用生理上感觉不出差异多大了。真有那计算量大的活，早点拥抱集群计算或超算才是正解，按时付费可以瞬间有一台高性能电脑，处理完了下线也不耽误。&lt;/p&gt;
&lt;p&gt;说到树莓派，我最近尝试了一下，做大型数据处理比较吃力，但锅不在树莓派而在写某些算法的人设计理念有问题。日常办公是完全没压力的，100刀基于树莓派的商品化小型终端计算机可以应对80%的生活自用场景。但用了几天，特别是买了一大堆传感器尝试了GPIO后，我发现甚至树莓派的计算能力在很多场景下都是多余的，很多需求一块开发版就能完成，也并不是要去学汇编与C，我这种半吊子python使用者看看文档然后知道面包板跟子母线咋用就可以排列组合做很多有意思的应用。R里面目前有个 RpiR 包对基于 C 的 WiringPi 进行了基础功能的封装，可以在树莓派里用，但稍微复杂点的功能就得自己造轮子了，以我之懒直接换 python 了，高级语言还是用户友好的，反正都是调包。不过如果自己打算写，那就是另一个概念了，发光二极管只需要高低电压就行，但随便一个复杂点的传感器或元器件都需要自己写驱动，把数字甚至模拟信号处理成你需要的格式。这种从电路到编程的体验还是很有意思的，不过这可是物联网的基本功。&lt;/p&gt;
&lt;p&gt;通用计算机有可能不如专用芯片在未来有前景，特别是网速足够时，专用芯片的在线集群要比个人本地终端更高效地处理特定类型的数据与任务，非开发目的而仅仅是应用的话这种针对特定任务的数据处理芯片会有前景，当然，如果任务本身的计算量终端完全可以负担，也没必要去在线处理。应该对计算任务评估一个计算当量，如果计算当量超过了日常办公一个数量级，那么就应该在线做或购买特异性的服务器而不要对不需要的功能付费。反之，如果本地可承担，那就使用本地资源，本地计算资源总是稀缺的，网速足够的话计算任务尽量放到云端来提高效率，只是这个事牵扯数据价值的归属问题，个人应可以向使用自己数据的实体发放付费或免费的授权，对应换取更多服务或折扣。好比集中供暖，家家户户烧炉子的社会效益低于集中供暖，开支相应更高，但要做好用户信息加密与反追踪。打个比方，行为数据模式的统计量对与公司是有价值的但不能追踪到个体，那么在数据收集后我们应该引入第三方或者一种算法将用户个体数据在信息不丢失的情况下做类似基因交叉互换的操作，这样公司或政府拿到的数据不会指向某一个真实的人，但其中的信息却可以提供价值与决策参考。&lt;/p&gt;
&lt;p&gt;经济上脱实向虚目前来看是弊大于利的，造成了财富分配不均现象的恶化，但网速提高造成的信息脱实向虚却很难给个简单的好坏判断，但无论如何都会是一种趋势。我们是否还需要一台全能电脑还是符合实际需求的电脑，抑或是特异性的芯片？这也是所有人会用脚投票的趋势。眼下有些技术还缺失，有些比较成熟，有些则属于过剩的泡沫，当然人的需求从来不是一成不变的，但面对改变重新认识自己应该是必要的，否则随波逐流实属对个人选择权的放弃。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>编译器漫谈</title>
      <link>https://yufree.cn/cn/2019/02/14/compiler/</link>
      <pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2019/02/14/compiler/</guid>
      <description>&lt;p&gt;最近借由一次R包安装失败，我又重新收集了下关于编译的概念与知识，为了让下一次失忆的我不至于再折腾半天，就归纳到这里。&lt;/p&gt;
&lt;p&gt;C语言源码编译运行过程是这样的：先预处理源码，调入模块，然后转换成汇编语言文件，汇编语言文件可以被汇编器转为机器码，然后通过连接器合并为可执行文件，最后加载到内存里运行。因为C语言是多数操作系统的基础，所以多数操作系统也自带对应的C编译器。更重要的是，C语言的库很丰富，也就是工具函数比较多，换别的就得自己写。这是很有意思的路径依赖案例，事实上任何语言都应该可以拿来写操作系统，不过C是在开发Unix时候设计出来的，现在流行的开发级操作系统都是unix/类unix操作系统，加上C在内存管理与CPU交互上的先天优势，历史机遇下成为了主流。&lt;/p&gt;
&lt;p&gt;C的核心地位还体现在很多高级语言的编译器是构建在C之上的，或者是C++之上，多数高级语言都通过限制自由度（例如不让操作内存、功能模块化等）来实现上手容易与较高的开发效率，但只要关注程序性能，肯定回去学C或C++的。GNU的GCC编译器是一个相对通用的编译器合集，可以用来编译包括C在内的多种语言。然而，GCC也是有历史包袱的，所有有人就另起炉灶单独针对C或C++重写了效率更高的编译器及其后台，这就是苹果的LLVM项目与clang编译器。但要注意的是LLVM支持的语言不如GCC多，所以如果你还要用到fortain或java编译器，那就还是老老实实用GCC吧，或者cmake的时候分别指定编译器，只要你不嫌麻烦。一般而言，效率与性能往往不能兼得。&lt;/p&gt;
&lt;p&gt;高级语言的编译过程跟相对底层的C或C++是不一样的，Java就是自己定义了一套运行环境JVM，编译出的文件也是JVM可读的，这就提高了Java的可移植性，降低了跨平台开发的难度，当然你得保证这些平台上可以运行JVM。其实很多高级语言是解释型的，REPL里可直接运行代码，但同样会有人为高级语言写编译器来提高运行性能，这个是按需求来。我个人感觉是用REPL的人一般是应用层的，关心有没有满足自己需求的函数；用编译语言的人一般是开发层的，关心软件工程及性能。然而，高级语言里如果打算提高运行效率，也会提供C或C++的接口让程序员可以通过外力来提高自由度。过度的功能封装实际也限制了高级语言的应用场景。&lt;/p&gt;
&lt;p&gt;说到效率，自然少不了并行计算，openmp就是一种并行化方案，可以支撑C与C++。很多R包会通过使用 openmp 来底层加速算法，但这样的包一般都需要单独编译。目前GCC与Clang在编译器层其实都实现了对 openmp 的支持，编译时加上 &lt;code&gt;-fopenmp&lt;/code&gt; 就可以。不过 mac os 自带的编译器是没有这个功能的，所以你需要 homebrew 来自己安装这些支持 openmp 的编译器然后在 &lt;code&gt;.R/Makevars&lt;/code&gt; 里把默认编译器换成新的就可以了。&lt;/p&gt;
&lt;p&gt;例如你装了llvm/Clang，可以写上：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CC=/usr/local/opt/llvm/bin/clang
CXX=/usr/local/opt/llvm/bin/clang++
CXX11=/usr/local/opt/llvm/bin/clang++
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或者GCC版（注意版本要对应）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CC=/usr/local/bin/gcc-8
CXX=/usr/local/bin/gcc-8
CXX11=/usr/local/bin/gcc-8
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或者更简单的方法就是不用并行计算，直接在&lt;code&gt;.R/Makevars&lt;/code&gt; 里参数留空强制跳过对openmp的编译要求：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SHLIB_OPENMP_CFLAGS=
SHLIB_OPENMP_CXXFLAGS=
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;同样的道理可以用在开发上，如果你编写R包涉及了相关并行计算功能，需要在&lt;code&gt;src&lt;/code&gt;目录下创建Makevars文件来帮助用户提前配置编译参数，不过这方面我就没经验了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>短链氯化石蜡的定量方法</title>
      <link>https://yufree.cn/cn/2018/08/06/sccp/</link>
      <pubDate>Mon, 06 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2018/08/06/sccp/</guid>
      <description>


&lt;p&gt;短链氯化石蜡的定量在环境分析化学中属于难度比较高的，在目的性分析里也算是最非传统的了，不过理解了这类物质的定量对于理解环境分析的复杂性很有帮助，鉴于目前教科书里肯定没有，我就总结在这里。&lt;/p&gt;
&lt;div id=&#34;定量分析&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;定量分析&lt;/h2&gt;
&lt;p&gt;分析化学里的定量模型比较简单，你知道待测物的某个指标，在不同含量下测定标准品中这个指标，找出含量与指标的关系模型。然后当测定样品时，将测得的指标带入关系模型就可以得到含量了。用色谱质谱联用来说明就是首先购买待测物的纯品，然后优化色谱质谱条件来获得待测物特定离子响应色谱峰的最佳灵敏度与分离效果，然后配置不同浓度标准溶液进样，根据浓度与峰面积绘制标准曲线。当然说是标准曲线，多数是用线性关系，有了标线就可以测定位置样品中浓度了。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;短链氯化石蜡&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;短链氯化石蜡&lt;/h2&gt;
&lt;p&gt;针对短链氯化石蜡，上面那个传统套路是走不通的，原因有三个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;短链氯化石蜡并不是一种化合物，而是碳原子数10-13个，氯原子数5-10个的短链烷烃。这是一类化合物，分子式就有24种，而结构式就要上万种了&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;标准品不是纯品，化学定量分析要求标准物纯度要非常高，但市售短链氯化石蜡的标准品大都不是单体，甚至同一种分子式都做不到，能买到的都是用氯含量来标定，也就是定量要通过氯含量来换算&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;分离困难，因为标准不纯，定量要同时测多种短链氯化石蜡，但不同短链氯化石蜡特征离子几乎一样，且中链氯化石蜡会形成干扰，后果就是质谱提取特征离子时有时包含了两种化合物，这样就存在系统性定量偏高，同样的，色谱分离也做不到基线分离，甚至看不到峰，标准品看到的也只是五指峰&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于上述原因，那些基于特征离子定量且又是低分辨质谱定量的方法几乎可以肯定是不靠谱的，线性关系一塌糊涂不说误差不是一般的高。有意思的是，网上可以搜到很多短链氯化石蜡定量方法专利，基本都是胡说八道，根本就没搞清楚问题的复杂性，要么就是隐藏技术细节，但其实很多文献里都有也不用隐藏，且根本就不该出现这类专利，因为学术期刊早就发表过了。我自己申请过也获得过专利，那帮审专利的人专业水平不是一般的低，很大程度是因为外行评价内行，不过这东西挺唬人的，但其实科技领域越前沿胡说八道就越多，这样才有讨论空间，不必看得太重。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;高分辨质谱定量方法&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;高分辨质谱定量方法&lt;/h2&gt;
&lt;p&gt;离子源肯定要是负化学源，EI源会把短链烷烃打碎，后果就是根本找不到特征离子，负化学源能保证你起码找得到分子离子峰。不过这么说也不对，因为即使在负化学源下，短链氯化石蜡的基峰也是掉一个氯原子的。而且只要有氯原子就会存在同位素峰，同位素峰之间在低分辨质谱上会互相覆盖，这里我们先不讨论低分辨定量，高分辨质谱是可以在质谱上实现每种分子式的分离的。&lt;/p&gt;
&lt;p&gt;那么色谱用什么呢？首先得是能接质谱的柱子，其次因为短链氯化石蜡沸点不高，我们可以用气相色谱柱来进行分离。当然也有用液相分离的，那个比较复杂就不提了。&lt;/p&gt;
&lt;p&gt;好了，我们来看技术难点，我们手上有的标准只标注了氯含量而不是单体含量，所以首先我们要构建氯含量与单体含量的关系。不同氯含量的标准会有不同质谱响应因子，也就是你进样量相同时，峰面积是不同的。这里文献中一般采用的策略就是假设所有短链氯化石蜡单体响应的不同主要来自于氯含量不同，而氯含量不同是因为组成不同。计算出氯均一化后总体氯化度对应的响应因子与样品峰面积就可以知道含量。&lt;/p&gt;
&lt;p&gt;首先，我们需要固定浓度不同氯化度的sccp标样，然后提取离子计算面积积分，各离子面积要先除以离子同位素丰度得到化合物面积，然后除氯原子个数得到单位氯原子化合物的响应，所有离子响应之和为单位氯原子的总面积，每个离子的氯化度是已知的，用其单位离子响应占总响应的比例乘以氯化度求和可得到样品中的总氯化度，单位氯原子的总面积除以质量得到标准响应因子，这个响应因子在不同氯化度下不一样，用不同氯化度标样分别测定，得到氯化度与响应因子的回归曲线。&lt;/p&gt;
&lt;p&gt;然后，测定样品时只要计算出样品中的总氯化度就可以知道样品响应因子，根据样品中计算出的单位氯原子的响应面积就可以知道其中sccp的总含量。同时也可以反推得到样品中不同单体的浓度组成。不过最好先用标准品作为样品检验下方法是否有系统偏差。&lt;/p&gt;
&lt;p&gt;我在&lt;code&gt;enviGCMS&lt;/code&gt;包里已经做了一个应用，你按照里面说的一步步来是可以实现定量的，不过请一定理解背后的假设，启动应用的代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;#39;enviGCMS&amp;#39;)
enviGCMS::runsccp()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时你的浏览器会弹出图形界面，按照指示做就可以了，记得去引 references 选项卡里的文献与这个软件包就可以了，我不是方法的发明人，但软件却是我写的。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;低分辨质谱定量方法&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;低分辨质谱定量方法&lt;/h2&gt;
&lt;p&gt;前面提到，在低分辨质谱上存在同一个离子实际是两种物质的情况，如果两组物质质量数干扰，就选两个共有离子，每个离子响应是来自两个物质响应的线形加和，由于两个离子的丰度比可以事先计算出来，所以可以构建下面的方程：&lt;/p&gt;
&lt;p&gt;物质A、B的两个共有离子峰面积X、Y，两者丰度比例 m、n：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[A + B = X\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[A\frac{n}{m}+B\frac{n}{m} = Y\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;联立求解可以得到A与B的真实峰面积。&lt;/p&gt;
&lt;p&gt;如此可以把相互干扰的离子对放到一起去测，然后求解各离子的实际比例。&lt;/p&gt;
&lt;p&gt;这是个穷人的方法，买不起高分辨就要用数据处理来矫正误差，这里我就不写图形界面了，穷要有穷的志气，穷还不愿思考动手，那我也不想帮忙了，线索已经给的很明确了。至于说高分辨为什么写原因也很简单，目前有人要用，而且我知道对方是理解背后原理的，我不希望不理解原理就套用模版套路来进行科研的人（但写作可以用模版方便交流），这样做出的东西意义不大，没有思考的科研也许有用，但丧失了乐趣。&lt;/p&gt;
&lt;p&gt;祝大家短链氯化石蜡定量快乐！&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>现代科研兵刃谱</title>
      <link>https://yufree.cn/cn/2018/07/14/sci-tools/</link>
      <pubDate>Sat, 14 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2018/07/14/sci-tools/</guid>
      <description>&lt;p&gt;工欲善其事，必先利其器。今天绝大多数知识都是工具生产出来的，也就是想使用知识，肯定要先学工具，而工具又需要知识铺垫，这就成了一个鸡生蛋蛋生鸡的问题。虽然事后总结都有千般道理，但就我人经验而言，工具与知识是相辅相成缺一不可的，过于关注知识会导致脱离实际而沉迷于工具选择则有很高的迁移成本。这里的忠告就是不要想太多，先迈开步子，随便找个工具用起来，用实战来丰富需求，根据需求定向选择最适合自己的工具而不做工具的奴隶，如有必要，自己创造工具。另外，尽量选择那些花费百分之二十的精力可以掌握百分之八十的内容或应用场景的工具。同时系统学习那些使用频率高的工具，其余的只要知道其存在即可，不要捡芝麻丢西瓜。&lt;/p&gt;
&lt;h2 id=&#34;文本编辑&#34;&gt;文本编辑&lt;/h2&gt;
&lt;p&gt;科研用文本编辑工具主要应对排版要求，早期排版系统基本都是通过 TeX 语言来实现的，后来由于个人电脑普及及新兴学科的出现，很多科研人员上手会用的都是可见即可得的文本编辑器。现在期刊投稿一般会支持基于 TeX 的投稿及常见可见即可得文档，这些都是本地编辑。另一个当前流行的可见即可得文本编辑方式是在线协作，例如&lt;a href=&#34;https://docs.google.com/&#34;&gt;谷歌文档&lt;/a&gt;、&lt;a href=&#34;https://shimo.im/&#34;&gt;石墨文档&lt;/a&gt;、&lt;a href=&#34;https://docs.qq.com/&#34;&gt;腾讯文档&lt;/a&gt;等。对于需要协作完成的论文，在线协作文档极大方便了实时交互与版本控制。其实利用基于Git的&lt;a href=&#34;https://github.com/&#34;&gt;GitHub&lt;/a&gt;也可以实现在线协作与修订，不过门槛比较高，但有希望成为一些期刊今后的投稿系统原型。&lt;/p&gt;
&lt;p&gt;还有些文本编辑器是基于纯文本的，通过文本中的控制语句来实现排版，TeX就是其中最流行的。&lt;a href=&#34;https://www.overleaf.com/&#34;&gt;Overleaf&lt;/a&gt;支持基于 TeX 的在线文档协作，甚至你可以直接用其向特定期刊投稿，同样的工具还有&lt;a href=&#34;https://www.sharelatex.com/&#34;&gt;sharelatex&lt;/a&gt;。不过，TeX的控制语句实在太丰富，学习起来比较困难。&lt;a href=&#34;https://pandoc.org/&#34;&gt;Pandoc&lt;/a&gt; 的出现方便了其他更简单的标记语言对 Tex 的转换，其中最容易上手的是&lt;a href=&#34;https://daringfireball.net/projects/markdown/&#34;&gt;Markdown&lt;/a&gt;。不过 Markdown 存在很多版本，其中基础版支持的排版功能非常有限，Pandoc 对其进行的&lt;a href=&#34;https://pandoc.org/MANUAL.html#pandocs-markdown&#34;&gt;扩展&lt;/a&gt;则支持了更丰富的功能方便排版。所以理论上你可以使用 Markdown 来写论文，不过这需要你的编辑器支持一些额外的功能。&lt;/p&gt;
&lt;p&gt;总结一下，作为现代科研工具，理想文本编辑器需要至少有以下功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持在线协作、评论与修订&lt;/li&gt;
&lt;li&gt;支持版本控制&lt;/li&gt;
&lt;li&gt;支持常见文献管理工具&lt;/li&gt;
&lt;li&gt;支持期刊样式排版&lt;/li&gt;
&lt;li&gt;容易上手&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;文献管理&#34;&gt;文献管理&lt;/h2&gt;
&lt;p&gt;现在的文献管理工具一般都支持常见文本编辑工具，也就是可以很方便的插入参考文献。然而，文献管理工具要同时具有收集、整理与分析的功能为佳。当前主流文献管理工具都已经支持浏览器层次的文献收集，也就是直接通过快捷键、脚本或浏览器扩展一键自动提取文章页面中参考文献信息并存入用户指定的文献库。要实现这个功能，多数需要知道文献数据库网页结构，当前很多文献数据库都推出了自己的文献收集应用，有的直接收购了文献管理软件。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://endnote.com/&#34;&gt;Endnote&lt;/a&gt;是比较老牌的文献管理工具，不同于前面所说的网页采集，其自身就有与常见数据库的搜索接口，国内科研机构图书馆大都提供培训。与之类似的&lt;a href=&#34;http://www.inoteexpress.com/aegean/&#34;&gt;NoteExpress&lt;/a&gt;则属于国产软件，据说对中文期刊格式支持更好，类似的还有&lt;a href=&#34;https://www.mendeley.com/&#34;&gt;Mendeley&lt;/a&gt;、&lt;a href=&#34;http://refer.medlive.cn/&#34;&gt;医学文献王&lt;/a&gt;、服务 TeX 里 BibTex 的 &lt;a href=&#34;http://www.jabref.org/&#34;&gt;JabRef&lt;/a&gt; 与Mac OS 下的&lt;a href=&#34;https://www.readcube.com/papers/mac&#34;&gt;Papers&lt;/a&gt;。这些工具起步较早，从单机时代就有用户，还有些工具诞生于互联网时代，有着更丰富的功能。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zotero.org/&#34;&gt;Zotero&lt;/a&gt; 属于互联网精神的产物，特别是前者本身就是基于火狐浏览器，其支持的文献格式样式都非常多，而且也有着丰富的文本分析扩展应用。&lt;a href=&#34;https://paperpile.com/app&#34;&gt;Paperpile&lt;/a&gt;则属于基于谷歌文档的应用，可以很方便地管理在谷歌文档中使用到的文献。&lt;a href=&#34;https://www.doi.org/&#34;&gt;DOI&lt;/a&gt;与&lt;a href=&#34;https://www.crossref.org/&#34;&gt;crossref&lt;/a&gt;的出现则更方便了文献的搜索定位。可以说基于互联网的团队化文献管理正在成为趋势。&lt;/p&gt;
&lt;p&gt;总结一下，作为现代科研工具，理想文献管理软件需要至少有以下功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持常见文本编辑器&lt;/li&gt;
&lt;li&gt;支持在线文献采集&lt;/li&gt;
&lt;li&gt;支持文献库协作与共享&lt;/li&gt;
&lt;li&gt;支持文献信息学探索&lt;/li&gt;
&lt;li&gt;容易上手&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;数据处理与绘图&#34;&gt;数据处理与绘图&lt;/h2&gt;
&lt;p&gt;数据处理方面很多学科只需要电子表格与基本的统计分析就可以了，很多在线服务就可以完成。然而，有些学科需要更丰富的功能例如多元统计分析与假设检验时，电子表格提供的功能可能就不那么明显了，有时需要学习使用电子表格的宏扩展来实现。此时，很多人容易陷入哪个分析一定要用哪个软件做的误区，其实多数数据分析软件的算法都差不多，只不过默认值可能不同，有些功能则藏的比较深，此时请善用搜索引擎。&lt;/p&gt;
&lt;p&gt;所见即所得的数据处理与绘图软件有很多，Excel、&lt;a href=&#34;https://www.originlab.com/&#34;&gt;Origin&lt;/a&gt;、&lt;a href=&#34;https://systatsoftware.com/&#34;&gt;SigmaPlot&lt;/a&gt; 与&lt;a href=&#34;https://www.ibm.com/analytics/spss-statistics-software&#34;&gt;SPSS&lt;/a&gt; 是科研中用的比较多的。这些软件都是图形界面操作且都收费，其内置很多现成的分析模块应对实际科研问题，但这些简化会导致使用者知其然不知其所以然，在分析方法使用上陷入误区。&lt;/p&gt;
&lt;p&gt;编程分析与绘图则属于基础的工具，&lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt; 、&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt;、&lt;a href=&#34;https://www.mathworks.com/products/matlab.html&#34;&gt;Matlab&lt;/a&gt; 与&lt;a href=&#34;https://www.sas.com/en_us/home.html&#34;&gt;SAS&lt;/a&gt; 都是这类工具的代表，应该说掌握其中任意一个就足够应对科研中需要的数据分析了。不过通常这类工具比较难学，最好是配合数据分析方法的学习同步掌握，而且要通过案例来理解方法，累积经验。如果推荐一个，那么基于 R 的 &lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;ggplot2&lt;/a&gt; 作图与其背后的 &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; 数据分析套装则是很好的起点。如果更进一步，可以用&lt;a href=&#34;https://www.rstudio.com/products/shiny/&#34;&gt;shiny&lt;/a&gt; 来制作交互式数据展示界面。&lt;/p&gt;
&lt;p&gt;此外，互联网上也有一些在线应用可以很方便地生成特殊图形例如&lt;a href=&#34;http://naotu.baidu.com/&#34;&gt;百度脑图&lt;/a&gt;可以用来生成流程图或思维导图、&lt;a href=&#34;https://www.autodraw.com/&#34;&gt;Autodraw&lt;/a&gt;可以用来画简笔画、&lt;a href=&#34;https://plot.ly/&#34;&gt;plotly&lt;/a&gt;可以在线完成绘图等。甚至网上还有直接上传数据后自动猜测你需要进行分析与制图的&lt;a href=&#34;https://www.charted.co/&#34;&gt;Charted&lt;/a&gt;。这样的工具只要搜索你所需要的分析然后加上“online”作为关键词就可以找到。&lt;/p&gt;
&lt;p&gt;总结一下，作为现代科研工具，理想数据分析与绘图软件至少有以下功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持科研用统计分析&lt;/li&gt;
&lt;li&gt;图片默认输出美观大方支持绘图自定义&lt;/li&gt;
&lt;li&gt;具备可重复性的宏功能或数据处理脚本&lt;/li&gt;
&lt;li&gt;容易上手&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;学术交流&#34;&gt;学术交流&lt;/h2&gt;
&lt;p&gt;学术交流是科研生活中可以说最重要的一环，现代科研体系的分工合作都要通过学术交流来实现。主流趋势包括论文预印本服务器、开放获取与线上学术交流。&lt;/p&gt;
&lt;p&gt;预印本指在通过同行评议发表之前事先将论文手稿托管在公开服务器的研究工作。预印本服务器可以加速新思想的交流，接受预印本发表的期刊可以从维基百科上&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_academic_journals_by_preprint_policy&#34;&gt;查到&lt;/a&gt;。比较知名的预印本服务器包括偏数学物理计算机科学的&lt;a href=&#34;https://arxiv.org/&#34;&gt;arxiv&lt;/a&gt;、偏生命科学的&lt;a href=&#34;https://www.biorxiv.org/&#34;&gt;biorxiv&lt;/a&gt; 与偏化学的&lt;a href=&#34;https://chemrxiv.org/&#34;&gt;chemrxiv&lt;/a&gt;。国内也有中科院的科技论文预发布[&lt;a href=&#34;http://chinaxiv.org/home.htm&#34;&gt;平台&lt;/a&gt;来服务国内科研人员。很多期刊出版方也在推广自己的预印本服务器来吸引高水平研究，所以可酌情选择。&lt;/p&gt;
&lt;p&gt;开放获取是另一个趋势，要求研究工作可以公开让大众阅读。目前很多科研基金都开始有了这方面的要求及预算。但值得注意的是虽然开放获取期刊可能有更好的阅读数与引用表现，但有很多机构的开放获取期刊属于掠夺性期刊，给钱就发表，对学术评价与学科发展非常不利，可以通过一些网络上的&lt;a href=&#34;https://beallslist.weebly.com/&#34;&gt;列表&lt;/a&gt;来鉴别。要实现开放获取或者说透明科研，&lt;a href=&#34;https://f1000research.com/&#34;&gt;f1000research&lt;/a&gt;、&lt;a href=&#34;https://peerj.org/&#34;&gt;PeerJ&lt;/a&gt;还有&lt;a href=&#34;https://www.plos.org/&#34;&gt;Plos&lt;/a&gt;都是还不错的先行者，它们在实践一些新理念，不过显然并不便宜。&lt;/p&gt;
&lt;p&gt;线上学术交流除了期刊外，实际还要包括学术博客、多媒体展示、学术出版与网络身份。制作学术博客的工具可以直接借助平台例如&lt;a href=&#34;http://blog.sciencenet.cn/&#34;&gt;科学网博客&lt;/a&gt;，也可以自己搭建例如使用&lt;a href=&#34;https://zh-cn.wordpress.com/&#34;&gt;Wordpress&lt;/a&gt;、&lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;Blogdown&lt;/a&gt;或者&lt;a href=&#34;https://www.netlify.com/&#34;&gt;Netlify&lt;/a&gt;等工具。幻灯片制作也最好使用网页模式方便交流，&lt;a href=&#34;https://github.com/yihui/xaringan&#34;&gt;xaringan&lt;/a&gt;、&lt;a href=&#34;https://rstudio.github.io/learnr/&#34;&gt;learnr&lt;/a&gt;等其他基于Markdown语言的幻灯片制作工具可以满足要求。学术出版物则可以通过&lt;a href=&#34;https://bookdown.org/&#34;&gt;bookdown&lt;/a&gt;或&lt;a href=&#34;https://github.com/rstudio/rticles&#34;&gt;rticles&lt;/a&gt;等工具来完成。线上的学术身份识别对于存在大量重名现象的中国科研人员也是很有必要的，&lt;a href=&#34;https://orcid.org/&#34;&gt;ORCID&lt;/a&gt;、&lt;a href=&#34;http://www.researcherid.com/&#34;&gt;Researcher ID&lt;/a&gt;、&lt;a href=&#34;https://www.scopus.com/&#34;&gt;Scopus Auther ID&lt;/a&gt;、&lt;a href=&#34;https://scholar.google.com&#34;&gt;谷歌学术个人主页&lt;/a&gt;及国内的&lt;a href=&#34;https://xueshu.baidu.com/&#34;&gt;百度学术个人主页&lt;/a&gt;都是不错的网上学术名片。而在线交流的手段则可通过&lt;a href=&#34;https://www.researchgate.net/&#34;&gt;ResearchGate&lt;/a&gt;、&lt;a href=&#34;https://www.academia.edu/&#34;&gt;Academia&lt;/a&gt;、&lt;a href=&#34;https://www.linkedin.com/&#34;&gt;Linkedin&lt;/a&gt;及&lt;a href=&#34;https://twitter.com/&#34;&gt;twitter&lt;/a&gt;来完成。&lt;/p&gt;
&lt;p&gt;审稿也是很重要的学术交流方式，建议使用 &lt;a href=&#34;https://publons.com/home/&#34;&gt;Publons&lt;/a&gt; 来构建自己的学术审稿记录。当然你可以在博客或微博上评论最新研究，甚至很多网络期刊网站的评论也有很好的思想碰撞，这里最关键的是要搞清楚你所在学科最活跃的网络交流平台，如果没有，自己搭建一个也无妨。&lt;/p&gt;
&lt;h2 id=&#34;数据分享&#34;&gt;数据分享&lt;/h2&gt;
&lt;p&gt;数据分享是一个很重要现代科研特征，越来越多的科研成果正在开放自己的原始数据供社区推动学科进步。其中，&lt;a href=&#34;https://figshare.com/&#34;&gt;figshare&lt;/a&gt;、&lt;a href=&#34;https://osf.io/&#34;&gt;Open Science Framework&lt;/a&gt;、&lt;a href=&#34;https://dataverse.org/&#34;&gt;Dataverse&lt;/a&gt;与&lt;a href=&#34;https://zenodo.org/&#34;&gt;Zenodo&lt;/a&gt;都是这一潮流的引领者。良好的数据分享不仅包含原始数据，还要包括处理后数据、数据收集相关信息与处理代码，另外对于共享数据的使用也要尊重数据生产者。&lt;/p&gt;
&lt;h2 id=&#34;代码管理&#34;&gt;代码管理&lt;/h2&gt;
&lt;p&gt;后续我们会看到所有学科都会不可逆引入编程，所以代码管理工具也非常重要。&lt;a href=&#34;https://github.com/&#34;&gt;Github&lt;/a&gt;与&lt;a href=&#34;https://bitbucket.org/&#34;&gt;Bitbucket&lt;/a&gt;都是非常实用的在线代码管理与版本控制平台。而&lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;Rmarkdown&lt;/a&gt;与&lt;a href=&#34;https://ipython.org/notebook.html&#34;&gt;Jupyter Notebook&lt;/a&gt;等工具背后提倡的文学化编程也是很重要的代码开发工具。此外应考虑为未来自己做好注释并记录运行环境保证重复性。&lt;a href=&#34;https://docs.docker.com/get-started/&#34;&gt;Docker image&lt;/a&gt;等完整的数据分析环境也可能成为现代科研的主流。代码的编写要能站到巨人肩上：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Good writers borrow from other authors, great authors steal outright&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;r包管理&#34;&gt;R包管理&lt;/h3&gt;
&lt;p&gt;对于R包的管理，建议打印相关Rstudio出品的&lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34;&gt;小抄&lt;/a&gt;作为参考。同时作为IDE，Rstiduo提供了包开发的模版，可以使用&lt;a href=&#34;https://yihui.name/formatr/&#34;&gt;formatR&lt;/a&gt; 与 &lt;a href=&#34;https://cran.r-project.org/web/packages/Rd2roxygen/index.html&#34;&gt;Rd2roxgen&lt;/a&gt;来重新格式化旧代码。同时使用&lt;a href=&#34;https://cran.r-project.org/web/packages/roxygen2/index.html&#34;&gt;roxygen2&lt;/a&gt;来编写开发文档。为了让包更容易使用，可以用Rmarkdown来写&lt;a href=&#34;http://r-pkgs.had.co.nz/vignettes.html&#34;&gt;小品文&lt;/a&gt;方便读者上手，另外就是使用&lt;a href=&#34;https://github.com/r-lib/testthat&#34;&gt;testthat&lt;/a&gt;来进行代码的单元测试。对于代码的执行效率，可以用&lt;a href=&#34;https://rstudio.github.io/profvis/&#34;&gt;Profvis&lt;/a&gt;进行可视化而集成在线测试则可以通过&lt;a href=&#34;https://travis-ci.org/&#34;&gt;travis-ci&lt;/a&gt;或&lt;a href=&#34;https://www.appveyor.com/&#34;&gt;appveyor&lt;/a&gt;来分别对R包进行Linux与Windows系统下的测试。当然，包完成后可通过 &lt;a href=&#34;https://github.com/r-lib/pkgdown&#34;&gt;pkgdown&lt;/a&gt;来制作网站并通过&lt;a href=&#34;https://rstudio.github.io/learnr/&#34;&gt;learnr&lt;/a&gt; 来制作交互式教程。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>基于blogdown的在线rss阅读器</title>
      <link>https://yufree.cn/cn/2018/03/24/blogdown-rss/</link>
      <pubDate>Sat, 24 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2018/03/24/blogdown-rss/</guid>
      <description>


&lt;p&gt;我们这批活着的地球人在有一点上是相对平等的，那就是网龄。2007年大二时我在网吧启用了 google reader，但相当长的一段时间里我都没有看。直到2010年初，去向定下来后我陷入一种无聊状态，此时开始使用 rss 阅读器，等刚搞明白怎么回事并感觉打开新世界后，谷歌就退出中国了。假如我开窍晚一个周，可能后面很多事就不可能发生了，有时想来真的是无因无果的随机扰动改变了一切。&lt;/p&gt;
&lt;p&gt;为了能正常使用GR，我很快开启了个人翻篱笆工程，而在GR上也逐渐积累了很多优质资源，顺藤摸瓜关注了很多能提供优质资源的用户，因为我兴趣比较分散，很快就发现了不同领域里对同一问题的对立观点并在评论区见识到了更多不同意见。应该说那时我才搞明白“和而不同”是啥意思，而篱笆的存在事实上分割了两种认知水平。&lt;/p&gt;
&lt;p&gt;我一度沉迷 GR 到每天有四五个小时都在上面吸收新知识，而过度的使用也让我意识到知识管理的重要性，不到半年时间，我就把 GR 每日使用时间锁定到了1个小时并严格执行，这样其实给了更多大脑自由整合知识的时间。同时，伴随 RSS 源的不断积累，我也逐渐克服阅读英文文章的障碍（其实大学英语水平完全够用，就看你想不想用），并纳入了科技论文查新体系。&lt;/p&gt;
&lt;p&gt;跟很多这个年龄段的毛头小子一样，毫无来由的见识优越感让我重启了另一个2006年起就不断重复的“从入门到放弃”工程：写作，或者更具体些，博客。我06年才把上网列入刚需，当时就开了百度空间，而qq空间印象中是需要邀请的，我懒得求人也对其较慢的加载速度不满，但大学学业还是比较重的，所以空间到最后关闭都没写几篇。08年买了笔记本后我很虚伪地限定自己只做跟学习有关的事，但其实当时还是沉迷技术论坛，那时看到自己的帖子被放在首页推荐我是肯定会跑食堂吃点好的的，那时没有现在对知识产权搞得这么功利化，就觉得分享是美德。但论坛归属感很快就因为社区里出现的一些事情而烟消云散，写的东西也不愿再看。但在接触 GR 后，我重新觉得可以写点东西了，然后就有了科学网博客，当时我可是等了好几天才实名通过认证的。而10年8月份我就写了&lt;a href=&#34;http://blog.sciencenet.cn/blog-430956-355369.html&#34;&gt;一篇&lt;/a&gt;专门介绍 GR 的，现在看里面很多不懂装懂的成分，不过有了输出后整合型思考开始出现，我更多从学他人变成了整合入自己的知识体系，这点其实很重要。&lt;/p&gt;
&lt;p&gt;好景不长，GR 退出大陆我是可接受的，因为有无数种办法可以重新连上。但谷歌放弃掉 GR 的内部分享时我意识到这个产品最大的特色已经丢掉了，很多分享者还没来得及说声谢谢就失联了。到2013年谷歌放弃 GR 时，我就开始考虑使用本地阅读器了。 feedly 等替代品在功能上是没问题的，但用户聚不到一起了，这就没多大意义了。随之而来的自媒体浪潮更是把很多博主改造成了公众号运营者或投向知识付费的大浪潮里，很现实的情况就是阵地还在，人没了，大家都进入黑暗森林了。就我个人的 rss 来源而言，原来每季度我都要找半天来整理，现在一年都不整理一次了，每次整理都会看到大量的源年输出量小于1，成了纪念馆。现在每天更新的大都是新闻跟有专人运营的博客，个体户少了，分享搬运工就更少了。内容产生的源头动力不足，分享者则根本找不到基于 rss 的在线评论分享平台，只能跑到朋友圈或微博里活动，而那里面你控制不了的东西太多。&lt;/p&gt;
&lt;p&gt;但凡事都有转机，谢大的 &lt;a href=&#34;https://github.com/rstudio/blogdown&#34;&gt;blogdown&lt;/a&gt; 可以说是我见过的最简单易用的内容生产工具，而静态网站也符合绝大多数内容生产者的实际需求。你可能说背后还不就是 &lt;a href=&#34;https://gohugo.io/&#34;&gt;hugo&lt;/a&gt;？类似的工具很多，github pages 背后的&lt;a href=&#34;https://jekyllrb.com/&#34;&gt;jekyll&lt;/a&gt;也挺简单。甚至谢大&lt;a href=&#34;https://yihui.name/cn/2018/03/netlify-cms/&#34;&gt;最近&lt;/a&gt;提到的&lt;a href=&#34;https://www.netlifycms.org/&#34;&gt;netlify&lt;/a&gt; 也提供了技术平台。如果你的需求就是文字，那确实够了，但稍微跟技术沾边的都需要点代码支持，特别是我相信文学化编程会是一个小众但不可忽视的趋势，让别人知道你如何实现一件事更真诚，也更符合我心中互联网应有的样子，所以支持 rmarkdown 这个&lt;a href=&#34;https://zh.wikipedia.org/zh-hans/%E6%9D%80%E6%89%8B%E7%BA%A7%E5%BA%94%E7%94%A8&#34;&gt;杀手级应用&lt;/a&gt;的 blogdown 自然是我的首选。没错，这仅是个人喜好，我当然知道大风气跟现状是什么，但每个人都有不顺应潮流的自由。&lt;/p&gt;
&lt;p&gt;但就算不断有人加入内容生产，另一个问题也很致命，那就是共享平台，特别是 rss 内容的共享平台。现在各个互联网公司都把能让用户留下作为重要目标，rss 这种上古工具自然不太受待见，广告不好发不说用户基本是匿名的，商业价值有限。GR 当初的优势在于用 google 账户可以进行用户区分与关注，眼下这样的平台是绝不会轻易提供类似 rss 阅读器这种大流量低附加值的服务的。不过 &lt;a href=&#34;https://disqus.com/&#34;&gt;Disqus&lt;/a&gt; 倒是个例外，因为其可以跨平台支持评论，而且也通过评论广告的模式可以运营下去，国内的类似平台多说则直接关闭了。当然还是老问题，需要科学上网。另一个类似的服务是 &lt;a href=&#34;https://github.com/imsun/gitment&#34;&gt;gitment&lt;/a&gt;，利用 github 的issue功能来提供评论服务，唯一的缺点就是第一条评论需要用户开启才能用。&lt;/p&gt;
&lt;p&gt;我个人的强项不是开发，但功能整合是可以做的，在跟谢大年初交流后，他提出了一个取代某广告横行网站的思路，后续我发现其实可以更进一步直接用 blogdown 做在线的 rss 阅读器，解析 rss 需要点 xml 或 json 基础，把解析的 rss 转成文本则只需要在 yaml 头文件里做点文章，利用 github 的命令行 PR 功能与 cron 的每日任务功能，我们可以构思出这样一个处理流程：&lt;/p&gt;
&lt;p&gt;每天固定时间启动一个脚本，首先检查一个 rss 地址列表，然后去爬源文件回来，看看跟之前一天有没有变化，如果有就写一个 yaml 文件，然后 hugo 会把这个文件解析成网页，当然只有摘要，因为作为一个在线阅读器，如果抓全文就有不必要的麻烦，但有原文链接。&lt;/p&gt;
&lt;p&gt;在网站的首页设计上，我套用了谢大的 xmag 模版，但做了&lt;a href=&#34;https://github.com/yufree/hugo-xmag&#34;&gt;修改&lt;/a&gt;：点击标题跳转原文，点击摘要则跳转一个摘要页面，这个页面是支持 gitment 或 Disqus 的，这样如果你想跟人交流就可以在这里讨论分享。当然为了让网站有点区别度，我又设计了一个日报机制，也就是说，你如果每天固定时间来看这个网站，都只能看到当天更新的内容，之前的内容是没有入口的，错过了就错过了，这样你大概永远不会看到 1000+。但我的恶趣味在于保留了页面数，虽然你能看到前面很多页，但对不起，首页里只有当天更新的入口。如果真的想看，这个 rss 阅读器本身也提供 rss 输出，你可用其他 rss 阅读器来看到过往文章。&lt;/p&gt;
&lt;p&gt;这个&lt;a href=&#34;http://dailyr.netlify.com/&#34;&gt;在线阅读器&lt;/a&gt;的一个优势在于完全自动且基于 GitHub，你如果想增加 rss 来源，只需要对这个&lt;a href=&#34;https://github.com/yufree/daily/blob/master/R/list.txt&#34;&gt;rss源文件&lt;/a&gt;PR就可以了，作为一个懒汉，我已经把个人干预降到了最低。同时，这也是个&lt;a href=&#34;https://github.com/yufree/daily&#34;&gt;模版网站&lt;/a&gt;，你完全可以自己建一个放到 netlify 上，例如我就为环境期刊搞了&lt;a href=&#34;https://envirss.netlify.com/&#34;&gt;一个&lt;/a&gt;，用来督促师弟师妹看文献。&lt;/p&gt;
&lt;p&gt;当然，这个也是有原型的，谢大的 &lt;a href=&#34;https://t.yihui.name/&#34;&gt;twitter-blogdown&lt;/a&gt;。这个网站已经悄悄运行了两个月了，目前的问题在于我并没有完全开放 gitment 的评论，而&lt;a href=&#34;https://github.com/yufree/hugo-xmag&#34;&gt;修改&lt;/a&gt;版的主题默认支持的是Disqus。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>无学位加拿大博后工作签流程与注意事项</title>
      <link>https://yufree.cn/cn/2016/03/04/canada-visa/</link>
      <pubDate>Fri, 04 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2016/03/04/canada-visa/</guid>
      <description>&lt;p&gt;拿到加村博后offer后，下一步就是工签了，网上查了下攻略感觉都比较保守，本着时间长（我得下半年才能过去，现在无学位）与不作死就不会死的精神我开启了网申工签的过程，以下为流水账与注意事项。&lt;/p&gt;
&lt;h2 id=&#34;准备offer&#34;&gt;准备offer&lt;/h2&gt;
&lt;p&gt;外导的秘书比你更清楚offer如何写，他／她会去cic上交一笔230加元的申请（不用你出），返回你一个收据的PDF文档，这就够了，不必要一定有IMM5802这个表。说白了只有offer跟收据需要外方出，其实收据没有也无所谓，知道申请号就行。其余的资料可以事先准备。&lt;/p&gt;
&lt;h2 id=&#34;网申&#34;&gt;网申&lt;/h2&gt;
&lt;p&gt;网申网站：http://www.cic.gc.ca/english/e-services/mycic.asp&lt;/p&gt;
&lt;p&gt;官方工签指南：http://www.cic.gc.ca/english/information/applications/guides/5487ETOC.asp&lt;/p&gt;
&lt;p&gt;回答问题：http://www.cic.gc.ca/ctc-vac/getting-started.asp&lt;/p&gt;
&lt;p&gt;先仔细阅读指南，搞清楚加国的工签要求，然后回答上一串问题，之后得到一个reference code，直接去注册个网站帐户（GCkey）就会导入编号内容，然后就可以进入到提交材料部分了。一定记得如实回答，不然就麻烦。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;这时候你最好有个visa或master或美国运通的信用卡，不然没法付费，卡可以不是你的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;扫描仪没必要，智能手机下个扫描app，印象笔记就可以（此处硬广）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;申请费一个人155加元，配偶子女我都没有，所以也就没管&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;电脑上一定安装Adobe Reader，因为很多PDF阅读器不支持填表功能而Adobe Reader别看体积大，功能都支持&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;博后不一定要已经拿到博士学位，事实上我现在还没有，offer也是要求拿到学位后生效，但不妨碍申请，这个后面资料里会详谈&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面就是材料的准备：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;IMM1295 这个表UCI可以不写，然后如实写就ok了，最后点validate，如果填写无误就不会报错，工作类型博后选Exception from Labour Market Impact Assessment&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;General Education and Employment Form 中英文都写&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Passport 手机扫描，信息页与盖章页&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;IMM5645 中英文都写&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Employment Contract offer 这个没什么注意的，直接上传offer，我也一直没收到纸质版，从头到尾是电子版&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;research proposal 这个我写了一页，说明背景，目标，方法，期待结果与研究资源就可以了&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CV 这个每个人都有吧，我的就两页&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Digital photo 这个就别便宜照相馆了，直接找个制作证件照的app，里面有加签模版，保留电子版收了1块&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Employment Reference Letter&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Employment Records&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Letter from Current Employer&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这三个我一博士生自然啥都没有，三项都直接上传了同一封信，模版如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Dear Sir/Madam,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;After I graduated from Shandong University, I went to University of X and studies as PhD student till now. I never worked as employee in China and have no employment records, current employer or past employer. I think my salary in the offer ($X/year) is enough to live in Canada. Besides I got increasing subsidy from X as student from 2010(around $X/year) to now(around $X/year).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Thanks,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;X&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Proof that you Meet the Requirements of the Job Being Offered 我上传了中英文在读证明，上面加了一句完成博士学位要求并将要在五月答辩，盖了研究所的章。另外就是英文成绩单与三封英文推荐信&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Education (diplomas/degrees) 这里木有硕士学位的我上传了本科学位证与学历证及学信网的认证（一份30，有效期一年），然后又提交了中英文在读证明，之后是学信网的硕士与博士学历认证（一份30，有效期一年），把所有这些资料按顺序排列成一个PDF文档，木有任何额外公证（主要是穷）&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面是可选文档，这个其实挺重要的，我仔细说下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;IMM5257 这个是签证，填起来很简单，也需要validate，放到可选是因为有些人已经有居住签证了，必填&lt;/li&gt;
&lt;li&gt;Letter of Explanation 这个最重要，我前面搞了那么多乱七八糟不靠谱的东西都是这封信里说明白的。下面是我的模版：&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Dear Sir/Madam,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;My name is X and I am applying for the work permit as postdoctoral fellow in Prof. X’group, University of X. Here is some explanations about my documents:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Proof that you Meet the Requirements of the Job Being Offered&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;My offer is conditioned on a PhD degree while I would defend my PhD degree at May 2016 and could not get the formal degree before the middle of the July according to the convention. However, my postdoctoral fellow research would begin at August 2016 due to the offer. According to C44, I attached the certification from the students affairs office of University of X to prove I am currently a student and I have completed the requirement of PhD degree.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;I also attached the transcript of my academic record in the University of X and three recommendations to prove my capability for the research of postdoctoral fellow in Canada as shown in the research proposal. Also such capability could be checked by my publications in CV.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Employment Reference Letter&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Employment Records&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Letter from Current Employer&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;After I graduated from X University, I went to University of X and studies as a full-time student till now. I never worked as employee in China
and have no employment records, current employer or past employer. I think my salary in the offer
($X/year) is enough to live in Canada. Besides I got increasing subsidy from X as student
from 2010(around $X/year) to now(around $X/year).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Education (diplomas/degrees)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;I submit the verification report of study record in X University and University of X along with the the scanned original document of my diplomas in X. I also submit the verification report of my Bachelor of Science and the scanned original document. Besides, I attached certification from the students affairs office of University of X.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;To make it clear, my study in X is actually a MD-PhD program, so I had a master study record but no master degree. Also the study record of the master said the Status of Student Record is “Transfered”.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;IMM5802 Offer of Employment to a Foreign National LMIA-Exempt&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;I have not get the copy of IMM5802 but attached the e-receipt with the IMM5802 application number(X) noted on it from the secretary of University of X.
The other documents were filled according to the Guide 5487.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Thank you very much for your consideration.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Sincerely,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;X&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;IMM5802 Offer of Employment to a Foreign National LMIA-Exempt 我就上传了个收据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然后交费，因为是加元，建议用多币种卡。交完钱我就回家过年去了。&lt;/p&gt;
&lt;h2 id=&#34;体检&#34;&gt;体检&lt;/h2&gt;
&lt;p&gt;年后收到体检通知，给了IMM 1017E表格去体检，我查了一圈都是1700的体检费，没辙，选了个瑞吉酒店旁边的和睦家医院，打了个电话预约，按要求带上那个体检表，还有护照，身份证以及护照首页复印件就风风火火去了，不用带相片。&lt;/p&gt;
&lt;p&gt;体检不要求空腹，我上午去晚了也没关系，反正下午也可以去。先交钱（可刷卡），进屋拍照，然后给你一套浴袍，鞋套与钥匙，到更衣室脱的只剩下内衣与袜子，穿着浴袍去体检。&lt;/p&gt;
&lt;p&gt;体检包括验尿、验血（包括梅毒跟HIV）、身高、体重、戴眼镜的视力、胸透等，最后大夫问诊，看下耳鼻喉，听下心跳，问下病史等等，然后就完了，1700就花完了。尿检结果很快，血检要等到下午。他们直接给了我一个盖章的表，说下午五点没收到电话就是没问题。其实那个表也没用，估计是能报销。值得一提的是这家医院好像是个私人诊所，说费用都要先问走不走保险，这倒是高收入人群的消费习惯。&lt;/p&gt;
&lt;h2 id=&#34;护照贴签&#34;&gt;护照贴签&lt;/h2&gt;
&lt;p&gt;几天后邮件通知可以去交护照了，你会收到IMM5740，直接打印出来带上护照就可以去加拿大签证中心了。那个地方在东四十条站，进去会有保安帮你刷卡进，你要说加拿大签证，坐电梯到12层右转就是。进去排号，人也不多，先去复印个护照首页（7角），也可以自己事先做好，这个网站指南没写。然后交钱，139.6元，可以现金，也可以刷储蓄卡，但信用卡不行。这个收费没有快递，因为我是来去都11路，快递来回好像四十多。然后过不了几天就可以收到cic的邮件说签证做好了，隔一天去取，会收到一个大黄信封，里面有给边检的信与贴好签证的护照。至此国内部分就完结了。&lt;/p&gt;
&lt;h2 id=&#34;日程&#34;&gt;日程&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;1月31日 提交网申&lt;/li&gt;
&lt;li&gt;2月23日 体检通知&lt;/li&gt;
&lt;li&gt;2月24日 预约&lt;/li&gt;
&lt;li&gt;2月25日 和睦家建国门诊所体检&lt;/li&gt;
&lt;li&gt;2月28日 体检通过通知&lt;/li&gt;
&lt;li&gt;3月1日 递交护照通知&lt;/li&gt;
&lt;li&gt;3月2日 加拿大签证中心送护照&lt;/li&gt;
&lt;li&gt;3月3日 签证通过通知&lt;/li&gt;
&lt;li&gt;3月4日 加拿大签证中心取护照&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总计一个多月，中间回家过了个年。&lt;/p&gt;
&lt;h2 id=&#34;花销&#34;&gt;花销&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;网签费用 155加元&lt;/li&gt;
&lt;li&gt;体检费用 1700元&lt;/li&gt;
&lt;li&gt;签证服务费 139.6元&lt;/li&gt;
&lt;li&gt;学信网认证费用 30*4 ＝ 120元&lt;/li&gt;
&lt;li&gt;照相 1元&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总计约2700元。&lt;/p&gt;
&lt;h2 id=&#34;其他&#34;&gt;其他&lt;/h2&gt;
&lt;p&gt;整体步骤请参考这里：http://www.cic.gc.ca/english/work/apply-who.asp&lt;/p&gt;
&lt;p&gt;别的上面没提到的资料都没用到也没准备，例如无犯罪证明还有一些公证什么的。我觉得学信网那个学位学历认证就不错，虽然只有一年有效期，但中英文都有比较方便，差点就得跑躺济南。&lt;/p&gt;
&lt;p&gt;事先可以打印出给边检看的资料，然后就去准备机票吧（我去年就买了）。&lt;/p&gt;
&lt;p&gt;网申加拿大工签并不难办，但重要的事还是要说三遍：真实，真实，真实。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>如何设计一个扫地机器人程序？</title>
      <link>https://yufree.cn/cn/2015/10/10/roomba/</link>
      <pubDate>Sat, 10 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2015/10/10/roomba/</guid>
      <description>&lt;p&gt;首先先搞清楚扫地机器人运行的抽象版，就是一个2D不随机行走，走过的地方要标记为干净，干净到一定程度停止。从设计角度要考虑面向对象编程，设定初始值与运行停止条件。也就是说我们要定义一个2D棋盘并将其网格化，然后定义一个棋子在上面游走。两者的基本联系是坐标，所以从继承的角度先定义坐标类型。围绕坐标标记或替换写出棋盘与棋子的类型，到时候调用就ok了。初始值与停止条件是解决实际问题必须考虑的，不能没法初始化运行，更不能完全停不下来。寻址算法效率什么的倒可以暂时放到后面去考虑或不考虑，让高级语言编译器代工。&lt;/p&gt;
&lt;p&gt;坐标的属性很简单，给出x与y即可。同时可以给坐标的移动写一个函数，下一步的坐标要通过现有速度与角度进行更新来得到新坐标。因为棋盘与棋子的行为不同，我们最好用不同的类型来区分并针对特殊的类型进行编程，例如平面就需要有标记是否被清洁的属性而棋子则要有移动属性。这两个类型中都要继承来自坐标的属性。&lt;/p&gt;
&lt;p&gt;先看平面类型，我们要对平面类型预定义长与宽，这样就可以按照单位长宽设计网格了。之后要进行初始化，预设全部都是脏的，那么所有网格都有个脏的属性。反映到编程上就是做一个列表，先存储所有的坐标点，这个列表要可更新。这只是初始状态，然后设计一个函数，当给出一个坐标后去检查是否在脏的列表之中：如果在，从列表中删掉；不在，继续。针对这个类型设计一个函数来返回干净坐标的比例，这个比例用来判定是否停止工作。同时，我们还面对一个大的初始化问题，那就是平面要可以给出一个随机点让棋子开始游走。有了平面坐标，脏净判断并初始化后，还要抽象一个边界，当随机行走坐标跑到预定义之外要返回一个逻辑判断，让棋子自行决定如何进行下一步。&lt;/p&gt;
&lt;p&gt;然后就是棋子类型了，棋子要做的就是继承一个棋盘类型并定义一个速度，初始的速度直接用棋盘里的随机点函数生成，这样可以保证不会一出生就跑到棋盘外的坐标上去。然后初始方向直接随机，接下来就是如何行走了。很简单，棋子所在点在单位时间已经干净了，直接调用坐标类型的移动函数进行移动，将初始化中的方向与速度放进去即可。但需要注意的是要调用前面棋盘类型中的函数对下一个点进行判断，看是否撞墙了，撞了墙就改方向，直到不撞墙为止。然后就是停止条件，直接通过棋盘类型中是否干净的比例进行判断，如果达到一定比例就别干活了。&lt;/p&gt;
&lt;p&gt;然后我们就可以写一个仿真程序来看看结果了，输入的参数包括房间大小、速度、行走方式及停止条件，程序可以返回一个估计时间来进行调试。对于行走方式而言，常见的一般就是撞墙变方向，也有比较疯狂的每一步都变方向。当然这都是基于算法的，如果基于其他传感信号，也可以写出更有针对性的算法例如脏的地方降低速度等来提高效果。这样通过程序我们已经可以模拟一个棋子连续遍历棋盘的过程了。也就是说这时候我们把这个程序跟参数输给扫地机器人，它就可以打扫空房间了。&lt;/p&gt;
&lt;p&gt;真实状态的房间除了墙还有桌子腿什么的，不过用上面的算法也可以跑，就是慢点。一般会通过传感器感应房间大小，要是没有也无所谓，第一天跑时间长点，把房型撞出来也不难。先预设一个1000平的大房间，然后跑，撞墙就在坐标上做标记，跑时间长点就可以记住房型了。然后我们可以设计内置一个优化算法，对已经撞出来的房型路线进行路线优化，得到时间最短的那个。如果再高级点，可以对平时打扫需要较长时间的地方（此时要依赖传感器了）进行坐标标记，关键位置提高功率。然后软件可以放到树莓派这种东西上跑，外接上一个压力传感器得到墙的信号，一个电机跟电池保障能动能扫地，最好有个判断干净与否的光学传感器就够了。作为搞分析化学的，我建议搞个拉曼顺道定性测下污染物是什么，小型化质谱也应该可以，现在不流行原位电离吗，上一个就得了。这样不但采集了室内灰尘，顺道连污染物一并测完了，然后断面研究或队列研究随便搞，发点健康类文章问题应该不会太大。咦，好像写跑偏了。&lt;/p&gt;
&lt;p&gt;不过鉴于宿舍里只有扫帚，我也不会真的去设计这玩意，只是完成了一次课程作业后的遐想。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spark中关键概念的理解</title>
      <link>https://yufree.cn/cn/2015/06/18/spark-concept/</link>
      <pubDate>Thu, 18 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2015/06/18/spark-concept/</guid>
      <description>&lt;p&gt;其实我自己对spark的应用场景是没什么需求的，但几个月前不知道怎么想的在edx上选了一门伯克利的&lt;a href=&#34;https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/info&#34;&gt;spark课&lt;/a&gt;，所以就入了坑。一共五周，现在开到第三周，因为对python不熟加上记性也不好，先把其中比较干货的东西捡出来，此外剩下两周的课可能由于外出开会耽误。我没有计算机科学背景，所以仅按照自己理解与讲义来写，疏漏之处见谅。&lt;/p&gt;
&lt;h2 id=&#34;spark简介&#34;&gt;Spark简介&lt;/h2&gt;
&lt;p&gt;Spark是用来解决大量数据处理问题的一个工具。由于现在数据产生非常快，单机在收集、储存与处理数据上是性能不足的。如果我们用集群的话收集与存储是没问题了，但如何快速处理数据让数据变成知识也是需要工具的。此外，集群出于成本考虑多采用分布式的结构，所以这个工具要做的就是从这些分布式集群中快速准确的提取信息，而这也是spark的设计初衷。&lt;/p&gt;
&lt;p&gt;我们来理解一个分布计算场景：这里有一大段文本，我们把它们分成N份去储存，现在我打算计算词频，该如何做？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;方案一：每个储存单元作为一个处理单元，进行分词后各自计算自己分到文本的词频，然后汇总发送到另一个独立处理单元单独作汇总。这个方案是分层的，高层汇总单元（比较贵）挂了也就挂了，而同时响应并发数据很容易把这个单元搞宕机。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方案二：既然两层会挂掉，那我就在中间继续添加独立汇总层，例如在进行最终汇总前面加两个处理单元，每个处理单元只处理下层有限个储存单元的数据然后汇总。这样由于存在缓冲层甚至多个缓冲层，我们的处理单元成本可以相对一致，整体处理的稳定性会好一些。但这仍然是分层逻辑，高层挂了还是全挂。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方案三：还是分层，但是这次是逻辑分层，在词频问题上就是我们的处理层不是一台中枢而是多个处理单元。每个处理单元只收集处理逻辑上的一部分，例如处理中心A只响应词频高于100的，B只响应50～100的…这样出现宕机只会损失一部分运算。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在这里前面负责分词那一部分可理解为数据的map，也就是映射成可独自处理的小部分，而后面根据词频分别汇总可理解为reduce，也就是处理为想要的部分。整个流程就是先map到想要处理的东西，然后reduce为处理后的东西，不断循环。类似R里面从原始数据中提取想要的数据后进行处理，只是应用场景换成比较高大上的集群，而规模一大就需要有工具来把底层分发处理工作高效化，这样看spark实际提供了一个处理对象，底层的黑活（例如某个处理单元挂了重启）通过spark完成，我们只管用熟悉的数据处理方式来处理spark对象就可以了。&lt;/p&gt;
&lt;p&gt;其实这个问题不是spark首先发现解决的，Hadoop也是来处理这个问题的，但由于Hadoop都是硬盘读写操作，大量的I/O会降低处理速度，spark的一大高明之处在于把硬盘读写省了，都在内存里玩，如果中间处理品需要，可以另外cache。&lt;/p&gt;
&lt;h2 id=&#34;resilient-distributed-datasetsrdd&#34;&gt;Resilient Distributed Datasets(RDD)&lt;/h2&gt;
&lt;p&gt;RDD是spark的核心概念，所有要处理的数据都以RDD形式存储或使用。RDD可以直接生成或通过其他格式转化，这个处理对象从硬盘数据生成后运行在内存里，然后你就可以用熟悉的编程语言来处理这个对象了，目前支持Scala，Java，R与Python，同时Spark也提供了不少自带的函数来进行数据分析，前提是你得学下Scala。&lt;/p&gt;
&lt;h2 id=&#34;driver-and-workers&#34;&gt;Driver and Workers&lt;/h2&gt;
&lt;p&gt;Spark里一个程序是由两部分组成：Driver与Workers。Workers工作在集群节点或线程中，而上面说的RDD是分布在这些Workers中。Driver就是你的应用需求了，把需求提给Workers就完成编程了。&lt;/p&gt;
&lt;h2 id=&#34;rdd的创建与操作&#34;&gt;RDD的创建与操作&lt;/h2&gt;
&lt;p&gt;以下讨论我使用的是PySpark中的术语，也就是使用Spark的Python接口包。&lt;/p&gt;
&lt;p&gt;首先是RDD的创建，RDD可以来自python的list对象，也可以来自RDD的转化与直接从硬盘读取。在RDD创建时，你可以指定RDD的分区，例如指定为5就是说会分发到5个集群节点去处理。这里是可以精细化配置的，当然你得对集群有概念，反正我没概念。&lt;/p&gt;
&lt;p&gt;然后是RDD的操作，有两种类型，一种是transformations，另一种是actions。当你指定transformations时操作不会立即执行，属于lazy loading，当指派了actions后，操作才会执行。此外如前所述，RDD可以缓存到内存或硬盘上，对于使用者而言只要缓存了就ok，底层工作让spark来做就够了。&lt;/p&gt;
&lt;p&gt;filter及与Hadoop类似的map功能是属于transformations的，也就是说我可以先写一大段transformations，但只要没有actions的功能例如计数（count）或收集（collect），这些语句是不被执行的。举个例子，我打算从集群的数据里map某个条目，然后filter其中符合某些特征的条目，最后计数。只有最后这个是actions，如果没有这个命令，前面那一套都不执行。&lt;/p&gt;
&lt;h2 id=&#34;spark程序生命周期&#34;&gt;Spark程序生命周期&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;从外部数据中建立RDD&lt;/li&gt;
&lt;li&gt;通过transform变成新的RDD&lt;/li&gt;
&lt;li&gt;cache()一些关键RDD为了复用&lt;/li&gt;
&lt;li&gt;执行actions来进行计算并输出结果&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;broadcast-variables&#34;&gt;Broadcast Variables&lt;/h2&gt;
&lt;p&gt;当某些变量需要只读的分发给所有workers，spark可以通过广播这些变量到所有workers。举例而言，当你需要反复利用同一个数据表做查询，如果每个workers都计算一遍就不如把这个表先生成广播到所有workers里来的高效。&lt;/p&gt;
&lt;h2 id=&#34;accumulators&#34;&gt;Accumulators&lt;/h2&gt;
&lt;p&gt;聚合所有workers结果回driver而workers之间不需要传送时，spark提供accumulators来汇总，提高性能。举例而言，当我做求和时需要汇总各个workers的数值到driver，我并不需要workers去读取driver上的数值，这时accumulators就可以在全局上进行汇总。&lt;/p&gt;
&lt;h2 id=&#34;pyspark实例&#34;&gt;PySpark实例&lt;/h2&gt;
&lt;h3 id=&#34;rdd的建立&#34;&gt;RDD的建立&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 从python list里构建&lt;/span&gt;
data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# 这个构建行为不是actions，只是指定构建方式，包括分发数&lt;/span&gt;
RDD &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize(data,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# 从Hadoop输入格式建立&lt;/span&gt;
distFile &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;textFile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;README.md&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;rdd的transformation与lambda函数&#34;&gt;RDD的transformation与lambda函数&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 构建RDD&lt;/span&gt;
rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;])
&lt;span style=&#34;color:#75715e&#34;&gt;# 用python的lambda函数来构建映射&lt;/span&gt;
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# RDD:	[1,2,2,4] → [2,4,4,8]	&lt;/span&gt;
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filter(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: x &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# RDD: [1,2,2,4] → [2,2,4]&lt;/span&gt;
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;distinct()	
&lt;span style=&#34;color:#75715e&#34;&gt;# RDD: [1,2,2,4] → [1,2,4]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从这里我们可以看出这个操作非常类似R中对数据框的操作，但因为是lazy的，没有action命令它们不会实际被执行。&lt;/p&gt;
&lt;h3 id=&#34;rdd的action与lambda函数&#34;&gt;RDD的action与lambda函数&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;])
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reduce(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; a,b: a&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;b)	
&lt;span style=&#34;color:#75715e&#34;&gt;# Value: 6&lt;/span&gt;
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# Value: [1,2]	&lt;/span&gt;
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect()	
&lt;span style=&#34;color:#75715e&#34;&gt;# Value: [1,2,3]	&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Spark的一大优点在于比Hadoop提供了更多的操作，这一点需要详查文档体会。&lt;/p&gt;
&lt;h3 id=&#34;broadcast-variables-1&#34;&gt;Broadcast Variables&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# driver端	&lt;/span&gt;
broadcastVar &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;broadcast([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;])	
&lt;span style=&#34;color:#75715e&#34;&gt;# worker端&lt;/span&gt;
broadcastVar&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;value
&lt;span style=&#34;color:#75715e&#34;&gt;# [1,2,3]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;accumulators-1&#34;&gt;Accumulators&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;accum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;accumulator(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)	
rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;])	
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(x):	
 &lt;span style=&#34;color:#66d9ef&#34;&gt;global&lt;/span&gt; accum
 accum &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; x			
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;foreach(f)	
accum&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;value
&lt;span style=&#34;color:#75715e&#34;&gt;# Value: 10	&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>基于Github Pages快速建站指南</title>
      <link>https://yufree.cn/cn/2014/12/09/github-pages/</link>
      <pubDate>Tue, 09 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2014/12/09/github-pages/</guid>
      <description>&lt;p&gt;最近整理了公开课笔记，开始是打算生成html扔到rpubs上，后来发现latex公式显示有问题，是可忍孰不可忍，就花了点时间整理出一个&lt;a href=&#34;yufree.github.io/notes/&#34;&gt;网站&lt;/a&gt;。做完了发现实际连半个小时都没用上，但就显示效果而言还过得去。进而想到科研人员给自己的项目搭建一个极简的网站其实是很实用的，但如果让每个科研人员去学前端后端估计不现实，但我们可以站在别人的肩上去工作的。&lt;/p&gt;
&lt;h2 id=&#34;git--github&#34;&gt;Git &amp;amp; Github&lt;/h2&gt;
&lt;p&gt;首先你可以在&lt;a href=&#34;https://try.github.io/levels/1/challenges/1&#34;&gt;在线模拟器&lt;/a&gt;上试一下。要点是Git是本地仓，Github是远程仓，你在本地仓的操作commit后可以push到远程仓更新。换句话讲，你在本地仓里写静态文本，然后推送到远程仓，远程仓通过&lt;code&gt;Github Pages&lt;/code&gt;这个Github提供的服务将你的文本转化为网页展示出去。所以具备这条前置知识后你应该至少在本地端安装了&lt;a href=&#34;http://git-scm.com/&#34;&gt;Git&lt;/a&gt;并注册有&lt;a href=&#34;https://github.com/&#34;&gt;Github&lt;/a&gt;账号，之后具备在本地Git端与服务器Github端仓库的交互能力。例如在本地名为test仓的a.txt文件可以推送到远程仓。学习基本语法加安装最多半小时左右就可以。&lt;/p&gt;
&lt;p&gt;如果你之前写过博客，这就相当于把Github作为博客的空间与域名提供商，你在本地提供内容就可以了。那么问题来了，什么样的内容可以转化成为网页？按说浏览器解析的是html语言，这样你应该先去学html语言… 等等，html语言，对，就是你在浏览器上右键查看源代码看到的那一坨，强烈不建议学习，我们需要一种更简单的语言，这就是markdown语言。&lt;/p&gt;
&lt;h2 id=&#34;markdown-语法&#34;&gt;markdown 语法&lt;/h2&gt;
&lt;p&gt;这个语法更简单，查看&lt;a href=&#34;https://help.github.com/articles/markdown-basics/&#34;&gt;这里&lt;/a&gt;就可以，如果你打算实现高级点的功能例如表格或代码高亮什么的，可以看Github对markdown语法的&lt;a href=&#34;https://help.github.com/articles/github-flavored-markdown/&#34;&gt;增强&lt;/a&gt;。这个学习过程可能就是几分钟的事。我在&lt;a href=&#34;http://yufree.github.io/blogcn/2014/10/25/kramdown.html&#34;&gt;博客&lt;/a&gt;中对markdown语法的一种方言进行了介绍，由于科研人员对数学公式的输出有要求，这种方言的学习很有必要。&lt;/p&gt;
&lt;h2 id=&#34;github-pages&#34;&gt;Github Pages&lt;/h2&gt;
&lt;p&gt;好了，现在你应该有个本地仓test，远端用testwebpage注册了github账号，里面有个按照markdown语法写好的文本文件例如a.md。把这个文档推送到远端就可以了，那么网址是什么呢？按照github规定，这个网址应该是：http://testwebpage.github.io/test/a.html。但你直接访问是打不开的，因为你得让github知道这是个需要生成网页的项目，按照规定github默认的网页必须放的一个名为gh-pages的git分支中，这样你手工新建一个这样的分支push到远端就可以了。但这仅仅是实现了网页而你还需要一个主页来整合网页，这时我们写一个index.md的文档把网页整合好放到根目录下就OK了。这样你访问http://testwebpage.github.io/test就能看到主页了。其实还有更简单的方法，你可以按照&lt;a href=&#34;https://pages.github.com/&#34;&gt;这里&lt;/a&gt;的介绍可视化生成网页。&lt;/p&gt;
&lt;h2 id=&#34;定制&#34;&gt;定制&lt;/h2&gt;
&lt;p&gt;按照前面的方法你大概可以做出一个能看的网页，但要做出好看有个性的网页仅仅依赖默认方法就不好办了。这里建议拿出一点时间来到&lt;a href=&#34;http://www.w3school.com.cn/&#34;&gt;w3school&lt;/a&gt;上面学下网页前端的基本概念，然后如果你选择一款有开发者模式的浏览器例如&lt;a href=&#34;https://www.google.com/chrome/&#34;&gt;chrome&lt;/a&gt;，打开我的&lt;a href=&#34;http://yufree.github.io/blogcn&#34;&gt;博客&lt;/a&gt;，对上面每个元素进行审查，看一下样式如何嵌套并修改观察效果。在试错中成长是最快的，如果不断实践，很快就可以摸索出修改方法，然后按自己的习惯定制就可以了。对于科研人员，这类网站不要花哨，简单明了重点在内容就可以了。阮一峰的一篇&lt;a href=&#34;http://www.ruanyifeng.com/blog/2012/08/blogging_with_jekyll.html&#34;&gt;介绍&lt;/a&gt;也很有价值，值得参考。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>kramdown 语法简介</title>
      <link>https://yufree.cn/cn/2014/10/25/kramdown/</link>
      <pubDate>Sat, 25 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2014/10/25/kramdown/</guid>
      <description>


&lt;p&gt;偶然发现之前文章的&lt;code&gt;markdown&lt;/code&gt;输出有问题，追根溯源发现是&lt;code&gt;Github Pages&lt;/code&gt;的解释器有问题。当年建站是东拼西凑的代码，其实对&lt;code&gt;jeklly&lt;/code&gt;没仔细学习，当时就觉得拿&lt;code&gt;markdown&lt;/code&gt;语法写就没问题了，其实这货也是有方言的。不同的人按自己的需要对标准语法缝缝补补以适应新的要求，&lt;a href=&#34;http://commonmark.org&#34;&gt;据说&lt;/a&gt;有些大牛开始着手&lt;code&gt;markdown&lt;/code&gt;的标准化问题了。不过&lt;code&gt;Github Pages&lt;/code&gt;目前只支持包括Maruku（默认），Redcarpet 和Kramdown在内为数不多的解释器。换言之，像我这种本地不装&lt;code&gt;jeklly&lt;/code&gt;而纯依赖推送原始&lt;code&gt;md&lt;/code&gt;文件的做法想要得到数学公式表格啥的支持就需要从这些解释器中找个最合适的用。如果你足够寂寞，装个&lt;code&gt;jeklly&lt;/code&gt;然后使用&lt;code&gt;pandoc&lt;/code&gt;的插件来本地生成&lt;code&gt;html&lt;/code&gt;文档去推送也没关系。但我足够懒，所以我选择找一个最接近&lt;code&gt;pandoc&lt;/code&gt;功能的解释器，也就是kramdown来凑合。本文旨在总结其与标准&lt;code&gt;markdown&lt;/code&gt;的区别，方便以后写作。&lt;/p&gt;
&lt;div id=&#34;SS&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setext Style&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;setext&lt;/code&gt; 样式算得上轻量级标记语言的始祖，在&lt;code&gt;kramdown&lt;/code&gt;中，这种样式可能与水平分割线冲突：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;para A
---
para B&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这种情况你不太好分辨&lt;code&gt;para A&lt;/code&gt;是标题还是两段为水平线所分离，这样&lt;code&gt;kramdown&lt;/code&gt;规定优先为标题，也就是&lt;code&gt;setext&lt;/code&gt; 样式优先级高于水平分割线，但不允许内容与分割线间有空行，&lt;code&gt;atx&lt;/code&gt;的井号模式也是如此，标准&lt;code&gt;markdown&lt;/code&gt;对此没有要求，允许多个空行。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;header-id&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Header ID&lt;/h2&gt;
&lt;p&gt;扩展功能，使用ID为你的标题提供页面内引用地址，可以像超链接一样跳转，不信你点&lt;a href=&#34;#SS&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Hello      {#id}

Say [hello](#id)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;代码块&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;代码块&lt;/h2&gt;
&lt;p&gt;扩展功能，可折叠代码块直接段落开头使用&lt;code&gt;tab&lt;/code&gt;键就可以了，默认不折叠。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  a &amp;lt;- c(1:100) #宇宙最高司令教导我们说可以把一行代码写的尽量长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长一些
  b &amp;lt;- c(2:101)
^
  c &amp;lt;- b-a&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;相连代码块用&lt;code&gt;^&lt;/code&gt;分割&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;围栏代码块&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;围栏代码块&lt;/h2&gt;
&lt;p&gt;扩展功能，用&lt;code&gt;~~~&lt;/code&gt;跟&lt;code&gt;~~~&lt;/code&gt;围起来就可以了。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;列表&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;列表&lt;/h2&gt;
&lt;p&gt;不允许混合无序列表与数字列表，标准版两个不同列表容易产生一个无序列表：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;* kram
+ down
- now

1. kram
2. down
3. now&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;同级别列表不允许有不同缩进。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;定义列表&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;定义列表&lt;/h2&gt;
&lt;p&gt;扩展功能。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kramdown
: A Markdown-superset converter

Maruku
:     Another Markdown-superset converter&lt;/code&gt;&lt;/pre&gt;
&lt;dl&gt;
&lt;dt&gt;kramdown&lt;/dt&gt;
&lt;dd&gt;
A Markdown-superset converter
&lt;/dd&gt;
&lt;dt&gt;Maruku&lt;/dt&gt;
&lt;dd&gt;
Another Markdown-superset converter
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div id=&#34;表格&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;表格&lt;/h2&gt;
&lt;p&gt;扩展功能。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;|-----------------+------------+-----------------+----------------|
| Default aligned |Left aligned| Center aligned  | Right aligned  |
|-----------------|:-----------|:---------------:|---------------:|
| First body part |Second cell | Third cell      | fourth cell    |
| Second line     |foo         | **strong**      | baz            |
| Third line      |quux        | baz             | bar            |
|-----------------+------------+-----------------+----------------|
| Second body     |            |                 |                |
| 2 line          |            |                 |                |
|=================+============+=================+================|
| Footer row      |            |                 |                |
|-----------------+------------+-----------------+----------------|&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;数学公式支持&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;数学公式支持&lt;/h2&gt;
&lt;p&gt;扩展功能。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$$
\begin{align*}
  &amp;amp; \phi(x,y) = \phi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right)
  = \sum_{i=1}^n \sum_{j=1}^n x_i y_j \phi(e_i, e_j) = \\
  &amp;amp; (x_1, \ldots, x_n) \left( \begin{array}{ccc}
      \phi(e_1, e_1) &amp;amp; \cdots &amp;amp; \phi(e_1, e_n) \\
      \vdots &amp;amp; \ddots &amp;amp; \vdots \\
      \phi(e_n, e_1) &amp;amp; \cdots &amp;amp; \phi(e_n, e_n)
    \end{array} \right)
  \left( \begin{array}{c}
      y_1 \\
      \vdots \\
      y_n
    \end{array} \right)
\end{align*}
$$&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
  &amp;amp; \phi(x,y) = \phi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right)
  = \sum_{i=1}^n \sum_{j=1}^n x_i y_j \phi(e_i, e_j) = \\
  &amp;amp; (x_1, \ldots, x_n) \left( \begin{array}{ccc}
      \phi(e_1, e_1) &amp;amp; \cdots &amp;amp; \phi(e_1, e_n) \\
      \vdots &amp;amp; \ddots &amp;amp; \vdots \\
      \phi(e_n, e_1) &amp;amp; \cdots &amp;amp; \phi(e_n, e_n)
    \end{array} \right)
  \left( \begin{array}{c}
      y_1 \\
      \vdots \\
      y_n
    \end{array} \right)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;html支持&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HTML支持&lt;/h2&gt;
&lt;p&gt;比标准更宽泛，支持任意位置插入&lt;code&gt;html&lt;/code&gt;代码并允许嵌套&lt;code&gt;markdown&lt;/code&gt;语句&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;脚注&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;脚注&lt;/h2&gt;
&lt;p&gt;扩展功能，其实就是加了上标的链接。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;That&amp;#39;s some text with a footnote.[^1]

[^1]: And that&amp;#39;s the footnote.

That&amp;#39;s the second paragraph.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s some text with a footnote.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;That’s the second paragraph.&lt;/p&gt;
&lt;p&gt;以上为常见功能的支持，有了这些写一篇带点公式代码的文章就不难了。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;And that’s the footnote.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
