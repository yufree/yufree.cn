<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tech on Miao Yu | 于淼 </title>
    <link>https://yufree.cn/tags/tech/</link>
    <description>Recent content in tech on Miao Yu | 于淼 </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://yufree.cn/tags/tech/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>竭泽而渔式非目的分析</title>
      <link>https://yufree.cn/cn/2022/02/17/exhaustive-nontargeted-analysis/</link>
      <pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2022/02/17/exhaustive-nontargeted-analysis/</guid>
      <description>&lt;p&gt;在高分辨质谱分析领域，偏生物样本的代谢组学与偏环境样本的非目的分析都是近期比较火的研究方向。这里关注的待测物一般是分子量1500Da或2000Da以下的小分子，相比蛋白质组学而言看起来是简单了但其实分析逻辑复杂度更高。从定性角度来说，基于质谱的定性要依赖精确质量数与其分子碎片的特异性，蛋白质组学的麻烦在于分子量大了以后精确质量数测定受同位素与多电荷影响谱图解析会有困难，实践里都是打碎了测肽段，然后利用算法重新拼出蛋白质序列，当然自上而下的测序现在也有些新思路，更多依赖仪器进步。到了小分子部分，蛋白质组学里存在的同位素与多电荷问题相对少一点，但这里就不存在肽段的概念了，也就是小分子测定里没有类似碱基或氨基酸的结构单元，此时单纯依赖高分辨质谱只能得到分子式，但要想知道结构，必须要有碎片信息。&lt;/p&gt;
&lt;p&gt;这里我先明确下为什么我不用所谓“非靶标”来翻译non-target 或untargeted 这个词。首先非靶标跟非目的就不是一回事，目的分析更多针对研究目的，靶标更多侧重已知物，前者是研究概念而后者是分析概念。在具体研究中，应该使用目的而非靶标，例如我想找一种已知物的代谢物，已知物我们有标准品而代谢物没有，那么这算靶标还是非靶标？没有标准算非靶标的话事实上你又知道其与已知物可能就存在一个质量差，也正是利用质量差来找代谢物；算靶标你又没有标准，只是有个筛选概念。但如果直接算目的性分析就没有问题，因为我研究目的就是代谢物，真正的非目的分析应该是完全意外的发现或基于效应引导分析的发现，例如田振宇老师对6PPD-quinone的发现。&lt;/p&gt;
&lt;p&gt;现在很多人名义上说是非目的分析，实际做的却是全扫描配合DDA/DIA之后去跟质谱数据库做比对。坦白说这跟你买了标准在自己实验室搞个本地数据库做比对从结果上看区别不大，后者无论如何也不能叫做非目的分析，因为逻辑上你根本就不打算测数据库之外的物质，你的目的实际上是讨论已知数据库里的物质。在我看来，能在研究开始前就得到标准谱图的物质都不能算到非目的分析里，只是高阶版目的分析或筛选，基于已知代谢物或污染物的下游分析例如代谢通路分析事实上都是在对已有的研究进行低质量验证工作，结论都只能基于已知代谢通路来讨论，谈不上有什么新发现。当然，新发现从来都是很困难的，能把已知通路影响做出来也算不错，目前很多人对真正非目的分析的可行性表示了怀疑，更多去关注已知通路的评价，也就是事实上做的都是目的性分析。这样的结论从分析角度更可靠，但似乎除了展示些花里胡哨的系统性影响外也很难有新假说提出，做的结果大都是看上去高端大气但读了也不知道具体在说啥，其他学科的人也始终对此持怀疑态度而应用受限。&lt;/p&gt;
&lt;p&gt;我觉得这个问题还是有解的，非目的分析关注的重点应该是信息收集而不是物质鉴定。如果你关注物质鉴定，那么肯定掉到数据库匹配的目的性分析老路上。但如果关注信息收集而把物质鉴定剥离出去，思路就会很明确，那就是最大限度收集样品里的小分子信息。看上去跟物质鉴定一样，但实际操作是不一样的，因为你需要保存的结果不是鉴定出的物质列表而是可能的谱图，前者是一个终点而后者可以反复去跟更新的数据库匹配来进行探索发现工作。举例而言就是你说我测到了一万个峰，鉴定出了一百个物质，但我更关心的是这一万个峰里不论有没有名字究竟有多少物质。很多所谓鉴定物质只有个名字而没有任何文献报道其功能，实质上跟没鉴定出来区别不大，你给他起名字叫物质XYZ都可以，真实研究关注的是其分子功能，但这个很多非目的分析的文章反而不去讨论，只去关注可鉴定物质间的关系，结果大概率还是其他通路研究的一个远程验证。&lt;/p&gt;
&lt;p&gt;那么具体在质谱上如何操作呢？第一步毫无疑问是全扫描，这里只说软电离部分，硬电离属于后面分子碎片的讨论。理论上软电离全扫描拿到的应该是物质的分子离子峰，但理论跟现实的距离好比卖家秀跟买家秀，看上去相似但其实根本不是一回事。即便是最常见的ESI源，单个物质软电离下会同时出现同位素峰、加合物峰、中性丢失峰还有一些莫名其妙的寡聚体与各种峰的多电荷峰。我下载了HMDB的LC-MS质谱数据，算了下大概平均每个物质的所谓MS1标准谱图也有26个峰，而GC-MS这种硬电离的平均峰数为90。这就是现实跟理论的差距，LC-MS的全扫描软电离数据里甚至还有碎片离子峰可以用来做定性，相关研究可以查阅广州工业大学薛靖川老师的工作。&lt;/p&gt;
&lt;p&gt;但我们的目的是收集物质信息，从信息角度一个物质我只需要一个峰，其余的峰给的信息都是冗余的。只要我找到单一物质的一个峰且最好是分子离子峰，那么我把这个峰送去做二级质谱得到结构信息，那么也就完成了信息收集。也就是说，如果我对每一个潜在物质找到一个峰，那么实际上样品里所有可以测到的物质信息我就都可以收集到二级谱图了，哪怕现在鉴定不了，起码我也可以根据谱图相似性做功能性推测。这也就是标题所说的“竭泽而渔”式非目的分析。&lt;/p&gt;
&lt;p&gt;那么怎么去掉冗余峰呢？常见思路就是给分别找出同位素峰与加合物，然后排除掉，这里就需要一个事先定义的质量差列表。不过我前面也说了，这还是目的性分析的思路，利用经验法则来排除。但真实样品里加合物是非常复杂的，套用已知质量差的列表会引入假阳性，例如样品里本没有某种加合物，但你去套这个列表就会误把不同的物质鉴定为加合物关系。我的思路很简单，既然只关注样品里存在的加合物，那么我就去算样品里的单独保留时间区间里的质量差，然后计算下各种质量差的出现频率，频率高的就标注为潜在加合物，这样就降低了假阳性的概率。其实，在真实样品中经常会发现很多不常见的高频率质量差，但也不用过度解释，直接归为未知加合物就可以了。然后，通过峰强度的相关性计算出潜在加合物间的相关性峰网络，这个网络大概率就来自于同一物质。之后，我们就可以根据一些自定义的规则来选择其中的一个峰作为潜在的先导离子，例如选强度最高的。这样我们就把全扫描的数据处理成了独立物质峰数据，一个峰代表一个物质，算是“竭泽而渔”，有些研究人员则称为伪目的分析（pseudo targeted analysis）。从我经验而看，如果20分钟全扫描可以得到一万个峰，那么独立物质峰个数大概只有一千这个量级。&lt;/p&gt;
&lt;p&gt;那么是不是这一千个峰就可以用目的性分析的方式作为先导离子来采集二级质谱呢？如果只进样一次是不行的，因为PRM模式下20分钟扫一千个先导离子灵敏度完全不够，二级谱质量非常差。这里需要一个技巧，那就是根据先导离子的保留时间写一个多次进样脚本，让每次进样都测定样品中一部分互相不干扰灵敏度的先导离子。从我经验而言，一千个先导离子大概需要10次重复进样就可以覆盖所有独立物质峰。这样在样品进样阶段，我们就收集了所有可能的物质结构信息（二级谱图），至于啥时候能鉴定出来，这个一来可以等后面出现更全的数据库，二来其实对很多研究而言，给个名字并不重要，例如我们发现很多物质都是甲氧基化的，那么可以直接去验证相关酶的活性而不是去费力研究所有甲氧基化物质具体是什么。生物信息重要的是过程而不是名字。&lt;/p&gt;
&lt;p&gt;前面介绍的就是我最近发表在 &lt;em&gt;Journal of Cheminformatics&lt;/em&gt; 上的PMDDA工作流，整体流程如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yufree/yufree.cn/master/static/images/PMDDAnew.png&#34; alt=&#34;PMDDAnew&#34;&gt;&lt;/p&gt;
&lt;p&gt;那么效果如何呢？我比对了利用已知加合物质量差列表的CAMERA或RAMClustR，这两个软件都有提供MS1谱的功能，可以直接给出潜在物质的精确质量数，我就用这些数据计算出先导离子荷质比并同样进行重复进样。最后我同时对比了PMDDA/CAMERA/RAMClustR 选择先导离子后采集的二级谱图在GNPS上的分子网络图（可理解为潜在物质）。结果如下图，可以看出这种基于样品自身信息选择先导离子的方法要全面优于基于已知质量差列表的方法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yufree/yufree.cn/master/static/images/comparepclust.png&#34; alt=&#34;comparepclust&#34;&gt;&lt;/p&gt;
&lt;p&gt;同时我其实也对比了常见的DDA（一年前的预印本里有，后来因为加入iterative DDA在正式论文里被移除掉了），DDA的问题是牺牲了灵敏度去采集二级谱图而其先导离子虽然依赖样品自己，但选到的峰其实质量非常差，这就出现了DDA测到的很多物质在MS1数据里压根就找不到的现象。这里我要说下很多现在发表的工作经常把MS1的数据与MS2的数据分开来讨论，其背后的小心思就在于MS2里好不容易发现的能解释得通的物质在MS1里完全找不到或数据不支持结论，这就是DDA滥用的后果。其实DDA倒也可以把MS1里选择的先导离子作为优先离子，但问题是如果你关注的就是MS1里能测到的物质，PRM就是能收集所有信息灵敏度最高的方法了，DDA肯定会把扫描时间浪费在MS1里没有的离子上。我还额外测试了iterative DDA，这项技术需要仪器支持，每次扫描只会扫前一次扫描里没有扫到的先导离子，效果跟PMDDA差不多，但让我比较吃惊的是PMDDA跟iterative DDA测到的MS1里的物质很多是不重叠的。也就是说，如果你真想竭泽而渔，那么最好的方法就是在样本量允许情况下同时进行PMDDA与PMDDA辅助的iterative DDA。&lt;/p&gt;
&lt;p&gt;这里我要补充下文章里没详细描述的一部分经验内容，那就是“竭泽而渔”的效果。从独立分子网络或者说潜在化合物的数目上看，即使使用PMDDA，先导离子里有二级谱图生成的也不多，大概只有一半多一些，而DDA收集的二级谱图从数目上看是足够多的，但问题是它选的先导离子并不存在于MS1的峰表里。形成这个现象的原因可能是我的MS1峰列表排除掉了样品中峰响应小于空白中同一峰响应三倍的峰，但进行这个峰过滤的原因在于这样的峰通常峰形很差，不能拿来定量，但DDA却会因为这些峰符合top10的选择原则就给送去做二级了。所以，我很难说“竭泽而渔”的PMDDA一定能拿到比DDA更多的二级谱图，但从其与MS1质量较高的峰表的对应效果而言还是有优势的。而且，我还发现传统DDA的重复进样可重现性非常差，下图展示了三次DDA进样同一样本时其选择的先导离子，基本一针一个样，这使得我更怀疑所有用DDA作出的研究成果了。有意思的是仪器厂家的软件根本就不提供这个功能，所以做科研必须要用开源软件且具备自己验证的能力，否则就是仪器厂家的免费甚至付费复读机。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yufree/yufree.cn/master/static/images/ddams1file.png&#34; alt=&#34;ddams1file&#34;&gt;&lt;/p&gt;
&lt;p&gt;PMDDA的另一优点在于不需要仪器配合，你完全可以用三重四极杆质谱来采集二级谱信息。这里的关键就是要通过数据分析提取MS1全扫数据通过多次进样来采集高质量的二级谱信息。不过，大连化物所许国旺老师也有类似思路的研究，这里最大区别就在于先导离子的选择策略。此外我对于研究的可重复性要求非常高，研究里用的样品是标准参考物质，数据可以直接从网上下载，所有处理代码是开源的，除了在&lt;a href=&#34;https://github.com/yufree/xcmsrocker/pkgs/container/xcmsrocker&#34;&gt;xcmsrocker&lt;/a&gt;镜像里有模版脚本外，我还在GitHub上建了一个&lt;a href=&#34;https://github.com/yufree/pmdda&#34;&gt;仓库&lt;/a&gt;保存了本项研究中所有分析细节，包括所有论文里图片的生成过程，而论文在预印本期间就已被多次引用。实际上我现在所在的实验室就用的PMDDA工作流来给其他课题组测样，可靠的工作流要经得起公开透明的各种验证并方便迁移到其他人的研究环境里，但很遗憾很多研究看着很好却因为过渡依赖商用软件很难实现本地化与验证。&lt;/p&gt;
&lt;p&gt;如果有问题或对此感兴趣也欢迎随时联系我，我会提供技术支持。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>质谱组学数据的批次效应</title>
      <link>https://yufree.cn/cn/2022/01/27/ms-data-batch-effect/</link>
      <pubDate>Thu, 27 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2022/01/27/ms-data-batch-effect/</guid>
      <description>&lt;p&gt;组学数据的一个显著特征就是需要同时测定一个样本里成千上万的信号，这个信号可以是基因、蛋白或代谢物。但这是说给外行听的，真实数据对于基因组来说一个基因对应测定的可能是十几个碱基序列片段或测序信号，对蛋白组来说一个蛋白的数据需要从众多肽段重组出来（也可以直接topdown测定，勿杠），对代谢组而说一个代谢物能产生十几个包括同位素、加合物、寡聚体在内的质谱峰。但凡分析化学背景出身的研究人员都会特别关心测量上的质量问题，因为如果本来就测得不对，后面的统计方法或可视化方法再怎么天花乱坠都属于空中楼阁。&lt;/p&gt;
&lt;p&gt;不过，真实情况要复杂很多。就算一个样品只需要20分钟分析，50个样品需要加上空白与混合样进行质量控制，一般每10个样品就需要进一针空白与混合样，这样实际要进样60个，考虑上每次进样前后都需要一些混合样来稳定柱效，这一批样品大概要跑24小时。每天50个样品连续跑一周也就能跑三百多个真实样品，但凡用过色谱的都知道一根柱子跑上几千个样品后基本也就该换了，连轴转的实验室基本半年一换。而且，做过分析的都知道，连轴转要比断断续续做对仪器稳定性来说更好，但即使再好，同一个样品隔一周测一次也能看到明显的变化，有时候是信号逐渐减弱，有时候则是响应突然断崖式跌一个数量级。从我个人经验而言，即使连续进样的数据也会出现类似状况，更不用说应对那些队列数据。&lt;/p&gt;
&lt;p&gt;队列数据收集起来不是一次成型的，所以样品天然就存在不同批次。今天采血的大哥拔针偏慢，下次采血的大姐喜欢快速收针，很容易造成样品天然背景就不一样。这时候即便你把样品混到一起随机进样，看到的也是基线高高低低的总离子流。不过这都不算事，因为队列数据经常不是同一时间收集的，所以要么攒到最后一起测，要么凑一波几十个样先测，前者需要保证样品储存不能产生影响，后者需要保证不同时间分析的仪器出信号要稳定。巧了，这两个保证现实中一个都做不到，即使零下八十摄氏度保存代谢物也会出现差异（我在实验室验证过），而仪器前面说了，隔一周都不够稳就不用说隔几个月甚至几年了。在论文里大家都会说是稳定的或者换个方式说“尽最大可能保证稳定”，至于不稳定的部分，学术界目前都是回避讨论。所以，用流行病学样本里的分子证据来证实某种疾病的机理是很困难的，一般都需要实验室条件下用老鼠或细胞实验来反复验证。我称之为测不准原理，不懂测量分析的研究人员容易沉浸于理论的逻辑自洽中，真实数据里的因果要比假设的复杂得多。&lt;/p&gt;
&lt;p&gt;好了，既然提到批次效应，我们就要想办法去掉这个影响。做目的性分析出身的人往往很不屑，因为他们有内标法。内标法一般是用稳定同位素标记过的物质在样品进样前加入，所有测到的目标物的响应都要去除内标的响应，这样如果分析上的波动是来自于仪器的不稳定，那么内标控制后的响应就可以排除掉这部分影响。不过内标法有一个前提，那就是标记物质与待测物在仪器上响应因子要一样，原汤化原食。但到了非目的分析，开全扫描模式，你根本不知道待测物是什么，此时加入内标并用内标矫正所有峰响应就属于胡闹了。不同物质的离子化过程是有差异的，在同一样品基质下，有些是信号增强，有些则是信号减弱，你要是用信号减弱的校准信号增强的，那么增强的信号就更强了。我到现在也不是很明白为啥有些非目的分析的研究中还在用内标，不是说内标没用，而是内标能校准的东西太少了。&lt;/p&gt;
&lt;p&gt;真正有用的混合样，也就是每一批次的样品都取出10微升来混成一个样，这样的样品应该会包含所有样品里的物质。那么，我们只要隔10个样品进一针混合样，然后就可以得到混合样里每一个物质的变化趋势了，然后只需要将这个变化趋势去除掉，剩下的就是样品物质的真实生物学变化了。这里需要注意的是这个变化趋势要是明显的才有矫正的意义，举例来说A物质在整个进样序列里的混合样中一直下降，那么对多个混合样里A物质的响应与其进样顺序做回归分析就会看到一条斜率为负的回归线，此时代入样品的进样顺序就会给出在这个回归线上的响应。所谓去除批次效应，就是用样品的真实响应去扣除掉进样顺序回归产生的响应。因为每个物质的质谱响应行为不同，所以回归线也不一样，有线性的，也有非线性的，但这个回归线的物理意义就是来自于进样顺序或批次的影响。因此，如果你进样序列里有混合样，那么后面发现批次效应就可以用这种方式进行原汤化原食的矫正。&lt;/p&gt;
&lt;p&gt;不过，有些实验数据过于莽撞，根本就没加入空白或混合样，这样有没有办法进行矫正呢？也有，但不全有。如果我们知道样本分组的话，那么数据还有救。所谓的样品，不过是一堆存在异质性的数据，如果我们的目标是寻找样品分组差异，那么所有不同于分组差异的显著性趋势都可以归类为某种批次效应。这样对多个物质而言，我们就先构建一个模型：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$响应 = 分组差异 + 其他差异$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这样我们就得到一个其他差异的初始值矩阵，然后我们对这个其他差异的矩阵进行主成分分析或svd分解，这样就可以提取出其他差异中的主要趋势，然后我们构建下面的模型：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$响应 = 其他差异主成分+分组差异+剩下的其他差异$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;拟合这个模型，然后继续在剩下的其他差异中找主要趋势，这里需要设计一个显著性的统计量，当统计量不再显著后就停止寻找，此时的模型就成了：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$响应 = 分组差异+其他差异所有主成分$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;此时，我们只提取分组差异就可以了，这个迭代过程就是替代变量的构建过程，这个方法可以在没有混合样但知道样品分组的条件下进行批次效应控制，效果非常好。那有朋友就会问了，如果我不知道分组差异呢？那我反问一个问题，你怎么知道存在批次效应呢？此时样品的波动究竟是来自于批次效应还是样品本来的差异根本就没法知道。如果你确定来自于批次效应，那么其实直接构建下面的模型：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$响应 = 生物学差异+批次$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;就可以了，但你至少也要知道批次或进样顺序，如果都不知道，我甚至都不知道你有什么理由认为存在批次效应。对于非目的分析，批次效应的难点在于不同物质的效应不一样，没办法用单一方法进行矫正，但这不代表盲目矫正就是对的。通常而言，如果你已经看到了批次效应，那么这个趋势就应该是可建模识别的，否则你根本说不清楚究竟是批次效应还是样本异质性导致出的现象。现在出现了很多自动化的批次效应检测矫正工具，且不论其中大部分都是重复造轮子，很多工具设计者本身就没有理解批次效应的复杂性简单进行函数套用来自动化决策过程，这种工程思维对软件开发是没问题的，但对科研是很不负责的。话虽这么说，这样的事其实我也没少干，这也是最近我在反思的问题，工具永远不能替代思考，否则人会陷到机械流程中无法自拔。&lt;/p&gt;
&lt;p&gt;此外，现在很多研究人员会只做目的性分析的代谢组学，也就是只去测定已知代谢物的数据并进行讨论。其实这是个聪明的选择，因为非目的分析通常会被物质鉴定卡住，做到最后其实也只能给出已知的在谱库里的数据，这只是另一种形式的目的性分析。不过，组学数据如果最后只能用已知信息来给出结论，那么额外测定的信息其实毫无价值。这可以部分解释为什么基因组或测序总能发现新东西而做蛋白的都喜欢光源冷冻电镜，代谢组学则始终在方法学层面打转，不是小分子里没有生物信息，也不是提取不出来，而是提取出来数据库里没有就给扔了。而且数据库匹配现在也是玄学，很多化合物标准品的谱图长一个样，放到生物样品里加上基质效应加上不同仪器状态就变另一个样了，考虑到代谢组学样品前处理跟没有差不多，就算你测到了已知物可能都无法匹配出来。&lt;/p&gt;
&lt;p&gt;这些现实存在的问题都需要解决，只进行“聪明的选择”永远也回答不了最难的科学问题，只是现在的我有点怀疑：是不是有些东西就是永远也测不准呢？如果是的话，那科研就一定要想办法与这种永恒的不确定性共存并寻求发展，这也是过去两三百年科学家们一直在做的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fountain 标记语言</title>
      <link>https://yufree.cn/cn/2021/10/25/fountain/</link>
      <pubDate>Mon, 25 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/10/25/fountain/</guid>
      <description>&lt;p&gt;这是去年留的一个坑，眼看着挖土填是来不及了，就灌水来填了。&lt;/p&gt;
&lt;p&gt;标记语言里最经典的是 HTML，但最出圈的应该是 Markdown。所谓标记语言，可近似理解为纯文本+标记，而标记是用来定义文本的结构、格式或属性。一款标记语言要有自己的标记环境，例如 HTML 喜欢用一对尖括号来进行标记，尖括号里的内容则指示了特定的结构、格式或属性。例如 &lt;code&gt;&amp;lt;p&amp;gt;段落&amp;lt;/p&amp;gt;&lt;/code&gt; 就对纯文本&amp;quot;段落&amp;quot;进行了标记，标记内容为&lt;code&gt;p&lt;/code&gt;，也就是段落的意思。&lt;/p&gt;
&lt;p&gt;绝大多数现代浏览器能识别 HTML ，把标记内容按一定样式可视化出来，当然样式也是可以自己定义的。但HTML或CSS对习惯读纯文本的人而言写起来还是比较繁琐的，所有就有了 Markdown 这种简化版标记语言，把成对尖括号这种反人类设计改成更直观的井号、星号还有方括号这些标记，然后开发了一个引擎，把这些标记翻译成 HTML 这类已经有固定标准的标记语言，最后被浏览器渲染成网页。&lt;/p&gt;
&lt;p&gt;我猜测最初 Markdown 的开发者只想提供一个极简的标记语言，因此只转译了例如标题、加粗、斜体、引用、列表、链接等很少的 HTML 标记，用20%的努力解决了80%的场景。因为最终展示的也是 HTML ，所以如果写文档的人想要更多的功能，可以在 Markdown 直接混写 HTML 标记语言，反正最后也都能展示出来。&lt;/p&gt;
&lt;p&gt;不过 Markdown 后来因为其简单易用出圈了，原来只有前端工程师才涉及的标记语言现在几乎各行各业的人都在用。学习 Markdown 可能几分钟就能搞定，但用起来感觉神清气爽。如果一个文字工作者被 Word 与各种在线编辑器折磨一段时间后，那么他不是已经用上了Markdown，就是在寻找这类工具的路上。例如我学Markdown完全是因为早就习惯了另一种标记语言：Emacs 内置的 Org-mode，我曾经用这玩意写过一段时间的日记，当然叫它日记算是抬举它了，一年也没几篇，后来因为电脑硬盘挂了就彻底没了。印象中 Org-mode 写维基特别好用，但后来转到 Markdown 也无外乎发现了我其实能用到的功能并不多，先写起来比纠结学哪个工具更重要。另一原因则是不管 Vim 还是 Emacs，我都因为用得少记不住快捷键，此时找个不依赖编辑器又简单易用的标记语言就成了自然需求。&lt;/p&gt;
&lt;p&gt;当然，但凡提标记语言就不能不提另一个山头：TeX。这个可以理解为排版用的标记语言，不同于转换为网页，TeX的终点是排版好的dvi文档，当然现在更流行的是PDF文档。这里就不多说 TeX 那一坨发展史了，反正本来就是某研究人员不爽传统排版而自己搞出来的标记语言，其发展初期长期被学术鄙视链顶端率先接触计算机的物理与数学研究人员采纳，而其他学科普通研究人员喝到计算机技术外溢到学术投稿系统的汤时，满大街都已经是微软 Office 的天下了。&lt;/p&gt;
&lt;p&gt;我接触 TeX 应该是读了王垠的一篇博文，不得不说这是个超级大坑，但也得承认 TeX 标记语言如果搞明白了，软件编译流程基本也就能想明白了。不过 TeX 终究也没火起来，它对标的是所见即所得的全功能编辑器，学习门槛比 Markdown 高太多。现在就算是研究人员大概也就还是搞数学跟计算机相关的学科出于对公式编辑器的喜爱还在用，再有就是对排版要求极高的个人，而出版社则反而不用，他们早就被 Word 给驯化了。&lt;/p&gt;
&lt;p&gt;不过，自从谢大的 knitr 包横空出世，编程语言与标记语言的混搭却意外得到了很多关注。早期 R 语言社区存在 Sweave 文档，此时混搭的是 R 与 LaTeX 文档，但前面说了，LaTeX 这个坑一般人是不愿意跳的，谢大的 knitr 包则选择了 Markdown 作为标记语言，这样一下就把门槛放平了。这里要说明的是 knitr 包很多人认为就是实现 RMarkdown 文档的编译，其实不全面，knitr 包也支持 Rnw 这种用 LaTeX 做标记语言的文档的编译，而且 knitr 包也不仅仅支持 R，其他编程语言配置好了也能进行文档编译，反而是针对 R 与 Markdown 的部分单独被 rmarkdown 包承包了。而 Python 社区现在则基本都是 Jupyter notebook 的天下了，当然 Jupyter 也支持 R、Julia等编程语言与 Markdown 作为标记语言。&lt;/p&gt;
&lt;p&gt;当然，有编程语言与标记语言的混排需求的人还是少数，更多人直接选了Markdown 编辑器来作为日常写作工具。例如我现在想记一些不涉及编程的东西最先开的是 Typora 这个编辑器，很多人也喜欢用支持 Markdown 的笔记软件。好了，终于要到 Fountain 标记语言了。&lt;/p&gt;
&lt;p&gt;我第一次知道 Fountain 标记语言是在Joplin这个笔记软件的选项里看到了其对这种标记语言的支持，当时就出于好奇查了一下，发现这是专门给编剧写剧本的。但我显然不是编剧，毕竟生活这出戏不用编就已经足够荒唐魔幻了，用不着虚构。但仔细了解了 Fountain 标记语言后，我发现这个格式对一个场景非常有用，而恰好这个场景也可能会是很多人可能用到的。&lt;/p&gt;
&lt;p&gt;首先允许我来个现炒现卖，剧本其实是个很神奇的排版方式：一页典型剧本开头要有场号，然后要有这场戏的梗概，之后就是内容，例如两个人的对白或转场。每一页英文标准剧本会严格限制字体与纸张大小，大概对应一分钟的戏。好了，知道这些就够了，毕竟我们也不是这个专业的。&lt;/p&gt;
&lt;p&gt;这里神奇的地方在于一页纸对应一分钟的戏，那么对于研究人员来说就可以用来准备讲稿，特别是学术会议。其实写还是按原来的写，Fountain 标记语言跟Markdown几乎一致，只不过是限制的更多，这样你导出的PDF文档每页大概就是一分钟的量，15分钟的报告也就需要15页稿子，有点像高中语文的1000字稿纸，方便练习。&lt;/p&gt;
&lt;p&gt;其实我去年开会就尝试过，稿子写了一上午，结果练习一次就发现自己实在是不习惯念稿，我更习惯看着幻灯片讲，一看稿子就分神。不过这可能是我个人问题，练习十遍能讲出十个版本，加上讲稿就成了十一个，我个人做报告会进入一种精神高度集中状态，经常临时给出新的想法或者故事线，练的多后反而对熟悉的东西没有兴致去讲好，影响报告效果。所以我这里推荐 Fountain 标记语言但实话说我自己反而用不好。&lt;/p&gt;
&lt;p&gt;另一个适合Fountain标记语言的场景是现在视频制作时的文案，固定字体与纸张大小后就会固定视频长度，而且可以利用剧本的转场或场景转换来让视频内容的丰富些，也就是把叙事从文字陈述改为文字陈述加上场景转换加上现场布置加上动作，这样文案就会更立体些，我称之为立体叙事或镜头叙事。这样的镜头叙事对于传达信息会更生动些，对于老师备课也有启发。这些东西落实到剧本上，体验感与互动感会比单纯的文案有所增强。只要保证好一页纸对应一分钟的内容，创作也会更好把握节奏。同样，我也尝试过，但也发现我根本不适合准备文案，更适合临场发挥。这里只是分享给有需要的朋友。&lt;/p&gt;
&lt;p&gt;这里需要说明的是你需要专门的软件来支持Fountain标记语言的实时预览来控制创作节奏，可以考虑用joplin，毕竟专业的剧本软件都是收费的。另一个问题就是Fountain标记语言的节奏控制是针对英文的，切到中文每一页不一定对应一分钟，但只要你固定了字号与格式，基本也可以用。&lt;/p&gt;
&lt;p&gt;另外，我也推荐去读下剧本，这样可以更好体会编剧对于镜头语言的控制，中文的有个花生剧本可以免费用但不支持Fountain标记语言，好处是有不少经典电影剧本可以看。我不是搞编剧的，但很明显编剧的镜头语言与场景控制能力在其他行业里也可以用得上，至于如何用或者用好，自己去体验下就知道了。&lt;/p&gt;
&lt;p&gt;其实我到最后也没具体去聊Fountain标记语言的语法，这不重要，都是给个关键词10分钟就能搜索掌握到的。但要是掌握了剧本这种创作形式，可能打开一个新世界。&lt;/p&gt;
&lt;p&gt;ONE PIECE 就在新世界的尽头。淡出～&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>看门人分子</title>
      <link>https://yufree.cn/cn/2021/06/17/gatekeeper/</link>
      <pubDate>Thu, 17 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/06/17/gatekeeper/</guid>
      <description>&lt;p&gt;今年ACS春季会议我做了一个口头报告，提出了“看门人分子”的概念。这大概是我又双叒叕挖坑的一个开端，现在预印本已经放到&lt;a href=&#34;https://chemrxiv.org/engage/chemrxiv/article-details/60c9e3b13fc2cb674c000d4e&#34;&gt;网上&lt;/a&gt;了，这里做下解释。&lt;/p&gt;
&lt;p&gt;这里我关注的科学问题来自于污染物暴露研究，如果一个人暴露给一种污染物，那么就存在导致某种健康状态的可能。在暴露组学研究中，污染物暴露通常作为某种疾病的预测变量，也就是有下面的关系：&lt;/p&gt;
&lt;p&gt;$$健康 = f(污染物，协变量)$$&lt;/p&gt;
&lt;p&gt;然而在代谢组学中，我们会认为健康状态是由代谢物水平变化直接导致的，也就是下面的关系：&lt;/p&gt;
&lt;p&gt;$$健康 = g(代谢物)$$&lt;/p&gt;
&lt;p&gt;这里没有放协变量是因为很多协变量在分子水平上本就可以被某些代谢物来指示，例如性激素水平大概就能指示性别，此时你把性别放到模型里会造成共线性，降低模型稳健度。&lt;/p&gt;
&lt;p&gt;观察这两个等式我们会发现，如果代谢物本身可以体现健康状况，那么污染物也就有可能是通过影响代谢物来影响健康状态。注意，此处代谢物是可以被替换为其他来自人体样本的分子信息的，例如蛋白质、组蛋白、基因甲基化水平等。但本质上是一个环境信息传递到分子生物学信息再传递到健康效应的信息流。考虑上遗传作用也就是：&lt;/p&gt;
&lt;p&gt;$$环境暴露/遗传 \rightarrow 内源响应 \rightarrow 健康$$&lt;/p&gt;
&lt;p&gt;此时我们会发现，遗传影响与环境影响其实都会体现在生物样本的分子信息里，当这三部分都能被观察到时，我们应该可以从分子水平解释环境或遗传对健康影响的机理。遗传那部分不是我关注的，后面只讨论环境影响，这里放上遗传是为了让这个模型逻辑上通顺些，实际不同健康状态的主导影响也是不一样的。&lt;/p&gt;
&lt;p&gt;环境流行病学会关注那些受环境影响更大的疾病，但描述暴露与疾病关系却通常是毒理学家在做，这里经常出现毒理学上证明有影响但流行病学数据看不出来（大多数）或者流行病学看到了影响但毒理学证据不充分（少数）。这里的不一致在我看来主要是因为毒理学所采用的控制实验体系过分简化了环境暴露，很多毒理学上用到的剂量现实生活中很难出现，或者说虽然剂量水平相当，但因为缺乏暴露途径或存在其他拮抗暴露影响的机制。更重要的在于很多时候评价指标如果太过宏观就会不够灵敏，而描述暴露跟健康的数值都可以算相对宏观的指标。&lt;/p&gt;
&lt;p&gt;因此，要搞清楚环境究竟如何影响健康，我们最好是直接测量人群样品的分子水平信息，这就是代谢组学跟蛋白质组学还有表观遗传组学在做的事。但人群样本不可能做到随机化，我们不太容易拿到个体水平的环境信息，例如我可以知道A地当天室外空气细颗粒物浓度，但不太容易拿到某个人当天暴露的细颗粒物浓度，最简单就是如果这个人自己在家做饭，那么他的细颗粒物暴露量是远高于室外浓度的。&lt;/p&gt;
&lt;p&gt;其实，外在暴露的变动范围其实是远大于内源响应的，室外温度可以从零下几十度到四五十度，但人的体温波动不会特别大。从这个角度出发，体内的代谢物分子可以分成两类，一类是对暴露敏感的用来响应与感知外界变化，另一类则是不敏感的用来维持生命系统。可以预想到，人体代谢平衡或者主要代谢通路更多是不敏感的那一类分子在维持，如果暴露真的造成了健康状态改变，那么肯定是先影响敏感分子然后突破敏感分子的信息传播阈值影响到了不太敏感的主代谢通路。那么，这部分敏感的分子就有必要找出来。&lt;/p&gt;
&lt;p&gt;这里我们就会发现一个问题，当进行组学研究时，我们是不预设敏感度的，也就是仪器能测到啥我们就报道啥，但这里就涉及一个统计学多重检验的问题，我们会在完全不敏感的代谢分子上浪费大量的统计功效。毕竟现在错误发现率的控制都是基于检验数的，检验数越多，判断出现差异的要求就越高。例如我去进行污染物A对代谢组影响的研究，结果测到了1000种代谢物，然后要做1000次假设检验才能知道哪种代谢物跟污染物有关，此时就要做错误发现率控制。不过其实从一开始我们就知道不会有太多出现差异的，但这部分知识没法反应到数据分析方法里。&lt;/p&gt;
&lt;p&gt;现在常用的一个做法就是降维或聚类，把趋势类似的代谢物组合为一个新的潜在变量，然后去跟污染物进行相关分析。且不论到头来你怎么再把影响回溯到具体的物质，这个思路并不解决我们收集了一大堆无关信息。我的想法就比较简单直接，在进行污染物与代谢物之间分析之前，先对代谢物内部相关性进行分析。在设定的相关性阈值之下，代谢物会形成代谢网络，绝大多数情况你会看到一个主干网络与零星分布的小代谢网络，这里我们把这类代谢物叫做大陆代谢物，但更多的代谢物是独狼或孤岛，根本无法与其他代谢物产生任何联系。此时，生物信息的流动是构建在代谢网络上的，因此我们只关注大陆代谢物。这是一个基于概率的假设，不可否认会丢掉一些有用信息，但却能显著降低研究难度。&lt;/p&gt;
&lt;p&gt;从我做的案例来看，常规代谢物检测在一个较高相关性阈值下90%以上的代谢物属于孤岛，这样我们通过区分孤岛与大陆就实现了一次基于信息含量的降维。而面对剩下的代谢物，我们会逐一检查其与暴露物的相关性。可以预见，并非所有代谢物都会与暴露物有关系，这样我们就能筛到直接与暴露物打交道的代谢物。因为这些代谢物一方面跟污染物有联系，另一方面又跟其他代谢物有联系，那么很有可能污染物的信息流的接受者就是他们。换句话说，这些代谢物是污染物影响代谢网络的最前线，所以我给他们起名为看门人（gatekeeper）分子。下面是一个简单的看门人分子示意图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yufree/presentation/gh-pages/figure/gkc.png&#34; alt=&#34;gatekeeper&#34;&gt;&lt;/p&gt;
&lt;p&gt;理论上会有两种看门人分子，一种是直接横在暴露物与代谢网络之间的，另一种则是互相为看门人分子。但不论哪一种，看门人分子都应该属于对暴露物比较敏感的，而且在真实数据中，我发现看门人分子可以响应不止一种暴露物。这个发现就说明代谢物层面有可能存在一个相对通用的看门人机制，其实本来环境毒理学就有外源代谢通路的说法，但如果能从小分子层面描述可能更有利于阐明影响机制。&lt;/p&gt;
&lt;p&gt;不过看门人分子只处理了环境暴露跟代谢物的关系，我们最开始的那个问题，也就是影响健康的机制还没有讨论，只是把讨论范围缩小到了具体的代谢物上。在我做的案例中，环境暴露无法直接发现跟健康的关系，但当我用这些暴露物的看门人分子来研究与健康的关系时，可以直接观察到看门人分子与健康状态的关系。也就是说，看门人分子要比直接用环境暴露研究健康影响更灵敏。这倒不难理解，毕竟环境影响的测量不确定性本来也比代谢物测量不确定性要高。&lt;/p&gt;
&lt;p&gt;其实看门人分子这个概念的本质还是基于网络分析的，基因组里比较流行的WGCNA这个包的核心也是发现网络结构后对每个独立网络结构做svd分解，然后取第一个主成分来代表网络进行下一步分析。不过，看门人模型从一开始就是直接针对具体的代谢物来建模的，通过筛选网络结构来找出有实际意义的代谢物，形成假设方便后续检验。也就是有下面的关系：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yufree/yufree.cn/master/static/images/EMDG.png&#34; alt=&#34;EMDG&#34;&gt;&lt;/p&gt;
&lt;p&gt;这个分析方法的重点就在于我把代谢组限定到了看门人分子这个子集，这些代谢物可能更适合进行环境影响相关的疾病研究（起码初步的结果是这样）。同时，看门人分子不仅仅可以是小分子代谢物，还可以是蛋白质或其他生命大分子。这样看门人分子实际就成为了可以把各种组学连在一起的概念，而且这部分分子要比其他分子对环境因素更敏感，更可能是健康防护的第一条防线。&lt;/p&gt;
&lt;p&gt;关于这个概念我已经做过四次报告了。从反馈上看，做生物信息的把这个方法看成了利用信息量降维的手段；做统计的第一反应是这玩意怎么看着像因果分析；做生命科学的认为这个概念比花里胡哨的分析方法简单，容易设计实验验证；做流行病的则强调一定要把协变量给考虑进去且考虑其在预防医学上的意义，特别要考虑灵敏度问题；做环境的更关心能不能直接给出一个基于人体代谢物的看门人分子数据库。我之所以费劲写这篇介绍就是因为预印本被很多不同背景的人改的一言难尽，所以干脆拿母语把核心概念重写出来算了。&lt;/p&gt;
&lt;p&gt;从我个人角度跟能力，其他方向我做不了但我乐意与之交流，学科间应该有更多的互动而不是互相睥睨。不过，这个概念倒是有可能跟我之前所说的反应组学进行联动。当前的网络结构是利用相关性来构建的，但基于质量差的反应组也可以用来构建网络，而且解释性可能更好些。其实我并不喜欢炒作概念，主要原因是很多新概念只是构建了学科间的壁垒，但看门人分子这个隐喻却有利于学科间的融合，唯一的问题就是当前研究人员是否乐意打破学科壁垒了。&lt;/p&gt;
&lt;p&gt;ps. 我本来起的名字是守门员（goalkeeper），至于为啥改成看门人，那又是个关于政治正确的故事，不提也罢。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>jupyter与容器技术</title>
      <link>https://yufree.cn/cn/2021/01/28/container/</link>
      <pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2021/01/28/container/</guid>
      <description>&lt;p&gt;我们现在评价科研成果，经常喜欢用当前的共识这个表述，但前沿领域的共识不见得是对的。更严格说，所有涉及观点站队问题的讨论都不属于科学讨论，面对质疑经过合格训练的科学家都不会简单采信某一方观点而是要去看原始数据或在自己的实验室重现，让事实说话要比空对空有意义的多。不过重复实验在现代社会能不能拿到经费是存疑的，在当前经费获取的方式下，重复研究别人的成果是费力不讨好的，更多人重复他人实验是为了基于此进行进一步研究，如果重复不出有些课题组会尝试其他路径，有些就去联系原作者询问实验细节。&lt;/p&gt;
&lt;p&gt;我博士阶段末期就收到过关于一组植物愈伤组织实验重复的询问，当时我特意去重复了实验还发了操作过程照片过去，结果那边还是重复不出来。我到现在也搞不清楚是哪里的问题，因为我重复后结果跟发表的论文是一样的，但对方咋做的就不知道了，而且他那边其实对我的体系进行了本地化修改，暴露物也换掉了，但在我看来应该不影响结果。因为这段往事让我意识到可重复性里面可能包含了质疑者与原作者都没意识到但很重要的步骤，而这个步骤直接导致了重现性不好，这就是真实科研或者说实验学科会遇到的问题。好比一份菜谱给出来，两个厨子炒出了两份口味不同的菜，如果菜谱符合要求，那么一定是存在菜谱外的东西影响了结果。这种情况在化学实验里不常见，因为化学实验体系通常比较简单，但生物实验就非常常见了，同样成分的培养基经常一个能养活细胞，另一个养啥死啥，这也是为什么生物论文实验里用的材料是要标注厂家信息的，因为很多实验对材料的要求只能用厂家品控来保证重复性。&lt;/p&gt;
&lt;p&gt;实验的可重复性眼下可控性里操作空间还是有的，但眼下实验完成后的数据处理与共享部分如果透明化，那么会极大挤压学术不端的空间。虽然同样一组数据在不同的分析方法或统计模型下可能得到完全不同的结论，但只要你能把数据如何分析的过程及原始数据共享出来，那么也是可接受的。打比方要做一组高维数据的机理与预测模型，你用随机森林选出一组重要变量并基于此构建了逻辑通顺的机理模型，隔天用同样的数据别人用线性混合模型又选出一组变量也构建了逻辑通顺的机理模型，但这两个模型降维上用了完全不同的策略与假设，导致两组模型虽然都说得通但都可能只解释了真相的一部分，这个也属于常见现象。&lt;/p&gt;
&lt;p&gt;当前期刊一般都会要求上传原始数据，但这是不够的，博士阶段我读了一篇论文看到了里面一种计算方法很有意思，但他们给的是数学公式，我是在matlab上重现后才能去验证。这个操作对于实验学科的人而言门槛过高，绝大多数实验学科研究人员的数据分析水平不会超过调用别人写好函数处理数据的阶段，指望自己写需要自学很多东西，这虽然应该是合格研究人员应具备的素质，但难度还是有的。我现在读很多论文可以感到明显的割裂感，就是实验部分与数据分析是分工来做的，这就导致很多描述非常不准，例如简单利用p值来说明结果而意识不到p值本身的问题，很多数据分析方法描述非常奇怪，明明一两句就能说清楚但却自己定义了一大堆东西来绕弯，很明显对分析方法原理没搞清楚。&lt;/p&gt;
&lt;p&gt;这属于无效协作，实验方与数据分析方想按照分工原理来提高效率，但最后展示出来的则是一团乱麻。一篇论文一定要有一个人能同时理顺实验与数据分析的所有步骤，这看上去很不合理但没办法，从我跟单位里所谓专业统计分析人员打交道的体验来看，那种强调要把数据做成他们那种行是样品列是特征的标准格式的要求是荒谬的，真实数据要转化到那种标准格式是要进行大量假设的，这部分实验方通常不了解，数据处理方又不管，最后给出的结果常常导致实验方无法验证而数据处理方又对数据中普遍存在的异常值与确实值大为恼火。在我看来根本就不能割裂开实验与数据分析，这两步需要同一个人来做，而且职业做实验的一定会被自动化仪器取代而职业处理标准数据分析的也一定会被自动化软件取代，唯独互相连接的人是无法被技术取代的。&lt;/p&gt;
&lt;p&gt;当前的解决方案对于实验人员而言就是保留完整的数据分析脚本，这个本来很正常的需求因为图形化商业软件的流行而被认为很不友好。说句不好听的，这就是被图形界面给惯坏了而忘了科研中对重复性的要求，而且大多数专业图形界面的数据分析软件其实也会记录操作步骤，你的每一步点击都会在一份记录中被保留，所以数据分析步骤与图形界面并不矛盾。不过从实际数据分析角度，如何给出脚本确实是个技术问题而jupyter项目则在一定程度上解决了这个问题。&lt;/p&gt;
&lt;p&gt;在介绍Jupyter项目之前，我想说如果你的数据分析完全依赖 R 与 Python，那么自带 RStudio 服务器版的 Rocker 镜像配合 Rmarkdown 文档就已经可以实现数据分析的完全可重现性了，甚至 Rmarkdown 文档本身就可以作为完整数据分析步骤的良好载体而附加在论文附件里来保证可重复性。当然如果你懂一点R包开发，把分析方法作为模版嵌到一个R包里也是没问题的。今天想说 jupyter 项目，纯粹是因为我最近考古发现现在 jupyter 已经从 Python 的轻量级在线开发环境成长为多语言支持的平台了，其部署上也非常容易。当然，其对学术写作生态的支持对比 Rmarkdown 的生态还是弱了非常多，更偏探索，当然依赖pandoc的核心都可以互相转换，但感觉学术界，特别是实验学科目前对R的接受度更高，毕竟学术数据分析所需要的统计工具 R 基本都有现成的，Python 在这方面虽然机器学习的包更全，但实际研究里需要的工具就不完整。R 包的社区里存在大量即懂专业知识又懂统计分析的人，会给出很多研究人员直接用的函数，当然很多开发者并不太在意效率问题。Python的社区里也有科研人员，但实验学科的不多，整体社区偏软件工程偏通用计算问题。不过理想状态是两种语言都掌握，一种做到开发级，另一种做到应用级对于绝大多数科研数据分析问题就都能处理了。&lt;/p&gt;
&lt;p&gt;Jupyter 项目最开始就是专门为 python 设计的，后来逐渐发展壮大，可以支持更多的语言，前提是你要把对应的核装上保证交互通信畅通。这里顺道也捋一捋 Python 的安装问题，现在人装软件都是全家桶，你单装一个 Python后面一样有大量的依赖问题，因此大多数都是去装一个anaconda，这是基于Python的数据处理和科学计算平台发行版，但其实也支持R。可以把 anaconda 理解为 TeX Live 之于 TeX 排版系统的关系，作为发行版，基本要囊括编程语言本身、集成开发环境（IDE）、常用包及包的管理器。例如，TeX live里就会打包排版引擎、参考文献处理引擎、文档格式转换、字体、常用宏包、包管理器 tlmgr、文档编辑器 TeXworks 等几乎所有你可能用到的工具与文档。anaconda 里你可以装 Jupyter notebook作为IDE，也可以装PyCharm作为IDE，甚至可以装RStudio，其包管理用的是 conda，用法上类似 Python 的 pip 安装，但 conda 不仅仅支持python，你是可以用 conda 来装 R 包的，而且其解决依赖问题也比较智能（pip其实也可以做到）。不过，因为 anaconda 本身也是个公司，有付费产品，免费产品界面也有点花哨，所以很多有洁癖的人会选 miniconda 这种精简版。&lt;/p&gt;
&lt;p&gt;Jupyter 经典版就是交互式笔记本，也是最早得以流行的核心功能，用户可以在代码块里写代码，然后运行代码块直接看到结果。熟悉 R 的会发现这跟 knitr 的功能类似，不同点在于 knitr 对于代码块的控制更多，侧重一次编译出带结果的成品文档，而Jupyter notebook 侧重实时输出结果或再现结果，更接近 REPL 但不算是个好的开发工具。在实际写文档时，相信多数人会选择调用包里的函数而不是现写一个，所以 Jupyter notebook 在交互要求高时也还算不错。虽然大多数人用 ipython 作为解释器，但通过 IRkernel 这个R包并初始化后其代码块也可以执行 R。在 RStudio 里，也可以创建类似的 R Notebook，并用 reticulate 包来使用 python。Jupyter 笔记本是完全采用网页界面形式进行交互（其实 RStudio 也是），笔记本的下一代产品是 JupyterLab ，这个界面更接近一个全功能的IDE了，也是基于 notebook的，可以装各种插件来提高效率，所以现在上手可以直接从 JupyterLab 开始。&lt;/p&gt;
&lt;p&gt;Jupyter项目中最吸引我的是Jupyter hub，前面的在线笔记本是一个人用的，Jupyter hub可以将笔记本发布到网上供多人使用。在R里面我们做网络应用一般用Shiny，后台跑的是R，如果借助Jupyter hub，我们也可以把一个交互式应用放到网上，这里可以用带有Jupyter hub的docker镜像进行快速部署，也可以借助k8s部署到集群上。如果你只打算在一台服务器上做一个轻量级的在线应用或分享一个笔记本，用户不超过100人，可以直接用The Littlest JupyterHub 来部署，这个应该是个很好的教学工具平台，不过R里也有learnr包作为对比。&lt;/p&gt;
&lt;p&gt;另一个值得关注的项目是binder，在这个项目里你可以直接分享一个笔记本，在这个&lt;a href=&#34;https://mybinder.org/&#34;&gt;网站&lt;/a&gt;，只需要告诉binder你的 Github 库地址就可以，当然这个库里得有笔记本。这个项目相当于给了公共计算资源，你的GitHub笔记本会被生成一个 docker 镜像，然后借助dockerhub展示给用户，这个项目目前是免费的，也支持R，请不要滥用。&lt;/p&gt;
&lt;p&gt;有了jupyter项目与容器技术，科研数据分析的流程就可以实现在线化与可重复性，其实如前所述单纯R的生态也可以做到。这样研究成果发表时应该同时附带对应的数据分析流程报告且最好这份报告也支持在线验证，这样就很容易从技术上堵掉图片误用这类说不清道不明的漏洞。上传原始数据并同时上传数据处理脚本，所有处理过程都让电脑来完成，这样审稿只需要关注处理方法是否合理就可以了，因为这里面从数据到结果没有可以人为干涉的空间。我相信重要的发现一定是可重复的，那么起码要保证数据到结果之间100%的重现性。&lt;/p&gt;
&lt;p&gt;在验证学术问题上，实验数据比专家口水更管用，代码自动化重现要比手动点击靠谱。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>诺谟图</title>
      <link>https://yufree.cn/cn/2020/04/08/nomography/</link>
      <pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2020/04/08/nomography/</guid>
      <description>&lt;p&gt;诺谟图（Nomography），又称列线图解法，于1884年由Philbert Maurice d&amp;rsquo;Ocagne (1862-1938)发明来用图形化计算解决复杂函数的求值方法。随着计算机数值计算的发展逐渐淡出人们的视线，但有必要将其拎出来重新讨论一下。一方面是重新演绎直观计算过程有助于消除科学的神秘性并增加趣味性，另一方面则是我个人的一种直觉：诺谟图似乎要在科学的某些领域复活并展示其巨大的威力，此外诺谟图的设计本身就是一种艺术创作，甚至同一个方程会出现两种完全不同的设计方式。这里我先大概翻译转述下Ron Doerfler 写的一篇&lt;a href=&#34;http://myreckonings.com/wordpress/wp-content/uploads/JournalArticle/The_Lost_Art_of_Nomography.pdf&#34;&gt;论文&lt;/a&gt;里我能看懂的部分来讲下原理，然后扩展讨论下历史，最后讲下其在当前统计学里的应用。&lt;/p&gt;
&lt;h2 id=&#34;原理篇&#34;&gt;原理篇&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/ctof.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;最简单的诺谟图是上面的摄氏度与华氏度的转化图,其本质也是一个计算尺，其区别在于计算尺可通过多步计算来实现更多种类的方程计算而诺谟图则侧重于一步解决一个特殊的方程。其实诺谟图的一般形式是通过将含三个或三个以上的变量的方程中的变量用标尺表示，然后通过等值线（isopleth）将所有已知变量连接，这样未知的变量也就知道了。当然不同变量的标尺的位置是通过前期设计得到的，而这种表示方式要比3D图更为直观也易于接受。&lt;/p&gt;
&lt;p&gt;平行线式诺谟图是最简单也是最经典的一种多变量诺谟图，其基本形式与图式如下:&lt;/p&gt;
&lt;p&gt;$$f_1(u) + f_2(v) = f_3(w)$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/mfm.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;code&gt;$m_1$&lt;/code&gt;，&lt;code&gt;$m_2$&lt;/code&gt;，&lt;code&gt;$m_3$&lt;/code&gt;代表的是比例因子，在一条等值线穿过这三条平行线后，根据相似三角形原理我们可以建立如下方程:&lt;/p&gt;
&lt;p&gt;$$\frac{m_1f_1(u)-m_3f_3(w)}{a} = \frac{m_3f_3(w) - m_2f_2(v)}{b}$$&lt;/p&gt;
&lt;p&gt;整理后可得:&lt;/p&gt;
&lt;p&gt;$$m_1f_1(u) + \frac{a}{b} m_2f_2(v) = (1+ \frac{a}{b})m_3f_3(w)$$&lt;/p&gt;
&lt;p&gt;对照最初的基本形式，我们要做的就是消去与&lt;code&gt;$m$&lt;/code&gt;、&lt;code&gt;$a$&lt;/code&gt;、&lt;code&gt;$b$&lt;/code&gt;相关的部分，如下:&lt;/p&gt;
&lt;p&gt;$$m_1 = \frac{a}{b}m_2 = (1+ \frac{a}{b})m_3$$&lt;/p&gt;
&lt;p&gt;整理可得到下面的公式:&lt;/p&gt;
&lt;p&gt;$$\frac{m_1}{m_2}=\frac{a}{b}$$&lt;/p&gt;
&lt;p&gt;$$m_3 = \frac{m_1*m_2}{m_1+m_2}$$&lt;/p&gt;
&lt;p&gt;好了，我们现在知道了比例因子的关系，同时我们也知道想要画的图的大小与我们关心的变量变动范围，而根据已知变量&lt;code&gt;$f_1(u)$&lt;/code&gt;与&lt;code&gt;$f_2(v)$&lt;/code&gt;的变动范围与图的高度我们可以确定 &lt;code&gt;$m_1$&lt;/code&gt;、&lt;code&gt;$m_2$&lt;/code&gt;与&lt;code&gt;$m_3$&lt;/code&gt;，根据 &lt;code&gt;$m$&lt;/code&gt;的取值与图的宽度可以定出直线 &lt;code&gt;$f_3(w)$&lt;/code&gt;的位置，这样由于在图上&lt;code&gt;$f_1(u)$&lt;/code&gt;、&lt;code&gt;$f_2(v)$&lt;/code&gt;与&lt;code&gt;$f_3(w)$&lt;/code&gt;的坐标都是线性均匀分布的，现在就可以用等值线来进行求解了。&lt;/p&gt;
&lt;p&gt;也许看到这里你会说求个线性方程费这么大的劲是没意义的，但线性方程可不仅仅就这一种形式，原则上可以转化为线性方程的方程都可以用诺谟图来求解，例如下面的两种转化&lt;/p&gt;
&lt;p&gt;$$log(cd) = log c + log d$$&lt;/p&gt;
&lt;p&gt;$$log(c^d) = d log c$$&lt;/p&gt;
&lt;p&gt;其实，这样做的同时你也会发现标尺不再是均匀的了，但没关系，因为我们在绘图时依然可以采用均匀的标尺，然后附加一个对数表就够了，要是有耐心可以把标尺按原始数据绘制上去，这都不影响我们计算的准确性。&lt;/p&gt;
&lt;p&gt;上面的理论分析太过枯燥，下面我们用一个例子来展示在处理复杂的工程经验方程时诺谟图的绘制。方程如下:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$(1.2*D + 0.47)^{0.68}*(0.91T)^{3/2} = N$$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;我们关心的范围是&lt;code&gt;$1.0&amp;lt;D&amp;lt;8.0$&lt;/code&gt;与&lt;code&gt;$1.0&amp;lt;v&amp;lt;2.0$&lt;/code&gt;，而&lt;code&gt;$u$&lt;/code&gt;与&lt;code&gt;$v$&lt;/code&gt;的长度都设置为11cm而宽度设置为6cm，首先进行方程变形:&lt;/p&gt;
&lt;p&gt;$$0.68 log(1.2D + 0.47) + 1.5 log T = logN − 1.5 log 0.91$$&lt;/p&gt;
&lt;p&gt;然后根据图形长度范围与参数范围求解 &lt;code&gt;$m$&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;$$m_1 =\frac{11}{0.68 log(1.2(8.0) + 0.47) − 0.68 log(1.2(1.0) + 0.47)}= 20.73$$&lt;/p&gt;
&lt;p&gt;$$m_2 =\frac{11}{1.5 log 2.0 − 1.5 log 1.0}= 24.36$$&lt;/p&gt;
&lt;p&gt;$$m_3 = \frac{m_1*m_2}{m_1 +m_2}= 11.20$$&lt;/p&gt;
&lt;p&gt;根据宽度范围求解第三条线的位置&lt;/p&gt;
&lt;p&gt;$$\frac{a}{b}= \frac{m_1}{m_2}= 0.851$$&lt;/p&gt;
&lt;p&gt;$$a + b = 6$$&lt;/p&gt;
&lt;p&gt;$$b = 3.241 cm, a = 2.759 cm$$&lt;/p&gt;
&lt;p&gt;好了，我们开始画图。&lt;/p&gt;
&lt;p&gt;在D轴上我们的起始点是1.0，然后按照下面的公式在11cm的范围中求坐标&lt;/p&gt;
&lt;p&gt;$$20.73 *(0.68 log(1.2D + 0.47) − 0.68 log(1.2(1.0) + 0.47))$$&lt;/p&gt;
&lt;p&gt;然后画&lt;code&gt;$T$&lt;/code&gt;的范围，起始点依旧是1.0。坐标公式如下:&lt;/p&gt;
&lt;p&gt;$$24.36(1.5 log T − 1.5 log 1.0)$$&lt;/p&gt;
&lt;p&gt;最后我们距离D轴2.759cm处垂直基线画一条N轴，起始点通过下面公式求出:&lt;/p&gt;
&lt;p&gt;$$(1.2(1.0) + 0.47)^{0.68}*(0.91(1.0))^{1.5} = 1.230$$&lt;/p&gt;
&lt;p&gt;坐标通过下面公式求出:&lt;/p&gt;
&lt;p&gt;$$11.20*(logN − 1.5 log(0.91))$$&lt;/p&gt;
&lt;p&gt;OK,现在我们可以得到这张诺谟图了。使用的时候我们只需要知道任意两轴上的数做延长或相交，就可以知道第三轴的数了。这个过程完全依赖直尺比划就可以，类似于查表（不知道现在还教不教）。我们可以看出诺谟图在求解复杂方程上是很方便的，甚至可以求解一些无解析解的方程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/expnomo.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;除了平行线型图，N型图可以用来解决带有除法的公式：&lt;/p&gt;
&lt;p&gt;$$f_3(w) = \frac{f_1(u)}{f_2(v)}$$&lt;/p&gt;
&lt;p&gt;例如这种求圆柱体积的图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/nnomo.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;还有涉及四个变量的比例图，原理也是相似三角形。例如下面这种理想气体公式，你需要画两条线，先连接已知的两个变量，此时与中间对角线有个交点，然后连接交点与另一个已知变量并延长，就知道剩下的那一个变量是什么了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/pvnrt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;类似的还有正弦定理：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/sinx.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;利用相似三角形的性质，如平行、垂直、 还可以得到下面的图形：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/tri.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;还有一种图的原理是依赖等腰三角形的，用来解决并联电阻求值问题。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/parallel.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;当然，你也可以将多种图组合求解一个复杂但可拆分的方程。但更本质的方法则是把所有的比例关系都转成行列式结构，通过网格还可以绘制更为复杂但直观的诺谟图，方便工程应用，也有一定的艺术价值。原理请直接读&lt;a href=&#34;http://myreckonings.com/wordpress/wp-content/uploads/JournalArticle/The_Lost_Art_of_Nomography.pdf&#34;&gt;论文&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/complexnomo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;通过对原始变量的转换，例如平移、旋转、剪切、拉伸、投影， 基本上可以把诺谟图应用到各个数值求解的过程中去。 而实际需求的数值范围则是设计图表中最重要的因素，通过一定的转换会让诺谟图更像是一件艺术品。其实日晷就是一个诺谟图应用的很好实例。这里放个Ron Doerfler自己设计的日晷图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/ShadowLinc.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;历史篇&#34;&gt;历史篇&lt;/h2&gt;
&lt;p&gt;说了这么多你可能会好奇，这个东西是怎么出现的？其实我们可以扯的远一些，聊下图（graph）是怎么出现的。也许很多人没意识到，直到1878年，“graph”这个词才被数学家引入到研究中，用来表示化学-代数关系，而在这之前，图更多使用在天文与地理研究之中，例如天象图与地图。值得注意的是历史中图并不是抽象表达而更多是对真实事物的描述，当然几何求解数值方程的方法也算是历史悠久了。古人用的星盘与四分仪就是基于这种方法设计的，这对宗教的传播与未知世界的探索提供了可靠的保障，但这些更多是测量方法，需要借助实物与观察。&lt;/p&gt;
&lt;p&gt;研究中用的图，特别是带有笛卡尔坐标系来展示真实数据的图其实出现的并不早。根据《Blood, Dirt, and Nomograms: A Particular History of Graphs》的记载，最早的图可以追溯到1770年左右，而且有三个起源。第一个起源是英国人 William Playfire 做统计图集时作的图，用来描述英国国债。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/debt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;第二个起源是 James Watts 搞的，用来绘制引擎的压力体积变化，这个图就不是人画的了，直接就是机器画。在那个年代其实计数对于机械过程是个麻烦事，所以就直接给机器接上个机械臂，其变动用机械臂的震动记录在纸上。这种物理信号直接输出的仿真信号图后来广泛应用在地震监测、心率监测等领域，当然会有坐标系来校准。&lt;/p&gt;
&lt;p&gt;第三个起源是 Johann Heinrich Lambert 的实验记录，此君是科学家出身，坚持自然哲学要用观察数据来量化测量，在这个过程中他制作了大量的图。这里要说明的是自然哲学是相对于博物学来说的，那个年代自然哲学侧重今天的物理学研究关注形而上的东西与分析手段而博物学侧重对自然的宏观观察记录，侧重经验感知与综合。国内经常说的科普其实很多是在博物学这个层面上的，例如逛博物馆，野外考察什么的，侧重知识性。博物学很重要但其实国内缺的是自然哲学这块的思维普及而不仅仅是一个个知识点。&lt;/p&gt;
&lt;p&gt;然而，这个概念搞出来后在四十年里几乎没有人关注，原因在于懂这个技术的人还不知道咋用或者需要用的人不知道有这个技术。1795年法国大革命后引入了计量系统，在对公众推广时发现公众对数字无感，这时有个叫 Pounchet 的棉花制造商发现了 William Playfire 的图，他觉得这个图很有前景。其做的第一个应用就是把乘法表给图表化了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/mt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;这个图咋用呢，打比方我想知道6乘7的结果，我就沿着6的横网格找7的竖网格，里面的等值线就可以告诉我乘积在40到50之间，这对于不懂算术的人应该是非常方便了。但他其实已经将原始但两个变量关系图推广到了三变量关系了。在另一方面，法国的工程师们开始关注投影几何对于堡垒建设的应用，例如堡垒需要修建多么高？怎样最快运送沙土？其后继者接触了 Pounchet 的图后突然发现，投影几何其实可以用在这种存在等值线的图之中。&lt;/p&gt;
&lt;p&gt;所谓投影几何，就是将一组几何关系投影到另一个自定义的坐标系里的几何方法，说白了就是坐标变化。投影几何里的经典作品就是下面这个描述拿破仑大撤退的图，在图形可视化领域里也是经典作品。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/napolun.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;有位叫 Lalanne 的法国工程师将 Pounchet 的这种图示方法广泛推广到工程领域，特别是他把等值线跟地图结合，就有了我们现在看到的等值线地形图，这样三维数据就可以二维展示了。其第二个贡献就在于对数坐标轴的引入，从绘图角度，曲线不如直线好画，他发现把数据对数化后曲线可以变直线，这就是投影几何的一种映射。此时，图已经有了计算属性了。&lt;/p&gt;
&lt;p&gt;到了1884年，Philbert Maurice d&amp;rsquo;Ocagne 进一步将图形计算与投影几何结合，这就有了我们前面看到的诺谟图。而诺谟图事实上通过等值线超越了笛卡尔坐标系，多个参数可以几何投影到多条线但保证其函数关系可以被等值线来描述，使用起来也非常方便，完全不需要学习几何学与函数理论，会划线就可以。在第一次世界大战中，几乎每个防空炮兵都有一个诺谟图，根据当时情况来计算开炮的方位与角度。&lt;/p&gt;
&lt;p&gt;如果你认为诺谟图就是工程上可以用就错了，诺谟图其实在生理学里曾经有统治地位。20世纪初，生理学研究人员意识到人体血液是一个及其复杂的系统，里面有各种各样的化合物，其浓度其实存在一个平衡范围，然而将这个系统展示出来就成了大问题，如果两两关系进行绘图，那么其实超过一百幅图。此时，诺谟图闪亮登场。下面就是一副1928年哈佛生理学家 L.J.Henderson 在耶鲁大学作报告时用的图，他认为这张图浓缩了当时人类对血液的所有知识。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/blood.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;确实，诺谟图在浓缩知识上可能是个顶峰，如果我们打算给后人留一些证明文明存在的证据，也许这样一张图比复杂的公式要更实用，也更直观。哲学家 F.S.C.Northrop 认为一个多组分复杂系统可以也只可以用诺谟图来描述其动力学过程。&lt;/p&gt;
&lt;p&gt;然而，时过境迁，今天连知道诺谟图的人都不多了，实验学科已经意识到单变量控制实验与现实的距离，然而多变量的系统或者时髦一点的组学，其实都已经不知道诺谟图这类描述系统的方法了。同当年的机器类似，或许在数据量充盈的今天，我们可以先画出图，然后反推背后的函数，甚至图本身就是解。有没有很眼熟？&lt;/p&gt;
&lt;h2 id=&#34;回归篇&#34;&gt;回归篇&lt;/h2&gt;
&lt;p&gt;现在还有没有诺谟图，答案是肯定的，且如我前面所说，有些模型确实可以可视化为诺谟图，且所有机器学习里的黑箱模型应该都可以可视化为诺谟图且可以以图片形式保存与传递。在统计学里，有一种数据展示方法叫做回归诺谟图。例如下面这一张：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://yufree.cn/cn/2020-04-08-nomography_files/rnomo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;这张图的用法跟之前说的有点区别，使用的是垂直等值线，也就是说，图里六个变量你根据实际取值画一条垂直线到最上面那条 Log OR 的轴上，然后你会得到六个值，将这六个值的和计算出来算一个坐标，然后在图上画一条垂直线到下面的风险概率里，然后你就知道总体风险了。这个方法非常直观表示了不同变量取值范围及其对风险值的贡献，比单纯放个系数表上去要直观很多，但我估计在目前大学教育对诺谟图的缺失大背景下，能看懂的人估计不多，这个活原理上非常技术但其实不难。我个人曾经见到审稿人因为看不懂图就说图太复杂不如用表的情况，这种智力倒退在专业细化的今天从来没少过，甚至更多了，有些人自己脑子处理不了信息量大的图就去要信息量少但不准确的图，掩耳盗铃。其实很多复杂些的模型例如支持向量机也可以用诺谟图来展示最终&lt;a href=&#34;http://stat.columbia.edu/~jakulin/Int/f309-jakulin.pdf&#34;&gt;结果&lt;/a&gt;。至于能不能做到数据直接生成，我认为并不困难，甚至比数值求解还要简单。&lt;/p&gt;
&lt;p&gt;当然，这个活不需要你自己做。在 CRAN 上，你可以用 &lt;code&gt;hdnom&lt;/code&gt; 包来对多变量生存分析进行诺谟图绘制。而 &lt;code&gt;nomogramEx&lt;/code&gt; 包可用来从诺谟图图片中反向提取方程。不过我看到诺谟图的使用还是线性模型居多，应用也是更多在生存分析方面，以后应该有更多的应用方向与场景。当然，诺谟图设计本身就是个技术与艺术的结合体，如果你想修身养性平复焦虑又不愿意搞那些练字画画织毛衣的事，或许你该拿起笔来，用现在天天更新的一些数据自己设计一张诺谟图，记录下当前的大流行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;扩展阅读资料&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;制作诺谟图的&lt;a href=&#34;http://pynomo.org/wiki/index.php?title=Main_Page&#34;&gt;软件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.nomographer.com/home/index.html&#34;&gt;爱好者网站&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://myreckonings.com/wordpress/wp-content/uploads/JournalArticle/The_Lost_Art_of_Nomography.pdf&#34;&gt;The Lost Art of Nomography&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jstor.org/stable/237474&#34;&gt;Blood, Dirt, and Nomograms: A Particular History of Graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://statmodeling.stat.columbia.edu/2006/05/24/nomograms/&#34;&gt;Nomograms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>网速与树莓派</title>
      <link>https://yufree.cn/cn/2019/12/16/rpi/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2019/12/16/rpi/</guid>
      <description>&lt;p&gt;从大概五年前开始，4G网络开始普及，到今天5G报道满天飞，其实改变了很多人的生活方式。有时我在想，如果我们的网速停在3G时代，智能手机可能不会像今天这样流行。我大概看了下手机里装的应用，很多在网速低了后体验会变化很多，从可有可无变成日常打卡，特别是视频应用与游戏。而互联网开始流行短视频与vlog，很大程度上也是因为从制作到观看中没了网速瓶颈：制作视频不用桌面级软件的后期渲染，很多运动相机或云台相机甚至手机直出就很不错，高清直播配合降低的制作成本让视频内容大爆炸；看视频等信息量大的行为也不用先缓存或降低画质了，4k流媒体其实在4G网速下就可以实现，而4k的信息量跟你到电影院看电影的35mm胶片差不多了。也就是说，如果你同样有10美元，去电影院如果不是图个氛围或者去看IMAX，不如直接订阅4k流媒体服务一个月在家里搞个4k投影仪观看来的值，观看体验是差不多的。同样的问题存在于蓝光影碟，一张4k蓝光影碟几十G的容量只要你有网也属于可有可无了，当然蓝光版跟流媒体版的内容区别可能是其价值所在。现在的父母可以拖出一箱子硬盘告诉孩子这是你成长的点点滴滴，未来的父母可能只需要对孩子的数据读取进行授权就可以，很多数码实体可能要消失。我家里的有我几岁时说话的录音带实体还在，但能出声的录音机确实不好找了，今天的日常操作过二十年看可能就是不可思议了。不过，眼下的问题不在于生产，我看了下自己拍了几万张照片了，但留有印象的寥寥无几，有了直播看回放就几乎绝迹了，反正总有新的东西看，高网速下的持续输入与输出正在塑造新的认知趋势并压缩思考的空间。你如果每时每刻都在记录，那就不会有时间与精力回顾，那么记录本身的意义也就不大了，但也许你就是在享受记录这件事本身，这种事直到上个世纪都不常见：自古人们从来都是选择性高成本记录那些值得回忆的瞬间并反复回顾，没了回顾的低成本记录在信息高速传递的今天其实很是异类。&lt;/p&gt;
&lt;p&gt;伴随网速提高，我们会看到更多东西的消逝或数字化。本地数据存储除了隐私需要外其实必要性在减弱，我们需要的展示结果的终端，至于说数据的处理负担，可以交给云服务器来做。通过生物识别，我们的个人数据可以在网上高速调取，虚拟化会成为生活方式的一部分。很多曾经的习惯会从面向个体信息获取与娱乐变成社交或场合价值优先，例如现在的网吧没有完全在个人电脑普及浪潮后消失，反而成了电子竞技爱好者的自留地或社交场所。同样的，这类生活方式会从服务个体变到服务场合气氛，看重场景下的情感需求对于信息载体的脱实向虚是很重要的趋势。很多人去实体店或买实体看重的其实不是信息本身，而是其附带的情感与价值认同，这也决定了很多事物会衰落，但不会完全消失。等VR与AR技术更进一步，可能人真的会搞不清楚什么是真实存在，什么是虚构，社交可能通过虚拟场景远程实现，而网速或者说信息传递速度是这个趋势的关键，当足够快与平滑时，生理感知无法区分，那么真假在某种意义上就可操纵了。&lt;/p&gt;
&lt;p&gt;网速与本地运算是互相矛盾的，网速足够快，本地运算要求就相应降低，这样本来我们网上向本地传数据然后本机运算，当网速够快我们只需要提需求让服务器或云端进行计算，然后传递结果，本地终端只需要具备提供网速支持与结果展示所需要的计算能力。运算能力也许面对需求永远是不够用的，但在哪里算就是另一个问题了，例如现在云输入法的备选词就是在服务器端计算通过网络返回结果而不是调用本地训练好的模型，这样做有两个优点，一个是更新快，另一个是本地运算要求不高。现在很多电子产品都自带一个电脑或单片机，但在信息传递的支持下可以远程做很多事，美其名曰智能家电。很多在线应用也完全可以替代当年的装机必备软件，办公、图片处理、数据报表等软件纷纷云端化与订阅化，网速足够时一套完整办公环境可以随时随地搭建，效率自然会提高很多。我不必为了加个水印去学ps的图层，只要在要搜的关键词后面加上 online 就可以了，屡试不爽。原来我外出都要带个优盘，现在记住密码或带着有指纹的手或刷脸就可以快速重现工作所需的工具场景，做报告的话我甚至不用带电脑，因为幻灯片在线就可以公开查看，我只需要关键词与能联网的搜索引擎就够了，我需要的是带输入输出的终端而不是处理器，真需要处理笔记本的性能面对台式机或服务器反而很尴尬。&lt;/p&gt;
&lt;p&gt;今年回国我本来想把家里电脑升级下，回去后才知道电脑已经半年多没开机了，爸妈所有的查询现在都是通过智能音箱与手机来做，键盘都不用了。我甚至在想如果下次升级电脑，是否需要去搞个高性能笔记本，还是换成树莓派与几年的云计算服务，价格上区别不大甚至更便宜，我个人娱乐手机配合树莓派就搞定了，而买的正版游戏都从未下载到本地过，办公几乎全部走云端主机，多出来的钱升级下网速换个好点的屏幕就挺好。现在来看有点魔幻，对我而言，学生时代笔记本电脑就是我的全部，娱乐、学习、工作、社交全都通过笔记本电脑来做，现在笔记本电脑更多是工作工具了，或者说能联网的终端都可以是工作工具。本机处理数据一般不大，大的数据我都远程登录办公室一台服务器或干脆扔学校集群上算了，以后可能扔云端上，我也没啥有价值的隐私数据，有需要搞个个人NAS也够了。我猜想很多人觉得还是手边有高性能设备好，但希望他们去尝试下树莓派之类的产品，计算能力日常使用生理上感觉不出差异多大了。真有那计算量大的活，早点拥抱集群计算或超算才是正解，按时付费可以瞬间有一台高性能电脑，处理完了下线也不耽误。&lt;/p&gt;
&lt;p&gt;说到树莓派，我最近尝试了一下，做大型数据处理比较吃力，但锅不在树莓派而在写某些算法的人设计理念有问题。日常办公是完全没压力的，100刀基于树莓派的商品化小型终端计算机可以应对80%的生活自用场景。但用了几天，特别是买了一大堆传感器尝试了GPIO后，我发现甚至树莓派的计算能力在很多场景下都是多余的，很多需求一块开发版就能完成，也并不是要去学汇编与C，我这种半吊子python使用者看看文档然后知道面包板跟子母线咋用就可以排列组合做很多有意思的应用。R里面目前有个 RpiR 包对基于 C 的 WiringPi 进行了基础功能的封装，可以在树莓派里用，但稍微复杂点的功能就得自己造轮子了，以我之懒直接换 python 了，高级语言还是用户友好的，反正都是调包。不过如果自己打算写，那就是另一个概念了，发光二极管只需要高低电压就行，但随便一个复杂点的传感器或元器件都需要自己写驱动，把数字甚至模拟信号处理成你需要的格式。这种从电路到编程的体验还是很有意思的，不过这可是物联网的基本功。&lt;/p&gt;
&lt;p&gt;通用计算机有可能不如专用芯片在未来有前景，特别是网速足够时，专用芯片的在线集群要比个人本地终端更高效地处理特定类型的数据与任务，非开发目的而仅仅是应用的话这种针对特定任务的数据处理芯片会有前景，当然，如果任务本身的计算量终端完全可以负担，也没必要去在线处理。应该对计算任务评估一个计算当量，如果计算当量超过了日常办公一个数量级，那么就应该在线做或购买特异性的服务器而不要对不需要的功能付费。反之，如果本地可承担，那就使用本地资源，本地计算资源总是稀缺的，网速足够的话计算任务尽量放到云端来提高效率，只是这个事牵扯数据价值的归属问题，个人应可以向使用自己数据的实体发放付费或免费的授权，对应换取更多服务或折扣。好比集中供暖，家家户户烧炉子的社会效益低于集中供暖，开支相应更高，但要做好用户信息加密与反追踪。打个比方，行为数据模式的统计量对与公司是有价值的但不能追踪到个体，那么在数据收集后我们应该引入第三方或者一种算法将用户个体数据在信息不丢失的情况下做类似基因交叉互换的操作，这样公司或政府拿到的数据不会指向某一个真实的人，但其中的信息却可以提供价值与决策参考。&lt;/p&gt;
&lt;p&gt;经济上脱实向虚目前来看是弊大于利的，造成了财富分配不均现象的恶化，但网速提高造成的信息脱实向虚却很难给个简单的好坏判断，但无论如何都会是一种趋势。我们是否还需要一台全能电脑还是符合实际需求的电脑，抑或是特异性的芯片？这也是所有人会用脚投票的趋势。眼下有些技术还缺失，有些比较成熟，有些则属于过剩的泡沫，当然人的需求从来不是一成不变的，但面对改变重新认识自己应该是必要的，否则随波逐流实属对个人选择权的放弃。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>编译器漫谈</title>
      <link>https://yufree.cn/cn/2019/02/14/compiler/</link>
      <pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2019/02/14/compiler/</guid>
      <description>&lt;p&gt;最近借由一次R包安装失败，我又重新收集了下关于编译的概念与知识，为了让下一次失忆的我不至于再折腾半天，就归纳到这里。&lt;/p&gt;
&lt;p&gt;C语言源码编译运行过程是这样的：先预处理源码，调入模块，然后转换成汇编语言文件，汇编语言文件可以被汇编器转为机器码，然后通过连接器合并为可执行文件，最后加载到内存里运行。因为C语言是多数操作系统的基础，所以多数操作系统也自带对应的C编译器。更重要的是，C语言的库很丰富，也就是工具函数比较多，换别的就得自己写。这是很有意思的路径依赖案例，事实上任何语言都应该可以拿来写操作系统，不过C是在开发Unix时候设计出来的，现在流行的开发级操作系统都是unix/类unix操作系统，加上C在内存管理与CPU交互上的先天优势，历史机遇下成为了主流。&lt;/p&gt;
&lt;p&gt;C的核心地位还体现在很多高级语言的编译器是构建在C之上的，或者是C++之上，多数高级语言都通过限制自由度（例如不让操作内存、功能模块化等）来实现上手容易与较高的开发效率，但只要关注程序性能，肯定回去学C或C++的。GNU的GCC编译器是一个相对通用的编译器合集，可以用来编译包括C在内的多种语言。然而，GCC也是有历史包袱的，所有有人就另起炉灶单独针对C或C++重写了效率更高的编译器及其后台，这就是苹果的LLVM项目与clang编译器。但要注意的是LLVM支持的语言不如GCC多，所以如果你还要用到fortain或java编译器，那就还是老老实实用GCC吧，或者cmake的时候分别指定编译器，只要你不嫌麻烦。一般而言，效率与性能往往不能兼得。&lt;/p&gt;
&lt;p&gt;高级语言的编译过程跟相对底层的C或C++是不一样的，Java就是自己定义了一套运行环境JVM，编译出的文件也是JVM可读的，这就提高了Java的可移植性，降低了跨平台开发的难度，当然你得保证这些平台上可以运行JVM。其实很多高级语言是解释型的，REPL里可直接运行代码，但同样会有人为高级语言写编译器来提高运行性能，这个是按需求来。我个人感觉是用REPL的人一般是应用层的，关心有没有满足自己需求的函数；用编译语言的人一般是开发层的，关心软件工程及性能。然而，高级语言里如果打算提高运行效率，也会提供C或C++的接口让程序员可以通过外力来提高自由度。过度的功能封装实际也限制了高级语言的应用场景。&lt;/p&gt;
&lt;p&gt;说到效率，自然少不了并行计算，openmp就是一种并行化方案，可以支撑C与C++。很多R包会通过使用 openmp 来底层加速算法，但这样的包一般都需要单独编译。目前GCC与Clang在编译器层其实都实现了对 openmp 的支持，编译时加上 &lt;code&gt;-fopenmp&lt;/code&gt; 就可以。不过 mac os 自带的编译器是没有这个功能的，所以你需要 homebrew 来自己安装这些支持 openmp 的编译器然后在 &lt;code&gt;.R/Makevars&lt;/code&gt; 里把默认编译器换成新的就可以了。&lt;/p&gt;
&lt;p&gt;例如你装了llvm/Clang，可以写上：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CC=/usr/local/opt/llvm/bin/clang
CXX=/usr/local/opt/llvm/bin/clang++
CXX11=/usr/local/opt/llvm/bin/clang++
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或者GCC版（注意版本要对应）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CC=/usr/local/bin/gcc-8
CXX=/usr/local/bin/gcc-8
CXX11=/usr/local/bin/gcc-8
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或者更简单的方法就是不用并行计算，直接在&lt;code&gt;.R/Makevars&lt;/code&gt; 里参数留空强制跳过对openmp的编译要求：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SHLIB_OPENMP_CFLAGS=
SHLIB_OPENMP_CXXFLAGS=
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;同样的道理可以用在开发上，如果你编写R包涉及了相关并行计算功能，需要在&lt;code&gt;src&lt;/code&gt;目录下创建Makevars文件来帮助用户提前配置编译参数，不过这方面我就没经验了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>短链氯化石蜡的定量方法</title>
      <link>https://yufree.cn/cn/2018/08/06/sccp/</link>
      <pubDate>Mon, 06 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2018/08/06/sccp/</guid>
      <description>


&lt;p&gt;短链氯化石蜡的定量在环境分析化学中属于难度比较高的，在目的性分析里也算是最非传统的了，不过理解了这类物质的定量对于理解环境分析的复杂性很有帮助，鉴于目前教科书里肯定没有，我就总结在这里。&lt;/p&gt;
&lt;div id=&#34;定量分析&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;定量分析&lt;/h2&gt;
&lt;p&gt;分析化学里的定量模型比较简单，你知道待测物的某个指标，在不同含量下测定标准品中这个指标，找出含量与指标的关系模型。然后当测定样品时，将测得的指标带入关系模型就可以得到含量了。用色谱质谱联用来说明就是首先购买待测物的纯品，然后优化色谱质谱条件来获得待测物特定离子响应色谱峰的最佳灵敏度与分离效果，然后配置不同浓度标准溶液进样，根据浓度与峰面积绘制标准曲线。当然说是标准曲线，多数是用线性关系，有了标线就可以测定位置样品中浓度了。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;短链氯化石蜡&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;短链氯化石蜡&lt;/h2&gt;
&lt;p&gt;针对短链氯化石蜡，上面那个传统套路是走不通的，原因有三个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;短链氯化石蜡并不是一种化合物，而是碳原子数10-13个，氯原子数5-10个的短链烷烃。这是一类化合物，分子式就有24种，而结构式就要上万种了&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;标准品不是纯品，化学定量分析要求标准物纯度要非常高，但市售短链氯化石蜡的标准品大都不是单体，甚至同一种分子式都做不到，能买到的都是用氯含量来标定，也就是定量要通过氯含量来换算&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;分离困难，因为标准不纯，定量要同时测多种短链氯化石蜡，但不同短链氯化石蜡特征离子几乎一样，且中链氯化石蜡会形成干扰，后果就是质谱提取特征离子时有时包含了两种化合物，这样就存在系统性定量偏高，同样的，色谱分离也做不到基线分离，甚至看不到峰，标准品看到的也只是五指峰&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于上述原因，那些基于特征离子定量且又是低分辨质谱定量的方法几乎可以肯定是不靠谱的，线性关系一塌糊涂不说误差不是一般的高。有意思的是，网上可以搜到很多短链氯化石蜡定量方法专利，基本都是胡说八道，根本就没搞清楚问题的复杂性，要么就是隐藏技术细节，但其实很多文献里都有也不用隐藏，且根本就不该出现这类专利，因为学术期刊早就发表过了。我自己申请过也获得过专利，那帮审专利的人专业水平不是一般的低，很大程度是因为外行评价内行，不过这东西挺唬人的，但其实科技领域越前沿胡说八道就越多，这样才有讨论空间，不必看得太重。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;高分辨质谱定量方法&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;高分辨质谱定量方法&lt;/h2&gt;
&lt;p&gt;离子源肯定要是负化学源，EI源会把短链烷烃打碎，后果就是根本找不到特征离子，负化学源能保证你起码找得到分子离子峰。不过这么说也不对，因为即使在负化学源下，短链氯化石蜡的基峰也是掉一个氯原子的。而且只要有氯原子就会存在同位素峰，同位素峰之间在低分辨质谱上会互相覆盖，这里我们先不讨论低分辨定量，高分辨质谱是可以在质谱上实现每种分子式的分离的。&lt;/p&gt;
&lt;p&gt;那么色谱用什么呢？首先得是能接质谱的柱子，其次因为短链氯化石蜡沸点不高，我们可以用气相色谱柱来进行分离。当然也有用液相分离的，那个比较复杂就不提了。&lt;/p&gt;
&lt;p&gt;好了，我们来看技术难点，我们手上有的标准只标注了氯含量而不是单体含量，所以首先我们要构建氯含量与单体含量的关系。不同氯含量的标准会有不同质谱响应因子，也就是你进样量相同时，峰面积是不同的。这里文献中一般采用的策略就是假设所有短链氯化石蜡单体响应的不同主要来自于氯含量不同，而氯含量不同是因为组成不同。计算出氯均一化后总体氯化度对应的响应因子与样品峰面积就可以知道含量。&lt;/p&gt;
&lt;p&gt;首先，我们需要固定浓度不同氯化度的sccp标样，然后提取离子计算面积积分，各离子面积要先除以离子同位素丰度得到化合物面积，然后除氯原子个数得到单位氯原子化合物的响应，所有离子响应之和为单位氯原子的总面积，每个离子的氯化度是已知的，用其单位离子响应占总响应的比例乘以氯化度求和可得到样品中的总氯化度，单位氯原子的总面积除以质量得到标准响应因子，这个响应因子在不同氯化度下不一样，用不同氯化度标样分别测定，得到氯化度与响应因子的回归曲线。&lt;/p&gt;
&lt;p&gt;然后，测定样品时只要计算出样品中的总氯化度就可以知道样品响应因子，根据样品中计算出的单位氯原子的响应面积就可以知道其中sccp的总含量。同时也可以反推得到样品中不同单体的浓度组成。不过最好先用标准品作为样品检验下方法是否有系统偏差。&lt;/p&gt;
&lt;p&gt;我在&lt;code&gt;enviGCMS&lt;/code&gt;包里已经做了一个应用，你按照里面说的一步步来是可以实现定量的，不过请一定理解背后的假设，启动应用的代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;#39;enviGCMS&amp;#39;)
enviGCMS::runsccp()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;此时你的浏览器会弹出图形界面，按照指示做就可以了，记得去引 references 选项卡里的文献与这个软件包就可以了，我不是方法的发明人，但软件却是我写的。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;低分辨质谱定量方法&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;低分辨质谱定量方法&lt;/h2&gt;
&lt;p&gt;前面提到，在低分辨质谱上存在同一个离子实际是两种物质的情况，如果两组物质质量数干扰，就选两个共有离子，每个离子响应是来自两个物质响应的线形加和，由于两个离子的丰度比可以事先计算出来，所以可以构建下面的方程：&lt;/p&gt;
&lt;p&gt;物质A、B的两个共有离子峰面积X、Y，两者丰度比例 m、n：&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[A + B = X\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[A\frac{n}{m}+B\frac{n}{m} = Y\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;联立求解可以得到A与B的真实峰面积。&lt;/p&gt;
&lt;p&gt;如此可以把相互干扰的离子对放到一起去测，然后求解各离子的实际比例。&lt;/p&gt;
&lt;p&gt;这是个穷人的方法，买不起高分辨就要用数据处理来矫正误差，这里我就不写图形界面了，穷要有穷的志气，穷还不愿思考动手，那我也不想帮忙了，线索已经给的很明确了。至于说高分辨为什么写原因也很简单，目前有人要用，而且我知道对方是理解背后原理的，我不希望不理解原理就套用模版套路来进行科研的人（但写作可以用模版方便交流），这样做出的东西意义不大，没有思考的科研也许有用，但丧失了乐趣。&lt;/p&gt;
&lt;p&gt;祝大家短链氯化石蜡定量快乐！&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>现代科研兵刃谱</title>
      <link>https://yufree.cn/cn/2018/07/14/sci-tools/</link>
      <pubDate>Sat, 14 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yufree.cn/cn/2018/07/14/sci-tools/</guid>
      <description>&lt;p&gt;工欲善其事，必先利其器。今天绝大多数知识都是工具生产出来的，也就是想使用知识，肯定要先学工具，而工具又需要知识铺垫，这就成了一个鸡生蛋蛋生鸡的问题。虽然事后总结都有千般道理，但就我人经验而言，工具与知识是相辅相成缺一不可的，过于关注知识会导致脱离实际而沉迷于工具选择则有很高的迁移成本。这里的忠告就是不要想太多，先迈开步子，随便找个工具用起来，用实战来丰富需求，根据需求定向选择最适合自己的工具而不做工具的奴隶，如有必要，自己创造工具。另外，尽量选择那些花费百分之二十的精力可以掌握百分之八十的内容或应用场景的工具。同时系统学习那些使用频率高的工具，其余的只要知道其存在即可，不要捡芝麻丢西瓜。&lt;/p&gt;
&lt;h2 id=&#34;文本编辑&#34;&gt;文本编辑&lt;/h2&gt;
&lt;p&gt;科研用文本编辑工具主要应对排版要求，早期排版系统基本都是通过 TeX 语言来实现的，后来由于个人电脑普及及新兴学科的出现，很多科研人员上手会用的都是可见即可得的文本编辑器。现在期刊投稿一般会支持基于 TeX 的投稿及常见可见即可得文档，这些都是本地编辑。另一个当前流行的可见即可得文本编辑方式是在线协作，例如&lt;a href=&#34;https://docs.google.com/&#34;&gt;谷歌文档&lt;/a&gt;、&lt;a href=&#34;https://shimo.im/&#34;&gt;石墨文档&lt;/a&gt;、&lt;a href=&#34;https://docs.qq.com/&#34;&gt;腾讯文档&lt;/a&gt;等。对于需要协作完成的论文，在线协作文档极大方便了实时交互与版本控制。其实利用基于Git的&lt;a href=&#34;https://github.com/&#34;&gt;GitHub&lt;/a&gt;也可以实现在线协作与修订，不过门槛比较高，但有希望成为一些期刊今后的投稿系统原型。&lt;/p&gt;
&lt;p&gt;还有些文本编辑器是基于纯文本的，通过文本中的控制语句来实现排版，TeX就是其中最流行的。&lt;a href=&#34;https://www.overleaf.com/&#34;&gt;Overleaf&lt;/a&gt;支持基于 TeX 的在线文档协作，甚至你可以直接用其向特定期刊投稿，同样的工具还有&lt;a href=&#34;https://www.sharelatex.com/&#34;&gt;sharelatex&lt;/a&gt;。不过，TeX的控制语句实在太丰富，学习起来比较困难。&lt;a href=&#34;https://pandoc.org/&#34;&gt;Pandoc&lt;/a&gt; 的出现方便了其他更简单的标记语言对 Tex 的转换，其中最容易上手的是&lt;a href=&#34;https://daringfireball.net/projects/markdown/&#34;&gt;Markdown&lt;/a&gt;。不过 Markdown 存在很多版本，其中基础版支持的排版功能非常有限，Pandoc 对其进行的&lt;a href=&#34;https://pandoc.org/MANUAL.html#pandocs-markdown&#34;&gt;扩展&lt;/a&gt;则支持了更丰富的功能方便排版。所以理论上你可以使用 Markdown 来写论文，不过这需要你的编辑器支持一些额外的功能。&lt;/p&gt;
&lt;p&gt;总结一下，作为现代科研工具，理想文本编辑器需要至少有以下功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持在线协作、评论与修订&lt;/li&gt;
&lt;li&gt;支持版本控制&lt;/li&gt;
&lt;li&gt;支持常见文献管理工具&lt;/li&gt;
&lt;li&gt;支持期刊样式排版&lt;/li&gt;
&lt;li&gt;容易上手&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;文献管理&#34;&gt;文献管理&lt;/h2&gt;
&lt;p&gt;现在的文献管理工具一般都支持常见文本编辑工具，也就是可以很方便的插入参考文献。然而，文献管理工具要同时具有收集、整理与分析的功能为佳。当前主流文献管理工具都已经支持浏览器层次的文献收集，也就是直接通过快捷键、脚本或浏览器扩展一键自动提取文章页面中参考文献信息并存入用户指定的文献库。要实现这个功能，多数需要知道文献数据库网页结构，当前很多文献数据库都推出了自己的文献收集应用，有的直接收购了文献管理软件。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://endnote.com/&#34;&gt;Endnote&lt;/a&gt;是比较老牌的文献管理工具，不同于前面所说的网页采集，其自身就有与常见数据库的搜索接口，国内科研机构图书馆大都提供培训。与之类似的&lt;a href=&#34;http://www.inoteexpress.com/aegean/&#34;&gt;NoteExpress&lt;/a&gt;则属于国产软件，据说对中文期刊格式支持更好，类似的还有&lt;a href=&#34;https://www.mendeley.com/&#34;&gt;Mendeley&lt;/a&gt;、&lt;a href=&#34;http://refer.medlive.cn/&#34;&gt;医学文献王&lt;/a&gt;、服务 TeX 里 BibTex 的 &lt;a href=&#34;http://www.jabref.org/&#34;&gt;JabRef&lt;/a&gt; 与Mac OS 下的&lt;a href=&#34;https://www.readcube.com/papers/mac&#34;&gt;Papers&lt;/a&gt;。这些工具起步较早，从单机时代就有用户，还有些工具诞生于互联网时代，有着更丰富的功能。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zotero.org/&#34;&gt;Zotero&lt;/a&gt; 属于互联网精神的产物，特别是前者本身就是基于火狐浏览器，其支持的文献格式样式都非常多，而且也有着丰富的文本分析扩展应用。&lt;a href=&#34;https://paperpile.com/app&#34;&gt;Paperpile&lt;/a&gt;则属于基于谷歌文档的应用，可以很方便地管理在谷歌文档中使用到的文献。&lt;a href=&#34;https://www.doi.org/&#34;&gt;DOI&lt;/a&gt;与&lt;a href=&#34;https://www.crossref.org/&#34;&gt;crossref&lt;/a&gt;的出现则更方便了文献的搜索定位。可以说基于互联网的团队化文献管理正在成为趋势。&lt;/p&gt;
&lt;p&gt;总结一下，作为现代科研工具，理想文献管理软件需要至少有以下功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持常见文本编辑器&lt;/li&gt;
&lt;li&gt;支持在线文献采集&lt;/li&gt;
&lt;li&gt;支持文献库协作与共享&lt;/li&gt;
&lt;li&gt;支持文献信息学探索&lt;/li&gt;
&lt;li&gt;容易上手&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;数据处理与绘图&#34;&gt;数据处理与绘图&lt;/h2&gt;
&lt;p&gt;数据处理方面很多学科只需要电子表格与基本的统计分析就可以了，很多在线服务就可以完成。然而，有些学科需要更丰富的功能例如多元统计分析与假设检验时，电子表格提供的功能可能就不那么明显了，有时需要学习使用电子表格的宏扩展来实现。此时，很多人容易陷入哪个分析一定要用哪个软件做的误区，其实多数数据分析软件的算法都差不多，只不过默认值可能不同，有些功能则藏的比较深，此时请善用搜索引擎。&lt;/p&gt;
&lt;p&gt;所见即所得的数据处理与绘图软件有很多，Excel、&lt;a href=&#34;https://www.originlab.com/&#34;&gt;Origin&lt;/a&gt;、&lt;a href=&#34;https://systatsoftware.com/&#34;&gt;SigmaPlot&lt;/a&gt; 与&lt;a href=&#34;https://www.ibm.com/analytics/spss-statistics-software&#34;&gt;SPSS&lt;/a&gt; 是科研中用的比较多的。这些软件都是图形界面操作且都收费，其内置很多现成的分析模块应对实际科研问题，但这些简化会导致使用者知其然不知其所以然，在分析方法使用上陷入误区。&lt;/p&gt;
&lt;p&gt;编程分析与绘图则属于基础的工具，&lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt; 、&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt;、&lt;a href=&#34;https://www.mathworks.com/products/matlab.html&#34;&gt;Matlab&lt;/a&gt; 与&lt;a href=&#34;https://www.sas.com/en_us/home.html&#34;&gt;SAS&lt;/a&gt; 都是这类工具的代表，应该说掌握其中任意一个就足够应对科研中需要的数据分析了。不过通常这类工具比较难学，最好是配合数据分析方法的学习同步掌握，而且要通过案例来理解方法，累积经验。如果推荐一个，那么基于 R 的 &lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;ggplot2&lt;/a&gt; 作图与其背后的 &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; 数据分析套装则是很好的起点。如果更进一步，可以用&lt;a href=&#34;https://www.rstudio.com/products/shiny/&#34;&gt;shiny&lt;/a&gt; 来制作交互式数据展示界面。&lt;/p&gt;
&lt;p&gt;此外，互联网上也有一些在线应用可以很方便地生成特殊图形例如&lt;a href=&#34;http://naotu.baidu.com/&#34;&gt;百度脑图&lt;/a&gt;可以用来生成流程图或思维导图、&lt;a href=&#34;https://www.autodraw.com/&#34;&gt;Autodraw&lt;/a&gt;可以用来画简笔画、&lt;a href=&#34;https://plot.ly/&#34;&gt;plotly&lt;/a&gt;可以在线完成绘图等。甚至网上还有直接上传数据后自动猜测你需要进行分析与制图的&lt;a href=&#34;https://www.charted.co/&#34;&gt;Charted&lt;/a&gt;。这样的工具只要搜索你所需要的分析然后加上“online”作为关键词就可以找到。&lt;/p&gt;
&lt;p&gt;总结一下，作为现代科研工具，理想数据分析与绘图软件至少有以下功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持科研用统计分析&lt;/li&gt;
&lt;li&gt;图片默认输出美观大方支持绘图自定义&lt;/li&gt;
&lt;li&gt;具备可重复性的宏功能或数据处理脚本&lt;/li&gt;
&lt;li&gt;容易上手&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;学术交流&#34;&gt;学术交流&lt;/h2&gt;
&lt;p&gt;学术交流是科研生活中可以说最重要的一环，现代科研体系的分工合作都要通过学术交流来实现。主流趋势包括论文预印本服务器、开放获取与线上学术交流。&lt;/p&gt;
&lt;p&gt;预印本指在通过同行评议发表之前事先将论文手稿托管在公开服务器的研究工作。预印本服务器可以加速新思想的交流，接受预印本发表的期刊可以从维基百科上&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_academic_journals_by_preprint_policy&#34;&gt;查到&lt;/a&gt;。比较知名的预印本服务器包括偏数学物理计算机科学的&lt;a href=&#34;https://arxiv.org/&#34;&gt;arxiv&lt;/a&gt;、偏生命科学的&lt;a href=&#34;https://www.biorxiv.org/&#34;&gt;biorxiv&lt;/a&gt; 与偏化学的&lt;a href=&#34;https://chemrxiv.org/&#34;&gt;chemrxiv&lt;/a&gt;。国内也有中科院的科技论文预发布[&lt;a href=&#34;http://chinaxiv.org/home.htm&#34;&gt;平台&lt;/a&gt;来服务国内科研人员。很多期刊出版方也在推广自己的预印本服务器来吸引高水平研究，所以可酌情选择。&lt;/p&gt;
&lt;p&gt;开放获取是另一个趋势，要求研究工作可以公开让大众阅读。目前很多科研基金都开始有了这方面的要求及预算。但值得注意的是虽然开放获取期刊可能有更好的阅读数与引用表现，但有很多机构的开放获取期刊属于掠夺性期刊，给钱就发表，对学术评价与学科发展非常不利，可以通过一些网络上的&lt;a href=&#34;https://beallslist.weebly.com/&#34;&gt;列表&lt;/a&gt;来鉴别。要实现开放获取或者说透明科研，&lt;a href=&#34;https://f1000research.com/&#34;&gt;f1000research&lt;/a&gt;、&lt;a href=&#34;https://peerj.org/&#34;&gt;PeerJ&lt;/a&gt;还有&lt;a href=&#34;https://www.plos.org/&#34;&gt;Plos&lt;/a&gt;都是还不错的先行者，它们在实践一些新理念，不过显然并不便宜。&lt;/p&gt;
&lt;p&gt;线上学术交流除了期刊外，实际还要包括学术博客、多媒体展示、学术出版与网络身份。制作学术博客的工具可以直接借助平台例如&lt;a href=&#34;http://blog.sciencenet.cn/&#34;&gt;科学网博客&lt;/a&gt;，也可以自己搭建例如使用&lt;a href=&#34;https://zh-cn.wordpress.com/&#34;&gt;Wordpress&lt;/a&gt;、&lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;Blogdown&lt;/a&gt;或者&lt;a href=&#34;https://www.netlify.com/&#34;&gt;Netlify&lt;/a&gt;等工具。幻灯片制作也最好使用网页模式方便交流，&lt;a href=&#34;https://github.com/yihui/xaringan&#34;&gt;xaringan&lt;/a&gt;、&lt;a href=&#34;https://rstudio.github.io/learnr/&#34;&gt;learnr&lt;/a&gt;等其他基于Markdown语言的幻灯片制作工具可以满足要求。学术出版物则可以通过&lt;a href=&#34;https://bookdown.org/&#34;&gt;bookdown&lt;/a&gt;或&lt;a href=&#34;https://github.com/rstudio/rticles&#34;&gt;rticles&lt;/a&gt;等工具来完成。线上的学术身份识别对于存在大量重名现象的中国科研人员也是很有必要的，&lt;a href=&#34;https://orcid.org/&#34;&gt;ORCID&lt;/a&gt;、&lt;a href=&#34;http://www.researcherid.com/&#34;&gt;Researcher ID&lt;/a&gt;、&lt;a href=&#34;https://www.scopus.com/&#34;&gt;Scopus Auther ID&lt;/a&gt;、&lt;a href=&#34;https://scholar.google.com&#34;&gt;谷歌学术个人主页&lt;/a&gt;及国内的&lt;a href=&#34;https://xueshu.baidu.com/&#34;&gt;百度学术个人主页&lt;/a&gt;都是不错的网上学术名片。而在线交流的手段则可通过&lt;a href=&#34;https://www.researchgate.net/&#34;&gt;ResearchGate&lt;/a&gt;、&lt;a href=&#34;https://www.academia.edu/&#34;&gt;Academia&lt;/a&gt;、&lt;a href=&#34;https://www.linkedin.com/&#34;&gt;Linkedin&lt;/a&gt;及&lt;a href=&#34;https://twitter.com/&#34;&gt;twitter&lt;/a&gt;来完成。&lt;/p&gt;
&lt;p&gt;审稿也是很重要的学术交流方式，建议使用 &lt;a href=&#34;https://publons.com/home/&#34;&gt;Publons&lt;/a&gt; 来构建自己的学术审稿记录。当然你可以在博客或微博上评论最新研究，甚至很多网络期刊网站的评论也有很好的思想碰撞，这里最关键的是要搞清楚你所在学科最活跃的网络交流平台，如果没有，自己搭建一个也无妨。&lt;/p&gt;
&lt;h2 id=&#34;数据分享&#34;&gt;数据分享&lt;/h2&gt;
&lt;p&gt;数据分享是一个很重要现代科研特征，越来越多的科研成果正在开放自己的原始数据供社区推动学科进步。其中，&lt;a href=&#34;https://figshare.com/&#34;&gt;figshare&lt;/a&gt;、&lt;a href=&#34;https://osf.io/&#34;&gt;Open Science Framework&lt;/a&gt;、&lt;a href=&#34;https://dataverse.org/&#34;&gt;Dataverse&lt;/a&gt;与&lt;a href=&#34;https://zenodo.org/&#34;&gt;Zenodo&lt;/a&gt;都是这一潮流的引领者。良好的数据分享不仅包含原始数据，还要包括处理后数据、数据收集相关信息与处理代码，另外对于共享数据的使用也要尊重数据生产者。&lt;/p&gt;
&lt;h2 id=&#34;代码管理&#34;&gt;代码管理&lt;/h2&gt;
&lt;p&gt;后续我们会看到所有学科都会不可逆引入编程，所以代码管理工具也非常重要。&lt;a href=&#34;https://github.com/&#34;&gt;Github&lt;/a&gt;与&lt;a href=&#34;https://bitbucket.org/&#34;&gt;Bitbucket&lt;/a&gt;都是非常实用的在线代码管理与版本控制平台。而&lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;Rmarkdown&lt;/a&gt;与&lt;a href=&#34;https://ipython.org/notebook.html&#34;&gt;Jupyter Notebook&lt;/a&gt;等工具背后提倡的文学化编程也是很重要的代码开发工具。此外应考虑为未来自己做好注释并记录运行环境保证重复性。&lt;a href=&#34;https://docs.docker.com/get-started/&#34;&gt;Docker image&lt;/a&gt;等完整的数据分析环境也可能成为现代科研的主流。代码的编写要能站到巨人肩上：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Good writers borrow from other authors, great authors steal outright&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;r包管理&#34;&gt;R包管理&lt;/h3&gt;
&lt;p&gt;对于R包的管理，建议打印相关Rstudio出品的&lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34;&gt;小抄&lt;/a&gt;作为参考。同时作为IDE，Rstiduo提供了包开发的模版，可以使用&lt;a href=&#34;https://yihui.name/formatr/&#34;&gt;formatR&lt;/a&gt; 与 &lt;a href=&#34;https://cran.r-project.org/web/packages/Rd2roxygen/index.html&#34;&gt;Rd2roxgen&lt;/a&gt;来重新格式化旧代码。同时使用&lt;a href=&#34;https://cran.r-project.org/web/packages/roxygen2/index.html&#34;&gt;roxygen2&lt;/a&gt;来编写开发文档。为了让包更容易使用，可以用Rmarkdown来写&lt;a href=&#34;http://r-pkgs.had.co.nz/vignettes.html&#34;&gt;小品文&lt;/a&gt;方便读者上手，另外就是使用&lt;a href=&#34;https://github.com/r-lib/testthat&#34;&gt;testthat&lt;/a&gt;来进行代码的单元测试。对于代码的执行效率，可以用&lt;a href=&#34;https://rstudio.github.io/profvis/&#34;&gt;Profvis&lt;/a&gt;进行可视化而集成在线测试则可以通过&lt;a href=&#34;https://travis-ci.org/&#34;&gt;travis-ci&lt;/a&gt;或&lt;a href=&#34;https://www.appveyor.com/&#34;&gt;appveyor&lt;/a&gt;来分别对R包进行Linux与Windows系统下的测试。当然，包完成后可通过 &lt;a href=&#34;https://github.com/r-lib/pkgdown&#34;&gt;pkgdown&lt;/a&gt;来制作网站并通过&lt;a href=&#34;https://rstudio.github.io/learnr/&#34;&gt;learnr&lt;/a&gt; 来制作交互式教程。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
